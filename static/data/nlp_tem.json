{
  "api templates": {
    "fir": [
      {
        "cat_Preprocessing": {
          "color": "rgb(246, 183, 60)",
          "num": 48,
          "codes": [
            "fir_4.js",
            "fir_16.js",
            "fir_17.js",
            "fir_23.js",
            "fir_37.js",
            "fir_40.js",
            "fir_1.js",
            "fir_2.js",
            "fir_3.js",
            "fir_6.js",
            "fir_8.js",
            "fir_9.js",
            "fir_10.js",
            "fir_11.js",
            "fir_12.js",
            "fir_13.js",
            "fir_14.js",
            "fir_18.js",
            "fir_19.js",
            "fir_20.js",
            "fir_21.js",
            "fir_25.js",
            "fir_29.js",
            "fir_30.js",
            "fir_31.js",
            "fir_33.js",
            "fir_34.js",
            "fir_41.js",
            "fir_42.js",
            "fir_43.js",
            "fir_45.js",
            "fir_47.js",
            "fir_48.js",
            "fir_24.js",
            "fir_5.js",
            "fir_7.js",
            "fir_28.js",
            "fir_35.js",
            "fir_36.js",
            "fir_38.js",
            "fir_39.js",
            "fir_44.js",
            "fir_49.js",
            "fir_50.js",
            "fir_22.js",
            "fir_27.js",
            "fir_32.js",
            "fir_26.js"
          ]
        }
      },
      {
        "fea_parsing": {
          "color": "rgb(246, 183, 60)",
          "num": 6,
          "codes": [
            "fir_4.js",
            "fir_16.js",
            "fir_17.js",
            "fir_23.js",
            "fir_37.js",
            "fir_40.js"
          ]
        }
      },
      {
        "fea_tokenization": {
          "color": "rgb(246, 183, 60)",
          "num": 33,
          "codes": [
            "fir_4.js",
            "fir_16.js",
            "fir_17.js",
            "fir_23.js",
            "fir_1.js",
            "fir_2.js",
            "fir_3.js",
            "fir_6.js",
            "fir_8.js",
            "fir_9.js",
            "fir_10.js",
            "fir_11.js",
            "fir_12.js",
            "fir_13.js",
            "fir_14.js",
            "fir_18.js",
            "fir_19.js",
            "fir_20.js",
            "fir_21.js",
            "fir_25.js",
            "fir_29.js",
            "fir_30.js",
            "fir_31.js",
            "fir_33.js",
            "fir_34.js",
            "fir_37.js",
            "fir_40.js",
            "fir_41.js",
            "fir_42.js",
            "fir_43.js",
            "fir_45.js",
            "fir_47.js",
            "fir_48.js"
          ]
        }
      },
      {
        "fea_lemmatization": {
          "color": "rgb(246, 183, 60)",
          "num": 14,
          "codes": [
            "fir_4.js",
            "fir_1.js",
            "fir_3.js",
            "fir_6.js",
            "fir_9.js",
            "fir_10.js",
            "fir_12.js",
            "fir_14.js",
            "fir_25.js",
            "fir_29.js",
            "fir_30.js",
            "fir_24.js",
            "fir_37.js",
            "fir_42.js"
          ]
        }
      },
      {
        "fea_word_vectors": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_nlp_datasets": {
          "color": "rgb(246, 183, 60)",
          "num": 29,
          "codes": [
            "fir_1.js",
            "fir_2.js",
            "fir_3.js",
            "fir_8.js",
            "fir_9.js",
            "fir_12.js",
            "fir_13.js",
            "fir_14.js",
            "fir_20.js",
            "fir_21.js",
            "fir_29.js",
            "fir_30.js",
            "fir_24.js",
            "fir_5.js",
            "fir_7.js",
            "fir_28.js",
            "fir_31.js",
            "fir_33.js",
            "fir_34.js",
            "fir_35.js",
            "fir_36.js",
            "fir_38.js",
            "fir_39.js",
            "fir_41.js",
            "fir_43.js",
            "fir_44.js",
            "fir_48.js",
            "fir_49.js",
            "fir_50.js"
          ]
        }
      },
      {
        "fea_stemming": {
          "color": "rgb(246, 183, 60)",
          "num": 6,
          "codes": [
            "fir_2.js",
            "fir_8.js",
            "fir_20.js",
            "fir_21.js",
            "fir_38.js",
            "fir_43.js"
          ]
        }
      },
      {
        "fea_regular_expression": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "fir_5.js"
          ]
        }
      },
      {
        "fea_word_frequency": {
          "color": "rgb(246, 183, 60)",
          "num": 3,
          "codes": [
            "fir_24.js",
            "fir_22.js",
            "fir_27.js"
          ]
        }
      },
      {
        "fea_tagger": {
          "color": "rgb(246, 183, 60)",
          "num": 7,
          "codes": [
            "fir_4.js",
            "fir_6.js",
            "fir_27.js",
            "fir_32.js",
            "fir_34.js",
            "fir_35.js",
            "fir_49.js"
          ]
        }
      },
      {
        "fea_text_simplify": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_n_grams": {
          "color": "rgb(246, 183, 60)",
          "num": 4,
          "codes": [
            "fir_6.js",
            "fir_26.js",
            "fir_44.js",
            "fir_47.js"
          ]
        }
      },
      {
        "cat_Basic_Analysis": {
          "color": "rgb(246, 183, 60)",
          "num": 27,
          "codes": [
            "fir_32.js",
            "fir_49.js",
            "fir_4.js",
            "fir_16.js",
            "fir_17.js",
            "fir_23.js",
            "fir_6.js",
            "fir_11.js",
            "fir_12.js",
            "fir_30.js",
            "fir_24.js",
            "fir_37.js",
            "fir_44.js",
            "fir_45.js",
            "fir_48.js",
            "fir_33.js",
            "fir_35.js",
            "fir_36.js",
            "fir_41.js",
            "fir_42.js",
            "fir_47.js",
            "fir_50.js",
            "fir_19.js",
            "fir_8.js",
            "fir_20.js",
            "fir_21.js",
            "fir_29.js"
          ]
        }
      },
      {
        "fea_chunking": {
          "color": "rgb(246, 183, 60)",
          "num": 2,
          "codes": [
            "fir_32.js",
            "fir_49.js"
          ]
        }
      },
      {
        "fea_Part_of_Speech": {
          "color": "rgb(246, 183, 60)",
          "num": 15,
          "codes": [
            "fir_4.js",
            "fir_16.js",
            "fir_17.js",
            "fir_23.js",
            "fir_6.js",
            "fir_11.js",
            "fir_12.js",
            "fir_30.js",
            "fir_24.js",
            "fir_32.js",
            "fir_37.js",
            "fir_44.js",
            "fir_45.js",
            "fir_48.js",
            "fir_49.js"
          ]
        }
      },
      {
        "fea_named_entity_recognition": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "fir_4.js"
          ]
        }
      },
      {
        "fea_dependency_parsing": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "fir_4.js"
          ]
        }
      },
      {
        "fea_spellcheck": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_text_frequency": {
          "color": "rgb(246, 183, 60)",
          "num": 7,
          "codes": [
            "fir_33.js",
            "fir_35.js",
            "fir_36.js",
            "fir_41.js",
            "fir_42.js",
            "fir_47.js",
            "fir_50.js"
          ]
        }
      },
      {
        "fea_text_similarity": {
          "color": "rgb(246, 183, 60)",
          "num": 2,
          "codes": [
            "fir_19.js",
            "fir_42.js"
          ]
        }
      },
      {
        "fea_text_scoring": {
          "color": "rgb(246, 183, 60)",
          "num": 4,
          "codes": [
            "fir_8.js",
            "fir_20.js",
            "fir_21.js",
            "fir_29.js"
          ]
        }
      },
      {
        "fea_text_segmentation": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "cat_Advanced_Analysis": {
          "color": "rgb(246, 183, 60)",
          "num": 16,
          "codes": [
            "fir_9.js",
            "fir_30.js",
            "fir_24.js",
            "fir_28.js",
            "fir_22.js",
            "fir_27.js",
            "fir_15.js",
            "fir_32.js",
            "fir_45.js",
            "fir_46.js",
            "fir_36.js",
            "fir_3.js",
            "fir_8.js",
            "fir_21.js",
            "fir_37.js",
            "fir_40.js"
          ]
        }
      },
      {
        "fea_chatbot": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "fir_9.js"
          ]
        }
      },
      {
        "fea_classification": {
          "color": "rgb(246, 183, 60)",
          "num": 9,
          "codes": [
            "fir_30.js",
            "fir_24.js",
            "fir_28.js",
            "fir_22.js",
            "fir_27.js",
            "fir_15.js",
            "fir_32.js",
            "fir_45.js",
            "fir_46.js"
          ]
        }
      },
      {
        "fea_clustering": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "fir_36.js"
          ]
        }
      },
      {
        "fea_language_detection": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_sentiment_analysis": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "fir_28.js"
          ]
        }
      },
      {
        "fea_summarizer": {
          "color": "rgb(246, 183, 60)",
          "num": 3,
          "codes": [
            "fir_3.js",
            "fir_8.js",
            "fir_21.js"
          ]
        }
      },
      {
        "fea_translation": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_tree": {
          "color": "rgb(246, 183, 60)",
          "num": 2,
          "codes": [
            "fir_37.js",
            "fir_40.js"
          ]
        }
      },
      {
        "fea_Neural_Network_Models": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      }
    ],
    "sec": [
      {
        "cat_Preprocessing": {
          "color": "rgb(246, 183, 60)",
          "num": 21,
          "codes": [
            "sec_7.js",
            "sec_10.js",
            "sec_34.js",
            "sec_37.js",
            "sec_39.js",
            "sec_48.js",
            "sec_50.js",
            "sec_2.js",
            "sec_12.js",
            "sec_31.js",
            "sec_33.js",
            "sec_6.js",
            "sec_35.js",
            "sec_38.js",
            "sec_40.js",
            "sec_41.js",
            "sec_9.js",
            "sec_27.js",
            "sec_32.js",
            "sec_1.js",
            "sec_8.js"
          ]
        }
      },
      {
        "fea_parsing": {
          "color": "rgb(246, 183, 60)",
          "num": 2,
          "codes": [
            "sec_7.js",
            "sec_10.js"
          ]
        }
      },
      {
        "fea_tokenization": {
          "color": "rgb(246, 183, 60)",
          "num": 5,
          "codes": [
            "sec_34.js",
            "sec_37.js",
            "sec_39.js",
            "sec_48.js",
            "sec_50.js"
          ]
        }
      },
      {
        "fea_lemmatization": {
          "color": "rgb(246, 183, 60)",
          "num": 4,
          "codes": [
            "sec_2.js",
            "sec_12.js",
            "sec_31.js",
            "sec_33.js"
          ]
        }
      },
      {
        "fea_word_vectors": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_nlp_datasets": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_stemming": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_regular_expression": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_word_frequency": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_tagger": {
          "color": "rgb(246, 183, 60)",
          "num": 8,
          "codes": [
            "sec_10.js",
            "sec_6.js",
            "sec_34.js",
            "sec_35.js",
            "sec_38.js",
            "sec_39.js",
            "sec_40.js",
            "sec_41.js"
          ]
        }
      },
      {
        "fea_text_simplify": {
          "color": "rgb(246, 183, 60)",
          "num": 4,
          "codes": [
            "sec_2.js",
            "sec_9.js",
            "sec_27.js",
            "sec_32.js"
          ]
        }
      },
      {
        "fea_n_grams": {
          "color": "rgb(246, 183, 60)",
          "num": 3,
          "codes": [
            "sec_27.js",
            "sec_1.js",
            "sec_8.js"
          ]
        }
      },
      {
        "cat_Basic_Analysis": {
          "color": "rgb(246, 183, 60)",
          "num": 8,
          "codes": [
            "sec_45.js",
            "sec_2.js",
            "sec_6.js",
            "sec_12.js",
            "sec_17.js",
            "sec_31.js",
            "sec_26.js",
            "sec_29.js"
          ]
        }
      },
      {
        "fea_chunking": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "sec_45.js"
          ]
        }
      },
      {
        "fea_Part_of_Speech": {
          "color": "rgb(246, 183, 60)",
          "num": 2,
          "codes": [
            "sec_2.js",
            "sec_6.js"
          ]
        }
      },
      {
        "fea_named_entity_recognition": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_dependency_parsing": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_spellcheck": {
          "color": "rgb(246, 183, 60)",
          "num": 3,
          "codes": [
            "sec_12.js",
            "sec_17.js",
            "sec_31.js"
          ]
        }
      },
      {
        "fea_text_frequency": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_text_similarity": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_text_scoring": {
          "color": "rgb(246, 183, 60)",
          "num": 2,
          "codes": [
            "sec_26.js",
            "sec_29.js"
          ]
        }
      },
      {
        "fea_text_segmentation": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "sec_31.js"
          ]
        }
      },
      {
        "cat_Advanced_Analysis": {
          "color": "rgb(246, 183, 60)",
          "num": 34,
          "codes": [
            "sec_9.js",
            "sec_20.js",
            "sec_4.js",
            "sec_5.js",
            "sec_13.js",
            "sec_36.js",
            "sec_44.js",
            "sec_12.js",
            "sec_19.js",
            "sec_23.js",
            "sec_11.js",
            "sec_18.js",
            "sec_31.js",
            "sec_32.js",
            "sec_46.js",
            "sec_26.js",
            "sec_29.js",
            "sec_15.js",
            "sec_21.js",
            "sec_24.js",
            "sec_25.js",
            "sec_28.js",
            "sec_30.js",
            "sec_33.js",
            "sec_37.js",
            "sec_42.js",
            "sec_43.js",
            "sec_47.js",
            "sec_48.js",
            "sec_49.js",
            "sec_50.js",
            "sec_14.js",
            "sec_16.js",
            "sec_22.js"
          ]
        }
      },
      {
        "fea_chatbot": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_classification": {
          "color": "rgb(246, 183, 60)",
          "num": 7,
          "codes": [
            "sec_9.js",
            "sec_20.js",
            "sec_4.js",
            "sec_5.js",
            "sec_13.js",
            "sec_36.js",
            "sec_44.js"
          ]
        }
      },
      {
        "fea_clustering": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_language_detection": {
          "color": "rgb(246, 183, 60)",
          "num": 8,
          "codes": [
            "sec_12.js",
            "sec_19.js",
            "sec_23.js",
            "sec_11.js",
            "sec_18.js",
            "sec_31.js",
            "sec_32.js",
            "sec_46.js"
          ]
        }
      },
      {
        "fea_sentiment_analysis": {
          "color": "rgb(246, 183, 60)",
          "num": 18,
          "codes": [
            "sec_26.js",
            "sec_29.js",
            "sec_15.js",
            "sec_20.js",
            "sec_21.js",
            "sec_24.js",
            "sec_25.js",
            "sec_28.js",
            "sec_30.js",
            "sec_32.js",
            "sec_33.js",
            "sec_37.js",
            "sec_42.js",
            "sec_43.js",
            "sec_47.js",
            "sec_48.js",
            "sec_49.js",
            "sec_50.js"
          ]
        }
      },
      {
        "fea_summarizer": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_translation": {
          "color": "rgb(246, 183, 60)",
          "num": 8,
          "codes": [
            "sec_12.js",
            "sec_14.js",
            "sec_16.js",
            "sec_19.js",
            "sec_22.js",
            "sec_23.js",
            "sec_33.js",
            "sec_46.js"
          ]
        }
      },
      {
        "fea_tree": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_Neural_Network_Models": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      }
    ],
    "thr": [
      {
        "cat_Preprocessing": {
          "color": "rgb(246, 183, 60)",
          "num": 31,
          "codes": [
            "thr_17.js",
            "thr_1.js",
            "thr_2.js",
            "thr_3.js",
            "thr_14.js",
            "thr_10.js",
            "thr_12.js",
            "thr_18.js",
            "thr_20.js",
            "thr_27.js",
            "thr_34.js",
            "thr_36.js",
            "thr_38.js",
            "thr_45.js",
            "thr_46.js",
            "thr_49.js",
            "thr_50.js",
            "thr_22.js",
            "thr_26.js",
            "thr_33.js",
            "thr_37.js",
            "thr_39.js",
            "thr_41.js",
            "thr_13.js",
            "thr_44.js",
            "thr_5.js",
            "thr_6.js",
            "thr_25.js",
            "thr_24.js",
            "thr_11.js",
            "thr_19.js"
          ]
        }
      },
      {
        "fea_parsing": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "thr_17.js"
          ]
        }
      },
      {
        "fea_tokenization": {
          "color": "rgb(246, 183, 60)",
          "num": 16,
          "codes": [
            "thr_1.js",
            "thr_2.js",
            "thr_3.js",
            "thr_14.js",
            "thr_10.js",
            "thr_12.js",
            "thr_18.js",
            "thr_20.js",
            "thr_27.js",
            "thr_34.js",
            "thr_36.js",
            "thr_38.js",
            "thr_45.js",
            "thr_46.js",
            "thr_49.js",
            "thr_50.js"
          ]
        }
      },
      {
        "fea_lemmatization": {
          "color": "rgb(246, 183, 60)",
          "num": 9,
          "codes": [
            "thr_3.js",
            "thr_17.js",
            "thr_12.js",
            "thr_22.js",
            "thr_26.js",
            "thr_33.js",
            "thr_37.js",
            "thr_39.js",
            "thr_41.js"
          ]
        }
      },
      {
        "fea_word_vectors": {
          "color": "rgb(246, 183, 60)",
          "num": 3,
          "codes": [
            "thr_17.js",
            "thr_13.js",
            "thr_44.js"
          ]
        }
      },
      {
        "fea_nlp_datasets": {
          "color": "rgb(246, 183, 60)",
          "num": 3,
          "codes": [
            "thr_17.js",
            "thr_5.js",
            "thr_6.js"
          ]
        }
      },
      {
        "fea_stemming": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_regular_expression": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "thr_27.js"
          ]
        }
      },
      {
        "fea_word_frequency": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "thr_25.js"
          ]
        }
      },
      {
        "fea_tagger": {
          "color": "rgb(246, 183, 60)",
          "num": 3,
          "codes": [
            "thr_24.js",
            "thr_41.js",
            "thr_45.js"
          ]
        }
      },
      {
        "fea_text_simplify": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "thr_27.js"
          ]
        }
      },
      {
        "fea_n_grams": {
          "color": "rgb(246, 183, 60)",
          "num": 3,
          "codes": [
            "thr_24.js",
            "thr_11.js",
            "thr_19.js"
          ]
        }
      },
      {
        "cat_Basic_Analysis": {
          "color": "rgb(246, 183, 60)",
          "num": 19,
          "codes": [
            "thr_12.js",
            "thr_24.js",
            "thr_34.js",
            "thr_36.js",
            "thr_37.js",
            "thr_38.js",
            "thr_39.js",
            "thr_48.js",
            "thr_14.js",
            "thr_18.js",
            "thr_31.js",
            "thr_32.js",
            "thr_35.js",
            "thr_47.js",
            "thr_49.js",
            "thr_4.js",
            "thr_8.js",
            "thr_10.js",
            "thr_7.js"
          ]
        }
      },
      {
        "fea_chunking": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_Part_of_Speech": {
          "color": "rgb(246, 183, 60)",
          "num": 8,
          "codes": [
            "thr_12.js",
            "thr_24.js",
            "thr_34.js",
            "thr_36.js",
            "thr_37.js",
            "thr_38.js",
            "thr_39.js",
            "thr_48.js"
          ]
        }
      },
      {
        "fea_named_entity_recognition": {
          "color": "rgb(246, 183, 60)",
          "num": 9,
          "codes": [
            "thr_14.js",
            "thr_18.js",
            "thr_31.js",
            "thr_32.js",
            "thr_35.js",
            "thr_37.js",
            "thr_47.js",
            "thr_48.js",
            "thr_49.js"
          ]
        }
      },
      {
        "fea_dependency_parsing": {
          "color": "rgb(246, 183, 60)",
          "num": 2,
          "codes": [
            "thr_4.js",
            "thr_47.js"
          ]
        }
      },
      {
        "fea_spellcheck": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "thr_8.js"
          ]
        }
      },
      {
        "fea_text_frequency": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_text_similarity": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "thr_10.js"
          ]
        }
      },
      {
        "fea_text_scoring": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "thr_7.js"
          ]
        }
      },
      {
        "fea_text_segmentation": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "cat_Advanced_Analysis": {
          "color": "rgb(246, 183, 60)",
          "num": 12,
          "codes": [
            "thr_1.js",
            "thr_15.js",
            "thr_17.js",
            "thr_9.js",
            "thr_7.js",
            "thr_6.js",
            "thr_36.js",
            "thr_38.js",
            "thr_2.js",
            "thr_3.js",
            "thr_4.js",
            "thr_14.js"
          ]
        }
      },
      {
        "fea_chatbot": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_classification": {
          "color": "rgb(246, 183, 60)",
          "num": 3,
          "codes": [
            "thr_1.js",
            "thr_15.js",
            "thr_17.js"
          ]
        }
      },
      {
        "fea_clustering": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_language_detection": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "thr_9.js"
          ]
        }
      },
      {
        "fea_sentiment_analysis": {
          "color": "rgb(246, 183, 60)",
          "num": 1,
          "codes": [
            "thr_7.js"
          ]
        }
      },
      {
        "fea_summarizer": {
          "color": "rgb(246, 183, 60)",
          "num": 3,
          "codes": [
            "thr_6.js",
            "thr_36.js",
            "thr_38.js"
          ]
        }
      },
      {
        "fea_translation": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_tree": {
          "color": "#12232E",
          "num": 0,
          "codes": []
        }
      },
      {
        "fea_Neural_Network_Models": {
          "color": "rgb(246, 183, 60)",
          "num": 6,
          "codes": [
            "thr_1.js",
            "thr_2.js",
            "thr_3.js",
            "thr_4.js",
            "thr_14.js",
            "thr_15.js"
          ]
        }
      }
    ]
  },
  "all templates": [
    {
      "name": "cat_Preprocessing",
      "definition": "Text processing is a method used under the NLP to clean the text and prepare it for the model building. ",
      "fir": 48,
      "sec": 21,
      "thr": 31
    },
    {
      "name": "fea_parsing",
      "definition": "Determining the syntactic structure of a text by analyzing its constituent words. e.g. “Tom ate an apple” -> (proper_noun: tom, verb: ate, determiner: an, noun: apple)",
      "fir": 6,
      "sec": 2,
      "thr": 1
    },
    {
      "name": "fea_tokenization",
      "definition": "Tokenization breaks the raw text into words, sentences called tokens. For example, the text “It is raining” can be tokenized into ‘It’, ‘is’, ‘raining’.",
      "fir": 33,
      "sec": 5,
      "thr": 16
    },
    {
      "name": "fea_lemmatization",
      "definition": "Reduce a given word to its root word. e.g. better -> good, given -> give",
      "fir": 14,
      "sec": 4,
      "thr": 9
    },
    {
      "name": "fea_word_vectors",
      "definition": "In a simple 1-of-N encoding every element in the vector is associated with a word in the vocabulary.",
      "fir": 0,
      "sec": 0,
      "thr": 3
    },
    {
      "name": "fea_nlp_datasets",
      "definition": "A large body of text",
      "fir": 29,
      "sec": 0,
      "thr": 3
    },
    {
      "name": "fea_stemming",
      "definition": "Reduce a given word to its root word. e.g. playing -> play, studies -> studi",
      "fir": 6,
      "sec": 0,
      "thr": 0
    },
    {
      "name": "fea_regular_expression",
      "definition": "Regular expressions or RegEx is a sequence of characters mainly used to find or replace patterns embedded in the text.",
      "fir": 1,
      "sec": 0,
      "thr": 1
    },
    {
      "name": "fea_word_frequency",
      "definition": "Count the times of the terms used in a text document or table",
      "fir": 3,
      "sec": 0,
      "thr": 1
    },
    {
      "name": "fea_tagger",
      "definition": "The process of classifying words into their parts of speech and labeling them accordingly. e.g. 'and' is a coordinating conjunction; 'now' is adverbs",
      "fir": 7,
      "sec": 8,
      "thr": 3
    },
    {
      "name": "fea_text_simplify",
      "definition": "Simplify the grammar and structure of the text.",
      "fir": 0,
      "sec": 4,
      "thr": 1
    },
    {
      "name": "fea_n_grams",
      "definition": "n-gram is a contiguous sequence of n items from a given sample of text or speech",
      "fir": 4,
      "sec": 3,
      "thr": 3
    },
    {
      "name": "cat_Basic_Analysis",
      "definition": "A combination of many basic NLP operations for a particular purpose.",
      "fir": 27,
      "sec": 8,
      "thr": 19
    },
    {
      "name": "fea_chunking",
      "definition": "",
      "fir": 2,
      "sec": 1,
      "thr": 0
    },
    {
      "name": "fea_Part_of_Speech",
      "definition": "POS is a special label assigned to each token (word) in a text corpus to indicate the part of speech and often also other grammatical categories such as tense, number (plural/singular), case etc",
      "fir": 15,
      "sec": 2,
      "thr": 8
    },
    {
      "name": "fea_named_entity_recognition",
      "definition": "NER is a technique to identify the key elements in a text, like names of people, places, brands, monetary values, etc.",
      "fir": 1,
      "sec": 0,
      "thr": 9
    },
    {
      "name": "fea_dependency_parsing",
      "definition": "Dependency parsing is a technique used to identify semantic relations between words in a sentence.",
      "fir": 1,
      "sec": 0,
      "thr": 2
    },
    {
      "name": "fea_spellcheck",
      "definition": "Spell-checking is the process of detecting and suggesting incorrect spelled words in a paragraph",
      "fir": 0,
      "sec": 3,
      "thr": 1
    },
    {
      "name": "fea_text_frequency",
      "definition": "",
      "fir": 7,
      "sec": 0,
      "thr": 0
    },
    {
      "name": "fea_text_similarity",
      "definition": "Text similarity has to determine how 'close' two pieces of text are both in surface closeness [lexical similarity] and meaning [semantic similarity]",
      "fir": 2,
      "sec": 0,
      "thr": 1
    },
    {
      "name": "fea_text_scoring",
      "definition": "Text scoring is a process to associate a numerical value with a sentence based on the used algorithm's priority.",
      "fir": 4,
      "sec": 2,
      "thr": 1
    },
    {
      "name": "fea_text_segmentation",
      "definition": "Text segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics.",
      "fir": 0,
      "sec": 1,
      "thr": 0
    },
    {
      "name": "cat_Advanced_Analysis",
      "definition": "Advanced analysis is a first-level concept, which contains 8 second-level concepts",
      "fir": 16,
      "sec": 34,
      "thr": 12
    },
    {
      "name": "fea_chatbot",
      "definition": "An NLP based chatbot is a computer program or artificial intelligence that communicates with a customer via textual or sound methods. ",
      "fir": 1,
      "sec": 0,
      "thr": 0
    },
    {
      "name": "fea_classification",
      "definition": "Text classification is a machine learning technique that assigns a set of predefined categories to open-ended text. ",
      "fir": 9,
      "sec": 7,
      "thr": 3
    },
    {
      "name": "fea_clustering",
      "definition": "",
      "fir": 1,
      "sec": 0,
      "thr": 0
    },
    {
      "name": "fea_language_detection",
      "definition": "Language identification or language detection is the problem of determining which natural language given content is in",
      "fir": 0,
      "sec": 8,
      "thr": 1
    },
    {
      "name": "fea_sentiment_analysis",
      "definition": "Sentiment analysis is to determine whether data is positive, negative or neutral.",
      "fir": 1,
      "sec": 18,
      "thr": 1
    },
    {
      "name": "fea_summarizer",
      "definition": "Text summarization in NLP is the process of summarizing the information in large texts for quicker consumption.",
      "fir": 3,
      "sec": 0,
      "thr": 3
    },
    {
      "name": "fea_translation",
      "definition": "Machine Translation (MT) is the task of automatically converting one natural language into another, preserving the meaning of the input text, and producing fluent text in the output language. ",
      "fir": 0,
      "sec": 8,
      "thr": 0
    },
    {
      "name": "fea_tree",
      "definition": "",
      "fir": 2,
      "sec": 0,
      "thr": 0
    },
    {
      "name": "fea_Neural_Network_Models",
      "definition": "",
      "fir": 0,
      "sec": 0,
      "thr": 6
    }
  ],
  "labeled files": {
    "fir_4": "<div class=\"codeBlock hljs python\" id=\"fir_4\"><pre id=\"fir_4_code\"><code class=\"python\">import nltk\nfrom nltk.parse.stanford import StanfordParser, StanfordDependencyParser\nfrom nltk.tag.stanford import StanfordPOSTagger\nfrom nltk.tag import StanfordNERTagger\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom config import *\n\nclass Text_processing:\n\n\n\tdef __init__(self):\n\t\t\n\t\t# user need to download Stanford Parser, NER and POS tagger from stanford website\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">self.constituent_parse_tree = Stanford<span class=\"fea_parsing_keys udls\">Parser</span>()</div>  #user need to set as environment variable\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_1\" style=\"display: inline;\">self.stanford_dependency = StanfordDependency<span class=\"fea_parsing_keys udls\">Parser</span>()</div> #user need to set as environment variable\n\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">self.<span class=\"fea_lemmatization_keys udls\">lemma</span> = WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer()</div>\n\t\tself.home = '/home/ramesh'\n\t\t#user needs to download stanford packages and change directory\n\t\t<div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">self.ner = StanfordNER<span class=\"fea_tagger_keys udls\">Tagge</span>r(self.home + '/stanford-ner-2017-06-09/classifiers/english.all.3class.distsim.crf.ser.gz',self.home + '/stanford-ner-2017-06-09/stanford-ner.jar')</div>\n\t\t<div class=\"highlights fea_tagger\" id=\"tagger_1\" style=\"display: inline;\">self.pos_tag = StanfordPOS<span class=\"fea_tagger_keys udls\">Tagge</span>r(self.home + '/stanford-pos<span class=\"fea_tagger_keys udls\">tagge</span>r-2017-06-09/models/english-bidirectional-distsim.<span class=\"fea_tagger_keys udls\">tagge</span>r',self.home + '/stanford-pos<span class=\"fea_tagger_keys udls\">tagge</span>r-2017-06-09/stanford-pos<span class=\"fea_tagger_keys udls\">tagge</span>r-3.8.0.jar')</div>\n\t\tself.CharacterOffsetEnd = 0 \n\t\tself.CharacterOffsetBegin = 0\n\t\t\n\n\t'''\n\tInput: sentence\n\tReturns: \n\t'''\n\n\n\tdef parser(self,sentence):\n\n\n\t\tself.parseResult = {'parseTree':[], 'text':[], 'dependencies':[],'words':[] }\n\t\tparseText, sentences = self.get_parseText(sentence)\n\t\t# print \"sentences \", sentences\n\t\t# if source/target sent consist of 1 sentence \n\t\tif len(sentences) == 1:\n\t\t\treturn parseText\n\t\t\n\t\twordOffSet = 0 # offset is number of words in first sentence \n\n\t\t# if source/target sentence has more than 1 sentence\n\n\t\tfor i in xrange(len(parseText['text'])):\n\t\t\tif i &gt; 0:\n\n\t\t\t\tfor j in xrange(len(parseText['dependencies'][i])):\n\t\t\t\t\t# [root, Root-0, dead-4]\n\t\t\t\t\tfor k in xrange(1,3):\n\t\t\t\t\t\ttokens = parseText['dependencies'][i][j][k].split('-')\n\n\t\t\t\t\t\tif tokens[0] == 'Root':\n\t\t\t\t\t\t\tnewWordIndex = 0\n\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tif not tokens[len(tokens)-1].isdigit():\n\t\t\t\t\t\t\t\tcontinue \n\n\t\t\t\t\t\t\tnewWordIndex = int(tokens[len(tokens)-1]) + wordOffSet\n\n\t\t\t\t\t\tif len(tokens) == 2:\n\n\t\t\t\t\t\t\t parseText['dependencies'][i][j][k] = tokens[0]+ '-' + str(newWordIndex)\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tw = ''\n\t\t\t\t\t\t\tfor l in xrange(len(tokens)-1):\n\t\t\t\t\t\t\t\tw += tokens[l]\n\t\t\t\t\t\t\t\tif l&lt;len(tokens)-2:\n\t\t\t\t\t\t\t\t\tw += '-'\n\n\t\t\t\t\t\t\tparseText['dependencies'][i][j][k] = w + '-' + str(newWordIndex)\n\n\t\t\twordOffSet += len(parseText['words'][i])\n\n\t\treturn parseText\n\n\n\t'''\n\tInput: parserResult \n\tReturns: [[charBegin,charEnd], wordIndex(starts from 1), word, word_lemma]] \n\t'''\n\n\n\tdef get_lemma(self,parserResult):\n\n\n\t\tres = []\t\t\n\t\twordIndex = 1\n\t\tfor i in xrange(len(parserResult['words'])):\n\t\t\t\n\t\t\tfor j in xrange(len(parserResult['words'][i])):\n\t\t\t\t\n\t\t\t\ttag = [ [parserResult['words'][i][j][1]['CharacterOffsetBegin'], parserResult['words'][i][j][1]['CharacterOffsetEnd']], wordIndex,parserResult['words'][i][j][0] ,parserResult['words'][i][j][1]['Lemma'] ]\n\t\t\t\twordIndex += 1\n\t\t\t\tres.append(tag)\n\n\t\treturn res\n\n\n\n\t'''\n\tUsing Stanford POS Tagger\n\tInput: parserResult \n\tReturns: [[charBegin,charEnd], wordIndex(starts from 1), word, word_POS]] \n\t'''\n\n\n\tdef combine_lemmaAndPosTags(self,parserResult):\n\n\t\tres = []\n\t\t\n\t\twordIndex = 1\n\t\tfor i in xrange(len(parserResult['words'])):\n\t\t\t\n\t\t\tfor j in xrange(len(parserResult['words'][i])):\n\t\t\t\t\n\t\t\t\ttag = [ [parserResult['words'][i][j][1]['CharacterOffsetBegin'], parserResult['words'][i][j][1]['CharacterOffsetEnd']], wordIndex,parserResult['words'][i][j][0] ,parserResult['words'][i][j][1]['Lemma'] ,parserResult['words'][i][j][1]['PartOfSpeech'] ]\n\t\t\t\twordIndex += 1\n\t\t\t\tres.append(tag)\n\n\t\treturn res\n\n\n\t'''\n\tInput: parserResult\n\tReturns: ([charOffsetBegin,charOffsetEnd], wordindex,word, NER ])\n\t'''\n\n\n\tdef nerWordAnnotator(self,parserResult):\n\n\t\tres = []\n\t\t\n\t\twordIndex = 1\n\t\tfor i in xrange(len(parserResult['words'])):\n\t\t\t\n\t\t\tfor j in xrange(len(parserResult['words'][i])):\n\t\t\t\t\n\t\t\t\ttag = [ [parserResult['words'][i][j][1]['CharacterOffsetBegin'], parserResult['words'][i][j][1]['CharacterOffsetEnd']], wordIndex,parserResult['words'][i][j][0] ,parserResult['words'][i][j][1]['NamedEntityTag'] ]\n\t\t\t\t# print \"tag \", tag\n\t\t\t\twordIndex += 1\n\t\t\t\t# if there is valid named entity then add in list\n\t\t\t\tif tag[3] != 'O':\n\n\t\t\t\t\tres.append(tag)\n\n\t\treturn res\n\n\n\t'''\n\tInput : ParserResult\n\tReturns : list containing NamedEntites\n\t1. Group words in same list if they share same NE (Location), \n    2. Save other words in list that have any entity\n\t'''\n\n\n\tdef get_ner(self,parserResult):\n\n\n\t\t<div class=\"highlights fea_named_entity_recognition\" id=\"named_entity_recognition_0\" style=\"display: inline;\">nerWordAnnotations = self.nerWordAnnotator(parserResult)</div> #[[ [charbegin,charEnd], wordIndex, word, NE ]]\n\t\tnamedEntities = []\n\t\tcurrentWord = []\n\t\tcurrentCharacterOffSets = []\n\t\tcurrentWordOffSets = []\n\n\t\tfor i in xrange(len(nerWordAnnotations)):\n\n\t\t\tif i == 0:\n\n\t\t\t\tcurrentWord.append(nerWordAnnotations[i][2]) # word having NE\n\t\t\t\tcurrentCharacterOffSets.append(nerWordAnnotations[i][0]) # [begin,end]\n\t\t\t\tcurrentWordOffSets.append(nerWordAnnotations[i][1]) # Word Index\n\t\t\t\t# if there is only one ner Word tag\n\t\t\t\tif (len(nerWordAnnotations) == 1):\n\t\t\t\t\tnamedEntities.append([ currentCharacterOffSets, currentWordOffSets, \\\n\t\t\t\t\t\tcurrentWord, nerWordAnnotations[i-1][3] ])\n\t\t\t\t\t# print \"named Entities \", namedEntities\n\t\t\t\t\tbreak \n\t\t\t\tcontinue\n\t\t\t# if consecutive tags have same NER Tag, save them in one list\n\t\t\tif nerWordAnnotations[i][3] == nerWordAnnotations[i-1][3] and \\\n\t\t\t\t\tnerWordAnnotations[i][1] == nerWordAnnotations[i-1][1] + 1:\n\t\t\t\t\n\t\t\t\tcurrentWord.append(nerWordAnnotations[i][2]) # word having NE\n\t\t\t\tcurrentCharacterOffSets.append(nerWordAnnotations[i][0]) # [begin,end]\n\t\t\t\tcurrentWordOffSets.append(nerWordAnnotations[i][1]) # Word Index\n\n\t\t\t\tif i == (len(nerWordAnnotations) - 1):\n\t\t\t\t\tnamedEntities.append([ currentCharacterOffSets, \\\n\t\t\t\t\t\tcurrentWordOffSets, currentWord, nerWordAnnotations[i][3] ])\n\t\t\t# if consecutive tags do not match\n\t\t\telse:\n\n\t\t\t\tnamedEntities.append([ currentCharacterOffSets, \\\n\t\t\t\t\t\tcurrentWordOffSets, currentWord, nerWordAnnotations[i-1][3] ])\n\t\t\t\tcurrentWord = [nerWordAnnotations[i][2]]\n\t\t\t\t# remove everything from currentCharacterOffSets and currentWordOffSets\n\t\t\t\tcurrentCharacterOffSets = []\n\t\t\t\tcurrentWordOffSets = []\n\t\t\t\t# add charac offsets and currentWordOffSets of current word\n\t\t\t\tcurrentCharacterOffSets.append(nerWordAnnotations[i][0])\n\t\t\t\tcurrentWordOffSets.append(nerWordAnnotations[i][1])\n\n\t\t\t\t# if it is last iteration then update named Entities\n\t\t\t\tif i == len(nerWordAnnotations)-1:\n\t\t\t\t\tnamedEntities.append([ currentCharacterOffSets, currentWordOffSets, \\\n\t\t\t\t\t\t\tcurrentWord, nerWordAnnotations[i][3] ])\n\t\t#sort out according to len of characters in ascending order\n\t\tnamedEntities = sorted(namedEntities, key=len)\n\n\t\treturn namedEntities\n\n\n\t'''\n\tInput: Word(Word whose NE is not found), NE(word already have NE Tag) \n\tReturns: Boolean; True if word is acronym\n\t\t\t\t\tFalse if word is not acronym\n\t'''\n\n\n\tdef is_Acronym(self,word,NE):\n\n\n\t\tqueryWord = word.replace('.','')\n\t\t# If all words of queryWord is not capital or length of word != \n\t\t\t\t#length of NE(word already have NE Tag) or \n\t\t   #  if word is 'a' or 'i' \n\t\tif not queryWord.isupper() or len(queryWord) != len(NE) or queryWord.lower() in ['a', 'i']:\n\t\t\treturn False\n\n\t\tacronym = True\n\n\t\t#we run for loop till length of query word(i.e 3)(if word is 'UAE')\n\t\t#Compare 1st letter(U) of query word with first letter of first element in named entity(U = U(united))\n\t\t# again we take second letter of canonical word (A) with second element in named entity(Arab)\n\t\t# and so on \n\t\tfor i in xrange(len(queryWord)):\n\t\t\t# print \"queryword[i], NE \", queryWord, NE\n\t\t\tif queryWord[i] != NE[i][0]:\n\t\t\t\tacronym = False\n\t\t\t\tbreak\n\n\t\treturn acronym\n\n\n\t'''\n\tInput: sentence\n\tReturns: parse(\t{ParseTree, text, Dependencies, \n\t  'word : [] NamedEntityTag, CharacterOffsetEnd, \n\t  \t\tCharacterOffsetBegin, PartOfSpeech, Lemma}']}) \n\t  \t\tsentence and\n\t'''\n\n\n\tdef get_parseText(self,sentence):\n\n\t\tself.count = 0\n\t\tself.length_of_sentence = [] # stores length of each sentence\n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>ized_sentence = sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>\n\t\t# print \"len of tokenized \",len(tokenized_sentence)\n\t\tif (len(tokenized_sentence) == 1):\n\t\t\tself.count += 1\n\t\t\tfor i in tokenized_sentence:\n\t\t\t\tparse = self.get_combine_words_param(i)\n\t\telse:\n\t\t\ttmp = 0\n\t\t\tfor i in tokenized_sentence:\n\t\t\t\tself.count += 1\n\t\t\t\tparse = self.get_combine_words_param(i)\n\t\t\t\ts = len(i) + tmp\n\t\t\t\tself.length_of_sentence.append(s)\n\t\t\t\ttmp = s\n\n\t\treturn parse,tokenized_sentence\n\t\t\n\n\t'''\n\tInput: sentences\n    Return: constituency tree that represents relations between sub-phrases in sentences\n\t'''\n\n\n\tdef get_constituency_Tree(self,sentence):\n\t\t\n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">sentence = sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_2\" style=\"display: inline;\">constituency_<span class=\"fea_parsing_keys udls\">parser</span> = self.constituent_parse_tree.raw_parse_sents(sentence)</div>\n\t\tfor parser in constituency_parser:\n\t\t\tfor sent in parser:\n\t\t\t\ttree = str(sent)\n\t\tparse_string = ' '.join(str(tree).split()) \n        \n\t\treturn parse_string\n\n\n\t'''\n\tInput: sentence\n\treturns: relation between words with their index\n\t'''\t\n\n\n\tdef get_dependencies(self,sentence):\n\n\n\t\tdependency_tree = []\n\t\t<div class=\"highlights fea_dependency_parsing\" id=\"dependency_parsing_0\" style=\"display: inline;\">dependency_parser = self.stanford_dependency.raw_parse(sentence)</div>\n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_2\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span> = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>\n\t\t<div class=\"highlights fea_dependency_parsing\" id=\"dependency_parsing_1\" style=\"display: inline;\">parsetree = list(self.stanford_dependency.raw_parse(sentence))[0]</div>\n\t\t# Find root(head) of the sentence \n\t\tfor k in parsetree.nodes.values():\n\t\t\tif k[\"head\"] == 0:\n\t\t\n\t\t\t\tdependency_tree.append([str(k[\"rel\"]), \"Root-\" + str(k[\"head\"]), str(k[\"word\"]) \n\t\t\t\t\t+ \"-\" + str(k[\"address\"]) ])\t    \t\n\t\t# Find relation between words in sentence\n\t\tfor dep in dependency_parser:\n\t\t\tfor triple in dep.triples():\n\t\t\t\tindex_word = token.index(triple[0][0]) + 1 # because index starts from 0 \n\t\t\t\tindex2_word = token.index(triple[2][0]) + 1\n\t\t\t\tdependency_tree.append([str(triple[1]),str(triple[0][0]) + \"-\" + str(index_word),\\\n\t\t\t\t\t\t\t str(triple[2][0]) + \"-\" + str(index2_word)])\n\n\t\treturn dependency_tree\n\n\n\t'''\n\tInput: sentence, word(of which offset to determine)\n\tReturn: [CharacterOffsetEnd,CharacterOffsetBegin] for each word\n\t'''\n\n\n\tdef get_charOffset(self,sentence, word):\n\n\t\t# word containing '.' causes problem in counting\n\n\t\tCharacterOffsetBegin = sentence.find(word)\n\t\tCharacterOffsetEnd = CharacterOffsetBegin + len(word)\n\t\t\n\t\treturn [CharacterOffsetEnd,CharacterOffsetBegin]\n\n\n\t'''\n\tInput: sentence\n\tReturns: dictionary: \n\t{ParseTree, text, Dependencies, \n\t  #'word : [] NamedEntityTag, CharacterOffsetEnd, CharacterOffsetBegin, PartOfSpeech, Lemma}']}\n\t'''\n\n\n\tdef get_combine_words_param(self,sentence):\n\t\t\n\n\t\twords_in_each_sentence = []\n\t\twords_list = [] \n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_3\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>ized_words = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>\n\t\t<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">pos</span><span class=\"fea_Part_of_Speech_keys udls\">Tag</span> = self.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>.<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(<span class=\"fea_Part_of_Speech_keys udls\">token</span>ized_words)</div>\n\t\t<div class=\"highlights fea_named_entity_recognition\" id=\"named_entity_recognition_1\" style=\"display: inline;\">ner = self.ner.tag(tokenized_words)</div>\n\t\t\n\t\t# if source sentence/target sentence has one sentence\n\t\tif (self.count == 1):\n\t\t\tfor i in xrange(len(tokenized_words)):\n\t\t\t\tword_lemma = str()\n\t\t\t\tword = tokenized_words[i]\n\t\t\t\tname_entity = ner[i]\n\t\t\t\tword_posTag = posTag[i][-1]  # access tuple [(United, NNP),..]\n\t\t\t\t# print \"word and pos tag \", word, word_posTag[0]\t\n\t\t\t\t#wordNet lemmatizer needs pos tag with words else it considers noun\n\t\t\t\tif (word_posTag[0] == 'V'):\n\t\t\t\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_1\" style=\"display: inline;\">word_<span class=\"fea_lemmatization_keys udls\">lemma</span> = self.<span class=\"fea_lemmatization_keys udls\">lemma</span>.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(tokenized_words[i], wordnet.VERB)</div>\n\n\t\t\t\telif (word_posTag[0] == 'J'):\n\t\t\t\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_2\" style=\"display: inline;\">word_<span class=\"fea_lemmatization_keys udls\">lemma</span> = self.<span class=\"fea_lemmatization_keys udls\">lemma</span>.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(tokenized_words[i], wordnet.ADJ)</div>\n\n\t\t\t\telif (word_posTag[0:1] == 'RB'):\n\t\t\t\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_3\" style=\"display: inline;\">word_<span class=\"fea_lemmatization_keys udls\">lemma</span> = self.<span class=\"fea_lemmatization_keys udls\">lemma</span>.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(tokenized_words[i], wordnet.ADV)</div>\n\n\t\t\t\telse:\n\t\t\t\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_4\" style=\"display: inline;\">word_<span class=\"fea_lemmatization_keys udls\">lemma</span> = self.<span class=\"fea_lemmatization_keys udls\">lemma</span>.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(tokenized_words[i])</div>\n\n\t\t\t\tself.CharacterOffsetEnd, self.CharacterOffsetBegin = self.get_charOffset(sentence,tokenized_words[i])\n\t\t\t\t\n\n\t\t\t\twords_list.append([word, {\"NamedEntityTag\" : str(name_entity[1]),\n\t\t\t\t\t\"CharacterOffsetEnd\" : str(self.CharacterOffsetEnd), \"CharacterOffsetBegin\" : str(self.CharacterOffsetBegin) \n\t\t\t\t\t,\"PartOfSpeech\" : str(word_posTag) , \"Lemma\" : str(word_lemma)}])\n\n\t\t\tself.parseResult['parseTree'] = [self.get_constituency_Tree(sentence)]\n\t\t\tself.parseResult['text'] = [sentence]\n\t\t\tself.parseResult['dependencies'] = [self.get_dependencies(sentence)]\n\t\t\tself.parseResult['words'] = [words_list]\n\n\t\telse:\n\n\t\t\tfor i in xrange(len(tokenized_words)):\n\t\t\t\tword = tokenized_words[i]\n\t\t\t\tname_entity = ner[i] \n\t\t\t\tword_posTag = posTag[i][-1]\n\n\t\t\t\tif (word_posTag[0] == 'V'):\n\t\t\t\t\tword_lemma = self.lemma.lemmatize(tokenized_words[i], wordnet.VERB)\n\n\t\t\t\telif (word_posTag[0] == 'J'):\n\t\t\t\t\tword_lemma = self.lemma.lemmatize(tokenized_words[i], wordnet.ADJ)\n\n\t\t\t\telif (word_posTag[0:1] == 'RB'):\n\t\t\t\t\tword_lemma = self.lemma.lemmatize(tokenized_words[i], wordnet.ADV)\n\n\t\t\t\telse:\n\t\t\t\t\tword_lemma = self.lemma.lemmatize(tokenized_words[i])\n\n\t\t\t\tend, begin = self.get_charOffset(sentence,tokenized_words[i])\n\t\t\t\tend = end + self.length_of_sentence[self.count-2] + 1\n\t\t\t\tbegin = begin + self.length_of_sentence[self.count-2] + 1\t\n\t\t\t\twords_list.append([word, {\"NamedEntityTag\" : str(name_entity[1]),\n\t\t\t\t\t\"CharacterOffsetEnd\" : str(end), \"CharacterOffsetBegin\" : str(begin) \n\t\t\t\t\t,\"PartOfSpeech\" : str(word_posTag) , \"Lemma\" : str(word_lemma)}])\n\t\t\tself.parseResult['parseTree'].append(self.get_constituency_Tree(sentence))\n\t\t\tself.parseResult['text'].append(sentence)\n\t\t\tself.parseResult['dependencies'].append(self.get_dependencies(sentence))\n\t\t\tself.parseResult['words'].append(words_list)\n\n\t\treturn self.parseResult\n\t\t#https://github.com/rameshjes/Semantic-Textual-Similarity/blob/master/monolingualWordAligner/nltkUtil</code></pre></div>",
    "fir_16": "<div class=\"codeBlock hljs python\" id=\"fir_16\"><pre id=\"fir_16_code\"><code class=\"python\"># Copyright 2020 The `Kumar Nityan Suman` (https://github.com/nityansuman/).\n# All Rights Reserved.\n#\n#                     GNU GENERAL PUBLIC LICENSE\n#                        Version 3, 29 June 2007\n#  Copyright (C) 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt;\n#  Everyone is permitted to copy and distribute verbatim copies\n#  of this license document, but changing it is not allowed.\n# ==============================================================================\n\nimport logging\nfrom typing import Tuple\n\nimport nltk as nlp\nimport numpy as np\n\n\nclass SubjectiveTest:\n\t\"\"\"Class abstraction for subjective test generation module.\n\t\"\"\"\n\n\tdef __init__(self, filepath: str):\n\t\t\"\"\"Class constructor.\n\n\t\tArgs:\n\t\t\tfilepath (str): Absolute filepath to the subject corpus.\n\t\t\"\"\"\n\t\tself.question_pattern = [\n\t\t\t\"Explain in detail \",\n\t\t\t\"Define \",\n\t\t\t\"Write a short note on \",\n\t\t\t\"What do you mean by \"\n\t\t]\n\n\t\tself.grammar = r\"\"\"\n\t\t\tCHUNK: {&lt;NN&gt;+&lt;IN|DT&gt;*&lt;NN&gt;+}\n\t\t\t{&lt;NN&gt;+&lt;IN|DT&gt;*&lt;NNP&gt;+}\n\t\t\t{&lt;NNP&gt;+&lt;NNS&gt;*}\n\t\t\"\"\"\n\n\t\ttry:\n\t\t\twith open(filepath, mode=\"r\") as fp:\n\t\t\t\tself.summary = fp.read()\n\t\texcept FileNotFoundError:\n\t\t\tlogging.exception(\"Corpus file not found.\", exc_info=True)\n\n\t@staticmethod\n\tdef word_tokenizer(sequence: str) -&gt; list:\n\t\t\"\"\"Tokenize string sequences to words.\n\n\t\tArgs:\n\t\t\tsequence (str): Corpus sequences.\n\n\t\tReturns:\n\t\t\tlist: Word tokens.\n\t\t\"\"\"\n\t\tword_tokens = list()\n\t\ttry:\n\t\t\tfor sent in <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">nlp.sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(sequence)</div>:\n\t\t\t\tfor w in <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">nlp.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sent)</div>:\n\t\t\t\t\tword_tokens.append(w)\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Word tokenization failed.\", exc_info=True)\n\t\treturn word_tokens\n\n\t@staticmethod\n\tdef create_vector(answer_tokens: list, tokens: list) -&gt; np.array:\n\t\t\"\"\"Create a one-hot encoded vector for the answer_tokens.\n\n\t\tArgs:\n\t\t\tanswer_tokens (list): Tokenized user response.\n\t\t\ttokens (list): Tokenized answer corpus.\n\n\t\tReturns:\n\t\t\tnp.array: A one-hot encoded vector of the answer.\n\t\t\"\"\"\n\t\treturn np.array([1 if tok in answer_tokens else 0 for tok in tokens])\n\n\t@staticmethod\n\tdef cosine_similarity_score(vector1: np.array, vector2: np.array) -&gt; float:\n\t\t\"\"\"Compute the euclidean distance between two vectors.\n\n\t\tArgs:\n\t\t\tvector1 (np.array): Actual answer vector.\n\t\t\tvector2 (np.array): User response vector.\n\n\t\tReturns:\n\t\t\tfloat: Euclidean distance between two vectors.\n\t\t\"\"\"\n\t\tdef vector_value(vector):\n\t\t\treturn np.sqrt(np.sum(np.square(vector)))\n\n\t\tv1 = vector_value(vector1)\n\t\tv2 = vector_value(vector2)\n\n\t\tv1_v2 = np.dot(vector1, vector2)\n\t\treturn (v1_v2 / (v1 * v2)) * 100\n\n\tdef generate_test(self, num_questions: int = 2) -&gt; Tuple[list, list]:\n\t\t\"\"\"Method to generate subjective test.\n\n\t\tArgs:\n\t\t\tnum_questions (int, optional): Maximum number of questions\n\t\t\t\tto be generated. Defaults to 2.\n\n\t\tReturns:\n\t\t\tTuple[list, list]: Generated `Questions` and `Answers` respectively\n\t\t\"\"\"\n\t\ttry:\n\t\t\tsentences = nlp.sent_tokenize(self.summary)\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Sentence tokenization failed.\", exc_info=True)\n\n\t\ttry:\n\t\t\t<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">cp = nlp.Regexp<span class=\"fea_parsing_keys udls\">Parser</span>(self.grammar)</div>\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Regex grammar train failed.\", exc_info=True)\n\n\t\tquestion_answer_dict = dict()\n\t\tfor sentence in sentences:\n\n\t\t\ttry:\n\t\t\t\t<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">tag</span>ged_words = nlp.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(nlp.word_<span class=\"fea_Part_of_Speech_keys udls\">token</span>ize(sentence))</div>\n\t\t\texcept Exception:\n\t\t\t\tlogging.exception(\"Word tokenization failed.\", exc_info=True)\n\n\t\t\ttree = cp.parse(tagged_words)\n\t\t\tfor subtree in tree.subtrees():\n\t\t\t\tif subtree.label() == \"CHUNK\":\n\t\t\t\t\ttemp = \"\"\n\t\t\t\t\tfor sub in subtree:\n\t\t\t\t\t\ttemp += sub[0]\n\t\t\t\t\t\ttemp += \" \"\n\t\t\t\t\ttemp = temp.strip()\n\t\t\t\t\ttemp = temp.upper()\n\t\t\t\t\tif temp not in question_answer_dict:\n\t\t\t\t\t\tif len(<div class=\"highlights fea_tokenization\" id=\"tokenization_2\" style=\"display: inline;\">nlp.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>) &gt; 20:\n\t\t\t\t\t\t\tquestion_answer_dict[temp] = sentence\n\t\t\t\t\telse:\n\t\t\t\t\t\tquestion_answer_dict[temp] += sentence\n\n\t\tkeyword_list = list(question_answer_dict.keys())\n\t\tquestion_answer = list()\n\n\t\tfor _ in range(3):\n\t\t\trand_num = np.random.randint(0, len(keyword_list))\n\t\t\tselected_key = keyword_list[rand_num]\n\t\t\tanswer = question_answer_dict[selected_key]\n\t\t\trand_num %= 4\n\t\t\tquestion = self.question_pattern[rand_num] + selected_key + \".\"\n\t\t\tquestion_answer.append({\"Question\": question, \"Answer\": answer})\n\n\t\tque = list()\n\t\tans = list()\n\t\twhile len(que) &lt; num_questions:\n\t\t\trand_num = np.random.randint(0, len(question_answer))\n\t\t\tif question_answer[rand_num][\"Question\"] not in que:\n\t\t\t\tque.append(question_answer[rand_num][\"Question\"])\n\t\t\t\tans.append(question_answer[rand_num][\"Answer\"])\n\t\t\telse:\n\t\t\t\tcontinue\n\t\treturn que, ans\n\n\tdef evaluate_subjective_answer(self, original_answer: str, user_answer: str) -&gt; float:\n\t\t\"\"\"Evaluate the subjective answer given by the user.\n\n\t\tArgs:\n\t\t\toriginal_answer (str): A string representing the original answer.\n\t\t\tuser_answer (str): A string representing the answer given by the user.\n\n\t\tReturns:\n\t\t\tfloat: Similarity/correctness score of the user answer\n\t\t\t\tbased on the original asnwer.\n\t\t\"\"\"\n\t\tscore_obt = 0\n\t\toriginal_ans_list = self.word_tokenizer(original_answer)\n\t\tuser_ans_list = self.word_tokenizer(user_answer)\n\n\t\toverall_list = original_ans_list + user_ans_list\n\n\t\tvector1 = self.create_vector(original_ans_list, overall_list)\n\t\tvector2 = self.create_vector(user_answer, overall_list)\n\n\t\tscore_obt = self.cosine_similarity_score(vector1, vector2)\n\t\treturn score_obt\n\t\t#https://github.com/nityansuman/marvin/blob/main/src/subjective</code></pre></div>",
    "fir_17": "<div class=\"codeBlock hljs python\" id=\"fir_17\"><pre id=\"fir_17_code\"><code class=\"python\"># Copyright 2020 The `Kumar Nityan Suman` (https://github.com/nityansuman/).\n# All Rights Reserved.\n#\n#                     GNU GENERAL PUBLIC LICENSE\n#                        Version 3, 29 June 2007\n#  Copyright (C) 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt;\n#  Everyone is permitted to copy and distribute verbatim copies\n#  of this license document, but changing it is not allowed.\n# ==============================================================================\n\nimport logging\nimport re\nfrom typing import Tuple\n\nimport nltk\nimport numpy as np\nfrom nltk.corpus import wordnet as wn\n\n\nclass ObjectiveTest:\n\t\"\"\"Class abstraction for objective test generation module.\n\t\"\"\"\n\n\tdef __init__(self, filepath: str):\n\t\t\"\"\"Class constructor.\n\n\t\tArgs:\n\t\t\tfilepath (str): filepath (str): Absolute filepath to the subject corpus.\n\t\t\"\"\"\n\t\t# Load subject corpus\n\t\ttry:\n\t\t\twith open(filepath, mode=\"r\") as fp:\n\t\t\t\tself.summary = fp.read()\n\t\texcept FileNotFoundError:\n\t\t\tlogging.exception(\"Corpus file not found.\", exc_info=True)\n\n\tdef generate_test(self, num_questions: int = 3) -&gt; Tuple[list, list]:\n\t\t\"\"\"Method to generate an objective test.\n\n\t\tArgs:\n\t\t\tnum_questions (int, optional): Number of questions in a test.\n\t\t\t\tDefaults to 3.\n\n\t\tReturns:\n\t\t\tTuple[list, list]: Questions and answer options respectively.\n\t\t\"\"\"\n\t\t# Identify potential question sets\n\t\tquestion_sets = self.get_question_sets()\n\n\t\t# Identify potential question answers\n\t\tquestion_answers = list()\n\t\tfor question_set in question_sets:\n\t\t\tif question_set[\"Key\"] &gt; 3:\n\t\t\t\tquestion_answers.append(question_set)\n\n\t\t# Create objective test set\n\t\tquestions, answers = list(), list()\n\t\twhile len(questions) &lt; num_questions:\n\t\t\trand_num = np.random.randint(0, len(question_answers))\n\t\t\tif question_answers[rand_num][\"Question\"] not in questions:\n\t\t\t\tquestions.append(question_answers[rand_num][\"Question\"])\n\t\t\t\tanswers.append(question_answers[rand_num][\"Answer\"])\n\t\treturn questions, answers\n\n\tdef get_question_sets(self) -&gt; list:\n\t\t\"\"\"Method to dentify sentences with potential objective questions.\n\n\t\tReturns:\n\t\t\tlist: Sentences with potential objective questions.\n\t\t\"\"\"\n\t\t# Tokenize corpus into sentences\n\t\ttry:\n\t\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">sentences = nltk.sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(self.summary)</div>\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Sentence tokenization failed.\", exc_info=True)\n\n\t\t# Identify potential question sets\n\t\t# Each question set consists:\n\t\t# \tQuestion: Objective question.\n\t\t# \tAnswer: Actual asnwer.\n\t\t#\tKey: Other options.\n\t\tquestion_sets = list()\n\t\tfor sent in sentences:\n\t\t\tquestion_set = self.identify_potential_questions(sent)\n\t\t\tif question_set is not None:\n\t\t\t\tquestion_sets.append(question_set)\n\t\treturn question_sets\n\n\tdef identify_potential_questions(self, sentence: str) -&gt; dict:\n\t\t\"\"\"Method to identiyf potential question sets.\n\n\t\tArgs:\n\t\t\tsentence (str): Tokenized sequence from corpus.\n\n\t\tReturns:\n\t\t\tdict: Question formed along with the correct answer in case of\n\t\t\t\tpotential sentence else return None.\n\t\t\"\"\"\n\t\t# POS tag sequences\n\t\ttry:\n\t\t\t<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">tag</span>s = nltk.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(sentence)</div>\n\t\t\tif tags[0][1] == \"RB\" or len(<div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">nltk.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>) &lt; 4:\n\t\t\t\treturn None\n\t\texcept Exception:\n\t\t\tlogging.exception(\"POS tagging failed.\", exc_info=True)\n\n\t\t# Define regex grammar to chunk keywords\n\t\tnoun_phrases = list()\n\t\tgrammar = r\"\"\"\n\t\t\tCHUNK: {&lt;NN&gt;+&lt;IN|DT&gt;*&lt;NN&gt;+}\n\t\t\t\t{&lt;NN&gt;+&lt;IN|DT&gt;*&lt;NNP&gt;+}\n\t\t\t\t{&lt;NNP&gt;+&lt;NNS&gt;*}\n\t\t\t\"\"\"\n\n\t\t# Create parser tree\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">chunker = nltk.Regexp<span class=\"fea_parsing_keys udls\">Parser</span>(grammar)</div>\n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_2\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>s = nltk.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>\n\t\t<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_1\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">token</span>s = nltk.<span class=\"fea_Part_of_Speech_keys udls\">tag</span>.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(<span class=\"fea_Part_of_Speech_keys udls\">token</span>s)</div>\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_1\" style=\"display: inline;\">tree = chunker.parse(pos_tokens)</div>\n\n\t\t# Parse tree to identify tokens\n\t\tfor subtree in tree.subtrees():\n\t\t\tif subtree.label() == \"CHUNK\":\n\t\t\t\ttemp = \"\"\n\t\t\t\tfor sub in subtree:\n\t\t\t\t\ttemp += sub[0]\n\t\t\t\t\ttemp += \" \"\n\t\t\t\ttemp = temp.strip()\n\t\t\t\tnoun_phrases.append(temp)\n\n\t\t# Handle nouns\n\t\treplace_nouns = []\n\t\tfor word, _ in tags:\n\t\t\tfor phrase in noun_phrases:\n\t\t\t\tif phrase[0] == '\\'':\n\t\t\t\t\t# If it starts with an apostrophe, ignore it\n\t\t\t\t\t# (this is a weird error that should probably be handled elsewhere)\n\t\t\t\t\tbreak\n\t\t\t\tif word in phrase:\n\t\t\t\t\t# Blank out the last two words in this phrase\n\t\t\t\t\t[replace_nouns.append(phrase_word) for phrase_word in phrase.split()[-2:]]\n\t\t\t\t\tbreak\n\t\t\t# If we couldn't find the word in any phrases\n\t\t\tif len(replace_nouns) == 0:\n\t\t\t\treplace_nouns.append(word)\n\t\t\tbreak\n\n\t\tif len(replace_nouns) == 0:\n\t\t\treturn None\n\n\t\tval = 99\n\t\tfor i in replace_nouns:\n\t\t\tif len(i) &lt; val:\n\t\t\t\tval = len(i)\n\t\t\telse:\n\t\t\t\tcontinue\n\n\t\ttrivial = {\n\t\t\t\"Answer\": \" \".join(replace_nouns),\n\t\t\t\"Key\": val\n\t\t}\n\n\t\tif len(replace_nouns) == 1:\n\t\t\t# If we're only replacing one word, use WordNet to find similar words\n\t\t\ttrivial[\"Similar\"] = self.answer_options(replace_nouns[0])\n\t\telse:\n\t\t\t# If we're replacing a phrase, don't bother - it's too unlikely to make sense\n\t\t\ttrivial[\"Similar\"] = []\n\n\t\treplace_phrase = \" \".join(replace_nouns)\n\t\tblanks_phrase = (\"__________\" * len(replace_nouns)).strip()\n\t\texpression = re.compile(re.escape(replace_phrase), re.IGNORECASE)\n\t\tsentence = expression.sub(blanks_phrase, str(sentence), count=1)\n\t\ttrivial[\"Question\"] = sentence\n\t\treturn trivial\n\n\t@staticmethod\n\tdef answer_options(word: str) -&gt; list:\n\t\t\"\"\"Method to identify incorrect answer options.\n\n\t\tArgs:\n\t\t\tword (str): Actual answer to the question which is to be used\n\t\t\t\tfor generating other deceiving options.\n\n\t\tReturns:\n\t\t\tlist: Answer options.\n\t\t\"\"\"\n\t\t# In the absence of a better method, take the first synset\n\t\ttry:\n\t\t\tsynsets = wn.synsets(word, pos=\"n\")\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Synsets creation failed.\", exc_info=True)\n\n\t\t# If there aren't any synsets, return an empty list\n\t\tif len(synsets) == 0:\n\t\t\treturn []\n\t\telse:\n\t\t\tsynset = synsets[0]\n\n\t\t# Get the hypernym for this synset (again, take the first)\n\t\thypernym = synset.hypernyms()[0]\n\n\t\t# Get some hyponyms from this hypernym\n\t\thyponyms = hypernym.hyponyms()\n\n\t\t# Take the name of the first lemma for the first 8 hyponyms\n\t\tsimilar_words = []\n\t\tfor hyponym in hyponyms:\n\t\t\tsimilar_word = hyponym.lemmas()[0].name().replace(\"_\", \" \")\n\t\t\tif similar_word != word:\n\t\t\t\tsimilar_words.append(similar_word)\n\t\t\tif len(similar_words) == 8:\n\t\t\t\tbreak\n\t\treturn similar_words\n\t\t#https://github.com/nityansuman/marvin/blob/main/src/objective</code></pre></div>",
    "fir_23": "<div class=\"codeBlock hljs python\" id=\"fir_23\"><pre id=\"fir_23_code\"><code class=\"python\">import wikipedia as wiki\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nimport re\n\nimport text2num as t2n\n\nfrom Quiz import Quiz\nfrom QuestionSentence import QuestionSentence\n\nclass Article():\n\n    def __init__ (self, name):\n        self.name = name\n        self.page = wiki.page(name)\n\n        self.quiz = Quiz([])\n\n        self.generate_questions_for(\n            self.page.content.encode('ascii', 'ignore'))\n\n    ''' \n    NOT CURRENTLY USED, but maye be useful at a later point when knowing the\n    section a question was sourced from might be of use.\n    '''\n    # def iterate_sections(self):\n    #     # Iterate through article's sections\n    #     for section in self.page.sections:\n    #         print section\n    #         sec = self.page.section(section).encode('ascii', 'ignore')\n    #         if sec is None: \n    #             continue\n    #         self.generate_questions_for(sec)\n\n    '''\n    tokenizes and chunks a sentence based on a simple grammar\n    '''\n    def get_question_data(self, s):\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>s = nltk.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(s)</div>\n        <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">tag</span>ged = nltk.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(<span class=\"fea_Part_of_Speech_keys udls\">token</span>s)</div>\n        grammar =   \"\"\"  \n                    NUMBER: {&lt;$&gt;*&lt;CD&gt;+&lt;NN&gt;*}\n                    LOCATION: {&lt;IN&gt;&lt;NNP&gt;+&lt;,|IN&gt;&lt;NNP&gt;+} \n                    PROPER: {&lt;NNP|NNPS&gt;&lt;NNP|NNPS&gt;+}\n                    \"\"\"       \n        # \n        # HIT!: {&lt;PROPER&gt;&lt;NN&gt;?&lt;VBZ|VBN&gt;+}\n        # DATE: {&lt;IN&gt;(&lt;$&gt;*&lt;CD&gt;+&lt;NN&gt;*)}\n\n        <div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">chunker = nltk.Regexp<span class=\"fea_parsing_keys udls\">Parser</span>(grammar)</div>\n        <div class=\"highlights fea_parsing\" id=\"parsing_1\" style=\"display: inline;\">result = chunker.parse(tagged)</div>\n        return result\n\n    '''\n    splits a Wikipedia section into sentences and then chunks/tokenizes each\n    sentence\n    '''\n    def generate_questions_for(self, sec):\n        # Rid of all parentheses for easier processing\n        _sec = \"\".join(re.split('\\(', \n            sec.decode(\"utf-8\").replace(\")\", \"(\"))[0::2])\n\n        for sentence in sent_tokenize(_sec):\n            if \"==\" not in sentence:\n                qdata = self.get_question_data(sentence)\n                if len(qdata) &gt;= 75 and len(qdata) &lt;= 150:\n                    qdata = []\n\n                self.create_questions(sentence, qdata)\n\n    '''\n    given a setence in chunked and original form, produce the params necessary\n    to create a Question, and then add that to our Quiz object\n    '''\n    def create_questions(self, sentence, chunked):\n        gaps = []\n        for word in chunked:\n            if type(word) != tuple:                \n                target = []\n                for y in word:\n                    target.append(y[0])\n                orig_phrase = \" \".join(target)\n\n                if word.label() == \"NUMBER\":\n                    modified_phrase = orig_phrase[:]\n\n                    try:\n                        # convert spelled out word to numerical value\n                        modified_phrase = t2n.text2num(phrase)\n                    except:\n                        try:\n                            test = int(modified_phrase) + float(modified_phrase)\n                        except:\n                            # if the word could not be converted and \n                            # was not already numerical, ignore it\n                            continue\n\n                    if self.probably_range(modified_phrase):\n                        return\n\n                    gaps.append((word.label(), orig_phrase, modified_phrase))\n                elif word.label() in [\"LOCATION\", \"PROPER\"]: \n                    gaps.append((word.label(), orig_phrase, orig_phrase))\n\n        if len(gaps) &gt;= 2 and len(gaps) == len(set(gaps)):\n            gaps_filtered = [gap for gap in gaps if gap[0] == 'NUMBER' or gap[0] == 'LOCATION']\n            if len(gaps_filtered) and len(gaps) - len(gaps_filtered) &gt; 2:\n                self.quiz.add(QuestionSentence(sentence, gaps_filtered))\n\n    '''\n    Wikipedia returns non-hyphenated number ranges, so we need to check for mushed together years\n    and remove them. Not a complete solution to the problem, but most of the incidents are years\n    ''' \n    def probably_range(self, val):\n        s = str(val)\n        if s.count(\"19\") &gt; 1 or s.count(\"20\") &gt; 1 or (s.count(\"19\") == 1 and s.count(\"20\") == 1):\n            return True\n        return False\n        #https://github.com/alexgreene/WikiQuiz/blob/master/python/Article</code></pre></div>",
    "fir_1": "<div class=\"codeBlock hljs python\" id=\"fir_1\"><pre id=\"fir_1_code\"><code class=\"python\"># Stopwords removal and lemmatizing them.\n# import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nstop_keywords = [\n    'i',\n    'stack',\n    'overflow',\n    'web',\n    'tutorials',\n    'lesson',\n    'tip',\n    'learn',\n    'reference',\n    'demo',\n    'name',\n    'company',\n    'w3schools',\n    'w3resource',\n    'online',\n    'this',\n]\n\n\ndef text_process(data):\n\n    return_words = []\n\n    for d in data:\n        stop_words = set(<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english')</div>)\n\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">word_<span class=\"fea_tokenization_keys udls\">token</span>s = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(d)</div>\n\n        filtered_sentence = []\n\n        for w in word_tokens:\n            if w not in stop_words:\n                filtered_sentence.append(w)\n        # print(filtered_sentence)\n\n        # Lemmatizing the words.\n        lemma_word = []\n        <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">wordnet_<span class=\"fea_lemmatization_keys udls\">lemma</span>tizer = WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer()\n        for w in filtered_sentence:\n            word1 = wordnet_<span class=\"fea_lemmatization_keys udls\">lemma</span>tizer.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(w, pos=\"n\")\n            word2 = wordnet_<span class=\"fea_lemmatization_keys udls\">lemma</span>tizer.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(word1, pos=\"v\")\n            worfir = wordnet_<span class=\"fea_lemmatization_keys udls\">lemma</span>tizer.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(word2, pos=(\"a\"))</div>\n            lemma_word.append(worfir.lower())\n\n        # print(lemma_word)\n        # Removing unwanted lemmatized words.\n        lemma_word = set(lemma_word)\n        for word in lemma_word.copy():\n            if ((len(word) &lt;= 3) | (word in stop_keywords)):\n                lemma_word.remove(word)\n        # lemma_word = list(lemma_word)\n\n        lemma_word = list(lemma_word)\n    return return_words\n    #https://github.com/dhyeythumar/Search-Engine/blob/master/Python_scripts/text_processing</code></pre></div>",
    "fir_2": "<div class=\"codeBlock hljs python\" id=\"fir_2\"><pre id=\"fir_2_code\"><code class=\"python\"># Tokenize and Stem Data\n# Convert words to Vector Space using TFIDF matrix\n# Calculate Cosine Similarity and generate the distance matrix\n# Uses Ward method to generate an hierarchy\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.manifold import MDS\nimport matplotlibplot as thr\nimport pandas as pd\nfrom scipy.cluster.hierarchy import ward, dendrogram\nimport os\n\n\n# Function to return a list of stemmed words\ndef tokenize_and_stem(text_file):\n    # declaring stemmer and stopwords language\n    <div class=\"highlights fea_stemming\" id=\"stemming_0\" style=\"display: inline;\">stemmer = SnowballStemmer(\"english\")</div>\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span>_<span class=\"fea_nlp_datasets_keys udls\">words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english'))</div>\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">words = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(text_file)</div>\n    filtered = [w for w in words if w not in stop_words]\n    stems = [stemmer.stem(t) for t in filtered]\n    return stems\n\n\ndef main():\n\n    path = os.path.abspath(os.path.dirname(__file__))\n    data = pd.read_csv(os.path.join(path, 'data\\headlines_cleaned.txt'), names=['text'])\n\n    # text data in dataframe and removing stops words\n    stop_words = set(stopwords.words('english'))\n    data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n\n    # Using TFIDF vectorizer to convert convert words to Vector Space\n    tfidf_vectorizer = TfidfVectorizer(max_features=200000,\n                                       use_idf=True,\n                                       stop_words='english',\n                                       tokenizer=tokenize_and_stem)\n    #                                   ngram_range=(1, 3))\n\n    # Fit the vectorizer to text data\n    tfidf_matrix = tfidf_vectorizer.fit_transform(data['text'])\n\n    # Calculating the distance measure derived from cosine similarity\n    distance = 1 - cosine_similarity(tfidf_matrix)\n\n    # Ward’s method produces a hierarchy of clusterings\n    linkage_matrix = ward(distance)\n    fig, ax = thr.subplots(figsize=(15, 20)) # set size\n    ax = dendrogram(linkage_matrix, orientation=\"top\", labels=data.values)\n    thr.tight_layout()\n    thr.title('News Headlines using Ward Hierarchical Method')\n    thr.savefig(os.path.join(path, 'results\\hierarchical.png'))\n\n\nif __name__ == '__main__':\n    main()\n    #https://github.com/maneeshavinayak/Clustering-News-Headlines/blob/master/clustering/hierarchical</code></pre></div>",
    "fir_3": "<div class=\"codeBlock hljs python\" id=\"fir_3\"><pre id=\"fir_3_code\"><code class=\"python\">import json\nimport numpy as np \nimport urllib\nfrom urllib.request import urlopen\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nimport requests\nimport re\nfrom flask import Flask,request,jsonify,make_response\nimport nltk\nfrom flask_cors import CORS\nimport newspaper\nimport pandas as pd\nimport json\nimport requests\nimport time\n\nurl = \"https://graph.facebook.com/v2.6/me/messages\"\nngrok_url = 'https://the-daily-news-app.herokuapp.com/api/'\n\nsource_csv = pd.read_csv('https://raw.githubusercontent.com/codequipo/TheDailyNews/deploy/sites.csv')\n\napp = Flask(__name__)\nCORS(app)\n\nPORT=8087\n\nimport csv\n\nli_all=[]\nkey_name_all=[]\n\n@app.route('/',methods = ['GET','POST'])\ndef base():\n\treturn 'hello world'\n\n\n@app.route('/webhook', methods=['GET', 'POST'])\ndef webhook():\n\treq = request.get_json(force=True)\n\tprint(\"-\"*80)\n\tprint(req.get('queryResult').get('intent').get('displayName'))\n\tprint(\"-\"*80)\n\tif req.get('queryResult').get('intent').get('displayName') == 'summarize_intent':\n\t\tdatabase_id = req.get('queryResult').get('queryText').strip('Summarize:')\n\t\tsummary_url = ngrok_url + 'getSummary'\n\t\tdata2 = {'unique_id': database_id }\n\t\tsummary = requests.post(summary_url,data = data2)\n\t\tarticle_dict = json.loads(summary.text)\n\t\tarticle_summary = article_dict['article'][0]['text']\n\t\tarticl_image = article_dict['article'][0]['top_image']\n\t\treturn {'fulfillmentText': article_summary}\n\n\tif req.get('queryResult').get('intent').get('displayName') == 'source_intent':\n\t\tsource_name = req.get('queryResult').get('queryText')[7:]\n\t\tsource_csv.columns = ['name','link']\n\t\tsource_link = source_csv['link'][source_csv.loc[source_csv['name']==source_name].index[0]]\n\t\tnews_url = ngrok_url + 'getnewsbysources'\n\t\tdata1 = {'main_urls':source_link }\n\t\tpost_articles = requests.post(news_url,data = data1)\n\t\tlist_of_articles = json.loads(post_articles.text)\n\t\tli = list_of_articles['articles']\n\t\tmessages = []\n\t\tprint(len(li))\n\t\tfor index,item in enumerate(li):\n\t\t\ttemp = dict()\n\t\t\t\n\t\t\treq = urllib.request.Request('https://raw.githubusercontent.com/codequipo/TheDailyNews/flask_deploy/format.json')\n\t\t\twith urllib.request.urlopen(req) as f:\n    \t\t\t\ttemp = json.load(f)\n\t\t\t\n\t\t\t\t\n\t\t\ttemp['card']['buttons'][0]['postback'] = 'Summarize:'+item['unique_id']\n\t\t\ttemp['card']['title'] = item['title']\n\t\t\ttemp['card']['imageUri'] = item['top_image']\n\t\t\tprint(item['url'])\n\t\t\ttemp['card']['buttons'][1]['postback'] = item['url']\n\t\t\tmessages.append(temp)\n\n\t\tmessages = messages[:10]\n\t\t# print(messages)\n\t\treturn jsonify({'fulfillmentMessages': messages })  \n\t\n\n\treturn{'fulfillmentText':\"Please check your responses again  \"}\n\ndef getSourceData():\n\turl = 'https://raw.githubusercontent.com/codequipo/TheDailyNews/deploy/sites.csv'\n\tdf = pd.read_csv(url, error_bads=False)\n\turl_list = []\n\tkey_list = []\n\turl_list = df[\"http://www.huffingtonpost.com\"].values.tolist()\n\tkey_list = df[\"huffingtonpost\"].values.tolist()\n\turl_list = [\"http://www.huffingtonpost.com\"] + url_list\n\tkey_list = [\"huffingtonpost\"] + key_list\n\treturn key_list,url_list\n\nkey_list, url_list= getSourceData()\n\n@app.route(\"/db\",methods=['POST','GET'])\ndef build_database():\n\ttic=time.time()\n\t# key_list,url_list = getSourceData()\n\tjson_body=request.get_json(force=True)\n\tcurrCount = int(json_body.get('currCount'))\n\tnumOfSources = int(json_body.get('numOfSources'))\n\tnumOfArticlesPerSources = int(json_body.get('numOfArticlesPerSources'))\n\tnum_of_sentences_in_summary = int(json_body.get('num_of_sentences_in_summary'))\n\n\tprint('currCount : '+str(currCount))\n\n\t\n\tresponse_data=dict()\n\tfor i in range(currCount,currCount+numOfSources):\n\t\turl=url_list[i]\n\t\tsource = newspaper.build( url, memoize_articles=True, language='en')\n\t\t\n\t\td=dict() # Holds articles from current selected source \n\t\tk=0\n\t\t\n\t\tfor article in source.articles:\n\t\t\ttry:\n\t\t\t\tarticle.download() \n\t\t\t\tarticle.parse() \n\t\t\t\tsummary = driver(article.text,num_of_sentences_in_summary)\n\t\t\t\t\n\t\t\t\tarticle_info=dict()\n\t\t\t\tarticle_info['url']=article.url\n\t\t\t\tarticle_info['title']=article.title\n\t\t\t\tprint('i:'+str(i)+'  k:'+str(k)+'  title  =&gt; '+article_info['title'])\n\t\t\t\tarticle_info['text']=summary\n\t\t\t\tarticle_info['top_image']=article.top_image\n\n\t\t\t\td[k]=article_info\n\t\t\t\t\n\t\t\t\t\n\t\t\t\tk+=1\n\t\t\t\tif k == numOfArticlesPerSources:\n\t\t\t\t\tbreak\n\t\t\texcept Exception as e:\n\t\t\t\tprint(\"Entered except block :\"+str(e))\n\t\t\t\tpass\n\t\td['length']=k\n\t\tresponse_data[url]=d\n\n\t\tprint(url+\"   NewArticles : \"+str(k))\n\n\tresult={\n\t\t'success':True,\n\t\t'alldata':response_data,\n\t\t'allsite':url_list[currCount:currCount+numOfSources],\n\t\t'allsite_key':key_list[currCount:currCount+numOfSources]\n\t}\n\ttoc=time.time()\n\tdiff=toc-tic\n\tprint(\"# Time required for function to execute is :\"+str(diff)+\" # \")\n\tprint()\n\tprint()\n\treturn json.dumps(result)\n\t\n\n\ndef clean(sentences):\n\tlemmatizer = WordNetLemmatizer()\n\tcleaned_sentences = []\n\tfor sentence in sentences:\n\t\tsentence = sentence.lower()\n\t\tsentence = re.sub(r'[^a-zA-Z]',' ',sentence)\n\t\tsentence = sentence.split()\n\t\tsentence = [<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_keys udls\">lemma</span>tizer.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(word)</div> for word in sentence if word not in <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english'))</div>]\n\t\tsentence = ' '.join(sentence)\n\t\tcleaned_sentences.append(sentence)\n\treturn cleaned_sentences\n\ndef init_probability(sentences):\n\tprobability_dict = {}\n\t<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">words = word_<span class=\"fea_tokenization_keys udls\">token</span>ize('. '.join(sentences))</div>\n\ttotal_words = len(set(words))\n\tfor word in words:\n\t\tif word!='.':\n\t\t\tif not probability_dict.get(word):\n\t\t\t\tprobability_dict[word] = 1\n\t\t\telse:\n\t\t\t\tprobability_dict[word] += 1\n\n\tfor word,count in probability_dict.items():\n\t\tprobability_dict[word] = count/total_words \n\t\n\treturn probability_dict\n\ndef update_probability(probability_dict,word):\n\tif probability_dict.get(word):\n\t\tprobability_dict[word] = probability_dict[word]**2\n\treturn probability_dict\n\ndef average_sentence_weights(sentences,probability_dict):\n\tsentence_weights = {}\n\tfor index,sentence in enumerate(sentences):\n\t\tif len(sentence) != 0:\n\t\t\taverage_proba = sum([probability_dict[word] for word in sentence if word in probability_dict.keys()])\n\t\t\taverage_proba /= len(sentence)\n\t\t\tsentence_weights[index] = average_proba \n\treturn sentence_weights\n\ndef generate_summary(sentence_weights,probability_dict,cleaned_article,tokenized_article,summary_length = 30):\n\tsummary = \"\"\n\tcurrent_length = 0\n\twhile current_length &lt; summary_length :\n\t\thighest_probability_word = max(probability_dict,key=probability_dict.get)\n\t\tsentences_with_max_word= [index for index,sentence in enumerate(cleaned_article) if highest_probability_word in <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">set(word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence))</div>]\n\t\tsentence_list = sorted([[index,sentence_weights[index]] for index in sentences_with_max_word],key=lambda x:x[1],reverse=True)\n\t\tsummary += tokenized_article[sentence_list[0][0]] + \"\\n\"\n\t\tfor word in <div class=\"highlights fea_tokenization\" id=\"tokenization_2\" style=\"display: inline;\">word_<span class=\"fea_tokenization_keys udls\">token</span>ize(cleaned_article[sentence_list[0][0]])</div>:\n\t\t\tprobability_dict = update_probability(probability_dict,word)\n\t\tcurrent_length+=1\n\treturn summary\n\ndef driver(article,required_length):\n\trequired_length = int(required_length)\n\t<div class=\"highlights fea_tokenization\" id=\"tokenization_3\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>ized_article = sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(article)</div>\n\tcleaned_article = clean(tokenized_article) \n\tprobability_dict = init_probability(cleaned_article)\n\tsentence_weights = average_sentence_weights(cleaned_article,probability_dict)\n\t<div class=\"highlights fea_summarizer\" id=\"summarizer_0\" style=\"display: inline;\"><span class=\"fea_summarizer_keys udls\">summar</span>y = generate_<span class=\"fea_summarizer_keys udls\">summar</span>y(<span class=\"fea_summarizer_keys udls\">sentence</span>_weights,probability_dict,cleaned_article,tokenized_article,required_length)</div>\n\treturn summary\n\nif __name__ == \"__main__\":\n    app.run(port=PORT)\n    #https://github.com/codequipo/TheDailyNews/blob/master/flask_server/app</code></pre></div>",
    "fir_6": "<div class=\"codeBlock hljs python\" id=\"fir_6\"><pre id=\"fir_6_code\"><code class=\"python\">import os,re,math,csv,string,random,logging,glob,itertools,operator,sys\nfrom os import listdir\nfrom os.path import isfile, join\nfrom collections import Counter, defaultdict, OrderedDict\nfrom itertools import chain, combinations\n\nimport pandas as pd\nimport numpy as np\nimport scipy\nfrom scipy import spatial\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet as wn\nfrom nltk.tag.stanford import StanfordPOSTagger\nfrom nltk.util import ngrams\n\nimport gensim\nfrom gensim.models import word2vec\n\ndef InitialCleanup(dataframe,\n                   minwords=2,\n                   use_filler_list=None,\n                   filler_regex_and_list=False):\n\n    \"\"\"\n    Perform basic text cleaning to prepare dataframe\n    for analysis. Remove non-letter/-space characters,\n    empty turns, turns below a minimum length, and\n    fillers.\n\n    By default, preserves turns 2 words or longer.\n    If desired, this may be changed by updating the\n    `minwords` argument.\n\n    By default, remove common fillers through regex.\n    If desired, remove other words by passing a list\n    of literal strings to `use_filler_list` argument,\n    and if both regex and list of additional literal\n    strings are to be used, update `filler_regex_and_list=True`.\n    \"\"\"\n\n    # only allow strings, spaces, and newlines to pass\n    WHITELIST = string.ascii_letters + '\\'' + ' '\n\n    # remove inadvertent empty turns\n    dataframe = dataframe[pd.notnull(dataframe['content'])]\n\n    # internal function: remove fillers via regular expressions\n    def applyRegExpression(textFiller):\n        textClean = re.sub('^(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]+\\s', ' ', textFiller) # at the start of a string\n        textClean = re.sub('\\s(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]+\\s', ' ', textClean) # within a string\n        textClean = re.sub('\\s(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]$', ' ', textClean) # end of a string\n        textClean = re.sub('^(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]$', ' ', textClean) # if entire turn string\n        return textClean\n\n    # create a new column with only approved text before cleaning per user-specified settings\n    dataframe['clean_content'] = dataframe['content'].apply(lambda utterance: ''.join([char for char in utterance if char in WHITELIST]).lower())\n\n    # DEFAULT: remove typical speech fillers via regular expressions (examples: \"um, mm, oh, hm, uh, ha\")\n    if use_filler_list is None and not filler_regex_and_list:\n        dataframe['clean_content'] = dataframe['clean_content'].apply(applyRegExpression)\n\n    # OPTION 1: remove speech fillers or other words specified by user in a list\n    elif use_filler_list is not None and not filler_regex_and_list:\n        dataframe['clean_content'] = dataframe['clean_content'].apply(lambda utterance: ' '.join([word for word in utterance.split(\" \") if word not in use_filler_list]))\n\n    # OPTION 2: remove speech fillers via regular expression and any additional words from user-specified list\n    elif use_filler_list is not None and filler_regex_and_list:\n        dataframe['clean_content'] = dataframe['clean_content'].apply(applyRegExpression)\n        dataframe['clean_content'] = dataframe['clean_content'].apply(lambda utterance: ' '.join([word for word in utterance.split(\" \") if word not in use_filler_list]))\n\n    # OPTION 3: nothing is filtered\n    else:\n        dataframe['clean_content'] = dataframe['clean_content']\n\n    # drop the old \"content\" column and rename the clean \"content\" column\n    dataframe = dataframe.drop(['content'],axis=1)\n    dataframe = dataframe.rename(index=str,\n                                 columns ={'clean_content': 'content'})\n\n    # remove rows that are now blank or do not meet `minwords` requirement, then drop length column\n    dataframe['utteranceLen'] = dataframe['content'].apply(lambda x: <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">word_<span class=\"fea_tokenization_keys udls\">token</span>ize(x)).str.len()</div>\n    dataframe = dataframe.drop(dataframe[dataframe.utteranceLen &lt; int(minwords)].index).drop(['utteranceLen'],axis=1)\n    dataframe = dataframe.reset_index(drop=True)\n\n    # return the cleaned dataframe\n    return dataframe\n\ndef AdjacentMerge(dataframe):\n\n    \"\"\"\n    Given a dataframe of conversation turns,\n    merge adjacent turns by the same speaker.\n    \"\"\"\n\n    repeat=1\n    while repeat==1:\n        l1=len(dataframe)\n        DfMerge = []\n        k = 0\n        if len(dataframe) &gt; 0:\n            while k &lt; len(dataframe)-1:\n                if dataframe['participant'].iloc[k] != dataframe['participant'].iloc[k+1]:\n                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])\n                    k = k + 1\n                elif dataframe['participant'].iloc[k] == dataframe['participant'].iloc[k+1]:\n                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k] + \" \" + dataframe['content'].iloc[k+1]])\n                    k = k + 2\n            if k == len(dataframe)-1:\n                DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])\n\n        dataframe=pd.DataFrame(DfMerge,columns=('participant','content'))\n        if l1==len(dataframe):\n            repeat=0\n\n    return dataframe\n\ndef Tokenize(text,nwords):\n    \"\"\"\n    Given list of text to be processed and a list\n    of known words, return a list of edited and\n    tokenized words.\n\n    Spell-checking is implemented using a\n    Bayesian spell-checking algorithm\n    (http://norvig.com/spell-correct.html).\n\n    By default, this is based on the Project Gutenberg\n    corpus, a collection of approximately 1 million texts\n    (http://www.gutenberg.org). A copy of this is included\n    within this package. If desired, users may specify a\n    different spell-check training corpus in the\n    `training_dictionary` argument of the\n    `prepare_transcripts()` function.\n\n    \"\"\"\n\n    # internal function: identify possible spelling errors for a given word\n    def edits1(word):\n        splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes    = [a + b[1:] for a, b in splits if b]\n        transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)&gt;1]\n        replaces   = [a + c + b[1:] for a, b in splits for c in string.ascii_lowercase if b]\n        inserts    = [a + c + b     for a, b in splits for c in string.ascii_lowercase]\n        return set(deletes + transposes + replaces + inserts)\n\n    # internal function: identify known edits\n    def known_edits2(word,nwords):\n        return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in nwords)\n\n    # internal function: identify known words\n    def known(words,nwords): return set(w for w in words if w in nwords)\n\n    # internal function: correct spelling\n    def correct(word,nwords):\n        candidates = known([word],nwords) or known(edits1(word),nwords) or known_edits2(word,nwords) or [word]\n        return max(candidates, key=nwords.get)\n\n    # expand out based on a fixed list of common contractions\n    contract_dict = { \"ain't\": \"is not\",\n        \"aren't\": \"are not\",\n        \"can't\": \"cannot\",\n        \"can't've\": \"cannot have\",\n        \"'cause\": \"because\",\n        \"could've\": \"could have\",\n        \"couldn't\": \"could not\",\n        \"couldn't've\": \"could not have\",\n        \"didn't\": \"did not\",\n        \"doesn't\": \"does not\",\n        \"don't\": \"do not\",\n        \"hadn't\": \"had not\",\n        \"hadn't've\": \"had not have\",\n        \"hasn't\": \"has not\",\n        \"haven't\": \"have not\",\n        \"he'd\": \"he had\",\n        \"he'd've\": \"he would have\",\n        \"he'll\": \"he will\",\n        \"he'll've\": \"he will have\",\n        \"he's\": \"he is\",\n        \"how'd\": \"how did\",\n        \"how'd'y\": \"how do you\",\n        \"how'll\": \"how will\",\n        \"how's\": \"how is\",\n        \"i'd\": \"i would\",\n        \"i'd've\": \"i would have\",\n        \"i'll\": \"i will\",\n        \"i'll've\": \"i will have\",\n        \"i'm\": \"i am\",\n        \"i've\": \"i have\",\n        \"isn't\": \"is not\",\n        \"it'd\": \"it would\",\n        \"it'd've\": \"it would have\",\n        \"it'll\": \"it will\",\n        \"it'll've\": \"it will have\",\n        \"it's\": \"it is\",\n        \"let's\": \"let us\",\n        \"ma'am\": \"madam\",\n        \"mayn't\": \"may not\",\n        \"might've\": \"might have\",\n        \"mightn't\": \"might not\",\n        \"mightn't've\": \"might not have\",\n        \"must've\": \"must have\",\n        \"mustn't\": \"must not\",\n        \"mustn't've\": \"must not have\",\n        \"needn't\": \"need not\",\n        \"needn't've\": \"need not have\",\n        \"o'clock\": \"of the clock\",\n        \"oughtn't\": \"ought not\",\n        \"oughtn't've\": \"ought not have\",\n        \"shan't\": \"shall not\",\n        \"sha'n't\": \"shall not\",\n        \"shan't've\": \"shall not have\",\n        \"she'd\": \"she would\",\n        \"she'd've\": \"she would have\",\n        \"she'll\": \"she will\",\n        \"she'll've\": \"she will have\",\n        \"she's\": \"she is\",\n        \"should've\": \"should have\",\n        \"shouldn't\": \"should not\",\n        \"shouldn't've\": \"should not have\",\n        \"so've\": \"so have\",\n        \"so's\": \"so as\",\n        \"that'd\": \"that had\",\n        \"that'd've\": \"that would have\",\n        \"that's\": \"that is\",\n        \"there'd\": \"there would\",\n        \"there'd've\": \"there would have\",\n        \"there's\": \"there is\",\n        \"they'd\": \"they would\",\n        \"they'd've\": \"they would have\",\n        \"they'll\": \"they will\",\n        \"they'll've\": \"they will have\",\n        \"they're\": \"they are\",\n        \"they've\": \"they have\",\n        \"to've\": \"to have\",\n        \"wasn't\": \"was not\",\n        \"we'd\": \"we would\",\n        \"we'd've\": \"we would have\",\n        \"we'll\": \"we will\",\n        \"we'll've\": \"we will have\",\n        \"we're\": \"we are\",\n        \"we've\": \"we have\",\n        \"weren't\": \"were not\",\n        \"what'll\": \"what will\",\n        \"what'll've\": \"what will have\",\n        \"what're\": \"what are\",\n        \"what's\": \"what is\",\n        \"what've\": \"what have\",\n        \"when's\": \"when is\",\n        \"when've\": \"when have\",\n        \"where'd\": \"where did\",\n        \"where's\": \"where is\",\n        \"where've\": \"where have\",\n        \"who'll\": \"who will\",\n        \"who'll've\": \"who will have\",\n        \"who's\": \"who is\",\n        \"who've\": \"who have\",\n        \"why's\": \"why is\",\n        \"why've\": \"why have\",\n        \"will've\": \"will have\",\n        \"won't\": \"will not\",\n        \"won't've\": \"will not have\",\n        \"would've\": \"would have\",\n        \"wouldn't\": \"would not\",\n        \"wouldn't've\": \"would not have\",\n        \"y'all\": \"you all\",\n        \"y'all'd\": \"you all would\",\n        \"y'all'd've\": \"you all would have\",\n        \"y'all're\": \"you all are\",\n        \"y'all've\": \"you all have\",\n        \"you'd\": \"you would\",\n        \"you'd've\": \"you would have\",\n        \"you'll\": \"you will\",\n        \"you'll've\": \"you will have\",\n        \"you're\": \"you are\",\n        \"you've\": \"you have\" }\n    contractions_re = re.compile('(%s)' % '|'.join(list(contract_dict.keys())))\n\n    # internal function:\n    def expand_contractions(text, contractions_re=contractions_re):\n        def replace(match):\n            return contract_dict[match.group(0)]\n        return contractions_re.sub(replace, text.lower())\n\n    # process all words in the text\n    cleantoken = []\n    text = expand_contractions(text)\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span> = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(text)</div>\n    for word in token:\n        if \"'\" not in word:\n            cleantoken.append(correct(word,nwords))\n        else:\n            cleantoken.append(word)\n    return cleantoken\n\n\ndef pos_to_wn(tag):\n    \"\"\"\n    Convert NLTK default tagger output into a format that Wordnet\n    can use in order to properly lemmatize the text.\n    \"\"\"\n\n    # create some inner functions for simplicity\n    def is_noun(tag):\n        return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n    def is_verb(tag):\n        return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n    def is_adverb(tag):\n        return tag in ['RB', 'RBR', 'RBS']\n    def is_adjective(tag):\n        return tag in ['JJ', 'JJR', 'JJS']\n\n    # check each tag against possible categories\n    <div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">if is_<span class=\"fea_n_grams_keys udls\">noun</span>(tag):\n        return wn.<span class=\"fea_n_grams_keys udls\">NOUN</span>\n    elif is_<span class=\"fea_n_grams_keys udls\">verb</span>(tag):\n        return wn.<span class=\"fea_n_grams_keys udls\">VERB</span>\n    elif is_ad<span class=\"fea_n_grams_keys udls\">verb</span>(tag):\n        return wn.ADV\n    elif is_ad<span class=\"fea_n_grams_keys udls\">ject</span>ive(tag):\n        return wn.ADJ\n    else:\n        return wn.<span class=\"fea_n_grams_keys udls\">NOUN</span></div>\n\n\ndef Lemmatize(tokenlist):\n    l<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">emmatizer = WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer()</div>\n    <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\">default<span class=\"fea_Part_of_Speech_keys udls\">Pos</span> = nltk.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(<span class=\"fea_Part_of_Speech_keys udls\">token</span>list)</div> # get the POS tags from NLTK default tagger\n    words_lemma = []\n    for item in defaultPos:\n        words_lemma.append(<div class=\"highlights fea_lemmatization\" id=\"lemmatization_1\" style=\"display: inline;\"><span class=\"fea_lemmatization_keys udls\">lemma</span>tizer.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(item[0],pos_to_wn(item[1]))</div>) # need to convert POS tags to a format (NOUN, VERB, ADV, ADJ) that wordnet uses to lemmatize\n    return words_lemma\n\n\ndef ApplyPOSTagging(df,\n                    filename,\n                    add_stanford_tags=False,\n                    stanford_pos_path=None,\n                    stanford_language_path=None):\n\n    \"\"\"\n    Given a dataframe of conversation turns, return a new\n    dataframe with part-of-speech tagging. Add filename\n    (given as string) as a new column in returned dataframe.\n\n    By default, return only tags from the NLTK default POS\n    tagger. Optionally, also return Stanford POS tagger\n    results by setting `add_stanford_tags=True`.\n\n    If Stanford POS tagging is desired, specify the\n    location of the Stanford POS tagger with the\n    `stanford_pos_path` argument. Also note that the\n    default language model for the Stanford tagger is\n    English (english-left3words-distsim.tagger). To change\n    language model, specify the location with the\n    `stanford_language_path` argument.\n\n    \"\"\"\n\n    # if desired, import Stanford tagger\n    if add_stanford_tags:\n        if stanford_pos_path is None or stanford_language_path is None:\n            raise ValueError('Error! Specify path to Stanford POS tagger and language model using the `stanford_pos_path` and `stanford_language_path` arguments')\n        else:\n            <div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">stanford_<span class=\"fea_tagger_keys udls\">tagge</span>r = StanfordPOS<span class=\"fea_tagger_keys udls\">Tagge</span>r(stanford_pos_path + stanford_language_path,\n                                                stanford_pos_path + 'stanford-pos<span class=\"fea_tagger_keys udls\">tagge</span>r.jar')</div>\n\n    # add new columns to dataframe\n    df['tagged_token'] = df['token'].apply(<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_1\" style=\"display: inline;\">nltk.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span></div>)\n    df['tagged_lemma'] = df['lemma'].apply(nltk.pos_tag)\n\n    # if desired, also tag with Stanford tagger\n    if add_stanford_tags:\n        df['tagged_stan_token'] = df['token'].apply(<div class=\"highlights fea_tagger\" id=\"tagger_1\" style=\"display: inline;\">stanford_<span class=\"fea_tagger_keys udls\">tagge</span>r.tag</div>)\n        df['tagged_stan_lemma'] = df['lemma'].apply(stanford_tagger.tag)\n\n    df['file'] = filename\n\n    # return finished dataframe\n    return df\n\ndef prepare_transcripts(input_files,\n                        output_file_directory,\n                        training_dictionary=None,\n                        minwords=2,\n                        use_filler_list=None,\n                        filler_regex_and_list=False,\n                        add_stanford_tags=False,\n                        stanford_pos_path=None,\n                        stanford_language_path=None,\n                        input_as_directory=True,\n                        save_concatenated_dataframe=True):\n\n    \"\"\"\n    Prepare transcripts for similarity analysis.\n\n    Given individual .txt files of conversations,\n    return a completely prepared dataframe of transcribed\n    conversations for later ALIGN analysis, including: text\n    cleaning, merging adjacent turns, spell-checking,\n    tokenization, lemmatization, and part-of-speech tagging.\n    The output serve as the input for later ALIGN\n    analysis.\n\n    Parameters\n    ----------\n\n    input_files : str (directory name) or list of str (file names)\n        Raw files to be cleaned. Behavior governed by `input_as_directory`\n        parameter as well.\n\n    output_file_directory : str\n        Name of directory where output for individual conversations will be\n        saved.\n\n    training_dictionary : str, optional (default: None)\n        Specify whether to train the spell-checking dictionary using a\n        provided file name (str) or the default Project\n        Gutenberg corpus [http://www.gutenberg.org] (None).\n\n    minwords : int, optional (2)\n        Specify the minimum number of words in a turn. Any turns with fewer\n        than the minimum number of words will be removed from the corpus.\n        (Note: `minwords` must be equal to or greater than `maxngram` provided\n        to `calculate_alignment()` and `calculate_baseline_alignment` in later\n        steps.)\n\n    use_filler_list : list of str, optional (default: None)\n        Specify whether words should be filtered from all conversations using a\n        list of filler words (list of str) or using regular expressions to\n        filter out common filler words (None). Behavior governed by\n        `filler_regex_and_list` parameter as well.\n\n    filler_regex_and_list : boolean, optional (default: False)\n        If providing a list to `use_filler_list` parameter, specify whether to\n        use only the provided list (False) or to use both the provided list and\n        the regular expression filter (True).\n\n    add_stanford_tags : boolean, optional (default: False)\n        Specify whether to return part-of-speech similarity scores based on\n        Stanford POS tagger in addition to the Penn POS tagger (True) or to\n        return only POS similarity scores from the Penn tagger (False). (Note:\n        Including Stanford POS tags will lead to a significant increase in\n        processing time.)\n\n    stanford_pos_path : str, optional (default: None)\n        If Stanford POS tagging is desired, specify local path to Stanford POS\n        tagger.\n\n    stanford_language_path : str, optional (default: None)\n        If Stanford POS tagging is desired, specify local path to Stanford POS\n        tagger for the desired language (str) or use the default English tagger\n        (None).\n\n    input_as_directory : boolean, optional (default: True)\n        Specify whether the value passed to `input_files` parameter should\n        be read as a directory (True) or a list of files to be processed\n        (False).\n\n    save_concatenated_dataframe : boolean, optional (default: True)\n        Specify whether to save the individual conversation output data only\n        as individual files in the `output_file_directory` (False) or to save\n        the individual files as well as a single concatenated dataframe (True).\n\n    Returns\n    -------\n\n    prepped_df : Pandas DataFrame\n        A single concatenated dataframe of all transcripts, ready for\n        processing with `calculate_alignment()` and\n        `calculate_baseline_alignment()`.\n\n    \"\"\"\n\n    # create an internal function to train the model\n    def train(features):\n        model = defaultdict(lambda: 1)\n        for f in features:\n            model[f] += 1\n        return model\n\n    # if no training dictionary is specified, use the Gutenberg corpus\n    if training_dictionary is None:\n\n        # first, get the name of the package directory\n        module_path = os.path.dirname(os.path.abspath(__file__))\n\n        # then construct the path to the text file\n        training_dictionary = os.path.join(module_path, 'data/gutenberg.txt')\n\n    # train our spell-checking model\n    nwords = train(re.findall('[a-z]+', (open(training_dictionary).read().lower())))\n\n    # grab the appropriate files\n    if not input_as_directory:\n        file_list = glob.glob(input_files)\n    else:\n        file_list = glob.glob(input_files+\"/*.txt\")\n\n    # cycle through all files\n    prepped_df = pd.DataFrame()\n    for fileName in file_list:\n\n        # let us know which file we're processing\n        print((\"Processing: \"+fileName))\n        dataframe = pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n\n        # clean up, merge, spellcheck, tokenize, lemmatize, and POS-tag\n        dataframe = InitialCleanup(dataframe,\n                                   minwords=minwords,\n                                   use_filler_list=use_filler_list,\n                                   filler_regex_and_list=filler_regex_and_list)\n        dataframe = AdjacentMerge(dataframe)\n\n        # tokenize and lemmatize\n        dataframe['token'] = dataframe['content'].apply(Tokenize,\n                                     args=(nwords,))\n        dataframe['lemma'] = dataframe['token'].apply(Lemmatize)\n\n        # apply part-of-speech tagging\n        dataframe = ApplyPOSTagging(dataframe,\n                                    filename=os.path.basename(fileName),\n                                    add_stanford_tags=add_stanford_tags,\n                                    stanford_pos_path=stanford_pos_path,\n                                    stanford_language_path=stanford_language_path)\n\n        # export the conversation's dataframe as a CSV\n        conversation_file = os.path.join(output_file_directory,os.path.basename(fileName))\n        dataframe.to_csv(conversation_file, encoding='utf-8',index=False,sep='\\t')\n        prepped_df = prepped_df.append(dataframe)\n\n    # save the concatenated dataframe\n    if save_concatenated_dataframe:\n        concatenated_file = os.path.join(output_file_directory,'../align_concatenated_dataframe.txt')\n        prepped_df.to_csv(concatenated_file,\n                    encoding='utf-8',index=False, sep='\\t')\n\n    # return the dataframe\n    return prepped_df\n    #https://github.com/nickduran/align-linguistic-alignment/blob/master/align/prepare_transcripts</code></pre></div>",
    "fir_8": "<div class=\"codeBlock hljs python\" id=\"fir_8\"><pre id=\"fir_8_code\"><code class=\"python\"># -*- coding: utf-8 -*-\n# Implementation from https://dev.to/davidisrawi/build-a-quick-summarizer-with-python-and-nltk\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\nfrom framework.parser.parser import Parser\n\ntext_str = '''\n&lt;header&gt;My name is Wil Wheaton. I Live With Chronic Depression and Generalized Anxiety. I Am Not Ashamed.&lt;/header&gt;&lt;p&gt;Before I begin, I want to warn you that this talk touches on many triggering subjects, including self-harm and suicide. I also want you to know that I’m speaking from my personal experience, and that if you or someone you know may be living with mental illness, please talk to a licensed and qualified medical professional, because I am not a doctor.&lt;/p&gt;&lt;p&gt;Okay, let’s do this.&lt;/p&gt;&lt;p&gt;Hi, I’m Wil Wheaton. I’m 45 years-old, I have a wonderful wife, two adult children who make me proud every day, and a daughter in-law who I love like she’s my own child. I work on the most popular comedy series in the world, I’ve been a New York Times Number One Bestselling Audiobook narrator, I have run out of space in my office for the awards I’ve received for my work, and as a white, heterosexual, cisgender man in America, I live life on the lowest difficulty setting — with the Celebrity cheat enabled.&lt;/p&gt;&lt;p&gt;&lt;b&gt;My life is, by every objective measurement, very very good.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;And in spite of all of that, I struggle every day with my self esteem, my self worth, and my value not only as an actor and writer, but as a human being.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;That’s because I live with Depression and Anxiety, the tag team champions of the World Wrestling With Mental Illness Federation.&lt;/p&gt;&lt;p&gt;And I’m not ashamed to stand here, in front of six hundred people in this room, and millions more online, and proudly say that I live with mental illness, and that’s okay. I say “with” because even though my mental illness tries its best, it doesn’t control me, it doesn’t define me, and I refuse to be stigmatized by it.&lt;/p&gt;&lt;p&gt;&lt;b&gt;So. My name is Wil Wheaton, and I have Chronic Depression.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;It took me over thirty years to be able to say those ten words, and I suffered for most of them as a result. I suffered because though we in America have done a lot to help people who live with mental illness, we have not done nearly enough to make it okay for our fellow travelers on the wonky brain express to reach out and accept that help.&lt;/p&gt;&lt;p&gt;I’m here today to talk with you about working to end the stigma and prejudice that surrounds mental illness in America, and as part of that, I want to share my story with you.&lt;/p&gt;&lt;p&gt;When I was a little kid, probably seven or eight years old, I started having panic attacks. Back then, we didn’t know that’s what they were, and because they usually happened when I was asleep, the adults in my life just thought I had nightmares. Well, I did have nightmares, but they were so much worse than just bad dreams. Night after night, I’d wake up in absolute terror, and night after night, I’d drag my blankets off my bed, to go to sleep on the floor in my sister’s bedroom, because I was so afraid to be alone.&lt;/p&gt;&lt;p&gt;There were occasional stretches of relief, sometimes for months at a time, and during those months, I felt like what I considered to be a normal kid, but the panic attacks always came back, and each time they came back, they seemed worse than before.&lt;/p&gt;&lt;p&gt;When I was around twelve or thirteen, my anxiety began to express itself in all sorts of delightful ways.&lt;/p&gt;&lt;p&gt;I worried about everything. I was tired all the time, and irritable most of the time. I had no confidence and terrible self-esteem. I felt like I couldn’t trust anyone who wanted to be close to me, because I was convinced that I was stupid and worthless and the only reason anyone would want to be my friend was to take advantage of my fame.&lt;/p&gt;&lt;p&gt;This is important context. When I was thirteen, I was in an internationally-beloved film called Stand by Me, and I was famous. Like, really famous, like, can’t-go-to-the-mall-with-my-friends-without-getting-mobbed famous, and that meant that all of my actions were scrutinized by my parents, my peers, my fans, and the press. All the weird, anxious feelings I had all the time? I’d been raised to believe that they were shameful. That they reflected poorly on my parents and my family. That they should be crammed down deep inside me, shared with nobody, and kept secret.&lt;/p&gt;&lt;p&gt;My panic attacks happened daily, and not just when I was asleep. When I tried to reach out to the adults in my life for help, they didn’t take me seriously. When I was on the set of a tv show or commercial, and I was having a hard time breathing because I was so anxious about making a mistake and getting fired? The directors and producers complained to my parents that I was being difficult to work with. When I was so uncomfortable with my haircut or my crooked teeth and didn’t want to pose for teen magazine photos, the publicists told me that I was being ungrateful and trying to sabotage my success. When I couldn’t remember my lines, because I was so anxious about things I can’t even remember now, directors would accuse me of being unprofessional and unprepared. And that’s when my anxiety turned into depression.&lt;/p&gt;&lt;header&gt;(I’m going to take a moment for myself right now, and I’m going to tear a hole in the fabric of spacetime and I’m going to tell all those adults from the past: give this kid a break. He’s scared. He’s confused. He is doing the best he can, and if you all could stop seeing him as a way to put money into your pockets, maybe you could see that he’s suffering and needs help.)&lt;/header&gt;&lt;p&gt;I was miserable a lot of the time, and it didn’t make any sense. I was living a childhood dream, working on Star Trek: The Next Generation, and getting paid to do what I loved. I had all the video games and board games I ever wanted, and did I mention that I was famous?&lt;/p&gt;&lt;p&gt;I struggled to reconcile the facts of my life with the reality of my existence. I knew something was wrong with me, but I didn’t know what. And because I didn’t know what, I didn’t know how to ask for help.&lt;/p&gt;&lt;p&gt;I wish I had known that I had a mental illness that could be treated! I wish I had known that that the way I felt wasn’t normal and it wasn’t necessary. I wish I had known that I didn’t deserve to feel bad, all the time.&lt;/p&gt;&lt;p&gt;And I didn’t know those things, because Mental Illness was something my family didn’t talk about, and when they did, they talked about it like it was something that happened to someone else, and that it was something they should be ashamed of, because it was a result of something they did. This prejudice existed in my family in spite of the ample incidence of mental illness that ran rampant through my DNA, featuring successful and unsuccessful suicide attempts by my relations, more than one case of bipolar disorder, clinical depression everywhere, and, because of self-medication, so much alcoholism, it was actually notable when someone didn’t have a drinking problem.&lt;/p&gt;&lt;p&gt;Now, I don’t blame my parents for how they addressed — or more accurately didn’t address — my mental illness, because I genuinely believe they were blind to the symptoms I was exhibiting. They grew up and raised me in the world I’ve spent the last decade of my life trying to change. They lived in a world where mental illness was equated with weakness, and shame, and as a result, I suffered until I was in my thirties.&lt;/p&gt;&lt;p&gt;And it’s not like I never reached out for help. I did! I just didn’t know what questions to ask, and the adults I was close to didn’t know what answers to give.&lt;/p&gt;&lt;p&gt;Mom, I know you’re going to read this or hear this and I know it’s going to make you upset. I want you to know that I love you, and I know that you did the very best you could. I’m telling my story, though, so someone else’s mom can see the things you didn’t, through no fault of your own.&lt;/p&gt;&lt;p&gt;I clearly remember being twenty-two, living in my own house, waking up from a panic attack that was so terrifying just writing about it for this talk gave me so much anxiety I almost cut this section from my speech. It was the middle of the night, and I drove across town, to my parents’ house, to sleep on the floor of my sister’s bedroom again, because at least that’s where I felt safe. The next morning, I tearfully asked my mom what was wrong with me. She knew that many of my blood relatives had mental illness, but she couldn’t or wouldn’t connect the dots. “You’re just realizing that the world is a scary place,” she said.&lt;/p&gt;&lt;p&gt;Yeah, no kidding. The world terrifies me every night of my life and I don’t know why or how to stop it.&lt;/p&gt;&lt;p&gt;Again, I don’t blame her and neither should you. She really was doing the best that she could for me, but stigma and the shame is inspires are powerful things.&lt;/p&gt;&lt;p&gt;I want to be very clear on this: Mom, I know you’re going to read this or hear this and I know it’s going to make you upset. I want you to know that I love you, and I know that you did the very best you could. I’m telling my story, though, so someone else’s mom can see the things you didn’t, through no fault of your own.&lt;/p&gt;&lt;p&gt;Through my twenties, I continued to suffer, and not just from nightmares and panic attacks. I began to develop obsessive behaviors that I’ve never talked about in public until right now. Here’s a very incomplete list: I began to worry that the things I did would affect the world around me in totally irrational ways. I would hold my breath underneath bridges when I was driving, because if I didn’t, maybe I’d crash my car. I would tap the side of an airplane with my hand while I was boarding, and tell it to take care of me when I flew places for work, because I was convinced that if I didn’t, the plane would crash. Every single time I said goodbye to someone I cared about, my brain would play out in vivid detail how I would remember this as the last time I saw them. Talking about those memories, even without getting into specifics, is challenging. It’s painful to recall, but I’m not ashamed, because all those thoughts — which I thankfully don’t have any more, thanks to medical science and therapy — were not my fault any more than the allergies that clog my sinuses when the trees in my neighborhood start doin’ it every spring are my fault. It’s just part of who I am. It’s part of how my brain is wired, and because I know that, I can medically treat it, instead of being a victim of it.&lt;/p&gt;&lt;p&gt;One of the primary reasons I speak out about my mental illness, is so that I can make the difference in someone’s life that I wish had been made in mine when I was young, because not only did I have no idea what Depression even was until I was in my twenties, once I was pretty sure that I had it, I suffered with it for another fifteen years, because I was ashamed, I was embarrassed, and I was afraid.&lt;/p&gt;&lt;p&gt;So I am here today to tell anyone who can hear me: if you suspect that you have a mental illness, there is no reason to be ashamed, or embarrassed, and most importantly, you do not need to be afraid. You do not need to suffer. There is nothing noble in suffering, and there is nothing shameful or weak in asking for help. This may seem really obvious to a lot of you, but it wasn’t for me, and I’m a pretty smart guy, so I’m going to say it anyway: There is no reason to feel embarrassed when you reach out to a professional for help, because the person you are reaching out to is someone who has literally dedicated their life to helping people like us live, instead of merely exist.&lt;/p&gt;&lt;p&gt;I missed out on a lot of things, during what are supposed to be the best years of my life, because I was paralyzed by What If-ing anxiety.&lt;/p&gt;&lt;p&gt;That difference, between existing and living, is something I want to focus on for a minute: before I got help for my anxiety and depression, I didn’t truly live my life. I wanted to go do things with my friends, but my anxiety always found a way to stop me. Traffic would just be too stressful, it would tell me. It’s going to be a real hassle to get there and find parking, it would helpfully observe. And if those didn’t stop me from leaving my house, there was always the old reliable: What if…? Ah, “What if… something totally unlikely to happen actually happens? What if the plane crashes? What if I sit next to someone who freaks me out? What if they laugh at me? What if I get lost? What if I get robbed? What if I get locked out of my hotel room? What if I slip on some ice I didn’t see? What if there’s an earthquake? What if what if what if what if…&lt;/p&gt;&lt;p&gt;When I look back on most of my life, it breaks my heart that when my brain was unloading an endless pile of what ifs on me, it never asked, “What if I go do this thing that I want to do, and it’s … fun? What if I enjoy myself, and I’m really glad I went?”&lt;/p&gt;&lt;p&gt;I have to tell you a painful truth: I missed out on a lot of things, during what are supposed to be the best years of my life, because I was paralyzed by What If-ing anxiety.&lt;/p&gt;&lt;p&gt;All the things that people do when they are living their lives … all those experiences that make up a life, my anxiety got in between me and doing them. So I wasn’t living. I was just existing.&lt;/p&gt;&lt;p&gt;And through it all, I never stopped to ask myself if this was normal, or healthy, or even if it was my fault. I just knew that I was nervous about stuff, and I worried a lot. For my entire childhood, my mom told me that I was a worry wart, and my dad said I was overly dramatic about everything, and that’s just the way it was.&lt;/p&gt;&lt;p&gt;Except it didn’t have to be that way, and it took me having a full blown panic attack and a complete meltdown at Los Angeles International Airport for my wife to suggest to me that I get help.&lt;/p&gt;&lt;p&gt;Like I said, I had suspected for years that I was clinically depressed, but I was afraid to admit it, until the most important person in my life told me without shame or judgment that she could see that I was suffering. So I went to see a doctor, and I will never forget what he said, when I told him how afraid I was: “Please let me help you.”&lt;/p&gt;&lt;p&gt;I think it was then, at about 34 years-old, that I realized that Mental Illness is not weakness. It’s just an illness. I mean, it’s right there in the name “Mental ILLNESS” so it shouldn’t have been the revelation that it was, but when the part of our bodies that is responsible for how we perceive the world and ourselves is the same part of our body that is sick, it can be difficult to find objectivity or perspective.&lt;/p&gt;&lt;p&gt;So I let my doctor help me. I started a low dose of an antidepressant, and I waited to see if anything was going to change.&lt;/p&gt;&lt;p&gt;And boy did it.&lt;/p&gt;&lt;p&gt;My wife and I were having a walk in our neighborhood and I realized that it was just a really beautiful day — it was warm with just a little bit of a breeze, the birds sounded really beautiful, the flowers smelled really great and my wife’s hand felt really good in mine.&lt;/p&gt;&lt;p&gt;And as we were walking I just started to cry and she asked me, “what’s wrong?”&lt;/p&gt;&lt;p&gt;I said “I just realized that I don’t feel bad and I just realized that I’m not existing, I’m living.”&lt;/p&gt;&lt;p&gt;At that moment, I realized that I had lived my life in a room that was so loud, all I could do every day was deal with how loud it was. But with the help of my wife, my doctor, and medical science, I found a doorway out of that room.&lt;/p&gt;&lt;p&gt;I had taken that walk with my wife almost every day for nearly ten years, before I ever noticed the birds or the flowers, or how loved I felt when I noticed that her hand was holding mine. Ten years — all of my twenties — that I can never get back. Ten years of suffering and feeling weak and worthless and afraid all the time, because of the stigma that surrounds mental illness.&lt;/p&gt;&lt;p&gt;I’m not religious, but I can still say Thank God for Anne Wheaton. Thank God for her love and support. Thank God that my wife saw that I was hurting, and thank God she didn’t believe the lie that Depression is weakness, or something to be ashamed of. Thank God for Anne, because if she hadn’t had the strength to encourage me to seek professional help, I don’t know how much longer I would have been able to even exist, to say nothing of truly living.&lt;/p&gt;&lt;p&gt;I started talking in public about my mental illness in 2012, and ever since then, people reach out to me online every day, and they ask me about living with depression and anxiety. They share their stories, and ask me how I get through a bad day, or a bad week.&lt;/p&gt;&lt;header&gt;Right now, there is a child somewhere who has the same panic attacks I had, and their parents aren’t getting them help, because they believe it reflects poorly on their parenting to have a child with mental illness.&lt;/header&gt;&lt;p&gt;Here’s one of the things I tell them:&lt;/p&gt;&lt;p&gt;One of the many delightful things about having Depression and Anxiety is occasionally and unexpectedly feeling like the whole goddamn world is a heavy lead blanket, like that thing they put on your chest at the dentist when you get x-rays, and it’s been dropped around your entire existence without your consent.&lt;/p&gt;&lt;p&gt;Physically, it weighs heavier on me in some places than it does in others. I feel it tugging at the corners of my eyes, and pressing down on the center of my chest. When it’s really bad, it can feel like one of those dreams where you try to move, but every step and every motion feels like you’re struggling to move through something heavy and viscous. Emotionally, it covers me completely, separating me from my motivation, my focus, and everything that brings me joy in my life.&lt;/p&gt;&lt;p&gt;When it drops that lead apron over us, we have to remind ourselves that one of the things Depression does, to keep itself strong and in charge, is tell us lies, like: I am the worst at everything. Nobody really likes me. I don’t deserve to be happy. This will never end. And so on and so on. We can know, in our rational minds, that this is a giant bunch of bullshit (and we can look at all these times in our lives when were WERE good at a thing, when we genuinely felt happy, when we felt awful but got through it, etc.) but in the moment, it can be a serious challenge to wait for Depression to lift the roadblock that’s keeping us from moving those facts from our rational mind to our emotional selves.&lt;/p&gt;&lt;p&gt;And that’s the thing about Depression: we can’t force it to go away. As I’ve said, if I could just “stop feeling sad” I WOULD. (And, also, Depression isn’t just feeling sad, right? It’s a lot of things together than can manifest themselves into something that is most easily simplified into “I feel sad.”)&lt;/p&gt;&lt;p&gt;So another step in our self care is to be gentle with ourselves. Depression is beating up on us already, and we don’t need to help it out. Give yourself permission to acknowledge that you’re feeling terrible (or bad, or whatever it is you are feeling), and then do a little thing, just one single thing, that you probably don’t feel like doing, and I PROMISE you it will help. Some of those things are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Take a shower. &lt;/li&gt;&lt;li&gt;Eat a nutritious meal. &lt;/li&gt;&lt;li&gt;Take a walk outside (even if it’s literally to the corner and back). &lt;/li&gt;&lt;li&gt;Do something — throw a ball, play tug of war, give belly rubs — with a dog. Just about any activity with my dogs, even if it’s just a snuggle on the couch for a few minutes, helps me. &lt;/li&gt;&lt;li&gt;Do five minutes of yoga stretching. &lt;/li&gt;&lt;li&gt;Listen to a guided meditation and follow along as best as you can. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Finally, please trust me and know that this shitty, awful, overwhelming, terrible way you feel IS NOT FOREVER. It will get better. It always gets better. You are not alone in this fight, and you are OK.&lt;/p&gt;&lt;p&gt;No person anywhere, especially here in the richest country in the world, should live in the shadows or suffer alone, because they can’t afford treatment. We have all the money in the world for weapons and corporate tax cuts, so I know that we can afford to prioritize not just health care in general, but mental health care, specifically.&lt;/p&gt;&lt;p&gt;Right now, there is a child somewhere who has the same panic attacks I had, and their parents aren’t getting them help, because they believe it reflects poorly on their parenting to have a child with mental illness. Right now, there is a teenager who is contemplating self harm, because they don’t know how to reach out and ask for help. Right now, there are too many people struggling just to get to the end of the day, because they can’t afford the help that a lot of us can’t live without. But there are also people everywhere who are picking up the phone and making an appointment. There are parents who have learned that mental illness is no different than physical illness, and they’re helping their children get better. There are adults who, like me, were terrified that antidepressant medication would make them a different person, and they’re hearing the birds sing for the first time, because they have finally found their way out of the dark room.&lt;/p&gt;&lt;p&gt;I spent the first thirty years of my life trapped in that dark, loud room, and I know how hopeless and suffocating it feels to be in there, so I do everything I can to help others find their way out. I do that by telling my story, so that my privilege and success does more than enrich my own life. I can live by example for someone else the way Jenny Lawson lives by example for me.&lt;/p&gt;&lt;p&gt;But I want to leave you today with some suggestions for things that we can all do, even if you’re not Internet Famous like I am, to help end the stigma of mental illness, so that nobody has to merely exist, when they could be living.&lt;/p&gt;&lt;p&gt;We can start by demanding that our elected officials fully fund mental health programs. No person anywhere, especially here in the richest country in the world, should live in the shadows or suffer alone, because they can’t afford treatment. We have all the money in the world for weapons and corporate tax cuts, so I know that we can afford to prioritize not just health care in general, but mental health care, specifically.&lt;/p&gt;&lt;p&gt;And until our elected officials get their acts together, we can support organizations like NAMI, that offer low and no-cost assistance to anyone who asks for it. We can support organizations like Project UROK, that work tirelessly to end stigmatization and remind us that we are sick, not weak.&lt;/p&gt;&lt;p&gt;We can remember, and we can remind each other, that there is no finish line when it comes to mental illness. It’s a journey, and sometimes we can see the path we’re on all the way to the horizon, while other times we can’t even see five feet in front of us because the fog is so thick. But the path is always there, and if we can’t locate it on our own, we have loved ones and doctors and medications to help us find it again, as long as we don’t give up trying to see it.&lt;/p&gt;&lt;p&gt;Finally, we who live with mental illness need to talk about it, because our friends and neighbors know us and trust us. It’s one thing for me to stand here and tell you that you’re not alone in this fight, but it’s something else entirely for you to prove it. We need to share our experiences, so someone who is suffering the way I was won’t feel weird or broken or ashamed or afraid to seek treatment. So that parents don’t feel like they have failed or somehow screwed up when they see symptoms in their kids.&lt;/p&gt;&lt;p&gt;People tell me that I’m brave for speaking out the way I do, and while I appreciate that, I don’t necessarily agree. Firefighters are brave. Single parents who work multiple jobs to take care of their kids are brave. The Parkland students are brave. People who reach out to get help for their mental illness are brave. I’m not brave. I’m just a writer and occasional actor who wants to share his privilege and good fortune with the world, who hopes to speak out about mental health so much that one day, it will be wholly unremarkable to stand up and say fifteen words:&lt;/p&gt;&lt;p&gt;My name is Wil Wheaton, I live with chronic depression, and I am not ashamed.&lt;/p&gt;\n'''\n\n# All weightage for structure doc\n# Important: These scores are for the experimenting purpose only\nWEIGHT_FOR_LIST = 5\nWEIGHT_FOR_HIGHLIGHTED = 10\nWEIGHT_FOR_NUMERICAL = 5\nWEIGHT_FIRST_PARAGRAPH = 5\nWEIGHT_BASIC = 1\n\n\ndef _create_frequency_table(paragraph_list) -&gt; dict:\n    \"\"\"\n    we create a dictionary for the word frequency table.\n    For this, we should only use the words that are not part of the stopWords array.\n\n    Removing stop words and making frequency table\n    Stemmer - an algorithm to bring words to its root word.\n    :rtype: dict\n    \"\"\"\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">Words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>(\"english\"))</div>\n\n    <div class=\"highlights fea_stemming\" id=\"stemming_0\" style=\"display: inline;\">ps = PorterStemmer()</div>\n\n    freqTable = dict()\n    for paragraph in paragraph_list:\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">words = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(paragraph.text)</div>\n\n        all_highlighted_sentences = [sent for sent in paragraph.get_highlighted()]\n        highlighted_words_text = \" \".join(all_highlighted_sentences)\n        highlighted_words = word_tokenize(highlighted_words_text)\n\n        for word in words:\n\n            if paragraph.is_list_set:\n                weight = WEIGHT_FOR_LIST\n            else:\n                weight = WEIGHT_BASIC\n\n            if word in highlighted_words:\n                weight += WEIGHT_FOR_HIGHLIGHTED\n\n            if word.isnumeric() and len(word) &gt;= 2:\n                weight += WEIGHT_FOR_NUMERICAL\n\n            if paragraph.is_first_paragraph:\n                weight += WEIGHT_FIRST_PARAGRAPH\n\n            word = ps.stem(word)\n            if word in stopWords:\n                continue\n\n            if word in freqTable:\n                freqTable[word] += weight\n            else:\n                freqTable[word] = weight\n\n    return freqTable\n\n\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">def _<span class=\"fea_text_scoring_keys udls\">score</span>_s<span class=\"fea_text_scoring_keys udls\">ent</span>ences(s<span class=\"fea_text_scoring_keys udls\">ent</span>ences, freqTable) -&gt; dict:</div>\n    \"\"\"\n    score a sentence by its words\n    Basic algorithm: adding the frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n    :rtype: dict\n    \"\"\"\n    # TODO: Can you make this multiprocess compatible in python?\n\n    sentenceValue = dict()\n\n    for sentence in sentences:\n        word_count_in_sentence = (len(word_tokenize(sentence)))\n        word_count_in_sentence_except_stop_words = 0\n        for wordValue in freqTable:\n            if wordValue in sentence.lower():\n                word_count_in_sentence_except_stop_words += 1\n                if sentence[:10] in sentenceValue:\n                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n                else:\n                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n\n        if sentence[:10] in sentenceValue:\n            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] / word_count_in_sentence_except_stop_words\n\n        '''\n        Notice that a potential issue with our score algorithm is that long sentences will have an advantage over short sentences. \n        To solve this, we're dividing every sentence score by the number of words in the sentence.\n        \n        Note that here sentence[:10] is the first 10 character of any sentence, this is to save memory while saving keys of\n        the dictionary.\n        '''\n\n    return sentenceValue\n\n\ndef _find_average_score(sentenceValue) -&gt; int:\n    \"\"\"\n    Find the average score from the sentence value dictionary\n    :rtype: int\n    \"\"\"\n    sumValues = 0\n    for entry in sentenceValue:\n        sumValues += sentenceValue[entry]\n\n    average = 0\n    # Average value of a sentence from original summary_text\n    if len(sentenceValue) &gt; 0:\n        average = (sumValues / len(sentenceValue))\n\n    return average\n\n\ndef _generate_summary(sentences, sentenceValue, threshold):\n    sentence_count = 0\n    summary = ''\n\n    for sentence in sentences:\n        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] &gt;= (threshold):\n            summary += \" \" + sentence\n            sentence_count += 1\n\n    # TODO: check if the sentences in the summarization is in the original order of occurrence.\n\n    return summary\n\n\ndef run_summarization(paragraph_list):\n    # 1 Create the word frequency table\n    freq_table = _create_frequency_table(paragraph_list)\n    # print (freq_table)\n\n    '''\n    We already have a sentence tokenizer, so we just need \n    to run the sent_tokenize() method to create the array of sentences.\n    '''\n\n    # 2 Tokenize the sentences\n    sentences = [paragraph.text for paragraph in paragraph_list]\n    # print(sentences)\n\n    # 3 Important Algorithm: score the sentences\n    sentence_scores = _score_sentences(sentences, freq_table)\n\n    # 4 Find the threshold\n    threshold = _find_average_score(sentence_scores)\n\n    # 5 Important Algorithm: Generate the summary\n    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n\n    return summary\n\n\nif __name__ == '__main__':\n    parser = Parser()\n    parser.feed(text_str)\n    <div class=\"highlights fea_summarizer\" id=\"summarizer_0\" style=\"display: inline;\">result = run_<span class=\"fea_summarizer_keys udls\">summar</span>ization(parser.paragraphs)</div>\n    print(result)\n    #https://github.com/akashp1712/summarize-webpage/blob/master/implementation/word_frequency_summarize_parser</code></pre></div>",
    "fir_9": "<div class=\"codeBlock hljs python\" id=\"fir_9\"><pre id=\"fir_9_code\"><code class=\"python\">import constants\nimport nltk\nimport random\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport string\nimport warnings\n\ndef get_formalities_response(formality) :\n    if any(remove_punctuation_marks(formality).lower() in remove_punctuation_marks(greet).lower() for greet in constants.GREETING_INPUTS) :\n        return random.choice(constants.GREETING_REPLIES)\n    elif any(remove_punctuation_marks(formality).lower() in remove_punctuation_marks(thanks).lower() for thanks in constants.THANKS_INPUTS) :\n        return random.choice(constants.THANKS_REPLIES)\n\ndef get_lemmatized_tokens(text) :\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">normalized_<span class=\"fea_tokenization_keys udls\">token</span>s = nltk.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(remove_punctuation_marks(text.lower()))</div>\n    return [<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">nltk.stem.WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer().<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(normalized_token)</div> for normalized_token in normalized_tokens]\n\ndef get_query_reply(query) :\n    documents.append(query)\n    tfidf_results = TfidfVectorizer(tokenizer = get_lemmatized_tokens, stop_words = 'english').fit_transform(documents)\n    cosine_similarity_results = cosine_similarity(tfidf_results[-1], tfidf_results).flatten()\n    # The last will be 1.0 because it is the Cosine Similarity between the first document and itself\n    best_index = cosine_similarity_results.argsort()[-2]\n    documents.remove(query)\n    if cosine_similarity_results[best_index] == 0 :\n        return \"I am sorry! I don't understand you...\"\n    else :\n        return documents[best_index]\n\ndef remove_punctuation_marks(text) :\n    punctuation_marks = dict((ord(punctuation_mark), None) for punctuation_mark in string.punctuation)\n    return text.translate(punctuation_marks)\n\nif __name__ == \"__main__\" :\n    warnings.filterwarnings(\"ignore\")\n\n    try :\n        nltk.data.find('tokenizers/punkt')\n    except LookupError:\n        nltk.download('punkt')\n\n    try :\n        nltk.data.find('corpora/wordnet')\n    except LookupError:\n        <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">nltk.download('wordnet')</div>\n\n    corpus = open('corpus.txt', 'r' , errors = 'ignore').read().lower()\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">documents = nltk.sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(corpus)</div>\n\n    <div class=\"highlights fea_chatbot\" id=\"chatbot_0\" style=\"display: inline;\">print('RyuzakiBot: My name is RyuzakiBot. I will answer your queries about World Wide Web. If you want to exit just type: Bye!')\n    end_chat = False\n    while end_chat == False :\n        input_text = input()\n        if remove_punctuation_marks(input_text).lower() != 'bye' :\n            formality_reply = get_formalities_response(input_text)\n            if  formality_reply :\n                print('RyuzakiBot: ' + formality_reply)\n            else :\n                print('RyuzakiBot: ' + get_query_reply(input_text))\n        else :\n            print('RyuzakiBot: Bye! Take care ' + random.choice(constants.CANDIES))\n            end_chat = True</div>\n            #https://github.com/LuciaLlavero/ryuzaki_bot/blob/master/ryuzaki_bot_desktop</code></pre></div>",
    "fir_10": "<div class=\"codeBlock hljs python\" id=\"fir_10\"><pre id=\"fir_10_code\"><code class=\"python\"># coding: utf-8\n\nimport constants\nfrom flask import Flask, jsonify, request\nfrom flask_cors import CORS\nfrom flask_restful import Resource, Api\nimport nltk\nimport random\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport string\nimport sys\nimport warnings\n\ndef get_formalities_reply(formality) :\n    if any(remove_punctuation_marks(formality).lower() in remove_punctuation_marks(greet).lower() for greet in constants.GREETING_INPUTS) :\n        return random.choice(constants.GREETING_REPLIES)\n    elif any(remove_punctuation_marks(formality).lower() in remove_punctuation_marks(thanks).lower() for thanks in constants.THANKS_INPUTS) :\n        return random.choice(constants.THANKS_REPLIES)\n\ndef get_lemmatized_tokens(text):\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">normalized_<span class=\"fea_tokenization_keys udls\">token</span>s = nltk.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(remove_punctuation_marks(text.lower()))</div>\n    return [<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">nltk.stem.WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer().<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(normalized_token)</div> for normalized_token in normalized_tokens]\n\ncorpus = open('corpus.txt', 'r' , errors = 'ignore').read().lower()\ndocuments = nltk.sent_tokenize(corpus)\n\ndef get_query_reply(query) :    \n    documents.append(query)\n    tfidf_results = TfidfVectorizer(tokenizer = get_lemmatized_tokens, stop_words = 'english').fit_transform(documents)\n    cosine_similarity_results = cosine_similarity(tfidf_results[-1], tfidf_results).flatten()\n    # The last will be 1.0 because it is the Cosine Similarity between the first document and itself\n    best_index = cosine_similarity_results.argsort()[-2]\n    documents.remove(query)\n    if cosine_similarity_results[best_index] == 0 :\n        return \"I am sorry! I don't understand you...\"\n    else :\n        return documents[best_index]\n\ndef remove_punctuation_marks(text) :\n    punctuation_marks = dict((ord(punctuation_mark), None) for punctuation_mark in string.punctuation)\n    return text.translate(punctuation_marks)\n\napp = Flask(__name__)\ncors = CORS(app)\napi = Api(app)\n\nclass Reply(Resource) :\n    def get(self) :\n        if request.args.get('q') :\n            formality_reply = get_formalities_reply(request.args.get('q'))\n            if  formality_reply :\n                return jsonify({'reply': formality_reply + ' ' + random.choice(constants.SWEETS)})\n            else :\n                return jsonify({'reply': get_query_reply(request.args.get('q'))})\n        else :\n            return jsonify({'error': 'query is empty'})\n\napi.add_resource(Reply, '/reply.json')\n\nif __name__ == \"__main__\" :\n\n    app.run()\n    #https://github.com/LuciaLlavero/ryuzaki_bot/blob/master/ryuzaki_bot</code></pre></div>",
    "fir_11": "<div class=\"codeBlock hljs python\" id=\"fir_11\"><pre id=\"fir_11_code\"><code class=\"python\">import nltk\nimport re\nfrom newspaper import Article\nfrom geograpy.labels import Labels\n\nclass Extractor(object):\n    '''\n    Extract geo context for text or from url\n    '''\n    def __init__(self, text=None, url=None, debug=False):\n        '''\n        Constructor\n        Args:\n\n            text(string): the text to analyze\n            url(string): the url to read the text to analyze from\n            debug(boolean): if True show debug information\n        '''\n        if not text and not url:\n            raise Exception('text or url is required')\n        self.debug=debug\n        self.text = text\n        self.url = url\n        self.places = []\n\n    def set_text(self):\n        '''\n        Setter for text\n        '''\n        if not self.text and self.url:\n            a = Article(self.url)\n            a.download()\n            a.parse()\n            self.text = a.text\n            \n    def split(self,delimiter=r\",\"):\n        '''\n        simpler regular expression splitter with not entity check\n        \n        hat tip: https://stackoverflow.com/a/1059601/1497139\n        '''\n        self.set_text()\n        self.places=re.split(delimiter,self.text)\n            \n    def find_geoEntities(self):\n        '''\n        Find geographic entities\n        \n        Returns:\n            list: \n                List of places\n        '''\n        self.find_entities(Labels.geo)\n        return self.places\n        \n    def find_entities(self,labels=Labels.default):\n        '''\n        Find entities with the given labels set self.places and returns it\n        Args:\n            labels: \n                Labels: The labels to filter\n        Returns:\n            list: \n                List of places\n        '''\n        self.set_text()\n\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">text = nltk.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(self.text)</div>\n        nes = nltk.ne_chunk(<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\">nltk.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(text)</div>)\n\n        for ne in nes:\n            if type(ne) is nltk.tree.Tree:\n                nelabel=ne.label()\n                if (nelabel in labels):\n                    leaves=ne.leaves()\n                    if self.debug:\n                        print(leaves)\n                    self.places.append(u' '.join([i[0] for i in leaves]))\n        return self.places\n        #https://github.com/somnathrakshit/geograpy3/blob/master/geograpy/extraction</code></pre></div>",
    "fir_12": "<div class=\"codeBlock hljs python\" id=\"fir_12\"><pre id=\"fir_12_code\"><code class=\"python\">import sys,re,collections,nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# patterns that used to find or/and replace particular chars or words\n# to find chars that are not a letter, a blank or a quotation\npat_letter = re.compile(r'[^a-zA-Z \\']+')\n# to find the 's following the pronouns. re.I is refers to ignore case\npat_is = re.compile(\"(it|he|she|that|this|there|here)(\\'s)\", re.I)\n# to find the 's following the letters\npat_s = re.compile(\"(?&lt;=[a-zA-Z])\\'s\")\n# to find the ' following the words ending by s\npat_s2 = re.compile(\"(?&lt;=s)\\'s?\")\n# to find the abbreviation of not\npat_not = re.compile(\"(?&lt;=[a-zA-Z])n\\'t\")\n# to find the abbreviation of would\npat_would = re.compile(\"(?&lt;=[a-zA-Z])\\'d\")\n# to find the abbreviation of will\npat_will = re.compile(\"(?&lt;=[a-zA-Z])\\'ll\")\n# to find the abbreviation of am\npat_am = re.compile(\"(?&lt;=[I|i])\\'m\")\n# to find the abbreviation of are\npat_are = re.compile(\"(?&lt;=[a-zA-Z])\\'re\")\n# to find the abbreviation of have\npat_ve = re.compile(\"(?&lt;=[a-zA-Z])\\'ve\")\n\n\nlmtzr = WordNetLemmatizer()\n\n\ndef get_words(file):  \n    with open (file) as f:  \n        words_box=[]\n        pat = re.compile(r'[^a-zA-Z \\']+')\n        for line in f:                           \n            #if re.match(r'[a-zA-Z]*',line): \n            #    words_box.extend(line.strip().strip('\\'\\\"\\.,').lower().split())\n            # words_box.extend(pat.sub(' ', line).strip().lower().split())\n            words_box.extend(merge(replace_abbreviations(line).split()))\n    return collections.Counter(words_box)  \n\n\ndef merge(words):\n    new_words = []\n    for word in words:\n        if word:\n            tag = <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\">nltk.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span></div>(<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">word_<span class=\"fea_tokenization_keys udls\">token</span>ize(word)</div>) # tag is like [('bigger', 'JJR')]\n            pos = get_wordnet_pos(tag[0][1])\n            if pos:\n                <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_keys udls\">lemma</span>tized_word = lmtzr.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(word, pos)</div>\n                new_words.append(lemmatized_word)\n            else:\n                new_words.append(word)\n    return new_words\n\n\ndef get_wordnet_pos(treebank_tag):\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">if treebank_tag.startswith('J'):\n        return nltk.corpus.wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return nltk.corpus.wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return nltk.corpus.wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return nltk.corpus.wordnet.ADV</div>\n    else:\n        return ''\n\n\ndef replace_abbreviations(text):\n    new_text = text\n    new_text = pat_letter.sub(' ', text).strip().lower()\n    new_text = pat_is.sub(r\"\\1 is\", new_text)\n    new_text = pat_s.sub(\"\", new_text)\n    new_text = pat_s2.sub(\"\", new_text)\n    new_text = pat_not.sub(\" not\", new_text)\n    new_text = pat_would.sub(\" would\", new_text)\n    new_text = pat_will.sub(\" will\", new_text)\n    new_text = pat_am.sub(\" am\", new_text)\n    new_text = pat_are.sub(\" are\", new_text)\n    new_text = pat_ve.sub(\" have\", new_text)\n    new_text = new_text.replace('\\'', ' ')\n    return new_text\n\n\ndef append_ext(words):\n    new_words = []\n    for item in words:\n        word, count = item\n        tag = nltk.pos_tag(word_tokenize(word))[0][1] # tag is like [('bigger', 'JJR')]\n        new_words.append((word, count, tag))\n    return new_words\n\ndef write_to_file(words, file='results.txt'):\n    f = open(file, 'w')\n    for item in words:\n        for field in item:\n            f.write(str(field)+',')\n        f.write('\\n')\n\n\nif __name__=='__main__':\n    book = sys.argv[1]\n    print \"counting...\"\n    words = get_words(book)\n    print \"writing file...\"\n    write_to_file(append_ext(words.most_common()))\n    #https://github.com/rocketk/wordcounter/blob/master/wordcounter/word_counter</code></pre></div>",
    "fir_13": "<div class=\"codeBlock hljs python\" id=\"fir_13\"><pre id=\"fir_13_code\"><code class=\"python\"># -*- coding: utf-8 -*-\nimport numpy as np\nimport json\nimport sys\nimport re\nimport os\nimport ast\nimport argparse\nimport argcomplete\nimport multiprocessing\nfrom functools import partial\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nimport gensim\nfrom gensim import utils, corpora, models\nimport io\n\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">ignore_<span class=\"fea_nlp_datasets_keys udls\">words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english'))</div>\n\n''' from the model that was created, you can calculate the topic probability distribution of unseen documents.\n    this is a command line interface using Gensim for preprocessing unseen\n    documents and calculating topic probability distributions over a given topology from an LDA model '''\n\ndef write_topn_words(output_dir, lda):\n    if not os.path.exists(output_dir + 'topn_words.json'):\n        print('Writing topn words for LDA model')\n        reg_ex = re.compile('(?&lt;![\\s/])/[^\\s/]+(?![\\S/])')\n        topn_words = {'Topic ' + str(i + 1): [reg_ex.sub('', word) for word, prob in lda.show_topic(i, topn=20)] for i in range(0, lda.num_topics)}\n        with open(output_dir + 'topn_words.json', 'w') as outfile:\n            json.dump(topn_words, outfile, sort_keys=True, indent=4)\n\ndef preprocess_tweet(document, lemma):\n    with io.open(document, 'r', encoding=\"utf-8\") as infile:\n        text = ' '.join(line.rstrip('\\n') for line in infile)\n    # convert string into unicode\n    text = gensim.utils.any2unicode(text)\n    # remove URL's\n    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n    # remove symbols excluding the @, # and \\s symbol\n    text = re.sub(r'[^\\w@#\\s]', '', text)\n    if lemma:\n        return utils.lemmatize(text, stopwords=ignore_words, min_length=3)\n    # tokenize words using NLTK Twitter Tokenizer\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">tknzr = Tweet<span class=\"fea_tokenization_keys udls\">Token</span>izer()\n    text = tknzr.<span class=\"fea_tokenization_keys udls\">token</span>ize(text)</div>\n    # lowercase, remove words less than len 2 &amp; remove numbers in tokenized list\n    return [word.lower() for word in text if len(word) &gt; 2 and not word.isdigit() and not word in ignore_words]\n\ndef get_document_vectors(user_id, **kwargs):\n    print('Getting document vectors for: ' + user_id)\n    if os.path.exists(kwargs['tweets_dir'] + user_id):\n        tweetpath = kwargs['tweets_dir'] + user_id\n    else:\n        return\n\n    if not user_id in kwargs['document_vectors']:\n        document = preprocess_tweet(tweetpath, kwargs['lemma'])\n        # if after preprocessing, the list is empty, then skip that user\n        if not document:\n            return\n        # create bag of words from input document\n        doc_bow = kwargs['dictionary'].doc2bow(document)\n        # queries the document against the LDA model and associates the data with probabalistic topics\n        doc_lda = get_doc_topics(kwargs['lda_model'], doc_bow)\n        dense_vec = gensim.matutils.sparse2full(doc_lda, kwargs['lda_model'].num_topics)\n        # build dictionary of user document vectors &lt;k, v&gt;(user_id, vec)\n        return (user_id, dense_vec.tolist())\n    else:\n        return (user_id, kwargs['document_vectors'][user_id])\n\n# http://stackoverflow.com/questions/17310933/document-topical-distribution-in-gensim-lda\ndef get_doc_topics(lda, bow):\n    gamma, _ = lda.inference([bow])\n    topic_dist = gamma[0] / sum(gamma[0])\n    return [(topic_id, topic_value) for topic_id, topic_value in enumerate(topic_dist)]\n\ndef community_document_vectors(doc_vecs, community):\n    comm_doc_vecs = {}\n    for user in ast.literal_eval(community):\n        try:\n            comm_doc_vecs[str(user)] = doc_vecs[str(user)]\n        except:\n            pass\n    return comm_doc_vecs\n\ndef read_json(file_name):\n    try:\n        with open(file_name, 'r') as comm_doc_vecs_file:\n            return json.load(comm_doc_vecs_file)\n    except:\n        return {}\n \ndef main():\n    # this program uses an LDA model to vectorize 'documents' and outputs a json file containing {user: [topic probability distribution vector]} results\n    # it also creates the directories for the communities generated from the topology file, putting each community document vectors json file in corresponding directory\n    parser = argparse.ArgumentParser(description='Create a corpus from a collection of tweets and/or build an LDA model')\n    parser.add_argument('-t', '--topology_file', required=True, action='store', dest='top_file', help='Location of topology file')\n    parser.add_argument('-p', '--dir_prefix', choices=['clique', 'community'], required=True, action='store', dest='dir_prefix', help='Select whether the topology contains cliques or communities')\n    parser.add_argument('-w', '--working_dir', required=True, action='store', dest='working_dir', help='Name of the directory you want to direct output to')\n    parser.add_argument('-l', '--lda_loc', required=True, action='store', dest='lda_loc', help='Location of the saved LDA model')\n    parser.add_argument('-d', '--dict_loc', required=True, action='store', dest='dict_loc', help='Location of dictionary for the model')\n    parser.add_argument('-u', '--unseen_docs', required=True, action='store', dest='unseen_docs', help='Directory containing unseen documents')\n    parser.add_argument('-m', '--lemma', action='store_true', dest='lemma', help='Use this option to lemmatize words')\n    argcomplete.autocomplete(parser)\n    args = parser.parse_args()\n\n    output_dir = os.path.join(args.working_dir, '')\n    if not os.path.exists(os.path.dirname(output_dir)):\n        os.makedirs(os.path.dirname(output_dir), 0o755)\n\n    # load dictionary\n    model_dict = corpora.Dictionary.load(args.dict_loc)\n    # load trained model from file\n    lda = models.LdaModel.load(args.lda_loc)\n    write_topn_words(output_dir, lda)\n\n    # create a set of all users from topology file\n    with open(args.top_file, 'r') as inp_file:\n        users = set(str(user) for community in inp_file for user in ast.literal_eval(community))\n\n    # opens up a 'job in progress' if ran this program and stopped it\n    try:\n        with open(output_dir + 'document_vectors.json', 'r') as all_community_file:\n            document_vectors = json.load(all_community_file)\n    except:\n        document_vectors = {}\n\n    # use multiprocessing to query document vectors\n    pool = multiprocessing.Pool(max(1, multiprocessing.cpu_count() - 1))\n    func = partial(get_document_vectors,\n                   tweets_dir=args.unseen_docs,\n                   document_vectors=document_vectors,\n                   dictionary=model_dict,\n                   lda_model=lda,\n                   lemma=args.lemma)\n    doc_vecs = pool.map(func, users)\n    doc_vecs = [item for item in doc_vecs if item is not None]\n    pool.close()\n    pool.join()\n    doc_vecs = dict(doc_vecs) # {user: [topic probability distribution vector]}\n\n    document_vectors.update(doc_vecs)\n    with open(output_dir + 'document_vectors.json', 'w') as document_vectors_file:\n        json.dump(document_vectors, document_vectors_file, sort_keys=True, indent=4)\n\n    print('Building directories')\n    with open(args.top_file, 'r') as topology_file:\n        for i, community in enumerate(topology_file):\n            community_dir = os.path.join(output_dir, args.dir_prefix + '_' + str(i) + '/')\n            if not os.path.exists(os.path.dirname(community_dir)):\n                os.makedirs(os.path.dirname(community_dir), 0o755)\n            comm_doc_vecs = community_document_vectors(doc_vecs, community)\n            with open(community_dir + 'community_doc_vecs.json', 'w') as comm_docs_file:\n                json.dump(comm_doc_vecs, comm_docs_file, sort_keys=True, indent=4)\n\nif __name__ == '__main__':\n    sys.exit(main())\n    #https://github.com/kethort/TwitterLDATopicModeling/blob/master/src/tweets_on_LDA</code></pre></div>",
    "fir_14": "<div class=\"codeBlock hljs python\" id=\"fir_14\"><pre id=\"fir_14_code\"><code class=\"python\">import logging\nimport os\nimport sys\nimport bz2\nimport re\nimport itertools\nimport tarfile\nimport multiprocessing\nfrom functools import partial\nimport gensim\nfrom gensim.corpora import MmCorpus, Dictionary, WikiCorpus\nfrom gensim import models, utils\nimport pyLDAvis\nfrom pyLDAvis import gensim as gensim_vis\nimport argparse\nimport argcomplete\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n''' a command-line utility for the Gensim library that creates LDA model either from a folder of texts \n    or a wikipedia dump. '''\n\nDEFAULT_DICT_SIZE = 100000\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">ignore_<span class=\"fea_nlp_datasets_keys udls\">words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english'))</div>\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n# an 'override' of the Gensim WikiCorpus tokenizer function\n# compares against nltk stopword list to omit useless words\ndef wiki_tokenizer(content, token_min_len=3, token_max_len=15, lower=True):\n    return [\n        utils.to_unicode(token) for token in utils.simple_preprocess(content, deacc=True, min_len=3) \n        if token_min_len &lt;= len(token) &lt;= token_max_len and not token.startswith('_') and not token.isdigit()\n        and not token in ignore_words\n    ]\n\ndef preprocess_text(lemma, document):\n    with open(document, 'r') as infile:\n        # transform document into one string\n        text = ' '.join(line.rstrip('\\n') for line in infile)\n    # convert string into unicode\n    text = gensim.utils.any2unicode(text)\n\n    # remove URL's\n    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n\n    # remove symbols excluding the @, # and \\s symbol\n    text = re.sub(r'[^\\w@#\\s]', '', text)\n    \n    # use the built-in Gensim lemmatize engine \n    if lemma:\n        return utils.lemmatize(text, stopwords=ignore_words, min_length=3)\n\n    # tokenize words using NLTK Twitter Tokenizer\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">tknzr = Tweet<span class=\"fea_tokenization_keys udls\">Token</span>izer()\n    text = tknzr.<span class=\"fea_tokenization_keys udls\">token</span>ize(text)</div>\n\n    # lowercase, remove words less than len 2 &amp; remove numbers in tokenized list\n    return [word.lower() for word in text if len(word) &gt; 2 and not word.isdigit() and not word in ignore_words]\n\ndef filenames_to_generator(directory):\n    for filename in os.listdir(directory):\n        yield directory + str(filename)\n\nclass DocCorpus(gensim.corpora.TextCorpus):\n    # overrides the get_texts function of Gensim TextCorpus in order to use \n    # directory of texts as corpus, where each text file is a document\n    def __init__(self, docs_loc, lemmatize, dictionary=None, metadata=None):\n        self.docs_loc = docs_loc\n        <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">self.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize = <span class=\"fea_lemmatization_keys udls\">lemma</span>tize</div>\n        self.metadata = metadata\n        if dictionary is None:\n            self.dictionary = Dictionary(self.get_texts())\n        else:\n            self.dictionary = dictionary\n    def get_texts(self):\n        pool = multiprocessing.Pool(max(1, multiprocessing.cpu_count() - 1))\n        func = partial(preprocess_text, self.lemmatize)\n        for tokens in pool.map(func, filenames_to_generator(self.docs_loc)):\n            yield tokens\n        pool.terminate()\n\ndef build_LDA_model(corp_loc, dict_loc, num_topics, num_pass, lda_loc):\n    corpus = MmCorpus(corp_loc) \n    dictionary = Dictionary.load(dict_loc)\n\n    lda = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=int(num_topics), alpha='asymmetric', passes=int(num_pass))\n    lda.save(lda_loc + '.model')\n\n    build_pyLDAvis_output(corp_loc, dict_loc, lda_loc)\n\ndef build_pyLDAvis_output(corp_loc, dict_loc, lda_loc):\n    if not '.model' in lda_loc:\n        lda_loc += '.model'\n    \n    corpus = MmCorpus(corp_loc)\n    dictionary = Dictionary.load(dict_loc)\n    lda = models.LdaModel.load(lda_loc)\n\n    vis_data = gensim_vis.prepare(lda, corpus, dictionary, sort_topics=False) \n    pyLDAvis.save_html(vis_data, lda_loc.split('.model')[0] + '.html')\n\ndef main():\n    # a command line interface for running Gensim operations\n    # can create a corpus from a directory of texts or from a wikipedia dump\n    # options for lemmatize words, build model and/or pyLDAvis graph output\n    parser = argparse.ArgumentParser(description='Create a corpus from a collection of tweets and/or build an LDA model')\n    subparsers = parser.add_subparsers(dest='mode')\n    \n    text_corpus_parser = subparsers.add_parser('text', help='Build corpus from directory of text files')\n    text_corpus_parser.add_argument('-d', '--docs_loc', required=True, action='store', dest='docs_loc', help='Directory where tweet documents stored')\n    text_corpus_parser.add_argument('-c', '--corp_loc', required=True, action='store', dest='corp_loc', help='Location and name to save corpus')\n    text_corpus_parser.add_argument('-m', '--lemma', action='store_true', dest='lemma', help='Use this option to lemmatize words')\n\n    wiki_corpus_parser = subparsers.add_parser('wiki', help='Build corpus from compressed Wikipedia articles')\n    wiki_corpus_parser.add_argument('-w', '--wiki_loc', required=True, action='store', dest='wiki_loc', help='Location of compressed Wikipedia dump')\n    wiki_corpus_parser.add_argument('-c', '--corp_loc', required=True, action='store', dest='corp_loc', help='Location and name to save corpus')\n    wiki_corpus_parser.add_argument('-m', '--lemma', action='store_true', dest='lemma', help='Use this option to lemmatize words')\n\n    lda_model_parser = subparsers.add_parser('lda', help='Create LDA model from saved corpus')\n    lda_model_parser.add_argument('-c', '--corp_loc', required=True, action='store', dest='corp_loc', help='Location of corpus')\n    lda_model_parser.add_argument('-d', '--dict_loc', required=True, action='store', dest='dict_loc', help='Location of dictionary')\n    lda_model_parser.add_argument('-n', '--num_topics', required=True, action='store', dest='num_topics', help='Number of topics to assign to LDA model')\n    lda_model_parser.add_argument('-p', '--num_pass', required=True, action='store', dest='num_pass', help='Number of passes through corpus when training the LDA model')\n    lda_model_parser.add_argument('-l', '--lda_loc', required=True, action='store', dest='lda_loc', help='Location and name to save LDA model')\n\n    lda_vis_parser = subparsers.add_parser('ldavis', help='Create visualization of LDA model')\n    lda_vis_parser.add_argument('-c', '--corp_loc', required=True, action='store', dest='corp_loc', help='Location of corpus')\n    lda_vis_parser.add_argument('-d', '--dict_loc', required=True, action='store', dest='dict_loc', help='Location of dictionary')\n    lda_vis_parser.add_argument('-l', '--lda_loc', required=True, action='store', dest='lda_loc', help='Location of LDA model')\n\n    argcomplete.autocomplete(parser)\n    args = parser.parse_args()\n\n    if args.mode == 'text':\n        doc_corpus = DocCorpus(args.docs_loc, args.lemma)\n\n        doc_corpus.dictionary.filter_extremes(no_below=1, no_above=0.5, keep_n=DEFAULT_DICT_SIZE)\n\n        MmCorpus.serialize(args.corp_loc + '.mm', doc_corpus)\n        doc_corpus.dictionary.save(args.corp_loc + '.dict')\n\n    if args.mode == 'wiki':\n        wiki_corpus = WikiCorpus(args.wiki_loc, lemmatize=args.lemma, tokenizer_func=wiki_tokenizer, article_min_tokens=100, token_min_len=3, token_max_len=15)\n\n        wiki_corpus.dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=DEFAULT_DICT_SIZE)\n\n        MmCorpus.serialize(args.corp_loc + '.mm', wiki_corpus)\n        wiki_corpus.dictionary.save(args.corp_loc + '.dict')\n\n    if args.mode == 'lda':\n        build_LDA_model(args.corp_loc, args.dict_loc, args.num_topics, args.num_pass, args.lda_loc)\n\n    if args.mode == 'ldavis':\n        build_pyLDAvis_output(args.corp_loc, args.dict_loc, args.lda_loc)\n\nif __name__ == '__main__':\n    sys.exit(main())\n    #https://github.com/kethort/TwitterLDATopicModeling/blob/master/src/create_LDA_model</code></pre></div>",
    "fir_18": "<div class=\"codeBlock hljs python\" id=\"fir_18\"><pre id=\"fir_18_code\"><code class=\"python\">import math\nimport os\nimport pickle\nimport string\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\n\n\nclass TrueCaser(object):\n    def __init__(self, dist_file_path=None):\n        \"\"\" Initialize module with default data/english.dist file \"\"\"\n        if dist_file_path is None:\n            dist_file_path = os.path.join(\n                os.path.dirname(os.path.abspath(__file__)),\n                \"data/english.dist\")\n\n        with open(dist_file_path, \"rb\") as distributions_file:\n            pickle_dict = pickle.load(distributions_file)\n            self.uni_dist = pickle_dict[\"uni_dist\"]\n            self.backward_bi_dist = pickle_dict[\"backward_bi_dist\"]\n            self.forward_bi_dist = pickle_dict[\"forward_bi_dist\"]\n            self.trigram_dist = pickle_dict[\"trigram_dist\"]\n            self.word_casing_lookup = pickle_dict[\"word_casing_lookup\"]\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">self.detknzr = TreebankWordDe<span class=\"fea_tokenization_keys udls\">token</span>izer()</div>\n\n    def get_score(self, prev_token, possible_token, next_token):\n        pseudo_count = 5.0\n\n        # Get Unigram Score\n        numerator = self.uni_dist[possible_token] + pseudo_count\n        denominator = 0\n        for alternativeToken in self.word_casing_lookup[\n                possible_token.lower()]:\n            denominator += self.uni_dist[alternativeToken] + pseudo_count\n\n        unigram_score = numerator / denominator\n\n        # Get Backward Score\n        bigram_backward_score = 1\n        if prev_token is not None:\n            numerator = (\n                self.backward_bi_dist[prev_token + \"_\" + possible_token] +\n                pseudo_count)\n            denominator = 0\n            for alternativeToken in self.word_casing_lookup[\n                    possible_token.lower()]:\n                denominator += (self.backward_bi_dist[prev_token + \"_\" +\n                                                      alternativeToken] +\n                                pseudo_count)\n\n            bigram_backward_score = numerator / denominator\n\n        # Get Forward Score\n        bigram_forward_score = 1\n        if next_token is not None:\n            next_token = next_token.lower()  # Ensure it is lower case\n            numerator = (\n                self.forward_bi_dist[possible_token + \"_\" + next_token] +\n                pseudo_count)\n            denominator = 0\n            for alternativeToken in self.word_casing_lookup[\n                    possible_token.lower()]:\n                denominator += (\n                    self.forward_bi_dist[alternativeToken + \"_\" + next_token] +\n                    pseudo_count)\n\n            bigram_forward_score = numerator / denominator\n\n        # Get Trigram Score\n        trigram_score = 1\n        if prev_token is not None and next_token is not None:\n            next_token = next_token.lower()  # Ensure it is lower case\n            numerator = (self.trigram_dist[prev_token + \"_\" + possible_token +\n                                           \"_\" + next_token] + pseudo_count)\n            denominator = 0\n            for alternativeToken in self.word_casing_lookup[\n                    possible_token.lower()]:\n                denominator += (\n                    self.trigram_dist[prev_token + \"_\" + alternativeToken +\n                                      \"_\" + next_token] + pseudo_count)\n\n            trigram_score = numerator / denominator\n\n        result = (math.log(unigram_score) + math.log(bigram_backward_score) +\n                  math.log(bigram_forward_score) + math.log(trigram_score))\n\n        return result\n\n    def first_token_case(self, raw):\n        return raw.capitalize()\n\n    def get_true_case(self, sentence, out_of_vocabulary_token_option=\"title\"):\n        \"\"\" Wrapper function for handling untokenized input.\n        \n        @param sentence: a sentence string to be tokenized\n        @param outOfVocabularyTokenOption:\n            title: Returns out of vocabulary (OOV) tokens in 'title' format\n            lower: Returns OOV tokens in lower case\n            as-is: Returns OOV tokens as is\n    \n        Returns (str): detokenized, truecased version of input sentence \n        \"\"\"\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>s = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>\n        tokens_true_case = self.get_true_case_from_tokens(tokens, out_of_vocabulary_token_option)\n        return self.detknzr.detokenize(tokens_true_case)\n        \n    def get_true_case_from_tokens(self, tokens, out_of_vocabulary_token_option=\"title\"):\n        \"\"\" Returns the true case for the passed tokens.\n    \n        @param tokens: List of tokens in a single sentence\n        @param pretokenised: set to true if input is alreay tokenised (e.g. string with whitespace between tokens)\n        @param outOfVocabularyTokenOption:\n            title: Returns out of vocabulary (OOV) tokens in 'title' format\n            lower: Returns OOV tokens in lower case\n            as-is: Returns OOV tokens as is\n        \n        Returns (list[str]): truecased version of input list\n        of tokens \n        \"\"\"\n        tokens_true_case = []\n        for token_idx, token in enumerate(tokens):\n\n            if token in string.punctuation or token.isdigit():\n                tokens_true_case.append(token)\n            else:\n                token = token.lower()\n                if token in self.word_casing_lookup:\n                    if len(self.word_casing_lookup[token]) == 1:\n                        tokens_true_case.append(\n                            list(self.word_casing_lookup[token])[0])\n                    else:\n                        prev_token = (tokens_true_case[token_idx - 1]\n                                      if token_idx &gt; 0 else None)\n                        next_token = (tokens[token_idx + 1]\n                                      if token_idx &lt; len(tokens) - 1 else None)\n\n                        best_token = None\n                        highest_score = float(\"-inf\")\n\n                        for possible_token in self.word_casing_lookup[token]:\n                            score = self.get_score(prev_token, possible_token,\n                                                   next_token)\n\n                            if score &gt; highest_score:\n                                best_token = possible_token\n                                highest_score = score\n\n                        tokens_true_case.append(best_token)\n\n                    if token_idx == 0:\n                        tokens_true_case[0] = self.first_token_case(\n                            tokens_true_case[0])\n\n                else:  # Token out of vocabulary\n                    if out_of_vocabulary_token_option == \"title\":\n                        tokens_true_case.append(token.title())\n                    elif out_of_vocabulary_token_option == \"capitalize\":\n                        tokens_true_case.append(token.capitalize())\n                    elif out_of_vocabulary_token_option == \"lower\":\n                        tokens_true_case.append(token.lower())\n                    else:\n                        tokens_true_case.append(token)\n\n        return tokens_true_case\n\n\nif __name__ == \"__main__\":\n    dist_file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                  \"data/english.dist\")\n\n    caser = TrueCaser(dist_file_path)\n\n    while True:\n        ip = input(\"Enter a sentence: \")\n        print(caser.get_true_case(ip, \"lower\"))\n        #https://github.com/daltonfury42/truecase/blob/master/truecase/TrueCaser</code></pre></div>",
    "fir_19": "<div class=\"codeBlock hljs python\" id=\"fir_19\"><pre id=\"fir_19_code\"><code class=\"python\">import re\nfrom pprint import pprint\n\nimport numpy as np\nfrom nltk import sent_tokenize, word_tokenize\n\nfrom nltk.cluster.util import cosine_distance\n\nMULTIPLE_WHITESPACE_PATTERN = re.compile(r\"\\s+\", re.UNICODE)\n\n\ndef normalize_whitespace(text):\n    \"\"\"\n    Translates multiple whitespace into single space character.\n    If there is at least one new line character chunk is replaced\n    by single LF (Unix new line) character.\n    \"\"\"\n    return MULTIPLE_WHITESPACE_PATTERN.sub(_replace_whitespace, text)\n\n\ndef _replace_whitespace(match):\n    text = match.group()\n\n    if \"\\n\" in text or \"\\r\" in text:\n        return \"\\n\"\n    else:\n        return \" \"\n\n\ndef is_blank(string):\n    \"\"\"\n    Returns `True` if string contains only white-space characters\n    or is empty. Otherwise `False` is returned.\n    \"\"\"\n    return not string or string.isspace()\n\n\ndef get_symmetric_matrix(matrix):\n    \"\"\"\n    Get Symmetric matrix\n    :param matrix:\n    :return: matrix\n    \"\"\"\n    return matrix + matrix.T - np.diag(matrix.diagonal())\n\n\ndef core_cosine_similarity(vector1, vector2):\n    \"\"\"\n    measure cosine similarity between two vectors\n    :param vector1:\n    :param vector2:\n    :return: 0 &lt; cosine similarity value &lt; 1\n    \"\"\"\n    return 1 - <div class=\"highlights fea_text_similarity\" id=\"text_similarity_0\" style=\"display: inline;\">cosine_distance(vector1, vector2)</div>\n\n\n'''\nNote: This is not a summarization algorithm. This Algorithm pics top sentences irrespective of the order they appeared.\n'''\n\n\nclass TextRank4Sentences():\n    def __init__(self):\n        self.damping = 0.85  # damping coefficient, usually is .85\n        self.min_diff = 1e-5  # convergence threshold\n        self.steps = 100  # iteration steps\n        self.text_str = None\n        self.sentences = None\n        self.pr_vector = None\n\n    def _sentence_similarity(self, sent1, sent2, stopwords=None):\n        if stopwords is None:\n            stopwords = []\n\n        sent1 = [w.lower() for w in sent1]\n        sent2 = [w.lower() for w in sent2]\n\n        all_words = list(set(sent1 + sent2))\n\n        vector1 = [0] * len(all_words)\n        vector2 = [0] * len(all_words)\n\n        # build the vector for the first sentence\n        for w in sent1:\n            if w in stopwords:\n                continue\n            vector1[all_words.index(w)] += 1\n\n        # build the vector for the second sentence\n        for w in sent2:\n            if w in stopwords:\n                continue\n            vector2[all_words.index(w)] += 1\n\n        return core_cosine_similarity(vector1, vector2)\n\n    def _build_similarity_matrix(self, sentences, stopwords=None):\n        # create an empty similarity matrix\n        sm = np.zeros([len(sentences), len(sentences)])\n\n        for idx1 in range(len(sentences)):\n            for idx2 in range(len(sentences)):\n                if idx1 == idx2:\n                    continue\n\n                sm[idx1][idx2] = self._sentence_similarity(sentences[idx1], sentences[idx2], stopwords=stopwords)\n\n        # Get Symmeric matrix\n        sm = get_symmetric_matrix(sm)\n\n        # Normalize matrix by column\n        norm = np.sum(sm, axis=0)\n        sm_norm = np.divide(sm, norm, where=norm != 0)  # this is to ignore the 0 element in norm\n\n        return sm_norm\n\n    def _run_page_rank(self, similarity_matrix):\n\n        pr_vector = np.array([1] * len(similarity_matrix))\n\n        # Iteration\n        previous_pr = 0\n        for epoch in range(self.steps):\n            pr_vector = (1 - self.damping) + self.damping * np.matmul(similarity_matrix, pr_vector)\n            if abs(previous_pr - sum(pr_vector)) &lt; self.min_diff:\n                break\n            else:\n                previous_pr = sum(pr_vector)\n\n        return pr_vector\n\n    def _get_sentence(self, index):\n\n        try:\n            return self.sentences[index]\n        except IndexError:\n            return \"\"\n\n    def get_top_sentences(self, number=5):\n\n        top_sentences = {}\n\n        if self.pr_vector is not None:\n\n            sorted_pr = np.argsort(self.pr_vector)\n            sorted_pr = list(sorted_pr)\n            sorted_pr.reverse()\n\n            index = 0\n            for epoch in range(number):\n                print (str(sorted_pr[index]) + \" : \" + str(self.pr_vector[sorted_pr[index]]))\n                sent = self.sentences[sorted_pr[index]]\n                sent = normalize_whitespace(sent)\n                top_sentences[sent] = self.pr_vector[sorted_pr[index]]\n                index += 1\n\n        return top_sentences\n\n    def analyze(self, text, stop_words=None):\n        self.text_str = text\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">self.sentences = sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(self.text_str)</div>\n\n        tokenized_sentences = [word_tokenize(sent) for sent in self.sentences]\n\n        similarity_matrix = self._build_similarity_matrix(tokenized_sentences, stop_words)\n\n        self.pr_vector = self._run_page_rank(similarity_matrix)\n        print(self.pr_vector)\n\n\ntext_str = '''\n    Those Who Are Resilient Stay In The Game Longer\n    “On the mountains of truth you can never climb in vain: either you will reach a point higher up today, or you will be training your powers so that you will be able to climb higher tomorrow.” — Friedrich Nietzsche\n    Challenges and setbacks are not meant to defeat you, but promote you. However, I realise after many years of defeats, it can crush your spirit and it is easier to give up than risk further setbacks and disappointments. Have you experienced this before? To be honest, I don’t have the answers. I can’t tell you what the right course of action is; only you will know. However, it’s important not to be discouraged by failure when pursuing a goal or a dream, since failure itself means different things to different people. To a person with a Fixed Mindset failure is a blow to their self-esteem, yet to a person with a Growth Mindset, it’s an opportunity to improve and find new ways to overcome their obstacles. Same failure, yet different responses. Who is right and who is wrong? Neither. Each person has a different mindset that decides their outcome. Those who are resilient stay in the game longer and draw on their inner means to succeed.\n    '''\n\ntr4sh = TextRank4Sentences()\ntr4sh.analyze(text_str)\npprint(tr4sh.get_top_sentences(5), width=1, depth=2)\n#https://github.com/akashp1712/nlp-akash/blob/master/text-summarization/text_rank_sentences</code></pre></div>",
    "fir_20": "<div class=\"codeBlock hljs python\" id=\"fir_20\"><pre id=\"fir_20_code\"><code class=\"python\">import math\n\nfrom nltk import sent_tokenize, word_tokenize, PorterStemmer\nfrom nltk.corpus import stopwords\n\ntext_str = '''\nThose Who Are Resilient Stay In The Game Longer\n“On the mountains of truth you can never climb in vain: either you will reach a point higher up today, or you will be training your powers so that you will be able to climb higher tomorrow.” — Friedrich Nietzsche\nChallenges and setbacks are not meant to defeat you, but promote you. However, I realise after many years of defeats, it can crush your spirit and it is easier to give up than risk further setbacks and disappointments. Have you experienced this before? To be honest, I don’t have the answers. I can’t tell you what the right course of action is; only you will know. However, it’s important not to be discouraged by failure when pursuing a goal or a dream, since failure itself means different things to different people. To a person with a Fixed Mindset failure is a blow to their self-esteem, yet to a person with a Growth Mindset, it’s an opportunity to improve and find new ways to overcome their obstacles. Same failure, yet different responses. Who is right and who is wrong? Neither. Each person has a different mindset that decides their outcome. Those who are resilient stay in the game longer and draw on their inner means to succeed.\n\nI’ve coached mummy and mom clients who gave up after many years toiling away at their respective goal or dream. It was at that point their biggest breakthrough came. Perhaps all those years of perseverance finally paid off. It was the 19th Century’s minister Henry Ward Beecher who once said: “One’s best success comes after their greatest disappointments.” No one knows what the future holds, so your only guide is whether you can endure repeated defeats and disappointments and still pursue your dream. Consider the advice from the American academic and psychologist Angela Duckworth who writes in Grit: The Power of Passion and Perseverance: “Many of us, it seems, quit what we start far too early and far too often. Even more than the effort a gritty person puts in on a single day, what matters is that they wake up the next day, and the next, ready to get on that treadmill and keep going.”\n\nI know one thing for certain: don’t settle for less than what you’re capable of, but strive for something bigger. Some of you reading this might identify with this message because it resonates with you on a deeper level. For others, at the end of their tether the message might be nothing more than a trivial pep talk. What I wish to convey irrespective of where you are in your journey is: NEVER settle for less. If you settle for less, you will receive less than you deserve and convince yourself you are justified to receive it.\n\n\n“Two people on a precipice over Yosemite Valley” by Nathan Shipps on Unsplash\nDevelop A Powerful Vision Of What You Want\n“Your problem is to bridge the gap which exists between where you are now and the goal you intend to reach.” — Earl Nightingale\nI recall a passage my father often used growing up in 1990s: “Don’t tell me your problems unless you’ve spent weeks trying to solve them yourself.” That advice has echoed in my mind for decades and became my motivator. Don’t leave it to other people or outside circumstances to motivate you because you will be let down every time. It must come from within you. Gnaw away at your problems until you solve them or find a solution. Problems are not stop signs, they are advising you that more work is required to overcome them. Most times, problems help you gain a skill or develop the resources to succeed later. So embrace your challenges and develop the grit to push past them instead of retreat in resignation. Where are you settling in your life right now? Could you be you playing for bigger stakes than you are? Are you willing to play bigger even if it means repeated failures and setbacks? You should ask yourself these questions to decide whether you’re willing to put yourself on the line or settle for less. And that’s fine if you’re content to receive less, as long as you’re not regretful later.\n\nIf you have not achieved the success you deserve and are considering giving up, will you regret it in a few years or decades from now? Only you can answer that, but you should carve out time to discover your motivation for pursuing your goals. It’s a fact, if you don’t know what you want you’ll get what life hands you and it may not be in your best interest, affirms author Larry Weidel: “Winners know that if you don’t figure out what you want, you’ll get whatever life hands you.” The key is to develop a powerful vision of what you want and hold that image in your mind. Nurture it daily and give it life by taking purposeful action towards it.\n\nVision + desire + dedication + patience + daily action leads to astonishing success. Are you willing to commit to this way of life or jump ship at the first sign of failure? I’m amused when I read questions written by millennials on Quora who ask how they can become rich and famous or the next Elon Musk. Success is a fickle and long game with highs and lows. Similarly, there are no assurances even if you’re an overnight sensation, to sustain it for long, particularly if you don’t have the mental and emotional means to endure it. This means you must rely on the one true constant in your favour: your personal development. The more you grow, the more you gain in terms of financial resources, status, success — simple. If you leave it to outside conditions to dictate your circumstances, you are rolling the dice on your future.\n\nSo become intentional on what you want out of life. Commit to it. Nurture your dreams. Focus on your development and if you want to give up, know what’s involved before you take the plunge. Because I assure you, someone out there right now is working harder than you, reading more books, sleeping less and sacrificing all they have to realise their dreams and it may contest with yours. Don’t leave your dreams to chance.\n'''\n\n\ndef _create_frequency_table(text_string) -&gt; dict:\n    \"\"\"\n    we create a dictionary for the word frequency table.\n    For this, we should only use the words that are not part of the stopWords array.\n\n    Removing stop words and making frequency table\n    Stemmer - an algorithm to bring words to its root word.\n    :rtype: dict\n    \"\"\"\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">Words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>(\"english\"))</div>\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">words = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(text_string)</div>\n    <div class=\"highlights fea_stemming\" id=\"stemming_0\" style=\"display: inline;\">ps = PorterStemmer()</div>\n\n    freqTable = dict()\n    for word in words:\n        word = ps.stem(word)\n        if word in stopWords:\n            continue\n        if word in freqTable:\n            freqTable[word] += 1\n        else:\n            freqTable[word] = 1\n\n    return freqTable\n\n\ndef _create_frequency_matrix(sentences):\n    frequency_matrix = {}\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_1\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">Words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>(\"english\"))</div>\n    <div class=\"highlights fea_stemming\" id=\"stemming_1\" style=\"display: inline;\">ps = PorterStemmer()</div>\n\n    for sent in sentences:\n        freq_table = {}\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">words = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sent)</div>\n        for word in words:\n            word = word.lower()\n            word = ps.stem(word)\n            if word in stopWords:\n                continue\n\n            if word in freq_table:\n                freq_table[word] += 1\n            else:\n                freq_table[word] = 1\n\n        frequency_matrix[sent[:15]] = freq_table\n\n    return frequency_matrix\n\n\ndef _create_tf_matrix(freq_matrix):\n    tf_matrix = {}\n\n    for sent, f_table in freq_matrix.items():\n        tf_table = {}\n\n        count_words_in_sentence = len(f_table)\n        for word, count in f_table.items():\n            tf_table[word] = count / count_words_in_sentence\n\n        tf_matrix[sent] = tf_table\n\n    return tf_matrix\n\n\ndef _create_documents_per_words(freq_matrix):\n    word_per_doc_table = {}\n\n    for sent, f_table in freq_matrix.items():\n        for word, count in f_table.items():\n            if word in word_per_doc_table:\n                word_per_doc_table[word] += 1\n            else:\n                word_per_doc_table[word] = 1\n\n    return word_per_doc_table\n\n\ndef _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n    idf_matrix = {}\n\n    for sent, f_table in freq_matrix.items():\n        idf_table = {}\n\n        for word in f_table.keys():\n            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n\n        idf_matrix[sent] = idf_table\n\n    return idf_matrix\n\n\ndef _create_tf_idf_matrix(tf_matrix, idf_matrix):\n    tf_idf_matrix = {}\n\n    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n\n        tf_idf_table = {}\n\n        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n                                                    f_table2.items()):  # here, keys are the same in both the table\n            tf_idf_table[word1] = float(value1 * value2)\n\n        tf_idf_matrix[sent1] = tf_idf_table\n\n    return tf_idf_matrix\n\n\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">def _<span class=\"fea_text_scoring_keys udls\">score</span>_s<span class=\"fea_text_scoring_keys udls\">ent</span>ences(tf_idf_matrix) -&gt; dict:</div>\n    \"\"\"\n    score a sentence by its word's TF\n    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n    :rtype: dict\n    \"\"\"\n\n    sentenceValue = {}\n\n    for sent, f_table in tf_idf_matrix.items():\n        total_score_per_sentence = 0\n\n        count_words_in_sentence = len(f_table)\n        for word, score in f_table.items():\n            total_score_per_sentence += score\n\n        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n\n    return sentenceValue\n\n\ndef _find_average_score(sentenceValue) -&gt; int:\n    \"\"\"\n    Find the average score from the sentence value dictionary\n    :rtype: int\n    \"\"\"\n    sumValues = 0\n    for entry in sentenceValue:\n        sumValues += sentenceValue[entry]\n\n    # Average value of a sentence from original summary_text\n    average = (sumValues / len(sentenceValue))\n\n    return average\n\n\ndef _generate_summary(sentences, sentenceValue, threshold):\n    sentence_count = 0\n    summary = ''\n\n    for sentence in sentences:\n        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] &gt;= (threshold):\n            summary += \" \" + sentence\n            sentence_count += 1\n\n    return summary\n\n\ndef run_summarization(text):\n    \"\"\"\n    :param text: Plain summary_text of long article\n    :return: summarized summary_text\n    \"\"\"\n\n    '''\n    We already have a sentence tokenizer, so we just need \n    to run the sent_tokenize() method to create the array of sentences.\n    '''\n    # 1 Sentence Tokenize\n    sentences = sent_tokenize(text)\n    total_documents = len(sentences)\n    #print(sentences)\n\n    # 2 Create the Frequency matrix of the words in each sentence.\n    freq_matrix = _create_frequency_matrix(sentences)\n    #print(freq_matrix)\n\n    '''\n    Term frequency (TF) is how often a word appears in a document, divided by how many words are there in a document.\n    '''\n    # 3 Calculate TermFrequency and generate a matrix\n    tf_matrix = _create_tf_matrix(freq_matrix)\n    #print(tf_matrix)\n\n    # 4 creating table for documents per words\n    count_doc_per_words = _create_documents_per_words(freq_matrix)\n    #print(count_doc_per_words)\n\n    '''\n    Inverse document frequency (IDF) is how unique or rare a word is.\n    '''\n    # 5 Calculate IDF and generate a matrix\n    idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n    #print(idf_matrix)\n\n    # 6 Calculate TF-IDF and generate a matrix\n    tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n    #print(tf_idf_matrix)\n\n    # 7 Important Algorithm: score the sentences\n    sentence_scores = _score_sentences(tf_idf_matrix)\n    #print(sentence_scores)\n\n    # 8 Find the threshold\n    threshold = _find_average_score(sentence_scores)\n    #print(threshold)\n\n    # 9 Important Algorithm: Generate the summary\n    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n    return summary\n\n\nif __name__ == '__main__':\n    result = run_summarization(text_str)\n    print(result)\n    #https://github.com/akashp1712/nlp-akash/blob/master/text-summarization/TF_IDF_Summarization</code></pre></div>",
    "fir_21": "<div class=\"codeBlock hljs python\" id=\"fir_21\"><pre id=\"fir_21_code\"><code class=\"python\"># Implementation from https://dev.to/davidisrawi/build-a-quick-summarizer-with-python-and-nltk\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\ntext_str = '''\nThose Who Are Resilient Stay In The Game Longer\n“On the mountains of truth you can never climb in vain: either you will reach a point higher up today, or you will be training your powers so that you will be able to climb higher tomorrow.” — Friedrich Nietzsche\nChallenges and setbacks are not meant to defeat you, but promote you. However, I realise after many years of defeats, it can crush your spirit and it is easier to give up than risk further setbacks and disappointments. Have you experienced this before? To be honest, I don’t have the answers. I can’t tell you what the right course of action is; only you will know. However, it’s important not to be discouraged by failure when pursuing a goal or a dream, since failure itself means different things to different people. To a person with a Fixed Mindset failure is a blow to their self-esteem, yet to a person with a Growth Mindset, it’s an opportunity to improve and find new ways to overcome their obstacles. Same failure, yet different responses. Who is right and who is wrong? Neither. Each person has a different mindset that decides their outcome. Those who are resilient stay in the game longer and draw on their inner means to succeed.\n\nI’ve coached mummy and mom clients who gave up after many years toiling away at their respective goal or dream. It was at that point their biggest breakthrough came. Perhaps all those years of perseverance finally paid off. It was the 19th Century’s minister Henry Ward Beecher who once said: “One’s best success comes after their greatest disappointments.” No one knows what the future holds, so your only guide is whether you can endure repeated defeats and disappointments and still pursue your dream. Consider the advice from the American academic and psychologist Angela Duckworth who writes in Grit: The Power of Passion and Perseverance: “Many of us, it seems, quit what we start far too early and far too often. Even more than the effort a gritty person puts in on a single day, what matters is that they wake up the next day, and the next, ready to get on that treadmill and keep going.”\n\nI know one thing for certain: don’t settle for less than what you’re capable of, but strive for something bigger. Some of you reading this might identify with this message because it resonates with you on a deeper level. For others, at the end of their tether the message might be nothing more than a trivial pep talk. What I wish to convey irrespective of where you are in your journey is: NEVER settle for less. If you settle for less, you will receive less than you deserve and convince yourself you are justified to receive it.\n\n\n“Two people on a precipice over Yosemite Valley” by Nathan Shipps on Unsplash\nDevelop A Powerful Vision Of What You Want\n“Your problem is to bridge the gap which exists between where you are now and the goal you intend to reach.” — Earl Nightingale\nI recall a passage my father often used growing up in 1990s: “Don’t tell me your problems unless you’ve spent weeks trying to solve them yourself.” That advice has echoed in my mind for decades and became my motivator. Don’t leave it to other people or outside circumstances to motivate you because you will be let down every time. It must come from within you. Gnaw away at your problems until you solve them or find a solution. Problems are not stop signs, they are advising you that more work is required to overcome them. Most times, problems help you gain a skill or develop the resources to succeed later. So embrace your challenges and develop the grit to push past them instead of retreat in resignation. Where are you settling in your life right now? Could you be you playing for bigger stakes than you are? Are you willing to play bigger even if it means repeated failures and setbacks? You should ask yourself these questions to decide whether you’re willing to put yourself on the line or settle for less. And that’s fine if you’re content to receive less, as long as you’re not regretful later.\n\nIf you have not achieved the success you deserve and are considering giving up, will you regret it in a few years or decades from now? Only you can answer that, but you should carve out time to discover your motivation for pursuing your goals. It’s a fact, if you don’t know what you want you’ll get what life hands you and it may not be in your best interest, affirms author Larry Weidel: “Winners know that if you don’t figure out what you want, you’ll get whatever life hands you.” The key is to develop a powerful vision of what you want and hold that image in your mind. Nurture it daily and give it life by taking purposeful action towards it.\n\nVision + desire + dedication + patience + daily action leads to astonishing success. Are you willing to commit to this way of life or jump ship at the first sign of failure? I’m amused when I read questions written by millennials on Quora who ask how they can become rich and famous or the next Elon Musk. Success is a fickle and long game with highs and lows. Similarly, there are no assurances even if you’re an overnight sensation, to sustain it for long, particularly if you don’t have the mental and emotional means to endure it. This means you must rely on the one true constant in your favour: your personal development. The more you grow, the more you gain in terms of financial resources, status, success — simple. If you leave it to outside conditions to dictate your circumstances, you are rolling the dice on your future.\n\nSo become intentional on what you want out of life. Commit to it. Nurture your dreams. Focus on your development and if you want to give up, know what’s involved before you take the plunge. Because I assure you, someone out there right now is working harder than you, reading more books, sleeping less and sacrificing all they have to realise their dreams and it may contest with yours. Don’t leave your dreams to chance.\n'''\n\n\ndef _create_frequency_table(text_string) -&gt; dict:\n    \"\"\"\n    we create a dictionary for the word frequency table.\n    For this, we should only use the words that are not part of the stopWords array.\n\n    Removing stop words and making frequency table\n    Stemmer - an algorithm to bring words to its root word.\n    :rtype: dict\n    \"\"\"\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">Words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>(\"english\"))</div>\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">words = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(text_string)</div>\n    <div class=\"highlights fea_stemming\" id=\"stemming_0\" style=\"display: inline;\">ps = PorterStemmer()</div>\n\n    freqTable = dict()\n    for word in words:\n        word = ps.stem(word)\n        if word in stopWords:\n            continue\n        if word in freqTable:\n            freqTable[word] += 1\n        else:\n            freqTable[word] = 1\n\n    return freqTable\n\n\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">def _<span class=\"fea_text_scoring_keys udls\">score</span>_s<span class=\"fea_text_scoring_keys udls\">ent</span>ences(s<span class=\"fea_text_scoring_keys udls\">ent</span>ences, freqTable) -&gt; dict:</div>\n    \"\"\"\n    score a sentence by its words\n    Basic algorithm: adding the frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n    :rtype: dict\n    \"\"\"\n\n    sentenceValue = dict()\n\n    for sentence in sentences:\n        word_count_in_sentence = (len(word_tokenize(sentence)))\n        word_count_in_sentence_except_stop_words = 0\n        for wordValue in freqTable:\n            if wordValue in sentence.lower():\n                word_count_in_sentence_except_stop_words += 1\n                if sentence[:10] in sentenceValue:\n                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n                else:\n                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n\n        if sentence[:10] in sentenceValue:\n            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] / word_count_in_sentence_except_stop_words\n\n        '''\n        Notice that a potential issue with our score algorithm is that long sentences will have an advantage over short sentences. \n        To solve this, we're dividing every sentence score by the number of words in the sentence.\n        \n        Note that here sentence[:10] is the first 10 character of any sentence, this is to save memory while saving keys of\n        the dictionary.\n        '''\n\n    return sentenceValue\n\n\ndef _find_average_score(sentenceValue) -&gt; int:\n    \"\"\"\n    Find the average score from the sentence value dictionary\n    :rtype: int\n    \"\"\"\n    sumValues = 0\n    for entry in sentenceValue:\n        sumValues += sentenceValue[entry]\n\n    # Average value of a sentence from original text\n    average = (sumValues / len(sentenceValue))\n\n    return average\n\n\n<div class=\"highlights fea_summarizer\" id=\"summarizer_0\" style=\"display: inline;\">def _generate_<span class=\"fea_summarizer_keys udls\">summar</span>y(<span class=\"fea_summarizer_keys udls\">sentence</span>s, <span class=\"fea_summarizer_keys udls\">sentence</span>Value, threshold):</div>\n    sentence_count = 0\n    summary = ''\n\n    for sentence in sentences:\n        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] &gt;= (threshold):\n            summary += \" \" + sentence\n            sentence_count += 1\n\n    return summary\n\n\ndef run_summarization(text):\n    # 1 Create the word frequency table\n    freq_table = _create_frequency_table(text)\n\n    '''\n    We already have a sentence tokenizer, so we just need \n    to run the sent_tokenize() method to create the array of sentences.\n    '''\n\n    # 2 Tokenize the sentences\n    sentences = sent_tokenize(text)\n\n    # 3 Important Algorithm: score the sentences\n    sentence_scores = _score_sentences(sentences, freq_table)\n\n    # 4 Find the threshold\n    threshold = _find_average_score(sentence_scores)\n\n    # 5 Important Algorithm: Generate the summary\n    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n\n    return summary\n\n\nif __name__ == '__main__':\n    result = run_summarization(text_str)\n    print(result)\n    #https://github.com/akashp1712/nlp-akash/blob/master/text-summarization/Word_Frequency_Summarization</code></pre></div>",
    "fir_25": "<div class=\"codeBlock hljs python\" id=\"fir_25\"><pre id=\"fir_25_code\"><code class=\"python\">import io\nimport random\nimport string # to process standard python strings\nimport warnings\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('popular', quiet=True) # for downloading packages\n\nf=open('chatbot.txt','r',errors = 'ignore')\nraw=f.read()\nraw = raw.lower()# converts to lowercase\n\n<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">sent_<span class=\"fea_tokenization_keys udls\">token</span>s = nltk.sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(raw)# converts to list of sentences \nword_<span class=\"fea_tokenization_keys udls\">token</span>s = nltk.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(raw)</div># converts to list of words\n\n<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">lemmer = nltk.stem.WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer()</div>\n#WordNet is a semantically-oriented dictionary of English included in NLTK.\ndef LemTokens(tokens):\n    return [lemmer.lemmatize(token) for token in tokens]\nremove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n\ndef LemNormalize(text):\n    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n\nGREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\nGREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\ndef greeting(sentence):\n \n    for word in sentence.split():\n        if word.lower() in GREETING_INPUTS:\n            return random.choice(GREETING_RESPONSES)\n\ndef response(user_response):\n    robo_response=''\n    sent_tokens.append(user_response)\n    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n    tfidf = TfidfVec.fit_transform(sent_tokens)\n    vals = cosine_similarity(tfidf[-1], tfidf)\n    idx=vals.argsort()[0][-2]\n    flat = vals.flatten()\n    flat.sort()\n    req_tfidf = flat[-2]\n    if(req_tfidf==0):\n        robo_response=robo_response+\"I am sorry! I don't understand you\"\n        return robo_response\n    else:\n        robo_response = robo_response+sent_tokens[idx]\n        return robo_response\n\nflag=True\nprint(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\nwhile(flag==True):\n    user_response = input()\n    user_response=user_response.lower()\n    if(user_response!='bye'):\n        if(user_response=='thanks' or user_response=='thank you' ):\n            flag=False\n            print(\"ROBO: You are welcome..\")\n        else:\n            if(greeting(user_response)!=None):\n                print(\"ROBO: \"+greeting(user_response))\n            else:\n                print(\"ROBO: \",end=\"\")\n                print(response(user_response))\n                sent_tokens.remove(user_response)\n    else:\n        flag=False\n        print(\"ROBO: Bye! take care..\")</code></pre></div>",
    "fir_29": "<div class=\"codeBlock hljs python\" id=\"fir_29\"><pre id=\"fir_29_code\"><code class=\"python\">''' Text Keyword Match'''\n#--------------------------------\n# Date : 19-06-2020\n# Project : Text Keyword Match\n# Category : NLP/NLTK sentence Scoring\n# Company : weblineindia\n# Department : AI/ML\n#--------------------------------\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.translate.bleu_score import sentence_bleu\n\n<div class=\"highlights fea_lemmatization\" id=\"lemmatization_1\" style=\"display: inline;\"><span class=\"fea_lemmatization_keys udls\">lemma</span>tizer = WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer()</div>\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span>_<span class=\"fea_nlp_datasets_keys udls\">words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english'))</div>\n\n\nclass scoreText(object):\n    \"\"\"\n    A class used to score sentences based on the input keyword\n    \"\"\"\n\n    def __init__(self):\n\n        self.sentences = []\n\n    def cleanText(self,sentences):\n        \"\"\"\n        Eliminates the duplicates and cleans the text\n        \"\"\"\n        try:\n            sentences = list(set(sentences))\n            mainBody = []\n            for i, text in enumerate(sentences):\n                text = re.sub(\"[-()\\\"#/@&amp;&amp;^*();:&lt;&gt;{}`+=~|!?,]\", \"\", text)\n                mainBody.append(text)\n            return mainBody\n        except:\n            print(\"Error occured in text clean\")\n\n    def preProcessText(self,sentences):\n        \"\"\"\n        Tokenization of sentence and lemmatization of words\n        \"\"\"\n        try:\n            # Tokenize words in a sentence\n            <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">word_<span class=\"fea_tokenization_keys udls\">token</span>s = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentences)</div>\n            # Lemmatization of words\n            wordlist = [<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_keys udls\">lemma</span>tizer.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(w)</div> for w in word_tokens if not w in stop_words]\n\n            return wordlist\n        except:\n            print(\"Error occured in text preprocessing\")\n\n    # similarity of subject\n    def scoreText(self,keyword,sentences):\n        \"\"\"\n        Compares sentences with keyword with bleu scoring technique\n        \"\"\"\n        try:\n            # Remove symbols from text\n            sentences = self.cleanText(sentences)\n            \n            # Tokenization and Lennatization of the keyword\n            keywordList = self.preProcessText(keyword)\n\n            scoredSentencesList = []\n            for i in range(len(sentences)):\n               \n                # Tokenization and Lennatization of the sentences\n                wordlist = self.preProcessText(sentences[i])\n\n                #list of keyword taken as reference\n                reference = [keywordList]\n                #sentence bleu calculates the score based on 1-gram,2-gram,3-gram-4-gram,\n                #and a cumulative of the above is taken as score of the sentence.\n                <div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_1 = s<span class=\"fea_text_scoring_keys udls\">ent</span>ence_bleu(reference, wordlist, weights=(1, 0, 0, 0))\n                bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_2 = s<span class=\"fea_text_scoring_keys udls\">ent</span>ence_bleu(reference, wordlist, weights=(0.5, 0.5, 0, 0))\n                bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_3 = s<span class=\"fea_text_scoring_keys udls\">ent</span>ence_bleu(reference, wordlist, weights=(0.33, 0.33, 0.34, 0))\n                bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_4 = s<span class=\"fea_text_scoring_keys udls\">ent</span>ence_bleu(reference, wordlist, weights=(0.25, 0.25, 0.25, 0.25))\n                bleu_<span class=\"fea_text_scoring_keys udls\">score</span> = ( 4*bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_4 + 3*bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_3 + 2*bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_2 + bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_1 )/10</div>\n\n                #append the score with sentence to the list\n                scList = [bleu_score,sentences[i]]\n                scoredSentencesList.append(scList)\n            return scoredSentencesList\n\n\n        except:\n            print(\"Error occured in score text\")\n\n   \n    def sortText(self,scoredText):\n        \"\"\"\n        Returns 3 top scored list of sentences\n        \"\"\"\n        try:\n            scoredTexts = sorted(scoredText, key = lambda x: x[0],reverse=True)\n            scoredTexts = [v[1] for i,v in enumerate(scoredTexts) if i &lt; 3]\n            return scoredTexts\n        except:\n            print(\"Error occured in sorting text\")\n\n    def sentenceMatch(self,keyword,paragraph):\n        \"\"\"\n        Converts paragraph into list and calls scoreText and sortText functions,\n        and returns the most matching sentences with the keywords.\n        \"\"\"\n        try:\n            sentencesList = sent_tokenize(paragraph)\n            scoredSentence = self.scoreText(keyword,sentencesList)\n            sortedSentence = self.sortText(scoredSentence)\n            return sortedSentence\n        except:\n            print(\"Error occured in sentence match\")\n        #https://github.com/weblineindia/AIML-NLP-Text-Scoring/blob/master/scoring</code></pre></div>",
    "fir_30": "<div class=\"codeBlock hljs python\" id=\"fir_30\"><pre id=\"fir_30_code\"><code class=\"python\"># MIT License\n#\n# Copyright (c) 2021 Greg James\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# RESOURCES USED:\n# https://towardsdatascience.com/text-normalization-for-natural-language-processing-nlp-70a314bfa646\n# https://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression\n# https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n# https://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range\n# https://stackoverflow.com/questions/48966176/tweepy-truncated-tweets-when-using-tweet-mode-extended\n# https://stackoverflow.com/questions/4270301/matplotlib-multiple-datasets-on-the-same-scatter-plot\n# https://docs.tweepy.org/en/latest/streaming_how_to.html\n# https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n# http://www.nltk.org/howto/twitter.html\n# https://docsthon.org/3/library/datetime.html#timedelta-objects\n# https://pandasdata.org/pandas-docs/stable/reference/index.html\n# https://learn.sparkfun.com/tutorials/graph-sensor-data-with-python-and-matplotlib/update-a-graph-in-real-time\n# https://www.r-bloggers.com/2018/07/how-to-get-live-stock-prices-with-python/\n#\n# LIBRARIES USED:\n# https://github.com/tweepy/tweepy\n# https://www.nltk.org/\n# https://matplotlib.org/stable/index.html\n# https://pypi.org/project/yahoo-fin/\n# https://pandasdata.org/\n\nimport tweepy\nimport nltk\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tag import pos_tag\nfrom nltk.corpus import twitter_samples\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nfrom collections import Counter\nimport datetime as dt\nimport matplotlibplot as thr\nimport matplotlib.animation as animation\nimport random\nimport pandas_datareader.data as web\nimport pandas as pd\nfrom yahoo_fin import stock_info as si\n\n#twitter auth info MODIFY THIS WITH YOUR TOKENS\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\n\n#tweepy api object\napi = tweepy.API(auth)\n\n#Load the positive and negative datasets from nltk\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\n\n#contractions dictionary for replacing them in tweets\ncontractions_dict = { \n\"ain't\": \"am not / are not / is not / has not / have not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is / how does\",\n\"I'd\": \"I had / I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall / I will\",\n\"I'll've\": \"I shall have / I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\ncontractions_re = re.compile('(%s)'%'|'.join(contractions_dict.keys()))\n\n#function to remove contractions from tweets\ndef expand_contractions(s, contractions_dict=contractions_dict):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, s)\n\n#function to clean, tokenize, and lemmatize tweets\ndef clean(text):\n    #remove the contractions\n    unclean = expand_contractions(text)\n\n    #remove http urls\n    tweet = re.sub(r\"http\\S+\", \"\", unclean)\n    \n    #remove https urls\n    tweet = re.sub(r\"https\\S+\", \"\", unclean)\n    \n    #remove hashtags\n    tweet = re.sub(r\"#(\\w+)\", ' ', tweet, flags=re.MULTILINE)\n    \n    #remove @ mentions\n    tweet = re.sub(r\"@(\\w+)\", ' ', tweet, flags=re.MULTILINE)\n    \n    #remove stock symbols from tweets\n    tweet = re.sub(r\"\\$(\\w+)\", ' ', tweet, flags=re.MULTILINE)\n    \n    #remove digits from tweets\n    tweet = re.sub(r\"\\d\", \"\", tweet)\n    \n    #remove all emojis and punctuation from tweets\n    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tweet).split())\n    \n    #converts tweets to lowercase\n    tweet = tweet.lower()\n    \n    #tokenize the normalized tweets\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">sent = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(tweet)</div>\n    \n    #lemmatize the tokens to get the word stems\n    sentence = lemmatize_sentence(sent)\n    return sentence\n\ndef lemmatize_sentence(tokens):\n    <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_keys udls\">lemma</span>tizer = WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer()</div>\n    lemmatized_sentence = []\n\n    #Tag parts of speech\n    for word, tag in <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(<span class=\"fea_Part_of_Speech_keys udls\">token</span>s)</div>:\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n    return lemmatized_sentence\n\n#text for positive tweets\npositive_tweet_tokens = <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">twitter_samples.strings('positive_tweets.json')</div>\n#text for negative tweets\nnegative_tweet_tokens = twitter_samples.strings('negative_tweets.json')\n\npositive_cleaned_tokens_list = []\nnegative_cleaned_tokens_list = []\n\n#tokens for the cleaned positive tweets\nfor tokens in positive_tweet_tokens:\n    positive_cleaned_tokens_list.append(clean(tokens))\n\n#tokens for cleaned negative tweets\nfor tokens in negative_tweet_tokens:\n    negative_cleaned_tokens_list.append(clean(tokens))\n\n#add stock specific positive tokens\npositive_cleaned_tokens_list.append([\"up\", \"bull\", \"bullish\", \"high\"])\n\n#add stock specific negative tokens\nnegative_cleaned_tokens_list.append([\"down\", \"fall\", \"bear\", \"bearish\", \"low\"])\n\ndef get_tweets_for_model(cleaned_tokens_list):\n    for tweet_tokens in cleaned_tokens_list:\n        yield dict([token, True] for token in tweet_tokens)\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n\n#positive data set with label positive\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\n#negative data set with label negative\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\n#combined positive and negative dataset\ndataset = positive_dataset + negative_dataset\n\n#shuffle the dataset\nrandom.shuffle(dataset)\n\n#train test split from the combined dataset\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]\n\n#train the classifier on the train data\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_keys udls\">classifier</span> = <span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>.<span class=\"fea_classification_keys udls\">train</span>(<span class=\"fea_classification_keys udls\">train</span>_data)</div>\n\n#find accuracy based on the test data\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\n#show most informative features for the model\nprint(classifier.show_most_informative_features(10))\n\nsentiments = []\n\n# sentiment analysis for only the top tweets for a ticker\n# ticker: the ticker to look up\n# mode: 'popular','recent', or 'mixed'\n# num: the number of tweets to get\ndef topOnly(ticker, mode, num):\n    #search for all the top tweets for a ticker\n    public_tweets = api.search(q=\"$\"+ticker + \" -filter:retweets\",lang=\"en\",result_type=mode,count=num,tweet_mode='extended')\n    for tweet in public_tweets:\n        #clean the tweet\n        cleaned = clean(tweet.full_text)\n        #find the sentiments from the model\n        sentiments.append(classifier.classify(dict([token, True] for token in cleaned)))\n    #count the number of positive tweets\n    pos = sentiments.count(\"Positive\")\n    #count the number of negative tweets\n    neg = sentiments.count(\"Negative\")\n    #find the overall score for the ticker\n    score = ((pos * 1) + (neg * -1))/(pos+neg)\n    print(score)\n\n#list to hold the dates and times\nxs = []\n\n#hold the avg of the scores \nys = []\n\n#array to hold normalized prices\nprices = []\n\n#list to hold the scores\nscores = []\n\n#live stream listening\nclass MyStreamListener(tweepy.StreamListener):\n    #when a new tweet is recieved\n    def on_status(self, status):\n        #clean the tweet\n        cleaned = clean(status.text)\n        \n        #calculate the sentiment for the tweet\n        #sentiments.append(classifier.classify(dict([token, True] for token in cleaned)))\n        \n        #calculate the score for the tweet\n        #pos = sentiments.count(\"Positive\")\n        #neg = sentiments.count(\"Negative\")\n        #score = ((pos * 1) + (neg * -1))/(pos+neg)\n        \n        #add the score to the array\n        sentiment = classifier.classify(dict([token, True] for token in cleaned))\n        \n        if sentiment == \"Positive\":\n            scores.append(1)\n        else:\n            scores.append(-1)\n\n    def on_error(self, status_code):\n        #stop the stream on error\n        return False\n\n# stream data live for a ticker\n# ticker: the ticker to stream\n# interval: how often to update the graph (in milliseconds)\n# numpoints: number of points to display on the graph\n# weeksback: how far back to go for the high/low to normalize stock data\ndef stream(ticker, interval, numpoints, weeksback):\n    #get 52 week high and low for normalizizing\n    start = dt.datetime.now() - dt.timedelta(weeks=weeksback)\n    end = dt.datetime.now()\n    \n    df = web.DataReader(ticker, 'yahoo', start, end)\n    close_px = df['Adj Close']\n\n    high = close_px.max()\n    low = close_px.min()\n\n    #matplotlib figure\n    fig = thr.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    \n    #clear the arrays\n    sentiments = []\n    \n    #start the stream\n    myStreamListener = MyStreamListener()\n    myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)\n    \n    #filter for the specified ticker\n    myStream.filter(track=[\"$\"+ticker], is_async=True)\n    \n    #animate the graphs\n    def animate(i, xs, ys, scores, prices):\n        #add the date to the array\n        if len(scores) != 0:\n            xs.append(dt.datetime.now().strftime('%H:%M:%S.%f'))\n            avgscore = sum(scores)/len(scores) \n            ys.append((avgscore-min(scores))/(max(scores)-min(scores)))\n            price = si.get_live_price(ticker)\n            prices.append((price-low)/(high-low))\n\n        # Limit x and y lists to 20 items\n        xs = xs[-numpoints:]\n        ys = ys[-numpoints:]\n        prices = prices[-numpoints:]\n\n        #clear scores array\n        #scores = []\n\n        # Draw x and y lists\n        ax.clear()\n        ax.plot(xs, ys, linestyle='--', marker='o', color='b', label=\"sentiment\")\n        ax.plot(xs, prices, linestyle='--', marker='x', color='r', label=\"price\")\n        ax.fill_between(xs, ys, prices, alpha=0.7)\n        ax.legend(\"sentiment\",\"price\")\n        \n        # Format plot\n        thr.xticks(rotation=45, ha='right')\n        thr.subplots_adjust(bottom=0.30)\n        thr.title(ticker.upper() + ' sentiment over time')\n        thr.ylabel('Sentiment')\n        thr.xlabel('Time')\n\n    # Set up plot to call animate() function periodically\n    ani = animation.FuncAnimation(fig, animate, fargs=(xs, ys, scores, prices), interval=interval)\n    thr.show()\n\n#topOnly(\"tsla\", \"popular\", 100)\nstream(\"tsla\", 60000, 60, 1)\n#https://github.com/gregyjames/twitter-stock-sentiment/blob/main/main</code></pre></div>",
    "fir_24": "<div class=\"codeBlock hljs python\" id=\"fir_24\"><pre id=\"fir_24_code\"><code class=\"python\"># importing libraries for persorming the sentiment analysis, cleaning data, training and saving model\nimport pickle\nimport random\nimport re\nimport string\n\nfrom nltk import FreqDist, NaiveBayesClassifier, classify\nfrom nltk.corpus import stopwords, twitter_samples\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import word_tokenize\n\n\ndef remove_noise(tweet_tokens, stop_words=()):\n    '''This function removes the links or hashtags presesnt in the text and change the verbs to its first form'''\n    cleaned_tokens = []\n\n    for token, tag in <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(tweet_<span class=\"fea_Part_of_Speech_keys udls\">token</span>s)</div>:\n        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+#]|[!*\\(\\),]|'\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)\n        token = re.sub(\"(@[A-Za-z0-9_]+)\", \"\", token)\n\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n\n        <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_keys udls\">lemma</span>tizer = WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer()</div>\n        <div class=\"highlights fea_lemmatization\" id=\"lemmatization_1\" style=\"display: inline;\">token = <span class=\"fea_lemmatization_keys udls\">lemma</span>tizer.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(token, pos)</div>\n\n        if len(token) &gt; 0 and token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens\n\n\ndef get_all_words(cleaned_tokens_list):\n    '''It acts as an generator for the tokens'''\n    for tokens in cleaned_tokens_list:\n        for token in tokens:\n            yield token\n\n\ndef get_tweets_for_model(cleaned_tokens_list):\n    '''This function takes the cleaned token list as input and reutrn a list which is suitable to fed to the classifier'''\n    for tweet_tokens in cleaned_tokens_list:\n        yield dict([token, True] for token in tweet_tokens)\n\n\ndef predict_sentiment(sentence, classifier):\n    '''predict_sentiment function predict the senitment of the text which was given as a argument'''\n    custom_tokens = remove_noise(word_tokenize(sentence))\n    return classifier.classify(\n        dict([token, True] for token in custom_tokens))\n\n\ndef save_model():\n    '''Saving the trained classifier'''\n    f = open('my_classifier.pickle', 'wb')\n    pickle.dump(classifier, f)\n    f.close()\n\n\nif __name__ == \"__main__\":\n\n    # loading dataset for model trainig\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">positive_tweets = twitter_samples.strings('positive_tweets.json')\n    negative_tweets = twitter_samples.strings('negative_tweets.json')</div>\n    text = twitter_samples.strings('tweets.20150430-223406.json')\n    tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n\n    # saving the stopwords from the nltk into a variable\n    stop_words = stopwords.words('english')\n\n    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n\n    positive_cleaned_tokens_list = []\n    negative_cleaned_tokens_list = []\n\n    for tokens in positive_tweet_tokens:\n        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n\n    for tokens in negative_tweet_tokens:\n        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n\n    all_pos_words = get_all_words(positive_cleaned_tokens_list)\n\n    <div class=\"highlights fea_word_frequency\" id=\"word_frequency_0\" style=\"display: inline;\"><span class=\"fea_word_frequency_keys udls\">freq</span>_dist_pos = <span class=\"fea_word_frequency_keys udls\">Freq</span>Dist(all_pos_<span class=\"fea_word_frequency_keys udls\">word</span>s)</div>\n    print(freq_dist_pos.most_common(10))\n\n    positive_tokens_for_model = get_tweets_for_model(\n        positive_cleaned_tokens_list)\n    negative_tokens_for_model = get_tweets_for_model(\n        negative_cleaned_tokens_list)\n\n    positive_dataset = [(tweet_dict, \"Positive\")\n                        for tweet_dict in positive_tokens_for_model]\n\n    negative_dataset = [(tweet_dict, \"Negative\")\n                        for tweet_dict in negative_tokens_for_model]\n\n    dataset = positive_dataset + negative_dataset\n\n    random.shuffle(dataset)\n\n    train_data = dataset[: 7000]\n    test_data = dataset[7000:]\n\n    <div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_keys udls\">classifier</span> = <span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>.<span class=\"fea_classification_keys udls\">train</span>(<span class=\"fea_classification_keys udls\">train</span>_data)</div>\n\n    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\n    print(classifier.show_most_informative_features(10))\n\n    custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n\n    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n\n    print(custom_tweet, classifier.classify(\n        dict([token, True] for token in custom_tokens)))\n        #https://github.com/g-paras/sentiment-analysis-api/blob/master/model_nltk</code></pre></div>",
    "fir_5": "<div class=\"codeBlock hljs python\" id=\"fir_5\"><pre id=\"fir_5_code\"><code class=\"python\">import string\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import re\n\nMIN_YEAR = 1900\nMAX_YEAR = 2100\n\n\ndef get_url_patern():\n    return <div class=\"highlights fea_regular_expression\" id=\"regular_expression_0\" style=\"display: inline;\">re.<span class=\"fea_regular_expression_keys udls\">compile</span>(\n        r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))'\n        r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})')</div>\n\n\ndef get_emojis_pattern():\n    try:\n        # UCS-4\n        <div class=\"highlights fea_regular_expression\" id=\"regular_expression_1\" style=\"display: inline;\">emojis_pattern = re.<span class=\"fea_regular_expression_keys udls\">compile</span>(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')</div>\n    except re.error:\n        # UCS-2\n        <div class=\"highlights fea_regular_expression\" id=\"regular_expression_2\" style=\"display: inline;\">emojis_pattern = re.<span class=\"fea_regular_expression_keys udls\">compile</span>(\n            u'([\\u2600-\\u27BF])|([\\uD83C][\\uDF00-\\uDFFF])|([\\uD83D][\\uDC00-\\uDE4F])|([\\uD83D][\\uDE80-\\uDEFF])')</div>\n    return emojis_pattern\n\n\ndef get_hashtags_pattern():\n    return <div class=\"highlights fea_regular_expression\" id=\"regular_expression_3\" style=\"display: inline;\">re.<span class=\"fea_regular_expression_keys udls\">compile</span>(r'#\\w*')</div>\n\n\ndef get_single_letter_words_pattern():\n    return <div class=\"highlights fea_regular_expression\" id=\"regular_expression_4\" style=\"display: inline;\"><p>re.<span class=\"fea_regular_expression_keys udls\">compile</span>(r'(?</p></div>\n\n\ndef get_blank_spaces_pattern():\n    return re.compile(r'\\s{2,}|\\t')\n\n\ndef get_twitter_reserved_words_pattern():\n    return re.compile(r'(RT|rt|FAV|fav|VIA|via)')\n\n\ndef get_mentions_pattern():\n    return re.compile(r'@\\w*')\n\ndef get_negations_pattern():\n    negations_ = {\"isn't\": \"is not\", \"can't\": \"can not\", \"couldn't\": \"could not\", \"hasn't\": \"has not\",\n                  \"hadn't\": \"had not\", \"won't\": \"will not\",\n                  \"wouldn't\": \"would not\", \"aren't\": \"are not\",\n                  \"haven't\": \"have not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n                  \"don't\": \"do not\", \"shouldn't\": \"should not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n                  \"mightn't\": \"might not\",\n                  \"mustn't\": \"must not\"}\n    return re.compile(r'\\b(' + '|'.join(negations_.keys()) + r')\\b')\n\n\ndef is_year(text):\n    if (len(text) == 3 or len(text) == 4) and (MIN_YEAR &lt; len(text) &lt; MAX_YEAR):\n        return True\n    else:\n        return False\n\n\nclass TwitterPreprocessor:\n\n    def __init__(self, text: str):\n        self.text = text\n\n    def fully_preprocess(self):\n        return self \\\n            .remove_urls() \\\n            .remove_mentions() \\\n            .remove_hashtags() \\\n            .remove_twitter_reserved_words() \\\n            .remove_punctuation() \\\n            .remove_single_letter_words() \\\n            .remove_blank_spaces() \\\n            .remove_stopwords() \\\n            .remove_numbers()\n\n    def remove_urls(self):\n        <div class=\"highlights fea_regular_expression\" id=\"regular_expression_5\" style=\"display: inline;\">self.text = re.sub(pattern=get_url_patern(), repl='', string=self.text)</div>\n        return self\n\n    def remove_punctuation(self):\n        self.text = self.text.translate(str.maketrans('', '', string.punctuation))\n        return self\n\n    def remove_mentions(self):\n        self.text = re.sub(pattern=get_mentions_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_hashtags(self):\n        self.text = re.sub(pattern=get_hashtags_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_twitter_reserved_words(self):\n        self.text = re.sub(pattern=get_twitter_reserved_words_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_single_letter_words(self):\n        self.text = re.sub(pattern=get_single_letter_words_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_blank_spaces(self):\n        self.text = re.sub(pattern=get_blank_spaces_pattern(), repl=' ', string=self.text)\n        return self\n\n    def remove_stopwords(self, extra_stopwords=None):\n        if extra_stopwords is None:\n            extra_stopwords = []\n        text = nltk.word_tokenize(self.text)\n        <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span>_<span class=\"fea_nlp_datasets_keys udls\">words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english'))</div>\n\n        new_sentence = []\n        for w in text:\n            if w not in stop_words and w not in extra_stopwords:\n                new_sentence.append(w)\n        self.text = ' '.join(new_sentence)\n        return self\n\n    def remove_numbers(self, preserve_years=False):\n        text_list = self.text.split(' ')\n        for text in text_list:\n            if text.isnumeric():\n                if preserve_years:\n                    if not is_year(text):\n                        text_list.remove(text)\n                else:\n                    text_list.remove(text)\n\n        self.text = ' '.join(text_list)\n        return self\n\n    def lowercase(self):\n        self.text = self.text.lower()\n        return self\n    \n    def handle_negations(self):  \n        self.text = re.sub(pattern=get_negations_pattern(), repl='', string=self.text)\n        return self\n        #https://github.com/vasisouv/tweets-preprocessor/blob/master/twitter_preprocessor</code></pre></div>",
    "fir_7": "<div class=\"codeBlock hljs python\" id=\"fir_7\"><pre id=\"fir_7_code\"><code class=\"python\">#! /usr/bin/env python2\n\n\"\"\"\nFilename: characterExtraction\nAuthor: Emily Daniels\nDate: April 2014\nPurpose: Extracts character names from a text file and performs analysis of\ntext sentences containing the names.\n\"\"\"\n\nimport json\nimport nltk\nimport re\n\nfrom collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom pattern.en import parse, Sentence, mood\nfrom pattern.db import csv\nfrom pattern.vector import Document, NB\n\ndef readText():\n    \"\"\"\n    Reads the text from a text file.\n    \"\"\"\n    with open(\"730.txt\", \"rb\") as f:\n        text = f.read().decode('utf-8-sig')\n    return text\n\n\ndef chunkSentences(text):\n    \"\"\"\n    Parses text into parts of speech tagged with parts of speech labels.\n\n    Used for reference: https://gist.github.com/onyxfish/322906\n    \"\"\"\n    sentences = nltk.sent_tokenize(text)\n    tokenizedSentences = [nltk.word_tokenize(sentence)\n                          for sentence in sentences]\n    taggedSentences = [nltk.pos_tag(sentence)\n                       for sentence in tokenizedSentences]\n    if nltk.__version__[0:2] == \"2.\":\n        chunkedSentences = nltk.batch_ne_chunk(taggedSentences, binary=True)\n    else:\n        chunkedSentences = nltk.ne_chunk_sents(taggedSentences, binary=True)\n    return chunkedSentences\n\n\ndef extractEntityNames(tree, _entityNames=None):\n    \"\"\"\n    Creates a local list to hold nodes of tree passed through, extracting named\n    entities from the chunked sentences.\n\n    Used for reference: https://gist.github.com/onyxfish/322906\n    \"\"\"\n    if _entityNames is None:\n        _entityNames = []\n    try:\n        if nltk.__version__[0:2] == \"2.\":\n            label = tree.node\n        else:\n            label = tree.label()\n    except AttributeError:\n        pass\n    else:\n        if label == 'NE':\n            _entityNames.append(' '.join([child[0] for child in tree]))\n        else:\n            for child in tree:\n                extractEntityNames(child, _entityNames=_entityNames)\n    return _entityNames\n\n\ndef buildDict(chunkedSentences, _entityNames=None):\n    \"\"\"\n    Uses the global entity list, creating a new dictionary with the properties\n    extended by the local list, without overwriting.\n\n    Used for reference: https://gist.github.com/onyxfish/322906\n    \"\"\"\n    if _entityNames is None:\n        _entityNames = []\n\n    for tree in chunkedSentences:\n        extractEntityNames(tree, _entityNames=_entityNames)\n\n    return _entityNames\n\n\ndef removeStopwords(entityNames, customStopWords=None):\n    \"\"\"\n    Brings in stopwords and custom stopwords to filter mismatches out.\n    \"\"\"\n    # Memoize custom stop words\n    if customStopWords is None:\n        with open(\"customStopWords.txt\", \"rb\") as f:\n            customStopwords = f.read().split(', ')\n\n    for name in entityNames:\n        if name in <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english')</div> or name in customStopwords:\n            entityNames.remove(name)\n\n\ndef getMajorCharacters(entityNames):\n    \"\"\"\n    Adds names to the major character list if they appear frequently.\n    \"\"\"\n    return {name for name in entityNames if entityNames.count(name) &gt; 10}\n\n\ndef splitIntoSentences(text):\n    \"\"\"\n    Split sentences on .?! \"\" and not on abbreviations of titles.\n    Used for reference: http://stackoverflow.com/a/8466725\n    \"\"\"\n    sentenceEnders = re.compile(r\"\"\"\n    # Split sentences on whitespace between them.\n    (?:               # Group for two positive lookbehinds.\n      (?&lt;=[.!?])      # Either an end of sentence punct,\n    | (?&lt;=[.!?]['\"])  # or end of sentence punct and quote.\n    )                 # End group of two positive lookbehinds.\n    (?&lt;!  Mr\\.   )    # Don't end sentence on \"Mr.\"\n    (?&lt;!  Mrs\\.  )    # Don't end sentence on \"Mrs.\"\n    (?&lt;!  Ms\\.   )    # Don't end sentence on \"Ms.\"\n    (?&lt;!  Jr\\.   )    # Don't end sentence on \"Jr.\"\n    (?&lt;!  Dr\\.   )    # Don't end sentence on \"Dr.\"\n    (?&lt;!  Prof\\. )    # Don't end sentence on \"Prof.\"\n    (?&lt;!  Sr\\.   )    # Don't end sentence on \"Sr.\"\n    \\s+               # Split on whitespace between sentences.\n    \"\"\", re.IGNORECASE | re.VERBOSE)\n    return sentenceEnders.split(text)\n\n\ndef compareLists(sentenceList, majorCharacters):\n    \"\"\"\n    Compares the list of sentences with the character names and returns\n    sentences that include names.\n    \"\"\"\n    characterSentences = defaultdict(list)\n    for sentence in sentenceList:\n        for name in majorCharacters:\n            if re.search(r\"\\b(?=\\w)%s\\b(?!\\w)\" % re.escape(name),\n                         sentence,\n                         re.IGNORECASE):\n                characterSentences[name].append(sentence)\n    return characterSentences\n\n\ndef extractMood(characterSentences):\n    \"\"\"\n    Analyzes the sentence using grammatical mood module from pattern.\n    \"\"\"\n    characterMoods = defaultdict(list)\n    for key, value in characterSentences.iteritems():\n        for x in value:\n            characterMoods[key].append(mood(Sentence(parse(str(x),\n                                                           lemmata=True))))\n    return characterMoods\n\n\ndef extractSentiment(characterSentences):\n    \"\"\"\n    Trains a Naive Bayes classifier object with the reviews.csv file, analyzes\n    the sentence, and returns the tone.\n    \"\"\"\n    nb = NB()\n    characterTones = defaultdict(list)\n    for review, rating in csv(\"reviews.csv\"):\n        nb.train(Document(review, type=int(rating), stopwords=True))\n    for key, value in characterSentences.iteritems():\n        for x in value:\n            characterTones[key].append(nb.classify(str(x)))\n    return characterTones\n\n\ndef writeAnalysis(sentenceAnalysis):\n    \"\"\"\n    Writes the sentence analysis to a text file in the same directory.\n    \"\"\"\n    with open(\"sentenceAnalysis.txt\", \"wb\") as f:\n        for item in sentenceAnalysis.items():\n            f.write(\"%s:%s\\n\" % item)\n\n\ndef writeToJSON(sentenceAnalysis):\n    \"\"\"\n    Writes the sentence analysis to a JSON file in the same directory.\n    \"\"\"\n    with open(\"sentenceAnalysis.json\", \"wb\") as f:\n        json.dump(sentenceAnalysis, f)\n\n\nif __name__ == \"__main__\":\n    text = readText()\n\n    chunkedSentences = chunkSentences(text)\n    entityNames = buildDict(chunkedSentences)\n    removeStopwords(entityNames)\n    majorCharacters = getMajorCharacters(entityNames)\n    \n    sentenceList = splitIntoSentences(text)\n    characterSentences = compareLists(sentenceList, majorCharacters)\n    characterMoods = extractMood(characterSentences)\n    characterTones = extractSentiment(characterSentences)\n\n    # Merges sentences, moods and tones together into one dictionary on each\n    # character.\n    sentenceAnalysis = defaultdict(list,\n                                   [(k, [characterSentences[k],\n                                         characterTones[k],\n                                         characterMoods[k]])\n                                    for k in characterSentences])\n    \n    writeAnalysis(sentenceAnalysis)\n    writeToJSON(sentenceAnalysis)\n    #https://github.com/emdaniels/character-extraction/blob/master/characterExtraction</code></pre></div>",
    "fir_28": "<div class=\"codeBlock hljs python\" id=\"fir_28\"><pre id=\"fir_28_code\"><code class=\"python\">from nltk.corpus import subjectivity\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.sentiment import SentimentAnalyzer # SentimentAnalyzer is a tool to implement and facilitate Sentiment Analysis.\nfrom nltk.sentiment.util import (mark_negation, extract_unigram_feats) # mark_negation(): Append _NEG suffix to words that appear in the scope between a negation and a punctuation mark. extract_unigram_feats(): Populate a dictionary of unigram features, reflecting the presence/absence in the document of each of the tokens in unigrams.\n\nn_instances = 100\nobj_docs = [(sent, 'obj') for sent in <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">subjectivity.sents(categories='obj')[:n_instances]]</div>\nsubj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\ntrain_obj_docs = obj_docs[:80]\ntest_obj_docs = obj_docs[80:100]\ntrain_subj_docs = subj_docs[:80]\ntest_subj_docs = subj_docs[80:100]\n\ntraining_docs = train_obj_docs + train_subj_docs\ntesting_docs = test_obj_docs + test_subj_docs\n\n<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">sentim_analyzer = <span class=\"fea_sentiment_analysis_keys udls\">Sentiment</span>Analyzer()</div>\nall_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n\nunigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n\nsentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n\ntraining_set = sentim_analyzer.apply_features(training_docs)\ntest_set = sentim_analyzer.apply_features(testing_docs)\n\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_keys udls\">train</span>er = <span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>.<span class=\"fea_classification_keys udls\">train</span>\n<span class=\"fea_classification_keys udls\">classifier</span> = sentim_analyzer.<span class=\"fea_classification_keys udls\">train</span>(<span class=\"fea_classification_keys udls\">train</span>er, <span class=\"fea_classification_keys udls\">train</span>ing_set)</div>\n\nfor key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n    print('{0}: {1}'.format(key, value))\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsentences = [\n    \"You are a piece of shit, and I will step on you.\",\n    \"THIS SUX!!!\",\n    \"This kinda sux...\",\n    \"You're good, man\",\n    \"HAHAHA YOU ARE THE BEST!!!!! VERY FUNNY!!!\"\n            ]\n\n\n<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_1\" style=\"display: inline;\">sid = <span class=\"fea_sentiment_analysis_keys udls\">Sentiment</span>IntensityAnalyzer()</div>\n\nfor sentence in sentences:\n    print('\\n' + sentence)\n    ss = sid.polarity_scores(sentence)\n    for k in sorted(ss):\n        print('{0}: {1}, '.format(k, ss[k]), end='')</code></pre></div>",
    "fir_22": "<div class=\"codeBlock hljs python\" id=\"fir_22\"><pre id=\"fir_22_code\"><code class=\"python\">\n# coding=utf-8\nimport utils\nimport nltk\n\ndata = utils.getTrainData()\n\ndef get_words_in_tweets(tweets):\n    all_words = []\n    for (words, sentiment) in tweets:\n      all_words.extend(words)\n    return all_words\n\ndef get_word_features(wordlist):\n    <div class=\"highlights fea_word_frequency\" id=\"word_frequency_0\" style=\"display: inline;\"><span class=\"fea_word_frequency_keys udls\">word</span>list = nltk.<span class=\"fea_word_frequency_keys udls\">Freq</span>Dist(<span class=\"fea_word_frequency_keys udls\">word</span>list)</div>\n    word_features = wordlist.keys()\n    return word_features\n\nword_features = get_word_features(get_words_in_tweets(data))\n\ndef extract_features(document):\n    document_words = set(document)\n    features = {}\n    for word in word_features:\n        features[word.decode(\"utf8\")] = (word in document_words)\n    return features\n\nallsetlength = len(data)\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_keys udls\">train</span>ing_set = nltk.classify.apply_features(extract_features, data[:allsetlength/10*8])</div>\ntest_set = data[allsetlength/10*8:]\n<div class=\"highlights fea_classification\" id=\"classification_1\" style=\"display: inline;\"><span class=\"fea_classification_keys udls\">classifier</span> = nltk.<span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>.<span class=\"fea_classification_keys udls\">train</span>(<span class=\"fea_classification_keys udls\">train</span>ing_set)</div>\n\ndef classify(tweet):\n\tprint classifier.classify(extract_features(tweet.split()))\n\nclassify(\"Bugün çok güzel bir gün\")\n#https://github.com/mertkahyaoglu/twitter-sentiment-analysis/blob/master/classify</code></pre></div>",
    "fir_27": "<div class=\"codeBlock hljs python\" id=\"fir_27\"><pre id=\"fir_27_code\"><code class=\"python\">from nltk.corpus import brown\nfrom nltk import FreqDist\n\n<div class=\"highlights fea_word_frequency\" id=\"word_frequency_0\" style=\"display: inline;\">suffix_fdist = <span class=\"fea_word_frequency_keys udls\">Freq</span>Dist()</div>\nfor word in brown.words():\n    word = word.lower()\n    suffix_fdist[word[-1:]] += 1\n    suffix_fdist[word[-2:]] += 1\n    suffix_fdist[word[-3:]] += 1\ncommon_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n\ndef pos_features(word):\n    features = {}\n    for suffix in common_suffixes:\n        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n    return features\n\n<div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\"><span class=\"fea_tagger_keys udls\">tagge</span>d_words = brown.<span class=\"fea_tagger_keys udls\">tagge</span>d_words(categories='news')</div>\nfeaturesets = [(pos_features(n), g) for (n,g) in tagged_words]\n\nfrom nltk import DecisionTreeClassifier\nfrom nltk.classify import accuracy\n\ncutoff = int(len(featuresets) * 0.1)\ntrain_set, test_set = featuresets[cutoff:], featuresets[:cutoff]\n\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_keys udls\">classifier</span> = DecisionTree<span class=\"fea_classification_keys udls\">Classifier</span>.<span class=\"fea_classification_keys udls\">train</span>(<span class=\"fea_classification_keys udls\">train</span>_set)</div> # NLTK is a teaching toolkit which is not really optimized for speed. Therefore, this may take forever. For speed, use scikit-learn for the classifiers.\n\naccuracy(classifier, test_set)\n\nclassifier.classify(pos_features('cats'))\n\nclassifier.pseudocode(depth=4)</code></pre></div>",
    "fir_26": "<div class=\"codeBlock hljs python\" id=\"fir_26\"><pre id=\"fir_26_code\"><code class=\"python\">s = \"Le temps est un grand maître, dit-on, le malheur est qu'il tue ses élèves.\"\ns = s.lower()\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(\"[a-zA-Z'`éèî]+\")\ns_tokenized = tokenizer.tokenize(s)\n\nfrom nltk.util import ngrams\ngenerated_4grams = []\n\nfor word in s_tokenized:\n    generated_4grams.append(list(<div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\"><span class=\"fea_n_grams_keys udls\">ngrams</span>(word, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')</div>)) # n = 4.\n\ngenerated_4grams = [word for sublist in generated_4grams for word in sublist]\n\nng_list_4grams = generated_4grams\nfor idx, val in enumerate(generated_4grams):\n    ng_list_4grams[idx] = ''.join(val)\n\nfreq_4grams = {}\n\nfor ngram in ng_list_4grams:\n    if ngram not in freq_4grams:\n        freq_4grams.update({ngram: 1})\n    else:\n        ngram_occurrences = freq_4grams[ngram]\n        freq_4grams.update({ngram: ngram_occurrences + 1})\n        \nfrom operator import itemgetter # The operator module exports a set of efficient functions corresponding to the intrinsic operators of Python. For example, operator.add(x, y) is equivalent to the expression x + y.\n\nfreq_4grams_sorted = sorted(freq_4grams.items(), key=itemgetter(1), reverse=True)[0:300] # We only keep the 300 most popular n-grams. This was suggested in the original paper written about n-grams.\n\nfrom nltk import everygrams\n\ns_clean = ' '.join(s_tokenized) # For the code below we need the raw sentence as opposed to the tokens.\n\ndef ngram_extractor(sent):\n    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4) \n            if ' ' not in ng and '\\n' not in ng and ng != ('_',)]\n\nngram_extractor(s_clean)</code></pre></div>",
    "fir_15": "<div class=\"codeBlock hljs python\" id=\"fir_15\"><pre id=\"fir_15_code\"><code class=\"python\"># -*- coding: utf-8 -*-\n# Maximum Entropy Part-of-Speech Tagger for NLTK (Natural Language Toolkit)\n# Author: Arne Neumann\n# Licence: GPL 3\n\n#__docformat__ = 'epytext en'\n\n\"\"\"\nA I{part-of-speech tagger} that uses NLTK's build-in L{Maximum Entropy\nmodels&lt;nltk.MaxentClassifier&gt;} to find the most likely I{part-of-speech\ntag} (POS) for each word in a given sequence.\n\nThe tagger will be trained on a corpus of tagged sentences. For every word\nin the corpus, a C{tuple} consisting of a C{dictionary} of features from\nthe word's context (e.g. preceding/succeeding words and tags, word\nprefixes/suffixes etc.) and the word's tag will be generated.\nThe maximum entropy classifier will learn a model from these tuples that\nwill be used by the tagger to find the most likely POS-tag for any given\nword, even unseen ones.\n\nThe tagger and the featuresets chosen for training are implemented as described\nin Ratnaparkhi, Adwait (1996). A Maximum Entropy Model for Part-Of-Speech\nTagging. In Proceedings of the ARPA Human Language Technology Workshop. Pages\n250-255.\n\nUsage notes:\n============\n\nPlease install the MEGAM package (http://hal3.name/megam),\notherwise training will take forever.\n\nTo use the demo, please install either 'brown' or 'treebank' with::\n\n    import nltk\n    nltk.download()\n\nin the Python interpreter. Proper usage of demo() and all other functions and\nmethods is described below.\n\"\"\"\n\nimport time\nimport re\nfrom collections import defaultdict\n\nfrom nltk import TaggerI, FreqDist, untag, config_megam\nfrom nltk.classify.maxent import MaxentClassifier\n                  \n\nPATH_TO_MEGAM_EXECUTABLE = \"/usr/bin/megam\"\nconfig_megam(PATH_TO_MEGAM_EXECUTABLE)\n\n\nclass MaxentPosTagger(TaggerI):\n    \"\"\"\n    MaxentPosTagger is a part-of-speech tagger based on Maximum Entropy models.\n    \"\"\"\n    def train(self, train_sents, algorithm='megam', rare_word_cutoff=5,\n              rare_feat_cutoff=5, uppercase_letters='[A-Z]', trace=3,\n              **cutoffs):\n        \"\"\"\n        MaxentPosTagger trains a Maximum Entropy model from a C{list} of tagged\n        sentences.\n\n        @type train_sents: C{list} of C{list} of tuples of (C{str}, C{str})\n        @param train_sents: A list of tagged sentences. Each sentence is\n        represented by a list of tuples. Each tuple holds two strings, a\n        word and its tag, e.g. ('company','NN').\n\n        @type algorithm: C{str}\n        @param algorithm: The algorithm that is used by\n        L{nltk.MaxentClassifier.train()} to train and optimise the model. It is\n        B{strongly recommended} to use the C{LM-BFGS} algorithm provided by the\n        external package U{megam&lt;http://hal3.name/megam/&gt;} as it is much faster\n        and uses less memory than any of the algorithms provided by NLTK (i.e.\n        C{GIS}, C{IIS}) or L{scipy} (e.g. C{CG} and C{BFGS}).\n\n        @type rare_word_cutoff: C{int}\n        @param rare_word_cutoff: Words with less occurrences than\n        C{rare_word_cutoff} will be treated differently by L{extract_feats}\n        than non-rare words (cf. Ratnaparkhi 1996).\n\n        @type rare_feat_cutoff: C{int}\n        @param rare_feat_cutoff: ignore features that occur less than\n        C{rare_feat_cutoff} during training.\n\n        @type uppercase_letters: C{regex}\n        @param uppercase_letters: a regular expression that covers all\n        uppercase letters of the language of your corpus (e.g. '[A-ZÄÖÜ]' for\n        German)\n\n        @type trace: C{int}\n        @param trace: The level of diagnostic output to produce. C{0} doesn't\n        produce any output, while C{3} will give all the output that C{megam}\n        produces plus the time it took to train the model.\n\n        @param cutoffs: Arguments specifying various conditions under\n            which the training should be halted. When using C{MEGAM}, only\n            C{max_iter} should be relevant. For other cutoffs see\n            L{nltk.MaxentClassifier}\n\n              - C{max_iter=v}: Terminate after C{v} iterations.\n       \"\"\"\n        self.uppercase_letters = uppercase_letters\n        self.word_freqdist = self.gen_word_freqs(train_sents)\n        self.featuresets = self.gen_featsets(train_sents,\n                rare_word_cutoff)\n        self.features_freqdist = self.gen_feat_freqs(self.featuresets)\n        self.cutoff_rare_feats(self.featuresets, rare_feat_cutoff)\n\n        t1 = time.time()\n        <div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">self.<span class=\"fea_classification_keys udls\">classifier</span> = Maxent<span class=\"fea_classification_keys udls\">Classifier</span>.<span class=\"fea_classification_keys udls\">train</span>(self.featuresets, algorithm,\n                                                 trace, **cutoffs)</div>\n        t2 = time.time()\n        if trace &gt; 0:\n            print \"time to train the classifier: {0}\".format(round(t2-t1, 3))\n\n    def gen_feat_freqs(self, featuresets):\n        \"\"\"\n        Generates a frequency distribution of joint features (feature, tag)\n        tuples. The frequency distribution will be used by the tagger to\n        determine which (rare) features should not be considered during\n        training (feature cutoff).\n\n        This is how joint features look like::\n            (('t-2 t-1', 'IN DT'), 'NN')\n            (('w-2', '&lt;START&gt;'), 'NNP')\n            (('w+1', 'of'), 'NN')\n\n        @type featuresets: {list} of C{tuples} of (C{dict}, C{str})\n        @param featuresets: a list of tuples that contain the featureset of a\n        word from the training set and its POS tag.\n\n        @rtype: C{FreqDist}\n        @return: a L{frequency distribution&lt;nltk.FreqDist()&gt;},\n        counting how often each (context information feature, tag) tuple occurs\n        in the training sentences.\n        \"\"\"\n        features_freqdist = defaultdict(int)\n        for (feat_dict, tag) in featuresets:\n            for (feature, value) in feat_dict.items():\n                features_freqdist[ ((feature, value), tag) ] += 1\n        return features_freqdist\n\n    def gen_word_freqs(self, train_sents):\n        \"\"\"\n        Generates word frequencies from the training sentences for the feature\n        extractor.\n\n        @type train_sents: C{list} of C{list} of tuples of (C{str}, C{str})\n        @param train_sents: A list of tagged sentences.\n\n        @rtype: C{FreqDist}\n        @return: a L{frequency distribution&lt;nltk.FreqDist()&gt;},\n        counting how often each word occurs in the training sentences.\n        \"\"\"\n        word_freqdist = FreqDist()\n        for tagged_sent in train_sents:\n            for (word, _tag) in tagged_sent:\n                word_freqdist[word] += 1\n        return word_freqdist\n\n    def gen_featsets(self, train_sents, rare_word_cutoff):\n        \"\"\"\n        Generates featuresets for each token in the training sentences.\n\n        @type train_sents: C{list} of C{list} of tuples of (C{str}, C{str})\n        @param train_sents: A list of tagged sentences.\n\n        @type rare_word_cutoff: C{int}\n        @param rare_word_cutoff: Words with less occurrences than\n        C{rare_word_cutoff} will be treated differently by L{extract_feats}\n        than non-rare words (cf. Ratnaparkhi 1996).\n\n        @rtype: {list} of C{tuples} of (C{dict}, C{str})\n        @return:  a list of tuples that contains the featureset of\n        a token and its POS-tag.\n        \"\"\"\n        featuresets = []\n        for tagged_sent in train_sents:\n            history = []\n            untagged_sent = untag(tagged_sent)\n            for (i, (_word, tag)) in enumerate(tagged_sent):\n                featuresets.append( (self.extract_feats(untagged_sent, i,\n                    history, rare_word_cutoff), tag) )\n                history.append(tag)\n        return featuresets\n\n\n    def cutoff_rare_feats(self, featuresets, rare_feat_cutoff):\n        \"\"\"\n        Cuts off rare features to reduce training time and prevent overfitting.\n\n        Example\n        =======\n\n            Let's say, the suffixes of this featureset are too rare to learn.\n\n            &gt;&gt;&gt; featuresets[46712]\n            ({'suffix(1)': 't',\n            'prefix(1)': 'L',\n            'prefix(2)': 'Le',\n            'prefix(3)': 'Lem',\n            'suffix(3)': 'ont',\n            'suffix(2)': 'nt',\n            'contains-uppercase': True,\n            'prefix(4)': 'Lemo',\n            'suffix(4)': 'mont'},\n            'NNP')\n\n            C{cutoff_rare_feats} would then remove the rare joint features::\n\n                (('suffix(1)', 't'), 'NNP')\n                (('suffix(3)', 'ont'), 'NNP')\n                ((suffix(2)': 'nt'), 'NNP')\n                (('suffix(4)', 'mont'), 'NNP')\n\n            and return a featureset that only contains non-rare features:\n\n            &gt;&gt;&gt; featuresets[46712]\n            ({'prefix(1)': 'L',\n            'prefix(2)': 'Le',\n            'prefix(3)': 'Lem',\n            'contains-uppercase': True,\n            'prefix(4)': 'Lemo'},\n            'NNP')\n\n\n        @type featuresets: {list} of C{tuples} of (C{dict}, C{str})\n        @param featuresets: a list of tuples that contain the featureset of a\n        word from the training set and its POS tag\n\n        @type rare_feat_cutoff: C{int}\n        @param rare_feat_cutoff: if a (context information feature, tag)\n        tuple occurs less than C{rare_feat_cutoff} times in the training\n        set, then its corresponding feature will be removed from the\n        C{featuresets} to be learned.\n        \"\"\"\n        never_cutoff_features = set(['w','t'])\n\n        for (feat_dict, tag) in featuresets:\n            for (feature, value) in feat_dict.items():\n                feat_value_tag = ((feature, value), tag)\n                if self.features_freqdist[feat_value_tag] &lt; rare_feat_cutoff:\n                    if feature not in never_cutoff_features:\n                        feat_dict.pop(feature)\n\n\n    def extract_feats(self, sentence, i, history, rare_word_cutoff=5):\n        \"\"\"\n        Generates a featureset from a word (in a sentence). The features\n        were chosen as described in Ratnaparkhi (1996) and his Java\n        software package U{MXPOST&lt;ftp://ftp.cis.upenn.edu/pub/adwait/jmx&gt;}.\n\n        The following features are extracted:\n\n            - features for all words: last tag (C{t-1}), last two tags (C{t-2\n              t-1}), last words (C{w-1}) and (C{w-2}), next words (C{w+1}) and\n              (C{w+2})\n            - features for non-rare words: current word (C{w})\n            - features for rare words: word suffixes (last 1-4 letters),\n              word prefixes (first 1-4 letters),\n              word contains number (C{bool}), word contains uppercase character\n              (C{bool}), word contains hyphen (C{bool})\n\n        Ratnaparkhi experimented with his tagger on the Wall Street Journal\n        corpus (Penn Treebank project). He found that the tagger yields\n        better results when words which occur less than 5 times are treated\n        as rare. As your mileage may vary, please adjust\n        L{rare_word_cutoff} accordingly.\n\n        Examples\n        ========\n\n            1. This is a featureset extracted from the nonrare (word, tag)\n            tuple ('considerably', 'RB')\n\n            &gt;&gt;&gt; featuresets[22356]\n            ({'t-1': 'VB',\n            't-2 t-1': 'TO VB',\n            'w': 'considerably',\n            'w+1': '.',\n            'w+2': '&lt;END&gt;',\n            'w-1': 'improve',\n            'w-2': 'to'},\n            'RB')\n\n            2. A featureset extracted from the rare tuple ('Lemont', 'NN')\n\n            &gt;&gt;&gt; featuresets[46712]\n            ({'suffix(1)': 't',\n            'prefix(1)': 'L',\n            'prefix(2)': 'Le',\n            'prefix(3)': 'Lem',\n            'suffix(3)': 'ont',\n            'suffix(2)': 'nt',\n            'contains-uppercase': True,\n            'prefix(4)': 'Lemo',\n            'suffix(4)': 'mont'},\n            'NNP')\n\n\n        @type sentence: C{list} of C{str}\n        @param sentence: A list of words, usually a sentence.\n\n        @type i: C{int}\n        @param i: The index of a word in a sentence, where C{sentence[0]} would\n        represent the first word of a sentence.\n\n        @type history: C{int} of C{str}\n        @param history: A list of POS-tags that have been assigned to the\n        preceding words in a sentence.\n\n        @type rare_word_cutoff: C{int}\n        @param rare_word_cutoff: Words with less occurrences than\n        C{rare_word_cutoff} will be treated differently than non-rare words\n        (cf. Ratnaparkhi 1996).\n\n        @rtype: C{dict}\n        @return: a dictionary of features extracted from a word's\n        context.\n        \"\"\"\n        features = {}\n        hyphen = re.compile(\"-\")\n        number = re.compile(\"\\d\")\n        uppercase = re.compile(self.uppercase_letters)\n\n        #get features: w-1, w-2, t-1, t-2.\n        #takes care of the beginning of a sentence\n        if i == 0: #first word of sentence\n            features.update({\"w-1\": \"&lt;START&gt;\", \"t-1\": \"&lt;START&gt;\",\n                             \"w-2\": \"&lt;START&gt;\", \"t-2 t-1\": \"&lt;START&gt; &lt;START&gt;\"})\n        elif i == 1: #second word of sentence\n            features.update({\"w-1\": sentence[i-1], \"t-1\": history[i-1],\n                             \"w-2\": \"&lt;START&gt;\",\n                             \"t-2 t-1\": \"&lt;START&gt; %s\" % (history[i-1])})\n        else:\n            features.update({\"w-1\": sentence[i-1], \"t-1\": history[i-1],\n                \"w-2\": sentence[i-2],\n                \"t-2 t-1\": \"%s %s\" % (history[i-2], history[i-1])})\n\n        #get features: w+1, w+2. takes care of the end of a sentence.\n        for inc in [1, 2]:\n            try:\n                features[\"w+%i\" % (inc)] = sentence[i+inc]\n            except IndexError:\n                features[\"w+%i\" % (inc)] = \"&lt;END&gt;\"\n\n        if self.word_freqdist[sentence[i]] &gt;= rare_word_cutoff:\n            #additional features for 'non-rare' words\n            features[\"w\"] = sentence[i]\n\n        else: #additional features for 'rare' or 'unseen' words\n            features.update({\"suffix(1)\": sentence[i][-1:],\n                \"suffix(2)\": sentence[i][-2:], \"suffix(3)\": sentence[i][-3:],\n                \"suffix(4)\": sentence[i][-4:], \"prefix(1)\": sentence[i][:1],\n                \"prefix(2)\": sentence[i][:2], \"prefix(3)\": sentence[i][:3],\n                \"prefix(4)\": sentence[i][:4]})\n            if hyphen.search(sentence[i]) != None:\n                #set True, if regex is found at least once\n                features[\"contains-hyphen\"] = True\n            if number.search(sentence[i]) != None:\n                features[\"contains-number\"] = True\n            if uppercase.search(sentence[i]) != None:\n                features[\"contains-uppercase\"] = True\n\n        return features\n\n\n    def tag(self, sentence, rare_word_cutoff=5):\n        \"\"\"\n        Attaches a part-of-speech tag to each word in a sequence.\n\n        @type sentence: C{list} of C{str}\n        @param sentence: a list of words to be tagged.\n\n        @type rare_word_cutoff: C{int}\n        @param rare_word_cutoff: words with less occurrences than\n        C{rare_word_cutoff} will be treated differently than non-rare words\n        (cf. Ratnaparkhi 1996).\n\n        @rtype: C{list} of C{tuples} of (C{str}, C{str})\n        @return: a list of tuples consisting of a word and its corresponding\n        part-of-speech tag.\n        \"\"\"\n        history = []\n        for i in xrange(len(sentence)):\n            featureset = self.extract_feats(sentence, i, history,\n                                               rare_word_cutoff)\n            tag = self.classifier.classify(featureset)\n            history.append(tag)\n        return zip(sentence, history)\n\n\ndef demo(corpus, num_sents):\n    \"\"\"\n    Loads a few sentences from the Brown corpus or the Wall Street Journal\n    corpus, trains them, tests the tagger's accuracy and tags an unseen\n    sentence.\n\n    @type corpus: C{str}\n    @param corpus: Name of the corpus to load, either C{brown} or C{treebank}.\n\n    @type num_sents: C{int}\n    @param num_sents: Number of sentences to load from a corpus. Use a small\n    number, as training might take a while.\n    \"\"\"\n    if corpus.lower() == \"brown\":\n        from nltk.corpus import brown\n        tagged_sents = brown.tagged_sents()[:num_sents]\n    elif corpus.lower() == \"treebank\":\n        from nltk.corpus import treebank\n        tagged_sents = treebank.tagged_sents()[:num_sents]\n    else:\n        print \"Please load either the 'brown' or the 'treebank' corpus.\"\n\n    size = int(len(tagged_sents) * 0.1)\n    train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n    maxent_tagger = MaxentPosTagger()\n    maxent_tagger.train(train_sents)\n    print \"tagger accuracy (test %i sentences, after training %i):\" % \\\n        (size, (num_sents - size)), maxent_tagger.evaluate(test_sents)\n    print \"\\n\\n\"\n    print \"classify unseen sentence: \", maxent_tagger.tag([\"This\", \"is\", \"so\",\n        \"slow\", \"!\"])\n    print \"\\n\\n\"\n    print \"show the 10 most informative features:\"\n    print maxent_tagger.classifier.show_most_informative_features(10)\n\n\nif __name__ == '__main__':\n    demo(\"treebank\", 200)\n    #~ featuresets = demo_debugger(\"treebank\", 10000)\n    print \"\\n\\n\\n\"\n\n#https://github.com/arne-cl/nltk-maxent-pos-tagger/blob/master/mxpost</code></pre></div>",
    "sec_7": "<div class=\"codeBlock hljs python\" id=\"sec_7\"><pre id=\"sec_7_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob.parsers import PatternParser\n<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">blob = TextBlob(\"Parsing is fun.\", <span class=\"fea_parsing_keys udls\">parser</span>=Pattern<span class=\"fea_parsing_keys udls\">Parser</span>())</div>\n<div class=\"highlights fea_parsing\" id=\"parsing_1\" style=\"display: inline;\">blob.parse()</div>\n#https://textblob.readthedocs.io/en/dev/advanced_usage.html</code></pre></div>",
    "sec_10": "<div class=\"codeBlock hljs python\" id=\"sec_10\"><pre id=\"sec_10_code\"><code class=\"python\">\nfrom flask import Flask, render_template, request\napp = Flask(__name__)\nfrom textblob import TextBlob\nimport nltk\nfrom textblob import Word\nimport sys\n\n\ndef parse(string):\n   \"\"\"\n   Parse a paragraph. Devide it into sentences and try to generate quesstions from each sentences.\n   \"\"\"\n   data = []\n   try:\n      txt = TextBlob(string)\n      # Each sentence is taken from the string input and passed to genQuestion() to generate questions.\n      for sentence in txt.sentences:\n         question = genQuestion(sentence)\n         if question != None:\n            data.append(question)\n      return data\n   except Exception as e:\n      raise e\n\n\n\ndef genQuestion(line):\n\n   \"\"\"\n   outputs question from the given text\n   \"\"\"\n   answer = line\n   if type(line) is str:\n      line = TextBlob(line) # Create object of type textblob.blob.TextBlob\n\n   bucket = {}               # Create an empty dictionary\n   for i,j in enumerate(<div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">line.tags</div>):  # line.tags are the parts-of-speach in English\n      if j[1] not in bucket:\n         bucket[j[1]] = i  # Add all tags to the dictionary or bucket variable\n    \n   if verbose:               # In verbose more print the key,values of dictionary\n      print('\\n','-'*20)\n      print(line ,'\\n')  \n      print(\"TAGS:\",line.tags, '\\n')  \n      print(bucket)\n    \n   question = ''            # Create an empty string \n\n    # These are the english part-of-speach tags used in this demo program.\n    #.....................................................................\n    # NNS     Noun, plural\n    # JJ  Adjective \n    # NNP     Proper noun, singular \n    # VBG     Verb, gerund or present participle \n    # VBN     Verb, past participle \n    # VBZ     Verb, 3rd person singular present \n    # VBD     Verb, past tense \n    # IN      Preposition or subordinating conjunction \n    # PRP     Personal pronoun \n    # NN  Noun, singular or mass \n    #.....................................................................\n\n    # Create a list of tag-combination\n\n   l1 = ['NNP', 'VBG', 'VBZ', 'IN']\n   l2 = ['NNP', 'VBG', 'VBZ']\n    \n\n   l3 = ['PRP', 'VBG', 'VBZ', 'IN']\n   l4 = ['PRP', 'VBG', 'VBZ']\n   l5 = ['PRP', 'VBG', 'VBD']\n   l6 = ['NNP', 'VBG', 'VBD']\n   l7 = ['NN', 'VBG', 'VBZ']\n\n   l8 = ['NNP', 'VBZ', 'JJ']\n   l9 = ['NNP', 'VBZ', 'NN']\n\n   l10 = ['NNP', 'VBZ']\n   l11 = ['PRP', 'VBZ']\n   l12 = ['NNP', 'NN', 'IN']\n   l13 = ['NN', 'VBZ']\n\n   l14 = ['DT', 'NNP', 'VBZ', 'JJ', 'IN']\n\n\n    # With the use of conditional statements the dictionary is compared with the list created above\n\n   if all(key in bucket for key in l14): #'NN', 'VBZ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + ' ' + line.words[bucket['JJ']] + ' ' + line.words[bucket['IN']] + '?'\n\n   elif all(key in  bucket for key in l1): #'NNP', 'VBG', 'VBZ', 'IN' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NNP']]+ ' '+ line.words[bucket['VBG']] + '?'\n\n    \n   elif all(key in  bucket for key in l2): #'NNP', 'VBG', 'VBZ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NNP']] +' '+ line.words[bucket['VBG']] + '?'\n\n    \n   elif all(key in  bucket for key in l3): #'PRP', 'VBG', 'VBZ', 'IN' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['PRP']]+ ' '+ line.words[bucket['VBG']] + '?'\n\n    \n   elif all(key in  bucket for key in l4): #'PRP', 'VBG', 'VBZ' in sentence.\n      question = 'What ' + line.words[bucket['PRP']] +' '+  ' does ' + line.words[bucket['VBG']]+ ' '+  line.words[bucket['VBG']] + '?'\n\n   elif all(key in  bucket for key in l7): #'NN', 'VBG', 'VBZ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NN']] +' '+ line.words[bucket['VBG']] + '?'\n\n   elif all(key in bucket for key in l8): #'NNP', 'VBZ', 'JJ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + '?'\n\n   elif all(key in bucket for key in l9): #'NNP', 'VBZ', 'NN' in sentence\n      question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + '?'\n\n   elif all(key in bucket for key in l11): #'PRP', 'VBZ' in sentence.\n      if line.words[bucket['PRP']] in ['she','he']:\n          question = 'What' + ' does ' + line.words[bucket['PRP']].lower() + ' ' + line.words[bucket['VBZ']].singularize() + '?'\n\n   elif all(key in bucket for key in l10): #'NNP', 'VBZ' in sentence.\n      question = 'What' + ' does ' + line.words[bucket['NNP']] + ' ' + line.words[bucket['VBZ']].singularize() + '?'\n\n   elif all(key in bucket for key in l13): #'NN', 'VBZ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NN']] + '?'\n    \n \n\n    # When the tags are generated 's is split to ' and s. To overcome this issue.\n   if 'VBZ' in bucket and line.words[bucket['VBZ']] == \"’\":\n      question = question.replace(\" ’ \",\"'s \")\n\n   # Print the genetated questions as output.\n   if question != '':\n      print('\\n', 'Question: ' + question )\n\n      return {'question':question,'answer':answer}\n      # print('\\n', 'Question: ' + question )\n   \n\n@app.route('/')\ndef student():\n   return render_template('form.html')\n\n@app.route('/result',methods = ['POST', 'GET'])\ndef result():\n   global verbose \n   verbose = False\n   text_input = ''\n   if request.method == 'POST':\n      result = request.form\n      for key, value in result.items():\n         text_input += value\n      data = (<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">parse(text_input)</div>)\n     \n      return render_template(\"result.html\",result = data)\n   else:\n      return render_template('form.html')\n\n\nif __name__ == '__main__':\n   app.run(debug = True)\n   #https://github.com/huudangdev/generator-question-textblob-nlp/blob/master/app</code></pre></div>",
    "sec_2": "<div class=\"codeBlock hljs python\" id=\"sec_2\"><pre id=\"sec_2_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob import Word\n\nword1 = Word(\"apples\")\nprint(\"apples:\", <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">word1.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize()</div>)\n\nword2 = Word(\"media\")\nprint(\"media:\", word2.lemmatize())\n\nworfir = Word(\"greater\")\nprint(\"greater:\", worfir.lemmatize(\"a\"))\n\nfor word, pos in <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\">text_blob_object.<span class=\"fea_Part_of_Speech_keys udls\">tag</span>s</div>:\n    print(word + \" =&gt; \" + pos)\n\ntext = (\"Football is a good game. It has many health benefit\")\ntext_blob_object = TextBlob(text)\n<div class=\"highlights fea_text_simplify\" id=\"text_simplify_0\" style=\"display: inline;\">print(text_blob_object.words.pluralize())\nprint(text_blob_object.words.singularize())</div>\n\n    #https://stackabuse.com/python-for-nlp-introduction-to-the-textblob-library/</code></pre></div>",
    "sec_12": "<div class=\"codeBlock hljs python\" id=\"sec_12\"><pre id=\"sec_12_code\"><code class=\"python\">from flask import Flask, request, jsonify\nfrom textblob import TextBlob, Word\nfrom textblob.exceptions import NotTranslated\napp = Flask(__name__)\n\nfrom signal import *\n\n@app.route(\"/sentiment\")\ndef singularize():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\t\n\treturn jsonify(blob.sentiment)\n\n@app.route(\"/singularize\")\ndef sentiment():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\t\n\treturn jsonify(blob.words.singularize())\n\n@app.route(\"/lemmatize\")\ndef lemmatize():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\t\n\treturn jsonify(<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">blob.words.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize()</div>)\n\n@app.route(\"/correct\")\ndef correct():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\t\n\treturn jsonify({'correct':str(<div class=\"highlights fea_spellcheck\" id=\"spellcheck_0\" style=\"display: inline;\">blob.correct()</div>)})\n\n@app.route(\"/spelling\")\ndef spelling():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\n\tsuggestions = {}\n\tfor token in blob.words:\n\t\tword = Word(token)\n\t\tsuggestions[token] = word.spellcheck()\n\t\t\n\treturn jsonify(suggestions)\n\n@app.route(\"/language\")\ndef language():\n\ttext = request.args.get('text').strip()\n\tblob = TextBlob(text)\n\t\n\treturn jsonify({\"language\":blob.detect_language()})\n\n@app.route(\"/translate\")\ndef translate():\n\ttext = request.args.get('text').strip()\n\tl_from = request.args.get('from')\n\tl_to = request.args.get('to')\n\n\tblob = TextBlob(text)\n\n\tif l_from is None:\n\t\t<div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">l_from = blob.<span class=\"fea_language_detection_keys udls\">detect</span>_<span class=\"fea_language_detection_keys udls\">language</span>()</div>\n\t\n\ttry:\n\t\t<div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">translated = blob.translate(from_lang = l_from, to = l_to)</div>\n\texcept NotTranslated:\n\t\ttranslated = text\t\t\n\n\treturn jsonify({\"translation\":str(translated)})\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=8593)\n    #https://github.com/dpasch01/textblob-service/blob/master/textblob-service</code></pre></div>",
    "sec_6": "<div class=\"codeBlock hljs python\" id=\"sec_6\"><pre id=\"sec_6_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob.taggers import NLTKTagger\n<div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">nltk_<span class=\"fea_tagger_keys udls\">tagge</span>r = NLTK<span class=\"fea_tagger_keys udls\">Tagge</span>r()</div>\nblob = TextBlob(\"Tag! You're It!\", pos_tagger=nltk_tagger)\n<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\">blob.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>s</div>\n#https://textblob.readthedocs.io/en/dev/advanced_usage.html</code></pre></div>",
    "sec_9": "<div class=\"codeBlock hljs python\" id=\"sec_9\"><pre id=\"sec_9_code\"><code class=\"python\">import textblob\n<div class=\"highlights fea_text_simplify\" id=\"text_simplify_0\" style=\"display: inline;\">stringText = textblob.TextBlob(str(list(dataset[\"Summary\"]))).lower()</div>\nwords = stringText.words\nwordCount = {}\nignore = ['a', 'an', 'the', \"'the\", 'and', 'to', 'of', 'in', 'into', 'is', 'was', 'on', 'at', 'from', 'with',\n          'while', 'for', \"'s\", 'as', 'not', 'by', 'after', 'during']\n\nfor word in words:\n    if word in ignore:\n        continue\n    if word in wordCount:\n        wordCount[word] = wordCount[word] + 1\n    else:\n        wordCount[word] = 1\n\nimport operator\nsorted_word = sorted(wordCount.items(), key=operator.itemgetter(1), reverse=True)[:500]\nwith open(\"sorted-word-count.txt\", \"w\") as f:\n    f.write(str(sorted_word))\n\nreasons = ['weather', 'fire', 'shot down', 'stall/runway', 'pilot/crew error', 'systems failure']\n\nexpresion = ['((poor|bad).*(weather|visibility)|thunderstorm|fog)','(caught fire)|(caught on fire)', \n           '(shot down) | (terrorist) | (terrorism)', '(stall)|(runway)', '(pilot|crew) (error|fatigue)',\n            '(engine.*(fire|fail))|(structural fail)|(fuel leak)|(langing gear)|(turbulence)|(electrical)|(out of fuel)|(fuel.*exhaust)']\n\ndataset['Label'] = pd.Series(np.nan, index=dataset.index)\n\ntrainData = []\nfor x in range(len(dataset)):\n    if dataset.loc[x,\"Summary\"] is np.nan:\n        dataset.loc[x,\"Label\"] = \"unknown\"\n    else:\n        for y in range(len(expresion)):\n            if re.search(expresion[y], dataset.loc[x,\"Summary\"].lower()):\n                dataset.loc[x,\"Label\"] = reasons[y]\n                temp = dataset.loc[x,\"Summary\"].lower(), dataset.loc[x,\"Label\"]\n                trainData.append(temp)\n                break\n\nfrom textblob.classifiers import NaiveBayesClassifier\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">cl = <span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>(<span class=\"fea_classification_keys udls\">train</span>Data)</div>\n\nreasons.append(\"unknown\")\nfor x in range(30,len(dataset)):\n    if dataset.loc[x,\"Label\"] in reasons:\n       continue\n    else:\n        dataset.loc[x,\"Label\"] = cl.classify(dataset.loc[x,\"Summary\"])\n        #https://github.com/arif-zaman/airplane-crash/blob/master/Airplane.ipynb</code></pre></div>",
    "sec_27": "<div class=\"codeBlock hljs python\" id=\"sec_27\"><pre id=\"sec_27_code\"><code class=\"python\">import argparse\nimport sys\nfrom textblob import TextBlob\n\nDEFAULT_SUBJECT_LIMIT = 50\nDEFAULT_BODY_LIMIT = 72\n\n\nclass CliColors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\n\ndef check_subject_is_separated_from_body(commit_message):\n    lines = commit_message.splitlines()\n    if len(lines) &gt; 1:\n        # The second line should be empty\n        check_result = not lines[1]\n    else:\n        # If there is just one line then this rule doesn't apply\n        check_result = True\n    print_result(check_result, \"Separate subject from body with a blank line\")\n\n    return check_result\n\n\ndef check_subject_is_not_too_long(commit_message, subject_limit):\n    lines = commit_message.splitlines()\n    check_result = len(lines[0]) &lt;= subject_limit\n    print_result(check_result, \"Limit the subject line to \" +\n                 str(subject_limit) + \" characters\")\n\n    return check_result\n\n\ndef check_subject_is_capitalized(commit_message):\n    lines = commit_message.splitlines()\n    # Check if first character is in upper case\n    check_result = lines[0][0].isupper()\n    print_result(check_result, \"Capitalize the subject line\")\n\n    return check_result\n\n\ndef check_subject_does_not_end_with_period(commit_message):\n    lines = commit_message.splitlines()\n    check_result = not lines[0].endswith(\".\")\n    print_result(check_result, \"Do not end the subject line with a period\")\n\n    return check_result\n\n\ndef check_subject_uses_imperative(commit_message):\n    first = commit_message.splitlines()[0]\n    <div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">third_person_singular_present_<span class=\"fea_n_grams_keys udls\">verb</span> = \"VBZ\"\n    non_third_person_singular_present_<span class=\"fea_n_grams_keys udls\">verb</span> = \"VBP\"</div>\n    # The default NLTK parser is not very good with imperative sentences\n    # so we prefix the commit message with a personal pronoun so to\n    # help it determine easier whether the upcoming word is a verb\n    # and not a noun.\n    # We will prefix in two different ways, so to avoid false results.\n    # Read more here: https://stackoverflow.com/a/30823202/6485320\n    # and here: https://stackoverflow.com/a/9572724/6485320\n    third_person_prefix = \"It \"\n    words_in_third_person_prefix_blob = len(third_person_prefix.split())\n    non_third_person_prefix = \"You \"\n    words_in_non_third_person_prefix_blob = len(\n        non_third_person_prefix.split())\n    # Turn the first character into a lowercase so to make it easier for\n    # the parser to determine whether the word is a verb and its tense\n    first_character_in_lowercase = first[0].lower()\n    first = first_character_in_lowercase + first[1:]\n    third_person_blob = TextBlob(third_person_prefix + first)\n    non_third_person_blob = TextBlob(non_third_person_prefix + first)\n\n    first_word, third_person_result = third_person_blob.tags[words_in_third_person_prefix_blob]\n    _, non_third_person_result = non_third_person_blob.tags[words_in_non_third_person_prefix_blob]\n\n    # We need to determine whether the first word is a non-third person verb\n    # when parsed in a non-third person blob. However, there were some\n    # false positives so we use a third person blob to ensure it is not a\n    # third person verb. Unfortunately, there were now some false negatives\n    # due to verbs in a non-third person form, being classified as being in\n    # third person, when parsed in the third person blob.\n    # So, we ultimately check if the verb ends with an 's' which is a pretty\n    # good indicator of a third person, simple present tense verb.\n    <div class=\"highlights fea_text_simplify\" id=\"text_simplify_0\" style=\"display: inline;\">check_res<span class=\"fea_text_simplify_keys udls\">ult</span> = non_third_person_res<span class=\"fea_text_simplify_keys udls\">ult</span> == non_third_person_singular_<span class=\"fea_text_simplify_keys udls\">pre</span>sent_verb and (\n        third_person_res<span class=\"fea_text_simplify_keys udls\">ult</span> != third_person_singular_<span class=\"fea_text_simplify_keys udls\">pre</span>sent_verb or not first_word.endswith(\"s\"))</div>\n    print_result(check_result, \"Use the imperative mood in the subject line\")\n\n    return check_result\n#https://github.com/platisd/bad-commit-message-blocker/blob/master/bad_commit_message_blocker</code></pre></div>",
    "sec_1": "<div class=\"codeBlock hljs python\" id=\"sec_1\"><pre id=\"sec_1_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob import Word\ndocument = (\"In computer science, artificial intelligence (AI), \\\n            sometimes called machine intelligence, is intelligence \\\n            demonstrated by machines, in contrast to the natural intelligence \\\n            displayed by humans and animals. Computer science defines AI \\\n            research as the study of \\\"intelligent agents\\\": any device that \\\n            perceives its environment and takes actions that maximize its\\\n            chance of successfully achieving its goals.[1] Colloquially,\\\n            the term \\\"artificial intelligence\\\" is used to describe machines\\\n            that mimic \\\"cognitive\\\" functions that humans associate with other\\\n            human minds, such as \\\"learning\\\" and \\\"problem solving\\\".[2]\")\ntext_blob_object = TextBlob(document)\n<div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">for <span class=\"fea_n_grams_keys udls\">noun</span>_phrase in text_blob_ob<span class=\"fea_n_grams_keys udls\">ject</span>.<span class=\"fea_n_grams_keys udls\">noun</span>_phrases:\n    print(<span class=\"fea_n_grams_keys udls\">noun</span>_phrase)</div>\n\ntext = \"I love to watch football, but I have never played it\"\ntext_blob_object = TextBlob(text)\nfor ngram in <div class=\"highlights fea_n_grams\" id=\"n_grams_1\" style=\"display: inline;\">text_blob_ob<span class=\"fea_n_grams_keys udls\">ject</span>.<span class=\"fea_n_grams_keys udls\">ngrams</span>(2)</div>:\n    print(ngram)\n    #https://stackabuse.com/python-for-nlp-introduction-to-the-textblob-library/</code></pre></div>",
    "sec_8": "<div class=\"codeBlock hljs python\" id=\"sec_8\"><pre id=\"sec_8_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob.np_extractors import ConllExtractor\nextractor = ConllExtractor()\nblob = TextBlob(\"Python is a high-level programming language.\", np_extractor=extractor)\n<div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">blob.<span class=\"fea_n_grams_keys udls\">noun</span>_phrases</div>\n#https://textblob.readthedocs.io/en/dev/advanced_usage.html</code></pre></div>",
    "sec_17": "<div class=\"codeBlock hljs python\" id=\"sec_17\"><pre id=\"sec_17_code\"><code class=\"python\">from textblob import TextBlob\nfrom spellchecker import SpellChecker\n\n\na = input('Enter an Incorrect String : ')\nprint('Original Text : ' + str(a))\nb = TextBlob(a)\n<div class=\"highlights fea_spellcheck\" id=\"spellcheck_0\" style=\"display: inline;\">print('Corrected <span class=\"fea_spellcheck_keys udls\">Text</span> : ' + str(b.correct()))</div>\n\n\nspell = SpellChecker()\n# Find those words that may be misspelled\nmisspelled = spell.unknown(['Good', 'Evening'])\n\nfor word in misspelled:\n    # getting the one `most likely` answer\n    print(spell.correction(word))\n    # getting a list of `likely` options\n    print(spell.candidates(word))\n    #https://github.com/OjasBarawal/Spell-Checker/blob/main/app</code></pre></div>",
    "sec_26": "<div class=\"codeBlock hljs python\" id=\"sec_26\"><pre id=\"sec_26_code\"><code class=\"python\"># The main package to help us with our text analysis\nfrom textblob import TextBlob\n\n# For reading input files in CSV format\nimport csv\n\n# For doing cool regular expressions\nimport re\n\n# For sorting dictionaries\nimport operator\n\n\n# For plotting results\nimport numpy as np\nimport matplotlib.mlab as mlab\nimport matplotlibplot as thr\n\n# Intialize an empty list to hold all of our tweets\ntweets = []\n\n\n# A helper function that removes all the non ASCII characters\n# from the given string. Retuns a string with only ASCII characters.\ndef strip_non_ascii(string):\n    ''' Returns the string without non ASCII characters'''\n    stripped = (c for c in string if 0 &lt; ord(c) &lt; 127)\n    return ''.join(stripped)\n\n\n\n# LOAD AND CLEAN DATA\n\n# Load in the input file and process each row at a time.\n# We assume that the file has three columns:\n# 0. The tweet text.\n# 1. The tweet ID.\n# 2. The tweet publish date\n#\n# We create a data structure for each tweet:\n#\n# id:       The ID of the tweet\n# pubdate:  The publication date of the tweet\n# orig:     The original, unpreprocessed string of characters\n# clean:    The preprocessed string of characters\n# TextBlob: The TextBlob object, created from the 'clean' string\n\nwith open('newtwitter.csv', 'rb') as csvfile:\n    reader = csv.reader(csvfile, delimiter=',')\n    reader.next()\n    for row in reader:\n\n        tweet= dict()\n        tweet['orig'] = row[0]\n        tweet['id'] = int(row[1])\n        tweet['pubdate'] = int(row[2])\n\n        # Ignore retweets\n        if re.match(r'^RT.*', tweet['orig']):\n            continue\n\n        tweet['clean'] = tweet['orig']\n\n        # Remove all non-ascii characters\n        tweet['clean'] = strip_non_ascii(tweet['clean'])\n\n        # Normalize case\n        tweet['clean'] = tweet['clean'].lower()\n\n        # Remove URLS. (I stole this regex from the internet.)\n        tweet['clean'] = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', tweet['clean'])\n\n        # Fix classic tweet linpython\n        tweet['clean'] = re.sub(r'\\bthats\\b', 'that is', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bive\\b', 'i have', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bim\\b', 'i am', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bya\\b', 'yeah', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bcant\\b', 'can not', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bwont\\b', 'will not', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bid\\b', 'i would', tweet['clean'])\n        tweet['clean'] = re.sub(r'wtf', 'what the fuck', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bwth\\b', 'what the hell', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\br\\b', 'are', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bu\\b', 'you', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bk\\b', 'OK', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bsux\\b', 'sucks', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bno+\\b', 'no', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bcoo+\\b', 'cool', tweet['clean'])\n\n        # Emoticons?\n        # NOTE: Turns out that TextBlob already handles emoticons well, so the\n        # following is not actually needed.\n        # See http://www.datagenetics.com/blog/october52012/index.html\n        # tweet['clean'] = re.sub(r'\\b:\\)\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b:D\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b:\\(\\b', 'sad', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b:-\\)\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b=\\)\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b\\(:\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b:\\\\\\b', 'annoyed', tweet['clean'])\n\n        # Create textblob object\n        tweet['TextBlob'] = TextBlob(tweet['clean'])\n\n        # Correct spelling (WARNING: SLOW)\n        #tweet['TextBlob'] = tweet['TextBlob'].correct()\n\n        tweets.append(tweet)\n\n\n\n# DEVELOP MODELS\n\n<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">for tweet in tweets:\n    tweet['<span class=\"fea_sentiment_analysis_keys udls\">polarity</span>'] = float(tweet['TextBlob'].<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span>)\n    tweet['subjec<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>ity'] = float(tweet['TextBlob'].<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.subjec<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>ity)\n\n    if tweet['<span class=\"fea_sentiment_analysis_keys udls\">polarity</span>'] &gt;= 0.1:\n        tweet['<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>'] = 'posi<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e'\n    elif tweet['<span class=\"fea_sentiment_analysis_keys udls\">polarity</span>'] &lt;= -0.1:\n        tweet['<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>'] = 'nega<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e'\n    else:\n        tweet['<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>'] = 'neutral'</div>\n\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">tweets_sorted = sorted(tweets, key=lambda k: k['polarity'])</div>\n\n\n# EVALUATE RESULTS\n\n# First, print out a few example tweets from each sentiment category.\n\nprint \"\\n\\nTOP NEGATIVE TWEETS\"\nnegative_tweets = [d for d in tweets_sorted if d['sentiment'] == 'negative']\nfor tweet in negative_tweets[0:100]:\n    print \"id=%d, polarity=%.2f, clean=%s\" % (tweet['id'], tweet['polarity'], tweet['clean'])\n\nprint \"\\n\\nTOP POSITIVE TWEETS\"\npositive_tweets = [d for d in tweets_sorted if d['sentiment'] == 'positive']\nfor tweet in positive_tweets[-100:]:\n    print \"id=%d, polarity=%.2f, clean=%s\" % (tweet['id'], tweet['polarity'], tweet['clean'])\n\nprint \"\\n\\nTOP NEUTRAL TWEETS\"\nneutral_tweets = [d for d in tweets_sorted if d['sentiment'] == 'neutral']\nfor tweet in neutral_tweets[0:500]:\n    print \"id=%d, polarity=%.2f, clean=%s\" % (tweet['id'], tweet['polarity'], tweet['clean'])\n\n#https://github.com/stepthom/textblob-sentiment-analysis/blob/master/doAnalysis</code></pre></div>",
    "sec_29": "<div class=\"codeBlock hljs python\" id=\"sec_29\"><pre id=\"sec_29_code\"><code class=\"python\">import sys\nimport json\nimport time\nimport re\nimport requests\nimport nltk\nimport argparse\nimport logging\nimport string\ntry:\n    import urllib.parse as urlparse\nexcept ImportError:\n    import urlparse\nfrom tweepy.streaming import StreamListener\nfrom tweepy import API, Stream, OAuthHandler, TweepError\nfrom textblob import TextBlob\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom bs4 import BeautifulSoup\nfrom elasticsearch import Elasticsearch\nfrom random import randint, randrange\nfrom datetime import datetime\nfrom newspaper import Article, ArticleException\n\n# import elasticsearch host, twitter keys and tokens\nfrom config import *\n\n\nSTOCKSIGHT_VERSION = '0.1-b.12'\n__version__ = STOCKSIGHT_VERSION\n\nIS_PY3 = sys.version_info &gt;= (3, 0)\n\nif not IS_PY3:\n    print(\"Sorry, stocksight requires Python 3.\")\n    sys.exit(1)\n\n# sentiment text-processing url\nsentimentURL = 'http://text-processing.com/api/sentiment/'\n\n# tweet id list\ntweet_ids = []\n\n# file to hold twitter user ids\ntwitter_users_file = './twitteruserids.txt'\n\nprev_time = time.time()\nsentiment_avg = [0.0,0.0,0.0]\n\ndef sentiment_analysis(text):\n    \"\"\"Determine if sentiment is positive, negative, or neutral\n    algorithm to figure out if sentiment is positive, negative or neutral\n    uses sentiment polarity from TextBlob, VADER Sentiment and\n    sentiment from text-processing URL\n    could be made better :)\n    \"\"\"\n\n    # pass text into sentiment url\n    if args.websentiment:\n        ret = get_sentiment_from_url(text, sentimentURL)\n        if ret is None:\n            sentiment_url = None\n        else:\n            sentiment_url, neg_url, pos_url, neu_url = ret\n    else:\n        sentiment_url = None\n\n    # pass text into TextBlob\n    text_tb = TextBlob(text)\n\n    # pass text into VADER Sentiment\n    analyzer = SentimentIntensityAnalyzer()\n    <div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">text_vs = analyzer.polarity_<span class=\"fea_text_scoring_keys udls\">score</span>s(text)</div>\n\n    # determine sentiment from our sources\n    <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">if <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>_url is None:\n        if text_tb.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &lt; 0 and text_vs['compound'] &lt;= -0.05:\n            <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"nega<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\"\n        elif text_tb.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &gt; 0 and text_vs['compound'] &gt;= 0.05:\n            <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"posi<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\"\n        else:\n            <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"neutral\"\n    else:\n        if text_tb.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &lt; 0 and text_vs['compound'] &lt;= -0.05 and <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>_url == \"nega<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\":\n            <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"nega<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\"\n        elif text_tb.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &gt; 0 and text_vs['compound'] &gt;= 0.05 and <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>_url == \"posi<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\":\n            <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"posi<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\"\n        else:\n            <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"neutral\"</div>\n\n    # calculate average polarity from TextBlob and VADER\n    polarity = (text_tb.sentiment.polarity + text_vs['compound']) / 2\n\n    # output sentiment polarity\n    print(\"************\")\n    print(\"Sentiment Polarity: \" + str(round(polarity, 3)))\n\n    # output sentiment subjectivity (TextBlob)\n    print(\"Sentiment Subjectivity: \" + str(round(text_tb.sentiment.subjectivity, 3)))\n\n    # output sentiment\n    print(\"Sentiment (url): \" + str(sentiment_url))\n    print(\"Sentiment (algorithm): \" + str(sentiment))\n    print(\"Overall sentiment (textblob): \", text_tb.sentiment) \n    print(\"Overall sentiment (vader): \", text_vs) \n    print(\"sentence was rated as \", round(text_vs['neg']*100, 3), \"% Negative\") \n    print(\"sentence was rated as \", round(text_vs['neu']*100, 3), \"% Neutral\") \n    print(\"sentence was rated as \", round(text_vs['pos']*100, 3), \"% Positive\") \n    print(\"************\")\n\n    return polarity, text_tb.sentiment.subjectivity, sentiment\n#https://github.com/shirosaidev/stocksight/blob/master/sentiment</code></pre></div>",
    "sec_15": "<div class=\"codeBlock hljs python\" id=\"sec_15\"><pre id=\"sec_15_code\"><code class=\"python\">from textblob import TextBlob\n\n\ndef tweet_sentiment(text, verbose=False):\n    \"\"\"\n    The sentiment function of textblob returns two properties, polarity, and subjectivity.\n    Polarity is float which lies in the range of [-1,1] where 1 means positive statement\n    and -1 means a negative statement.\n    Subjective sentences generally refer to personal opinion, emotion or judgment whereas\n    objective refers to factual information. Subjectivity is also a float which lies in\n    the range of [0,1].\n    \"\"\"\n    # parse the tweet into textblob object\n    blob = TextBlob(text)\n    # we define the sentiment of sentence to be the product of its polarity and subjectivity\n    # tweet sentiment is the sum of sentiment for all sentences in a tweet\n    <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = sum(s.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> * s.subjec<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>ity for s in blob.sentences)</div>\n    # print if verbose\n    if verbose:\n        polarity = sum(s.polarity for s in blob.sentences)\n        subjectivity = sum(s.subjectivity for s in blob.sentences)\n        num_sentence = len(blob.sentences)\n        return text, num_sentence, polarity, subjectivity, sentiment\n    else:\n        return sentiment\n\n\ndef test():\n    sentences = [\n        '$AAPL so is this the price that gets split? If so, looks like it’ll be $125.50 a share on Monday. Nice.',\n        'Stocks head into September in high gear as Apple and Tesla split, and markets await the August jobs report',\n        'S&amp;P 500 SETS FRESH RECORD CLOSING HIGH OF 3,508.01',\n        'Massive $tsla dump be careful out there short term oversold tho $spy $amzn',\n        '$SPX is overbought but momentum is very very strong. My bet is unless we correct quickly this week, we are looking for a blow off top. ',\n        '$SPY reached 350 2 points from our target of 352.. RSI is overbought - sell and wait ti buy for later. Short $SHOP and $NVAX.',\n        'Slight setback, nothing to worry about. Outlook dismal. 28 trade session left - Target $SPX 2394.25',\n        'Russell looks bad. Big bearish RSI divergence and ejected from the channel after riding up the bottom rail.',\n    ]\n    print(' | '.join(['',' #','Sentence'+' '*92,'# sentence','polarity','subjectivity','sentiment','']))\n    print('-'*162)\n    for i,sentence in enumerate(sentences):\n        text, num_sentence, polarity, subjectivity, sentiment = tweet_sentiment(sentence, verbose=True)\n        print(f' | {i+1:2d} | {text[:100]: &lt;100} | {num_sentence: &gt;10} | {polarity:+8.2f} | {subjectivity:+12.2f} | {sentiment:+9.2f} |')\n\n\nif __name__ == '__main__':\n    test()\n    #https://github.com/quantumsnowball/AppleDaily20200907/blob/master/sentiment</code></pre></div>",
    "sec_20": "<div class=\"codeBlock hljs python\" id=\"sec_20\"><pre id=\"sec_20_code\"><code class=\"python\">import random\nimport re\nimport csv\nimport string\nimport operator\n\nfrom textblob import TextBlob\nfrom textblob.classifiers import NaiveBayesClassifier # update sentiment, if textblob returns neutral\n\ndef determineSentiment(sent_dict):\n\t# takes in a dictionary or sub-dictionary to return the sentiment in a list\n\tfinal_sent_dict = {}\n\tsentence_list = []\n\tfor speech in sent_dict:\n\t\ttext_sent = TextBlob(sent_dict[speech])\n\t\t#text_tag = text_sent.tags\n\t\tcounter = 1\n\t\tfor sentence in text_sent.sentences:\n\t\t\t#print(speech)\n\t\t\tfinal_sent_dict[speech + '_' + str(counter)] = (sentence.sentiment, sentence)\n\t\t\tcounter += 1 # each sub-sentence in a speech has it's own dictionary key\n\t<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">final_sent_dict[\"_average\"] = text_sent.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span></div> # beginning of an ordered dict\n\treturn final_sent_dict\n\ndef trainSentiment():\n\t# if the sentence is neutral, update to attribute sentiment based on key words\n\t# example: villian -&gt; negative, dying -&gt; negative, etc...\n\t# https://textblob.readthedocs.io/en/dev/classifiers.html#classifiers\n\n\t# train classifers on actual hamlet data\n\thamlet_train = [\n\t# act 1\n\t\t('this dreaded sight', 'neg'),\n\t\t('o god!', 'neg'),\n\t\t('o fie!', 'neg'),\n\t\t('break my heart,  for i must hold my tongue!', 'neg'),\n\t\t('funeral', 'neg'),\n\t\t('he was a man,  take him for all in all, i shall not look upon his like again', 'neg'),\n\t\t('i doubt some foul play would the night were come!', 'neg'),\n\t\t('foul deeds will rise', 'neg'),\n\t\t('pooh!', 'neg'),\n\t\t('angels and ministers of grace defend us!', 'neg'),\n\t\t('you shall not go', 'neg'),\n\t\t('hold off your hands', 'neg'),\n\t\t('my fate cries out' , 'neg'),\n\t\t(\"i'll make a ghost of him that lets me\" , 'neg'),\n\t\t('something is rotten in the state of denmark', 'neg'),\n\t\t('harrow up thy soul', 'neg'),\n\t\t('revenge', 'neg'),\n\t\t('incest', 'neg'),\n\t\t('adulterate', 'neg'),\n\t\t('beast', 'neg'),\n\t\t('lust', 'neg'),\n\t\t('a serpent stung me', 'neg'),\n\t\t('villain', 'neg'),\n\t\t('perturbed spirit!', 'neg')\n\t]\n\n\thamlet_test = [\n\t# act 2\n\t\t('dishonour', 'neg'),\n\t\t('taints of liberty', 'neg'),\n\t\t('flash and outbreak of a fiery mind', 'neg'),\n\t\t('falsehood', 'neg'),\n\t\t('fouled', 'neg'),\n\t\t('piteous', 'neg'),\n\t\t('i do not know', 'neg'),\n\t\t('i do fear it', 'neg'),\n\t\t('madness wherein now he raves', 'neg'),\n\t\t('madness', 'neg'),\n\t\t('indifferent children of the earth', 'neg'),\n\t\t('beggars bodies', 'neg'),\n\t\t('murder', 'neg'),\n\t\t('that he should weep for her?', 'neg'),\n\t\t('am i a coward?', 'neg'),\n\t\t('who calls me villain?', 'neg'),\n\t]\n\t<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">cl = <span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>(hamlet_<span class=\"fea_classification_keys udls\">train</span>)</div>\n\treturn cl\n\t#https://github.com/cyschneck/Billy-Bot/blob/master/shakespeare_sentiment</code></pre></div>",
    "sec_21": "<div class=\"codeBlock hljs python\" id=\"sec_21\"><pre id=\"sec_21_code\"><code class=\"python\">from TwitterSearch import *\nfrom textblob import TextBlob\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf8')\n\nfilepath = \"2017.txt\"\ntry:\n\tfp = open(\"2017.txt\",\"r\")\n\tdi = { }\n\tfor line in fp.read().splitlines():\n\t\tcnt = 0\n\t\tscore = 0\n\t\ttemp = 0;\n\t\t#print line\n\t\n\t\t\n\t\ttso = TwitterSearchOrder() \n\t\ttso.set_keywords([line]) \n\t\ttso.set_language('en') \n\t\ttso.set_include_entities(False)\n\t\ttso.set_count(100)\n\n# it's about time to create a TwitterSearch object with our secret tokens\n\t\tts = TwitterSearch(\n\t\t\t\tconsumer_key = \"XXXX\",\n        \t\tconsumer_secret = \"YY\",\n        \t\taccess_token = \"ZZ\",\n        \t\taccess_token_secret = \"MM\"\n\t\t )\n\n\n # this is where the fun actually starts :)\n\t\tfor tweet in ts.search_tweets_iterable(tso):\n\t\t\tif(cnt&lt;20):\n\t\t\t\t<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">analysis = TextBlob(tweet['text'])\n\t\t\t\tcnt=cnt+1;\n\t\t\t\ttemp = temp+1;\n\t\t\t\tif analysis.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &gt; 0:\n\t\t\t\t\t#print '1'\n\t\t\t\t\tscore=score+1\n\t\t\t\telif analysis.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> == 0:\n\t\t\t\t\t#print '0'\n\t\t\t\t\tscore = score;\n\t\t\t\telse:\n\t\t\t\t\tscore=score-1</div>\n\t\t\telse:\n\t\t\t\tbreak\n\t#F.write(tweet['text'])\n\t\t\t#print(tweet['text'])\n\t\tx = float(score)/float(temp)\n\t\t#print x\n\t\tdi.update({line : x})\n\td_view = [ (v,k) for k,v in di.iteritems() ]\n\td_view.sort(reverse=True) # natively sort tuples by first element\n\tfor v,k in d_view:\n\t\t\tprint v, k\n\t\t\nexcept TwitterSearchException as e: # take care of all those ugly errors if there are some\n\tprint(e)\n\t#https://github.com/avaiyang/Movie-Rating-and-Prediction-Model/blob/master/twittersearch</code></pre></div>",
    "sec_24": "<div class=\"codeBlock hljs python\" id=\"sec_24\"><pre id=\"sec_24_code\"><code class=\"python\">import os\nfrom datetime import datetime, timedelta\nfrom pickle import load\n\nimport pytz\nfrom flask import Flask, jsonify, redirect, render_template, request, session, url_for\nfrom flask_sqlalchemy import SQLAlchemy\nfrom textblob import TextBlob\n\nfrom model_nltk import predict_sentiment\n\napp = Flask(__name__, template_folder=\"templates\")\n\n# \"sqlite:///data.sqlite\"\n# /// for relative path\n# //// for absolute path\napp.config[\"SQLALCHEMY_DATABASE_URI\"] = os.environ.get(\n    \"DATABASE_URL\", \"sqlite:///data.sqlite\"\n)\napp.config[\"SQLALCHEMY_TRACK_MODIFICATIONS\"] = False\napp.config[\"SECRET_KEY\"] = os.environ.get(\"SECRET_KEY\", \"thisissecret\")\napp.config[\"PERMANENT_SESSION_LIFETIME\"] = timedelta(hours=12)\n\ndb = SQLAlchemy(app)\n\n# since the app is hosted on heroku so this line of code is to change the timezone\nIST = pytz.timezone(\"Asia/Kolkata\")\n\n\n# I have creted two models but I am using model_nltk because of its high accurcy and less execution time.\n# textblob is used for ploting the subjectivity and polarity curve for the input data\n\n# class for creating and initialising database\nclass New_Data(db.Model):\n\n    Id = db.Column(db.Integer, primary_key=True)\n    Text = db.Column(db.Text)\n    Sentiment = db.Column(db.String(20))\n    # .now(IST).strftime('%Y-%m-%d %H:%M:%S'))\n    Date = db.Column(\n        db.DateTime, default=datetime.now(IST).strftime(\"%Y-%m-%d %H:%M:%S\")\n    )\n\n    def __init__(self, Text, Sentiment):\n        self.Text = Text\n        self.Sentiment = Sentiment\n\n\n# loading classifier\nwith open(\"my_classifier.pickle\", \"rb\") as f:\n    classifier = load(f)\n\n\ndef allowed_file(filename):\n    \"\"\"Checking file extension i.e. text file or not\"\"\"\n    return \".\" in filename and filename.split(\".\")[1] == \"txt\"\n\n\n# route for home page\n@app.route(\"/\", methods=[\"POST\", \"GET\"])\ndef home():\n    if request.method == \"POST\":\n        sentence = str(request.form.get(\"twt\"))\n\n        <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = predict_<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>(sentence, classifier)</div>\n\n        # adding emoji to the sentiment\n        if sentiment == \"Positive\":\n            sentiment += \" \\U0001f600\"\n\n        elif sentiment == \"Negative\":\n            sentiment += \" \\U0001F641\"\n\n        else:\n            pass\n\n        # creating an instance of the data table for the database and commiting the changes\n        usr_data = New_Data(sentence, sentiment.split()[0])\n        try:\n            db.session.add(usr_data)\n            db.session.commit()\n        except:\n            pass\n\n        text = 'You have entered \"' + sentence + '\"'\n        return render_template(\n            \"index.html\", text=text, sentiment=\"Sentiment: \" + sentiment\n        )\n\n    return render_template(\"index.html\")\n\n\n# route for about page\n@app.route(\"/about\")\ndef about():\n    return render_template(\"about.html\")\n\n\n# route for members page\n@app.route(\"/member\")\ndef contact():\n    return render_template(\"members.html\")\n\n\n# route for fastapi\n# setting default value for the api\n@app.route(\"/fast-api/\", defaults={\"sentence\": \"Great\"})\n@app.route(\"/fast-api/&lt;sentence&gt;\")\ndef fast_api(sentence):\n    sentiment = predict_sentiment(sentence, classifier)\n\n    return jsonify({\"sentence\": sentence, \"sentiment\": sentiment})\n\n\n# setting post method for the api\n@app.route(\"/fastapi\", methods=[\"POST\"])\ndef fastapi():\n    text = request.form[\"text\"]\n    <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_1\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_keys udls\">polarity</span> = TextBlob(text).<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span>\n    if <span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &gt; 0:\n        <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"Posi<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\"\n    elif <span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &lt; 0:\n        <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"Nega<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\"\n    else:\n        <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"Neutral\"\n    return jsonify({\"<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>\": <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>})</div>\n#https://github.com/g-paras/sentiment-analysis-api/blob/master/app</code></pre></div>",
    "sec_25": "<div class=\"codeBlock hljs python\" id=\"sec_25\"><pre id=\"sec_25_code\"><code class=\"python\">import facebook as fb\nimport requests\nimport argparse\nimport textblob as tb\n\nFLAGS = None\n\ndef sentiment_analysis(post):\n\n    # Here's where the magic happens\n    <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">tb_msg = tb(post['message'])\n    score = tb_msg.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span></div>\n\n    print(\"Date: %s, From: %s\\n\", post['created_time'], post['from'])\n    print(\"%s\\nShared: %s, Score: %f\", post['message'], post['share'], score)\n\n\n\ndef connect(access_token, user):\n    graph = fb.GraphAPI(access_token)\n    profile = graph.get_object(user)\n\n    return graph, profile\n\n\ndef main():\n\n    access_token = FLAGS.access_token\n    user = FLAGS.profile\n\n    graph, profile = connect(access_token, user)\n    \n    posts = graph.get_connections(profile['id'], 'posts')\n\n\n    #Let's grab all the posts and analyze them!\n    while True:\n        try:\n            [sentiment_analysis(post=post) for post in posts['data']]\n            posts= requests.get(posts['paging']['next']).json()\n        except KeyError:\n            break\n            \n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Simple Facebook Sentiment Analysis Script')\n    parser.add_argument('--access_token', type=str, required=True, default='', help='Your Facebook API Access Token: https://developers.facebook.com/docs/graph-api/overview')\n    parser.add_argument('--profile', type=str, required=True, default='', help='The profile name to retrieve the posts from')\n    FLAGS = parser.parse_args()\n    main()\n    #https://github.com/cosimoiaia/Facebook-Sentiment-Analysis/blob/master/simple_facebook_sentiment_analysis</code></pre></div>",
    "sec_28": "<div class=\"codeBlock hljs python\" id=\"sec_28\"><pre id=\"sec_28_code\"><code class=\"python\">import re\nimport tweepy\nfrom tweepy import OAuthHandler\nfrom textblob import TextBlob\n \nclass TwitterClient(object):\n    '''\n    Generic Twitter Class for sentiment analysis.\n    '''\n    def __init__(self):\n        '''\n        Class constructor or initialization method.\n        '''\n       \n        consumer_key = 'XXXXXXXXXXXX'\n        consumer_secret = 'XXXXXXXXXXXX'\n        access_token = 'XXXXXXXXXXXX'\n        access_token_secret = 'XXXXXXXXXXXX'\n \n       \n        try:\n         \n            self.auth = OAuthHandler(consumer_key, consumer_secret)\n         \n            self.auth.set_access_token(access_token, access_token_secret)\n        \n            self.api = tweepy.API(self.auth)\n        except:\n            print(\"Error: Authentication Failed\")\n \n    def clean_tweet(self, tweet):\n        '''\n        Utility function to clean tweet text by removing links, special characters\n        using simple regex statements.\n        '''\n        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n \n    def get_tweet_sentiment(self, tweet):\n        '''\n        Utility function to classify sentiment of passed tweet\n        using textblob's sentiment method\n        '''\n        \n        <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">analysis = TextBlob(self.clean_tweet(tweet))\n       \n        if analysis.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &gt; 0:\n            return 'posi<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e'\n        elif analysis.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> == 0:\n            return 'neutral'\n        else:\n            return 'nega<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e'</div>\n    #https://github.com/vinitshahdeo/jobtweets/blob/master/jobtweets</code></pre></div>",
    "sec_30": "<div class=\"codeBlock hljs python\" id=\"sec_30\"><pre id=\"sec_30_code\"><code class=\"python\">import sys,tweepy,csv,re\nfrom textblob import TextBlob\nimport matplotlibplot as thr\n\n\nclass SentimentAnalysis:\n\n    def __init__(self):\n        self.tweets = []\n        self.tweetText = []\n\n    def DownloadData(self):\n        # authenticating\n        consumerKey = 'your key here'\n        consumerSecret = 'your key here'\n        accessToken = 'your key here'\n        accessTokenSecret = 'your key here'\n        auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n        auth.set_access_token(accessToken, accessTokenSecret)\n        api = tweepy.API(auth)\n\n        # input for term to be searched and how many tweets to search\n        searchTerm = input(\"Enter Keyword/Tag to search about: \")\n        NoOfTerms = int(input(\"Enter how many tweets to search: \"))\n\n        # searching for tweets\n        self.tweets = tweepy.Cursor(api.search, q=searchTerm, lang = \"en\").items(NoOfTerms)\n\n        # Open/create a file to append data to\n        csvFile = open('result.csv', 'a')\n\n        # Use csv writer\n        csvWriter = csv.writer(csvFile)\n\n\n        # creating some variables to store info\n        polarity = 0\n        positive = 0\n        wpositive = 0\n        spositive = 0\n        negative = 0\n        wnegative = 0\n        snegative = 0\n        neutral = 0\n\n\n        # iterating through tweets fetched\n        for tweet in self.tweets:\n            #Append to temp so that we can store in csv later. I use encode UTF-8\n            self.tweetText.append(self.cleanTweet(tweet.text).encode('utf-8'))\n            # print (tweet.text.translate(non_bmp_map))    #print tweet's text\n            analysis = TextBlob(tweet.text)\n            # print(analysis.sentiment)  # print tweet's polarity\n            <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_keys udls\">polarity</span> += analysis.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span></div>  # adding up polarities to find the average later\n\n            if (analysis.sentiment.polarity == 0):  # adding reaction of how people are reacting to find average later\n                neutral += 1\n            elif (analysis.sentiment.polarity &gt; 0 and analysis.sentiment.polarity &lt;= 0.3):\n                wpositive += 1\n            elif (analysis.sentiment.polarity &gt; 0.3 and analysis.sentiment.polarity &lt;= 0.6):\n                positive += 1\n            elif (analysis.sentiment.polarity &gt; 0.6 and analysis.sentiment.polarity &lt;= 1):\n                spositive += 1\n            elif (analysis.sentiment.polarity &gt; -0.3 and analysis.sentiment.polarity &lt;= 0):\n                wnegative += 1\n            elif (analysis.sentiment.polarity &gt; -0.6 and analysis.sentiment.polarity &lt;= -0.3):\n                negative += 1\n            elif (analysis.sentiment.polarity &gt; -1 and analysis.sentiment.polarity &lt;= -0.6):\n                snegative += 1\n\n\n        # Write to csv and close csv file\n        csvWriter.writerow(self.tweetText)\n        csvFile.close()\n\n        # finding average of how people are reacting\n        positive = self.percentage(positive, NoOfTerms)\n        wpositive = self.percentage(wpositive, NoOfTerms)\n        spositive = self.percentage(spositive, NoOfTerms)\n        negative = self.percentage(negative, NoOfTerms)\n        wnegative = self.percentage(wnegative, NoOfTerms)\n        snegative = self.percentage(snegative, NoOfTerms)\n        neutral = self.percentage(neutral, NoOfTerms)\n\n        # finding average reaction\n        polarity = polarity / NoOfTerms\n\n        # printing out data\n        print(\"How people are reacting on \" + searchTerm + \" by analyzing \" + str(NoOfTerms) + \" tweets.\")\n        print()\n        print(\"General Report: \")\n\n        if (polarity == 0):\n            print(\"Neutral\")\n        elif (polarity &gt; 0 and polarity &lt;= 0.3):\n            print(\"Weakly Positive\")\n        elif (polarity &gt; 0.3 and polarity &lt;= 0.6):\n            print(\"Positive\")\n        elif (polarity &gt; 0.6 and polarity &lt;= 1):\n            print(\"Strongly Positive\")\n        elif (polarity &gt; -0.3 and polarity &lt;= 0):\n            print(\"Weakly Negative\")\n        elif (polarity &gt; -0.6 and polarity &lt;= -0.3):\n            print(\"Negative\")\n        elif (polarity &gt; -1 and polarity &lt;= -0.6):\n            print(\"Strongly Negative\")\n\n        print()\n        print(\"Detailed Report: \")\n        print(str(positive) + \"% people thought it was positive\")\n        print(str(wpositive) + \"% people thought it was weakly positive\")\n        print(str(spositive) + \"% people thought it was strongly positive\")\n        print(str(negative) + \"% people thought it was negative\")\n        print(str(wnegative) + \"% people thought it was weakly negative\")\n        print(str(snegative) + \"% people thought it was strongly negative\")\n        print(str(neutral) + \"% people thought it was neutral\")\n\n        self.plotPieChart(positive, wpositive, spositive, negative, wnegative, snegative, neutral, searchTerm, NoOfTerms)\n\n\n    def cleanTweet(self, tweet):\n        # Remove Links, Special Characters etc from tweet\n        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) | (\\w +:\\ / \\ / \\S +)\", \" \", tweet).split())\n\n    # function to calculate percentage\n    def percentage(self, part, whole):\n        temp = 100 * float(part) / float(whole)\n        return format(temp, '.2f')\n\n    def plotPieChart(self, positive, wpositive, spositive, negative, wnegative, snegative, neutral, searchTerm, noOfSearchTerms):\n        labels = ['Positive [' + str(positive) + '%]', 'Weakly Positive [' + str(wpositive) + '%]','Strongly Positive [' + str(spositive) + '%]', 'Neutral [' + str(neutral) + '%]',\n                  'Negative [' + str(negative) + '%]', 'Weakly Negative [' + str(wnegative) + '%]', 'Strongly Negative [' + str(snegative) + '%]']\n        sizes = [positive, wpositive, spositive, neutral, negative, wnegative, snegative]\n        colors = ['yellowgreen','lightgreen','darkgreen', 'gold', 'red','lightsalmon','darkred']\n        patches, texts = thr.pie(sizes, colors=colors, startangle=90)\n        thr.legend(patches, labels, loc=\"best\")\n        thr.title('How people are reacting on ' + searchTerm + ' by analyzing ' + str(noOfSearchTerms) + ' Tweets.')\n        thr.axis('equal')\n        thr.tight_layout()\n        thr.show()\n\n\n\nif __name__== \"__main__\":\n    sa = SentimentAnalysis()\n    sa.DownloadData()\n    #https://github.com/the-javapocalypse/Twitter-Sentiment-Analysis/blob/master/main</code></pre></div>",
    "sec_14": "<div class=\"codeBlock hljs python\" id=\"sec_14\"><pre id=\"sec_14_code\"><code class=\"python\">from textblob import TextBlob                   ##language translation API\nfrom tkinter.scrolledtext import ScrolledText   ##for scrollable text box\n\n#-----translation function----------------------------------------------------------------------\ndef toLang(lang):\n    try:\n        output.delete(\"1.0\",END)            ##to delete previous entry in the output box\n        inputSTR = input_str.get(\"1.0\",END)\n        obj = TextBlob(str(inputSTR))\n        <div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">outputSTR = obj.translate(to=lang)</div>\n        output.insert(END,str(outputSTR))   ##insert output to the output box\n    except:\n        output.insert(END, \"**Please enter a meaningful word/sentence**\")\n        #https://github.com/DeepakJha01/GUI-Translator/blob/master/main/gui_translator</code></pre></div>",
    "sec_16": "<div class=\"codeBlock hljs python\" id=\"sec_16\"><pre id=\"sec_16_code\"><code class=\"python\">import webbrowser\nfrom textblob import TextBlob, exceptions\nfrom wox import Wox, WoxAPI\n\nLANGUAGE = 'ru'\n\ndef translate(query):\n    query_modified = query.strip().lower()\n    en = set(chr(i) for i in range(ord('a'), ord('z') + 1))\n    results = []\n    if query_modified:\n        try:\n            from_lang, to_lang = ('en', LANGUAGE) if query_modified[0] in en else (LANGUAGE, 'en')\n            translation = TextBlob(query_modified).translate(from_lang, to_lang)\n            results.append({\n                \"Title\": str(translation),\n                \"SubTitle\": query,\n                \"IcoPath\":\"Images/app.png\",\n                \"JsonRPCAction\":{'method': 'openUrl',\n                                 'parameters': [r'http://translate.google.com/#{}/{}/{}'.format(from_lang, to_lang, query)],\n                                 'dontHideAfterAction': False}\n            })\n        except exceptions.NotTranslated:\n            pass\n    if not results:\n        results.append({\n                \"Title\": 'Not found',\n                \"SubTitle\": '',\n                \"IcoPath\":\"Images/app.png\"\n            })\n    return results\n\nclass Translate(Wox):\n    def query(self, query):\n        return <div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">translate(query)</div>\n\n    def openUrl(self, url):\n        webbrowser.open(url)\n\nif __name__ == \"__main__\":\n    Translate()\n    #https://github.com/RomanKornev/Translate/blob/master/main</code></pre></div>",
    "sec_19": "<div class=\"codeBlock hljs python\" id=\"sec_19\"><pre id=\"sec_19_code\"><code class=\"python\">import googletrans\nfrom googletrans import Translator\n\n#from pygoogletranslation import Translator\n\nfrom textblob import TextBlob\n\nimport time\nepis = {}\n\ntranslator = Translator(service_urls = ['translate.google.com', 'translate.google.co.kr'])\n#translator = Translator()\n\n#bad_result_message = '**!!! BAD RESULT OF RECOGNITION. U CAN TRY AGAIN**'\n\n\nlang_dic = {value.title(): key for key, value in googletrans.LANGUAGES.items()}\nlang_dic_reversed = {key: f'*{value.capitalize()}*' for key, value in googletrans.LANGUAGES.items()}\n\nall_langs = list(lang_dic.keys())\n\n\n\ndef from_code_to_name(language):\n    return  lang_dic_reversed[language]\n\ndef smart_to_tidy(langs):\n    return [lang_dic_reversed[l] for l in langs]\n\ndef get_code_from_lang(lang):\n    return lang_dic[lang.tolower()]\n\n\ndef log_text(text, lang_list = ['en','ru']):\n    \n    result = []\n    \n    if len(text) &lt; 3:\n        result.append(f'*too shirt text*: {text}')\n        return result\n    \n\n    lang_of_text = translator.detect(text).lang\n    #if len(lang_of_text) == 0: lang_of_text = 'en'\n    #print(lang_of_text)\n\n    bool_list = [r != lang_of_text for r in lang_list]\n    \n    if all(bool_list):\n        bool_list.append(False)\n        lang_list.append(lang_of_text)\n    \n    for lang, it in zip(lang_list, bool_list):\n        result.append(f'{lang_dic_reversed[lang].upper()}:')\n        if it:\n            time.sleep(0.7)\n            #print(f\"{text}, {lang}, {lang_of_text}\")\n            txt = translator.translate(text, dest = lang, src = lang_of_text).text\n            result.append(txt)\n        else:\n            txt = text\n            result.append(f'_(original text)_ {text}')\n        result.append('')\n    \n    return result\n\n\ndef log_text_better(text, lang_list = ['en','ru']):\n    \n    result = []\n    \n    if len(text) &lt; 3:\n        result.append(f'*too shirt text*: {text}')\n        return result\n    \n    blob = TextBlob(text)\n\n    <div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">lang_of_text = blob.<span class=\"fea_language_detection_keys udls\">detect</span>_<span class=\"fea_language_detection_keys udls\">language</span>()</div>\n\n    bool_list = [r != lang_of_text for r in lang_list]\n    \n    if all(bool_list):\n        bool_list.append(False)\n        lang_list.append(lang_of_text)\n    \n    for lang, it in zip(lang_list, bool_list):\n        result.append(f'{lang_dic_reversed[lang].upper()}:')\n        if it:\n            time.sleep(1.3)\n            <div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">txt = str(blob.translate(from_lang = lang_of_text, to=lang))</div>\n            result.append(txt)\n        else:\n            txt = text\n            result.append(f'_(original text)_ {text}')\n        result.append('')\n    \n    return result\n\n\ndef get_langs_from_numbers(numbers):\n    \n    l1 = [all_langs[k-1] for k in numbers]\n    \n    return l1, [lang_dic[k] for k in l1]\n\n\n\n\n\n\nif __name__ == '__main__':\n\n    #trans = Translator()\n    #print(trans.detect('Привет'))\n    #print(trans.detect('Hello').lang)\n    #print(trans.translate('Привет'))\n\n\n\n\n   \n    defs = ['en','ru']\n    \n    r = log_text('hello my friend',defs)\n    \n    print('\\n'.join(r))\n\n    r = log_text('Ich will',defs)\n    \n    print('\\n'.join(r))\n    \n    print(defs)\n    \n    r = log_text_better('hello my friend',defs)\n    \n    print('\\n'.join(r))\n\n    lang = Translator().detect('Hello boy')\n    print(lang)\n\n#https://github.com/PasaOpasen/TranslatorBot/blob/master/translator_tools</code></pre></div>",
    "sec_22": "<div class=\"codeBlock hljs python\" id=\"sec_22\"><pre id=\"sec_22_code\"><code class=\"python\"># -*- coding: utf-8 -*-\n\"\"\"\nTranslator module that uses the Google Translate API.\n\nAdapted from Terry Yin's google-translate-python.\nLanguage detection added by Steven Loria.\n\"\"\"\n# I(Dhyey thumar) have done some modifications in this file.\n\n# import codecs\n# import re\nimport ctypes\nimport json\nimport sys\n\nfrom textblob.compat import PY2, request, urlencode\n# from textblob.exceptions import TranslatorError, NotTranslated\n\nsource_lang_code = str(sys.argv[1])\ninput_string = str(sys.argv[2])\ndest_lang_code = str(sys.argv[3])\n\n\nclass Translator(object):\n\n    \"\"\"A language translator and detector.\n\n    Usage:\n    ::\n        &gt;&gt;&gt; from textblob.translate import Translator\n        &gt;&gt;&gt; t = Translator()\n        &gt;&gt;&gt; t.translate('hello', from_lang='en', to_lang='fr')\n        u'bonjour'\n        &gt;&gt;&gt; t.detect(\"hola\")\n        u'es'\n    \"\"\"\n\n    url = \"http://translate.google.com/translate_a/t?client=webapp&amp;dt=bd&amp;dt=ex&amp;dt=ld&amp;dt=md&amp;dt=qca&amp;dt=rw&amp;dt=rm&amp;dt=ss&amp;dt=t&amp;dt=at&amp;ie=UTF-8&amp;oe=UTF-8&amp;otf=2&amp;ssel=0&amp;tsel=0&amp;kc=1\"\n\n    headers = {\n        'Accept': '*/*',\n        'Connection': 'keep-alive',\n        'User-Agent': (\n            'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36')\n    }\n\n    def translate(self, source, from_lang='auto', to_lang='en', host=None, type_=None):\n        \"\"\"Translate the source text from one language to another.\"\"\"\n        if PY2:\n            source = source.encode('utf-8')\n        data = {\"q\": source}\n        url = u'{url}&amp;sl={from_lang}&amp;tl={to_lang}&amp;hl={to_lang}&amp;tk={tk}'.format(\n            url=self.url,\n            from_lang=from_lang,\n            to_lang=to_lang,\n            tk=_calculate_tk(source),\n        )\n        response = self._request(url, host=host, type_=type_, data=data)\n        result = json.loads(response)\n        if isinstance(result, list):\n            try:\n                result = result[0]  # ignore detected language\n            except IndexError:\n                pass\n        # self._validate_translation(source, result)\n        return result\n\n    def detect(self, source, host=None, type_=None):\n        \"\"\"Detect the source text's language.\"\"\"\n        if PY2:\n            source = source.encode('utf-8')\n        # if len(source) &lt; 3:\n        #     return 1\n            # raise TranslatorError('Must provide a string with at least 3 characters.')\n        data = {\"q\": source}\n        url = u'{url}&amp;sl=auto&amp;tk={tk}'.format(\n            url=self.url, tk=_calculate_tk(source))\n        response = self._request(url, host=host, type_=type_, data=data)\n        result, language = json.loads(response)\n        return language\n\n    # def _validate_translation(self, source, result):\n    #     \"\"\"Validate API returned expected schema, and that the translated text\n    #     is different than the original string.\n    #     \"\"\"\n    #     if not result:\n    #         raise NotTranslated('Translation API returned and empty response.')\n    #     if PY2:\n    #         result = result.encode('utf-8')\n    #     if result.strip() == source.strip():\n    #         raise NotTranslated('Translation API returned the input string unchanged.')\n\n    def _request(self, url, host=None, type_=None, data=None):\n        encoded_data = urlencode(data).encode('utf-8')\n        req = request.Request(url=url, headers=self.headers, data=encoded_data)\n        if host or type_:\n            req.set_proxy(host=host, type=type_)\n        resp = request.urlopen(req)\n        content = resp.read()\n        return content.decode('utf-8')\n\n\n# def _unescape(text):\n#     \"\"\"Unescape unicode character codes within a string.\n#     \"\"\"\n#     pattern = r'\\\\{1,2}u[0-9a-fA-F]{4}'\n#     decode = lambda x: codecs.getdecoder('unicode_escape')(x.group())[0]\n#     return re.sub(pattern, decode, text)\n\n\ndef _calculate_tk(source):\n    \"\"\"Reverse engineered cross-site request protection.\"\"\"\n    # Source: https://github.com/soimort/translate-shell/issues/94#issuecomment-165433715\n    # Source: http://www.liuxiatool.com/t.php\n\n    tkk = [406398, 561666268 + 1526272306]\n    b = tkk[0]\n\n    if PY2:\n        d = map(ord, source)\n    else:\n        d = source.encode('utf-8')\n\n    def RL(a, b):\n        for c in range(0, len(b) - 2, 3):\n            d = b[c + 2]\n            d = ord(d) - 87 if d &gt;= 'a' else int(d)\n            xa = ctypes.c_uint32(a).value\n            d = xa &gt;&gt; d if b[c + 1] == '+' else xa &lt;&lt; d\n            a = a + d &amp; 4294967295 if b[c] == '+' else a ^ d\n        return ctypes.c_int32(a).value\n\n    a = b\n\n    for di in d:\n        a = RL(a + di, \"+-a^+6\")\n\n    a = RL(a, \"+-3^+b+-f\")\n    a ^= tkk[1]\n    a = a if a &gt;= 0 else ((a &amp; 2147483647) + 2147483648)\n    a %= pow(10, 6)\n\n    tk = '{0:d}.{1:d}'.format(a, a ^ b)\n    return tk\n\n\n<div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">translator_instance = Translator()</div>\nif source_lang_code == \"null\":\n    source_lang_code = translator_instance.detect(input_string)\n\ntranslated_string = translator_instance.translate(input_string, source_lang_code, dest_lang_code)\n\nif translated_string:\n    translated_string = str(translated_string)\n    result = []\n    for char in translated_string:\n        result.append(str(ord(char)))\n    seperator = ','\n    trans_string = seperator.join(result)\n    print(trans_string, end='\\n')\nelse:\n    print('empty response')\n    #https://github.com/dhyeythumar/Search-Engine/blob/master/Python_scripts/lang_trans1</code></pre></div>",
    "sec_23": "<div class=\"codeBlock hljs python\" id=\"sec_23\"><pre id=\"sec_23_code\"><code class=\"python\">''' This is language translation script '''\nfrom textblob import TextBlob\nimport sys\n\n\nsource_lang_code = str(sys.argv[1])\ninput_string = str(sys.argv[2])\ndest_lang_code = str(sys.argv[3])\n\ninput_blob = TextBlob(input_string)\n\nif source_lang_code == \"null\":\n    try:\n        <div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">source_lang_code = input_blob.<span class=\"fea_language_detection_keys udls\">detect</span>_<span class=\"fea_language_detection_keys udls\">language</span>()</div>\n        # print(\"Detected language:  \", source_lang_code)\n    except Exception as e:  # What if the input_string language is not detected\n        print(\"Error_1\", e)\n\ntry:\n    translated_string = <div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">input_blob.translate(\n        from_lang=source_lang_code, to=dest_lang_code)</div>\n    # print(translated_string) give character in unicode format.\n    # translated_string =&gt; is a &lt;class 'textblob.blob.TextBlob'&gt; type of object\n    # str(translated_string) =&gt; is a &lt;class 'str'&gt; type of object\n    # str(translated_string).encode('utf8) =&gt; is a &lt;class 'bytes'&gt; type of object\n\n    translated_string = str(translated_string)\n    result = []\n    for char in translated_string:\n        result.append(str(ord(char)))\n    seperator = ', '\n    trans_string = seperator.join(result)\n    print(trans_string, end='\\n')\n\nexcept Exception as e:  # What if the dest_lang code is null\n    print(\"Error_2\", e)\n    #https://github.com/dhyeythumar/Search-Engine/blob/master/Python_scripts/lang_trans</code></pre></div>",
    "sec_11": "<div class=\"codeBlock hljs python\" id=\"sec_11\"><pre id=\"sec_11_code\"><code class=\"python\">from textblob import TextBlob\nfrom time import sleep\nimport csv\nimport pandas as pd\n\ndf = pd.read_csv(\"cleaned_data01.csv\")\ntexts = df['cleaned_text']\n\ncounter = 188277\nremains = texts.shape[0] - 188277\nreq_counter = 0\n\n#========================= textblob ==========================\n\nwith open('tweet_lang02.csv', 'a', encoding=\"utf-8-sig\") as csvFile:\n    csvWriter = csv.writer(csvFile)\n    csvWriter.writerow(['text','language'])\n    for i in range(188277, texts.shape[0]):\n        counter +=1\n        remains -=1\n        req_counter +=1\n        print(counter, ' ', remains)\n        t = texts[i]\n        s = t.replace(\"#\",\"\")\n        s = s.replace(\"_\", \" \")\n        if req_counter == 10:\n            sleep(3)\n            req_counter = 0\n\n        b = TextBlob(s)\n        <div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">l = b.<span class=\"fea_language_detection_keys udls\">detect</span>_<span class=\"fea_language_detection_keys udls\">language</span>()</div>\n        csvWriter.writerow([t,l])\n        #https://github.com/khaledabbud/SA_of_Tweets_After_QS_Assassination_AR_FA/blob/master/textblob_lang_classification</code></pre></div>",
    "sec_18": "<div class=\"codeBlock hljs python\" id=\"sec_18\"><pre id=\"sec_18_code\"><code class=\"python\">from textblob import TextBlob\ndef log_text(text, lang_of_text=None, lang_list = ['en','ru'], trans_list = [True, True]):\n    \n    if len(text) &lt; 3:\n        print_on_yellow('too small text:',end=' ')\n        print(text)\n        return\n    \n    blob = TextBlob(text)\n    if lang_of_text == None:\n        <div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">lang_of_text = blob.<span class=\"fea_language_detection_keys udls\">detect</span>_<span class=\"fea_language_detection_keys udls\">language</span>()</div>\n\n    bool_list = [r != lang_of_text for r in lang_list]\n    \n    for lang, it, tc in zip(lang_list, bool_list, trans_list):\n        print(colored(f'\\t {lang}:', color = 'cyan', attrs=['bold']), end=' ')\n        if it:\n           <div class=\"highlights fea_language_detection\" id=\"language_detection_1\" style=\"display: inline;\"> txt = str(blob.translate(from_lang = lang_of_text, to = lang))</div>\n            print(txt)\n        else:\n            txt = text\n            print(f'{text} (original text)')\n        \n        if tc:\n            pron = epis[lang].transliterate(txt)\n            print('\\t\\t\\t',end=' ')\n            print_on_magenta(f'[{pron}]')\n#https://github.com/PasaOpasen/SpeechLogger/blob/master/ThirdTry/text_logger5</code></pre></div>",
    "sec_4": "<div class=\"codeBlock hljs python\" id=\"sec_4\"><pre id=\"sec_4_code\"><code class=\"python\">\nfrom textblob.classifiers import NaiveBayesClassifier\n\ntrain = [\n    ('amor', \"spanish\"),\n    (\"perro\", \"spanish\"),\n    (\"playa\", \"spanish\"),\n    (\"sal\", \"spanish\"),\n    (\"oceano\", \"spanish\"),\n    (\"love\", \"english\"),\n    (\"dog\", \"english\"),\n    (\"beach\", \"english\"),\n    (\"salt\", \"english\"),\n    (\"ocean\", \"english\")\n]\ntest = [\n    (\"ropa\", \"spanish\"),\n    (\"comprar\", \"spanish\"),\n    (\"camisa\", \"spanish\"),\n    (\"agua\", \"spanish\"),\n    (\"telefono\", \"spanish\"),\n    (\"clothes\", \"english\"),\n    (\"buy\", \"english\"),\n    (\"shirt\", \"english\"),\n    (\"water\", \"english\"),\n    (\"telephone\", \"english\")\n]\n\ndef extractor(word):\n    '''Extract the last letter of a word as the only feature.'''\n    feats = {}\n    last_letter = word[-1]\n    feats[\"last_letter({0})\".format(last_letter)] = True\n    return feats\n\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">lang_detector = <span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>(<span class=\"fea_classification_keys udls\">train</span>, feature_extractor=extractor)</div>\nprint(lang_detector.accuracy(test))\nprint(lang_detector.show_informative_features(5))\n#https://gist.github.com/sloria/6342158</code></pre></div>",
    "sec_5": "<div class=\"codeBlock hljs python\" id=\"sec_5\"><pre id=\"sec_5_code\"><code class=\"python\">from textblob.classifiers import NaiveBayesClassifier\nfrom textblob import TextBlob\ntrain = [\n    ('I love this sandwich.', 'pos'),\n    ('This is an amazing place!', 'pos'),\n    ('I feel very good about these beers.', 'pos'),\n    ('This is my best work.', 'pos'),\n    (\"What an awesome view\", 'pos'),\n    ('I do not like this restaurant', 'neg'),\n    ('I am tired of this stuff.', 'neg'),\n    (\"I can't deal with this\", 'neg'),\n    ('He is my sworn enemy!', 'neg'),\n    ('My boss is horrible.', 'neg')\n]\ntest = [\n    ('The beer was good.', 'pos'),\n    ('I do not enjoy my job', 'neg'),\n    (\"I ain't feeling dandy today.\", 'neg'),\n    (\"I feel amazing!\", 'pos'),\n    ('Gary is a friend of mine.', 'pos'),\n    (\"I can't believe I'm doing this.\", 'neg')\n]\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">cl = <span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>(<span class=\"fea_classification_keys udls\">train</span>)</div>\n# Classify some text\nprint(cl.classify(\"Their burgers are amazing.\"))  # \"pos\"\nprint(cl.classify(\"I don't like their pizza.\"))   # \"neg\"\n# Classify a TextBlob\nblob = TextBlob(\"The beer was amazing. But the hangover was horrible. \"\n\"My boss was not pleased.\", classifier=cl)\nprint(blob)\nprint(blob.classify())\nfor sentence in blob.sentences:\nprint(sentence)\nprint(sentence.classify())\n# Compute accuracy\nprint(\"Accuracy: {0}\".format(cl.accuracy(test)))\n# Show 5 most informative features\ncl.show_informative_features(5)\n#https://gist.github.com/sloria/6338202#file-tweet_classify-py</code></pre></div>",
    "sec_13": "<div class=\"codeBlock hljs python\" id=\"sec_13\"><pre id=\"sec_13_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob.sentiments import NaiveBayesAnalyzer\nimport matplotlibplot as thr\nimport random\n\ndef twitter_analysis(string):\n\t## Aain function starts\n\t## =====\n\n\tprocessedTweet = []\n\tpos = 0\n\tneg = 0\n\tneutral = 0\n\n\t#start process_tweet\n\tdef processTweet(tweet):\n\t    # process the tweets\n\t    \n\t    #Convert to lower case\n\t    tweet = tweet.lower()\n\t    #Convert www.* or https?://* to URL\n\t    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n\t    #Convert @username to AT_USER\n\t    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n\t    #Remove additional white spaces\n\t    tweet = re.sub('[\\s]+', ' ', tweet)\n\t    #Replace #word with word\n\t    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n\t    #trim\n\t    tweet = tweet.strip('\\'\"')\n\t    return tweet\n\t#end\n\n\ttd = TwitterData()\n\trawtweet = td.getData(string)\n\n\t#print \"1. Tweets colleted and pre-processing steps started\"\n\n\t#pre-processing tweets    \n\tfor i in range(1,len(rawtweet)):\n\t    processedTweet.append(processTweet(rawtweet[i]))\n\n\t#print \"2. preprocessing over and classifer begins\"\n\n\t# classifying the processed tweets by NaiveBayesAnalyzer\n\n\tfor i in range(1,len(processedTweet)):\n\t   <div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"> <span class=\"fea_classification_keys udls\">classifier</span> = TextBlob(processedTweet[i], analyzer=<span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span>Analyzer())\n\t    classification = <span class=\"fea_classification_keys udls\">classifier</span>.sentiment.classification</div>\n\t    #print processedTweet[i],\"Polarity=\",classification\n\t    \n\t    if classification == \"pos\":\n\t        pos = pos + 1\n\t        #print pos;\n\t    elif classification == \"neg\":     \n\t        neg = neg + 1\n\t        #print neg\n\t    else:\n\t        neutral = neutral + 1 \n\n\tfinal = []\n\tfinal.append(neg);\n\tfinal.append(neutral);\n\tfinal.append(pos);\n\n\treturn final  \n\t#https://github.com/muthuvenki/Trend-Analysis/blob/master/sentimental_anlysis/views</code></pre></div>",
    "thr_1": "<div class=\"codeBlock hljs python\" id=\"thr_1\"><pre id=\"thr_1_code\"><code class=\"python\">#!/usr/bin/env python\n# coding: utf8\n\"\"\"Train a convolutional neural network text classifier on the\nIMDB dataset, using the TextCategorizer component. The dataset will be loaded\nautomatically via Thinc's built-in dataset loader. The model is added to\nspacy.pipeline, and predictions are available via `doc.cats`. For more details,\nsee the documentation:\n* Training: https://spacy.io/usage/training\n\nCompatible with: spaCy v2.0.0+\n\"\"\"\nimport random\nfrom pathlib import Path\nimport thinc.extra.datasets\nimport spacy\nfrom spacy.util import minibatch, compounding\n\n\ndef <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_6\" style=\"background-color: #f6b73c; display: inline;\">main(model=None, output_dir=None, n_iter=20, n_texts=2000):</div>\n    if model is not None:\n        <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_0\" style=\"background-color: #f6b73c; display: inline;\">nlp = spacy.load(model)</div>  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank('en')  # create blank Language class\n        print(\"Created blank 'en' model\")\n\n    # add the text classifier to the pipeline if it doesn't exist\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_1\" style=\"background-color: #f6b73c; display: inline;\">if 'textcat' not in nlp.pipe_names:\n        textcat = nlp.create_pipe('textcat')\n        nlp.add_pipe(textcat, last=True)</div>\n    # otherwise, get it, so we can add labels to it\n    <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_2\" style=\"background-color: #f6b73c; display: inline;\">else:\n        textcat = nlp.get_pipe('textcat')</div>\n\n    # add label to text classifier\n    <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_4\" style=\"background-color: #f6b73c; display: inline;\">textcat.add_label('POSITIVE')</div>\n\n    # load the IMDB dataset\n    print(\"Loading IMDB data...\")\n    (train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\n    print(\"Using {} examples ({} training, {} evaluation)\"\n          .format(n_texts, len(train_texts), len(dev_texts)))\n    train_data = list(zip(train_texts,\n                          [{'cats': cats} for cats in train_cats]))\n\n    # get names of other pipes to disable them during training\n    <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_3\" style=\"background-color: #f6b73c; display: inline;\">other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n    with nlp.disable_pipes(*other_pipes):  # only train textcat\n        optimizer = nlp.begin_training()\n        print(\"Training the model...\")\n        print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n        for i in range(n_iter):\n            losses = {}\n            # batch up the examples using spaCy's minibatch\n            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n                           losses=losses)\n            with textcat.model.use_params(optimizer.averages):\n                # evaluate on the dev data split off in load_data()\n                scores = <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"background-color: #f6b73c; display: inline;\">evaluate(nlp.<span class=\"fea_tokenization_keys udls\">token</span>izer</div>, textcat, dev_texts, dev_cats)\n            print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n                  .format(losses['textcat'], scores['textcat_p'],\n                          scores['textcat_r'], scores['textcat_f']))</div>\n\n    # test the trained model\n    <div class=\"highlights fea_classification\" id=\"classification_0\" style=\"background-color: #f6b73c; display: inline;\">test_text = \"This movie sucked\"\n    doc = nlp(test_text)\n    print(test_text, doc.cats)</div>\n\n    <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_5\" style=\"background-color: #f6b73c; display: inline;\">if output_dir is not None:\n        output_dir = Path(output_dir)\n        if not output_dir.exists():\n            output_dir.mkdir()\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)</div>\n\n        # test the saved model\n        print(\"Loading from\", output_dir)\n        nlp2 = spacy.load(output_dir)\n        doc2 = nlp2(test_text)\n        print(test_text, doc2.cats)\n\n\ndef load_data(limit=0, split=0.8):\n    \"\"\"Load data from the IMDB dataset.\"\"\"\n    # Partition off part of the train data for evaluation\n    train_data, _ = thinc.extra.datasets.imdb()\n    random.shuffle(train_data)\n    train_data = train_data[-limit:]\n    texts, labels = zip(*train_data)\n    cats = [{'POSITIVE': bool(y)} for y in labels]\n    split = int(len(train_data) * split)\n    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n\n\ndef evaluate(tokenizer, textcat, texts, cats):\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"background-color: #f6b73c; display: inline;\">docs = (<span class=\"fea_tokenization_keys udls\">token</span>izer(text) for text in texts)</div>\n    tp = 1e-8  # True positives\n    fp = 1e-8  # False positives\n    fn = 1e-8  # False negatives\n    tn = 1e-8  # True negatives\n    for i, doc in enumerate(textcat.pipe(docs)):\n        gold = cats[i]\n        for label, score in doc.cats.items():\n            if label not in gold:\n                continue\n            if score &gt;= 0.5 and gold[label] &gt;= 0.5:\n                tp += 1.\n            elif score &gt;= 0.5 and gold[label] &lt; 0.5:\n                fp += 1.\n            elif score &lt; 0.5 and gold[label] &lt; 0.5:\n                tn += 1\n            elif score &lt; 0.5 and gold[label] &gt;= 0.5:\n                fn += 1\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f_score = 2 * (precision * recall) / (precision + recall)\n    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}</code></pre></div>",
    "thr_2": "<div class=\"codeBlock hljs python\" id=\"thr_2\"><pre id=\"thr_2_code\"><code class=\"python\">import spacy\nimport pandas as pd\nimport json\nfrom itertools import groupby\n\n# Download spaCy models:\n<div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_0\" style=\"background-color: #f6b73c; display: inline;\">models = {\n    'en_core_web_sm': spacy.load(\"en_core_web_sm\"),\n    'en_core_web_lg': spacy.load(\"en_core_web_lg\")\n}</div>\n\n# This function converts spaCy docs to the list of named entity spans in Label Studio compatible JSON format:\ndef doc_to_spans(doc):\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"background-color: #f6b73c; display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>s = [(tok.text, tok.idx, tok.ent_type_) for tok in doc]</div>\n    results = []\n    entities = set()\n    for entity, group in groupby(tokens, key=lambda t: t[-1]):\n        if not entity:\n            continue\n        group = list(group)\n        _, start, _ = group[0]\n        word, last, _ = group[-1]\n        text = ' '.join(item[0] for item in group)\n        end = last + len(word)\n        results.append({\n            'from_name': 'label',\n            'to_name': 'text',\n            'type': 'labels',\n            'value': {\n                'start': start,\n                'end': end,\n                'text': text,\n                'labels': [entity]\n            }\n        })\n        entities.add(entity)\n\n    return results, entities\n\n# Now load the dataset and include only lines containing \"Easter \":\ndf = pd.read_csv('lines_clean.csv')\ndf = df[df['line_text'].str.contains(\"Easter \", na=False)]\nprint(df.head())\ntexts = df['line_text']\n\n# Prepare Label Studio tasks in import JSON format with the model predictions:\nentities = set()\ntasks = []\n<div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_1\" style=\"background-color: #f6b73c; display: inline;\">for text in texts:\n    predictions = []\n    for model_name, nlp in models.items():\n        doc = nlp(text)\n        spans, ents = doc_to_spans(doc)\n        entities |= ents\n        predictions.append({'model_version': model_name, 'result': spans})\n    tasks.append({\n        'data': {'text': text},\n        'predictions': predictions\n    })</div>\n\n# Save Label Studio tasks.json\nprint(f'Save {len(tasks)} tasks to \"tasks.json\"')\nwith open('tasks.json', mode='w') as f:\n    json.dump(tasks, f, indent=2)\n    \n# Save class labels as a txt file\nprint('Named entities are saved to \"named_entities.txt\"')\nwith open('named_entities.txt', mode='w') as f:\n    f.write('\\n'.join(sorted(entities)))</code></pre></div>",
    "thr_3": "<div class=\"codeBlock hljs python\" id=\"thr_3\"><pre id=\"thr_3_code\"><code class=\"python\">import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport base64\nimport string\nimport re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('english')\ndf = pd.read_csv('research_paper.csv')\n\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, test_size=0.33, random_state=42)\nprint('Research title sample:', train['Title'].iloc[0])\nprint('Conference of this paper:', train['Conference'].iloc[0])\nprint('Training Data Shape:', train.shape)\nprint('Testing Data Shape:', test.shape)\n\nimport spacy\n<div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_0\" style=\"background-color: #f6b73c; display: inline;\">nlp = spacy.load('en_core_web_sm')</div>\npunctuations = string.punctuation\ndef cleanup_text(docs, logging=False):\n    texts = []\n    counter = 1\n    for doc in docs:\n        if counter % 1000 == 0 and logging:\n            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n        counter += 1\n        <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_1\" style=\"background-color: #f6b73c; display: inline;\">doc = nlp(doc, disable=['parser', 'ner'])</div>\n        tokens = [<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"background-color: #f6b73c; display: inline;\">tok.<span class=\"fea_lemmatization_keys udls\">lemma</span>_.lower().strip() for tok in doc if tok.<span class=\"fea_lemmatization_keys udls\">lemma</span>_ != '-PRON-'</div>]\n        tokens = [<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"background-color: #f6b73c; display: inline;\">tok for tok in <span class=\"fea_tokenization_keys udls\">token</span>s if tok not in stopwords and tok not in punctuations</div>]\n        tokens = ' '.join(tokens)\n        texts.append(tokens)\n    return pd.Series(texts)\nINFO_text = [text for text in train[train['Conference'] == 'INFOCOM']['Title']]\nIS_text = [text for text in train[train['Conference'] == 'ISCAS']['Title']]\nINFO_clean = cleanup_text(INFO_text)\nINFO_clean = ' '.join(INFO_clean).split()\nIS_clean = cleanup_text(IS_text)\nIS_clean = ' '.join(IS_clean).split()\nINFO_counts = Counter(INFO_clean)\nIS_counts = Counter(IS_clean)\nINFO_common_words = [word[0] for word in INFO_counts.most_common(20)]\nINFO_common_counts = [word[1] for word in INFO_counts.most_common(20)]\nfig = plt.figure(figsize=(18,6))\nsns.barplot(x=INFO_common_words, y=INFO_common_counts)\nplt.title('Most Common Words used in the research papers for conference INFOCOM')\nplt.show()</code></pre></div>",
    "thr_4": "<div class=\"codeBlock hljs python\" id=\"thr_4\"><pre id=\"thr_4_code\"><code class=\"python\">#!/usr/bin/env python\n# coding: utf8\n\"\"\"A simple example of extracting relations between phrases and entities using\nspaCy's named entity recognizer and the dependency parse. Here, we extract\nmoney and currency values (entities labelled as MONEY) and then check the\ndependency tree to find the noun phrase they are referring to – for example:\n$9.4 million --&gt; Net income.\n\nCompatible with: spaCy v2.0.0+\nLast tested with: v2.2.1\n\"\"\"\nfrom __future__ import unicode_literals, print_function\n\nimport plac\nimport spacy\n\n\nTEXTS = [\n    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n]\n\n\n@plac.annotations(\n    model=(\"Model to load (needs parser and NER)\", \"positional\", None, str)\n)\n<div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_0\" style=\"background-color: #f6b73c; display: inline;\">def main(model=\"en_core_web_sm\"):\n    nlp = spacy.load(model)\n    print(\"Loaded model '%s'\" % model)</div>\n    print(\"Processing %d texts\" % len(TEXTS))\n\n    <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_1\" style=\"background-color: #f6b73c; display: inline;\">for text in TEXTS:\n        doc = nlp(text)</div>\n        relations = extract_currency_relations(doc)\n        for r1, r2 in relations:\n            print(\"{:&lt;10}\\t{}\\t{}\".format(r1.text, r2.ent_type_, r2.text))\n\n\ndef filter_spans(spans):\n    # Filter a sequence of spans so they don't contain overlaps\n    # For spaCy 2.1.4+: this function is available as spacy.util.filter_spans()\n    get_sort_key = lambda span: (span.end - span.start, -span.start)\n    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n    result = []\n    seen_tokens = set()\n    for span in sorted_spans:\n        # Check for end - 1 here because boundaries are inclusive\n        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n            result.append(span)\n        seen_tokens.update(range(span.start, span.end))\n    result = sorted(result, key=lambda span: span.start)\n    return result\n\n\n<div class=\"highlights fea_dependency_parsing\" id=\"dependency_parsing_0\" style=\"background-color: #f6b73c; display: inline;\">def extract_currency_relations(doc):\n    # Merge entities and noun chunks into one token\n    spans = list(doc.ents) + list(doc.noun_chunks)\n    spans = filter_spans(spans)\n    with doc.retokenize() as retokenizer:\n        for span in spans:\n            retokenizer.merge(span)\n\n    relations = []\n    for money in filter(lambda w: w.ent_type_ == \"MONEY\", doc):\n        if money.dep_ in (\"attr\", \"dobj\"):\n            subject = [w for w in money.head.lefts if w.dep_ == \"nsubj\"]\n            if subject:\n                subject = subject[0]\n                relations.append((subject, money))\n        elif money.dep_ == \"pobj\" and money.head.dep_ == \"prep\":\n            relations.append((money.head.head, money))\n    return relations</div>\n\n\nif __name__ == \"__main__\":\n    plac.call(main)\n\n    # Expected output:\n    # Net income      MONEY   $9.4 million\n    # the prior year  MONEY   $2.7 million\n    # Revenue         MONEY   twelve billion dollars\n    # a loss          MONEY   1b</code></pre></div>",
    "thr_14": "<div class=\"codeBlock hljs python\" id=\"thr_14\"><pre id=\"thr_14_code\"><code class=\"python\">#!/usr/bin/env python\n# coding: utf8\n\"\"\"Example of training spaCy's named entity recognizer, starting off with an\nexisting model or a blank model.\n\nFor more details, see the documentation:\n* Training: https://spacy.io/usage/training\n* NER: https://spacy.io/usage/linguistic-features#named-entities\n\nCompatible with: spaCy v2.0.0+\nLast tested with: v2.2.4\n\"\"\"\nfrom __future__ import unicode_literals, print_function\n\nimport plac\nimport random\nimport warnings\nfrom pathlib import Path\nimport spacy\nfrom spacy.util import minibatch, compounding\n\n\n# training data\nTRAIN_DATA = [\n    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n]\n\n\n@plac.annotations(\n    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n)\n<div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_0\" style=\"background-color: #f6b73c; display: inline;\">def main(model=None, output_dir=None, n_iter=100):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")</div>\n\n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    <div class=\"highlights fea_named_entity_recognition\" id=\"named_entity_recognition_0\" style=\"background-color: #f6b73c; display: inline;\">if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")</div>\n\n    # add labels\n    <div class=\"highlights fea_named_entity_recognition\" id=\"named_entity_recognition_1\" style=\"background-color: #f6b73c; display: inline;\">for _, annotations in TRAIN_DATA:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])</div>\n\n    # get names of other pipes to disable them during training\n    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n    # only train NER\n    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n        # show warnings for misaligned entity spans once\n        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n\n        # reset and initialize the weights randomly – but only if we're\n        # training a new model\n        <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_1\" style=\"background-color: #f6b73c; display: inline;\">if model is None:\n            nlp.begin_training()\n        for itn in range(n_iter):\n            random.shuffle(TRAIN_DATA)\n            losses = {}</div>\n            # batch up the examples using spaCy's minibatch\n            <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_2\" style=\"background-color: #f6b73c; display: inline;\">batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.5,  # dropout - make it harder to memorise data\n                    losses=losses,\n                )\n            print(\"Losses\", losses)</div>\n\n    # test the trained model\n    for text, _ in TRAIN_DATA:\n        doc = nlp(text)\n        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n        print(\"Tokens\", [<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"background-color: #f6b73c; display: inline;\">(t.text, t.ent_type_, t.ent_iob) for t in doc</div>])\n\n    # save model to output directory\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        if not output_dir.exists():\n            output_dir.mkdir()\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)\n\n        # test the saved model\n        print(\"Loading from\", output_dir)\n        nlp2 = spacy.load(output_dir)\n        for text, _ in TRAIN_DATA:\n            doc = nlp2(text)\n            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n\n\nif __name__ == \"__main__\":\n    plac.call(main)\n\n    # Expected output:\n    # Entities [('Shaka Khan', 'PERSON')]\n    # Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n    # ('Khan', 'PERSON', 1), ('?', '', 2)]\n    # Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n    # Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n    # ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]</code></pre></div>",
    "thr_15": "<div class=\"codeBlock hljs python\" id=\"thr_15\"><pre id=\"thr_15_code\"><code class=\"python\">#!/usr/bin/env python\n# coding: utf8\n\"\"\"Train a convolutional neural network text classifier on the\nIMDB dataset, using the TextCategorizer component. The dataset will be loaded\nautomatically via Thinc's built-in dataset loader. The model is added to\nspacy.pipeline, and predictions are available via `doc.cats`. For more details,\nsee the documentation:\n* Training: https://spacy.io/usage/training\n\nCompatible with: spaCy v2.0.0+\n\"\"\"\nfrom __future__ import unicode_literals, print_function\nimport plac\nimport random\nfrom pathlib import Path\nimport thinc.extra.datasets\n\nimport spacy\nfrom spacy.util import minibatch, compounding\n\n\n@plac.annotations(\n    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n    n_texts=(\"Number of texts to train from\", \"option\", \"t\", int),\n    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n    init_tok2vec=(\"Pretrained tok2vec weights\", \"option\", \"t2v\", Path),\n)\n<div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_0\" style=\"background-color: #f6b73c; display: inline;\">def main(model=None, output_dir=None, n_iter=20, n_texts=2000, init_tok2vec=None):</div>\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        if not output_dir.exists():\n            output_dir.mkdir()\n\n    <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_1\" style=\"background-color: #f6b73c; display: inline;\">if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")</div>\n\n    # add the text classifier to the pipeline if it doesn't exist\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_2\" style=\"background-color: #f6b73c; display: inline;\">if \"textcat\" not in nlp.pipe_names:\n        textcat = nlp.create_pipe(\n            \"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"}\n        )\n        nlp.add_pipe(textcat, last=True)\n    # otherwise, get it, so we can add labels to it\n    else:\n        textcat = nlp.get_pipe(\"textcat\")</div>\n\n    # add label to text classifier\n    <div class=\"highlights fea_classification\" id=\"classification_0\" style=\"background-color: #f6b73c; display: inline;\">textcat.add_label(\"POSITIVE\")\n    textcat.add_label(\"NEGATIVE\")</div>\n\n    # load the IMDB dataset\n    print(\"Loading IMDB data...\")\n    (train_texts, train_cats), (dev_texts, dev_cats) = load_data()\n    train_texts = train_texts[:n_texts]\n    train_cats = train_cats[:n_texts]\n    print(\n        \"Using {} examples ({} training, {} evaluation)\".format(\n            n_texts, len(train_texts), len(dev_texts)\n        )\n    )\n    train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n\n    # get names of other pipes to disable them during training\n    pipe_exceptions = [\"textcat\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n   <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_3\" style=\"background-color: #f6b73c; display: inline;\"> with nlp.disable_pipes(*other_pipes):  # only train textcat\n        optimizer = nlp.begin_training()</div>\n        if init_tok2vec is not None:\n            with init_tok2vec.open(\"rb\") as file_:\n                textcat.model.tok2vec.from_bytes(file_.read())\n        print(\"Training the model...\")\n        print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n        <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_4\" style=\"background-color: #f6b73c; display: inline;\">batch_sizes = compounding(4.0, 32.0, 1.001)\n        for i in range(n_iter):\n            losses = {}</div>\n            # batch up the examples using spaCy's minibatch\n            <div class=\"highlights fea_Neural_Network_Models\" id=\"Neural_Network_Models_5\" style=\"background-color: #f6b73c; display: inline;\">random.shuffle(train_data)\n            batches = minibatch(train_data, size=batch_sizes)\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n            with textcat.model.use_params(optimizer.averages):\n                # evaluate on the dev data split off in load_data()\n                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)</div>\n            print(\n                \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(  # print a simple table\n                    losses[\"textcat\"],\n                    scores[\"textcat_p\"],\n                    scores[\"textcat_r\"],\n                    scores[\"textcat_f\"],\n                )\n            )\n\n    # test the trained model\n    test_text = \"This movie sucked\"\n    doc = nlp(test_text)\n    print(test_text, doc.cats)\n\n    if output_dir is not None:\n        with nlp.use_params(optimizer.averages):\n            nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)\n\n        # test the saved model\n        print(\"Loading from\", output_dir)\n        nlp2 = spacy.load(output_dir)\n        doc2 = nlp2(test_text)\n        print(test_text, doc2.cats)\n\n\ndef load_data(limit=0, split=0.8):\n    \"\"\"Load data from the IMDB dataset.\"\"\"\n    # Partition off part of the train data for evaluation\n    train_data, _ = thinc.extra.datasets.imdb()\n    random.shuffle(train_data)\n    train_data = train_data[-limit:]\n    texts, labels = zip(*train_data)\n    cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in labels]\n    split = int(len(train_data) * split)\n    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n\n\ndef evaluate(tokenizer, textcat, texts, cats):\n    docs = (tokenizer(text) for text in texts)\n    tp = 0.0  # True positives\n    fp = 1e-8  # False positives\n    fn = 1e-8  # False negatives\n    tn = 0.0  # True negatives\n    for i, doc in enumerate(textcat.pipe(docs)):\n        gold = cats[i]\n        for label, score in doc.cats.items():\n            if label not in gold:\n                continue\n            if label == \"NEGATIVE\":\n                continue\n            if score &gt;= 0.5 and gold[label] &gt;= 0.5:\n                tp += 1.0\n            elif score &gt;= 0.5 and gold[label] &lt; 0.5:\n                fp += 1.0\n            elif score &lt; 0.5 and gold[label] &lt; 0.5:\n                tn += 1\n            elif score &lt; 0.5 and gold[label] &gt;= 0.5:\n                fn += 1\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    if (precision + recall) == 0:\n        f_score = 0.0\n    else:\n        f_score = 2 * (precision * recall) / (precision + recall)\n    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}\n\n\nif __name__ == \"__main__\":\n    plac.call(main)</code></pre></div>",
    "thr_17": "<div class=\"codeBlock hljs python\" id=\"thr_17\"><pre id=\"thr_17_code\"><code class=\"python\">import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\n\ndf_amazon = pd.read_csv (\"datasets/amazon_alexa.tsv\", sep=\"\\t\")\n\nimport string\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\n# Create our list of punctuation marks\npunctuations = string.punctuation\n\n# Create our list of stopwords\nnlp = spacy.load('en')\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span>_<span class=\"fea_nlp_datasets_keys udls\">words</span> = spacy.lang.en.<span class=\"fea_nlp_datasets_keys udls\">stop</span>_<span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">STOP</span>_<span class=\"fea_nlp_datasets_keys udls\">WORDS</span></div>\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nparser = English()\n\n# Creating our tokenizer function\ndef spacy_tokenizer(sentence):\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    <div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">mytokens = <span class=\"fea_parsing_keys udls\">parser</span>(sentence)</div>\n\n    # Lemmatizing each token and converting each token into lowercase\n    mytokens = [ <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">word.<span class=\"fea_lemmatization_keys udls\">lemma</span>_.lower()</div>.strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n\n    # Removing stop words\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n\n    # return preprocessed list of tokens\n    return mytokens\n\n# Custom transformer using spaCy\nclass predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        # Cleaning Text\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\n# Basic function to clean the text\ndef clean_text(text):\n    # Removing spaces and converting text into lowercase\n    return text.strip().lower()\n\n<div class=\"highlights fea_word_vectors\" id=\"word_vectors_0\" style=\"display: inline;\">bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))</div>\n\ntfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)\n\nfrom sklearn.model_selection import train_test_split\n\nX = df_amazon['verified_reviews'] # the features we want to analyze\nylabels = df_amazon['feedback'] # the labels, or answers, we want to test against\n\nX_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)\n\n# Logistic Regression Classifier\nfrom sklearn.linear_model import LogisticRegression\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_keys udls\">classifier</span> = LogisticRegression()</div>\n\n# Create pipeline using Bag of Words\npipe = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', bow_vector),\n                 ('classifier', classifier)])\n\n# model generation\npipe.fit(X_train,y_train)\n\nfrom sklearn import metrics\n# Predicting with a test dataset\npredicted = pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\nprint(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\nprint(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))</code></pre></div>",
    "thr_10": "<div class=\"codeBlock hljs python\" id=\"thr_10\"><pre id=\"thr_10_code\"><code class=\"python\"># Check if word vector is available\nimport spacy\n\n# Loading a spacy model\nnlp = spacy.load(\"en_core_web_md\")\ntokens = nlp(\"I am an excellent cook\")\n\nfor token in tokens:\n  print(token.text ,' ',token.has_vector)\n  print(token.text,' ',<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>.vector_norm</div>)\n\nreview_1=nlp(' The food was amazing')\nreview_2=nlp('The food was excellent')\nreview_3=nlp('I did not like the food')\nreview_4=nlp('It was very bad experience')\n\n<div class=\"highlights fea_text_similarity\" id=\"text_similarity_0\" style=\"display: inline;\">score_1=review_1.similarity(review_2)</div>\nprint('Similarity between review 1 and 2',score_1)\n\n<div class=\"highlights fea_text_similarity\" id=\"text_similarity_1\" style=\"display: inline;\">score_2=review_3.similarity(review_4)</div>\nprint('Similarity between review 3 and 4',score_2)\n\n#https://www.machinelearningplus.com/spacy-tutorial-nlp/#mergingandsplittingtokenswithretokenize</code></pre></div>",
    "thr_12": "<div class=\"codeBlock hljs python\" id=\"thr_12\"><pre id=\"thr_12_code\"><code class=\"python\">\"\"\"\nThis script extracts features from the transcript txt file and saves them to .csv files\nso they can be used in any toolkkit.\n\"\"\"\n\nimport csv\nimport spacy\n\n\ndef main():\n    \"\"\"Loads the model and processes it.\n    \n    The model used can be installed by running this command on your CMD/Terminal:\n\n    python -m spacy download es_core_news_md\n    \n    \"\"\"\n\n    corpus = open(\"transcript_clean.txt\", \"r\", encoding=\"utf-8\").read()\n    nlp = spacy.load(\"es_core_news_md\")\n\n    # Our corpus is bigger than the default limit, we will set\n    # a new limit equal to its length.\n    nlp.max_length = len(corpus)\n\n    doc = nlp(corpus)\n\n    get_tokens(doc)\n    get_entities(doc)\n    get_sentences(doc)\n\n\ndef <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">get_<span class=\"fea_tokenization_keys udls\">token</span>s(doc)</div>:\n    \"\"\"Get the tokens and save them to .csv\n\n    Parameters\n    ----------\n    doc : spacy.doc\n        A doc object.\n\n    \"\"\"\n\n    data_list = [[\"text\", \"text_lower\", \"lemma\", \"lemma_lower\",\n                  \"part_of_speech\", \"is_alphabet\", \"is_stopword\"]]\n\n    for token in doc:\n        data_list.append([\n            <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>.text, <span class=\"fea_tokenization_keys udls\">token</span>.lower_</div>, <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">token.<span class=\"fea_lemmatization_keys udls\">lemma</span>_, token.<span class=\"fea_lemmatization_keys udls\">lemma</span>_.lower()</div>,\n            <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">token</span>.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_</div>, token.is_alpha, token.is_stop\n        ])\n\n    with open(\"./tokens.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as tokens_file:\n        csv.writer(tokens_file).writerows(data_list)\n\n\ndef get_entities(doc):\n    \"\"\"Get the entities and save them to .csv\n\n    Parameters\n    ----------\n    doc : spacy.doc\n        A doc object.\n\n    \"\"\"\n\n    data_list = [[\"text\", \"text_lower\", \"label\"]]\n\n    for ent in doc.ents:\n        data_list.append([ent.text, ent.lower_, ent.label_])\n\n    with open(\"./entities.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as entities_file:\n        csv.writer(entities_file).writerows(data_list)\n\n\ndef get_sentences(doc):\n    \"\"\"Get the sentences, score and save them to .csv\n\n    You will require to download the dataset (zip) from the following url:\n\n    https://www.kaggle.com/rtatman/sentiment-lexicons-for-81-languages\n\n    Once downloaded you will require to extract 2 .txt files:\n\n    negative_words_es.txt\n    positive_words_es.txt\n\n    Parameters\n    ----------\n    doc : spacy.doc\n        A doc object.\n\n    \"\"\"\n\n    # Load positive and negative words into lists.\n    with open(\"positive_words_es.txt\", \"r\", encoding=\"utf-8\") as temp_file:\n        positive_words = temp_file.read().splitlines()\n\n    with open(\"negative_words_es.txt\", \"r\", encoding=\"utf-8\") as temp_file:\n        negative_words = temp_file.read().splitlines()\n\n    data_list = [[\"text\", \"score\"]]\n\n    for sent in doc.sents:\n\n        # Only take into account real sentences.\n        if len(sent.text) &gt; 10:\n\n            score = 0\n\n            # Start scoring the sentence.\n            for word in sent:\n\n                if word.lower_ in positive_words:\n                    score += 1\n\n                if word.lower_ in negative_words:\n                    score -= 1\n\n            data_list.append([sent.text, score])\n\n\n    with open(\"./sentences.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as sentences_file:\n        csv.writer(sentences_file).writerows(data_list)\n\n\nif __name__ == \"__main__\":\n\n    main()\n    https://github.com/PhantomInsights/mexican-government-report/blob/master/scripts/step2</code></pre></div>",
    "thr_18": "<div class=\"codeBlock hljs python\" id=\"thr_18\"><pre id=\"thr_18_code\"><code class=\"python\">survey_text = ('Out of 5 people surveyed, James Robert,'\n               ' Julie Fuller and Benjamin Brooks like'\n               ' apples. Kelly Cox and Matthew Evans'\n               ' like oranges.')\n\ndef replace_person_names(token):\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">if <span class=\"fea_tokenization_keys udls\">token</span>.ent_iob != 0 and <span class=\"fea_tokenization_keys udls\">token</span>.ent_type_ == 'PERSON':</div>\n        return '[REDACTED] '\n    return token.string\n\ndef <div class=\"highlights fea_named_entity_recognition\" id=\"named_entity_recognition_0\" style=\"display: inline;\">redact_names(nlp_doc):</div>\n    for ent in nlp_doc.ents:\n        ent.merge()\n    tokens = map(replace_person_names, nlp_doc)\n    return ''.join(tokens)\n\nsurvey_doc = nlp(survey_text)\nredact_names(survey_doc)</code></pre></div>",
    "thr_20": "<div class=\"codeBlock hljs python\" id=\"thr_20\"><pre id=\"thr_20_code\"><code class=\"python\">one_about_text = ('Gus Proto is a Python developer'\n    ' currently working for a London-based Fintech company')\none_about_doc = nlp(one_about_text)\n# Extract children of `developer`\nprint([token.text for token in one_about_doc[5].children])\n\n# Extract previous neighboring node of `developer`\nprint (one_about_doc[5].nbor(-1))\n\n# Extract next neighboring node of `developer`\nprint (one_about_doc[5].nbor())\n\n# Extract all tokens on the left of `developer`\nprint([token.text for token in one_about_doc[5].lefts])\n\n# Extract tokens on the right of `developer`\nprint([<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>.text for <span class=\"fea_tokenization_keys udls\">token</span> in one_about_doc[5].rights</div>])\n\n# Print subtree of `developer`\nprint (list(one_about_doc[5].subtree))\n\ndef flatten_tree(tree):\n    return ''.join([token.text_with_ws for token in list(tree)]).strip()\n\n# Print flattened subtree of `developer`\nprint (flatten_tree(one_about_doc[5].subtree))</code></pre></div>",
    "thr_27": "<div class=\"codeBlock hljs python\" id=\"thr_27\"><pre id=\"thr_27_code\"><code class=\"python\">import re\nimport spacy\nfrom spacy.tokenizer import Tokenizer\ncustom_nlp = spacy.load('en_core_web_sm')\n<div class=\"highlights fea_text_simplify\" id=\"text_simplify_0\" style=\"display: inline;\"><span class=\"fea_text_simplify_keys udls\">pre</span>fix_re = spacy.util.compile_<span class=\"fea_text_simplify_keys udls\">pre</span>fix_regex(custom_nlp.Defa<span class=\"fea_text_simplify_keys udls\">ult</span>s.<span class=\"fea_text_simplify_keys udls\">pre</span>fixes)\nsuffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defa<span class=\"fea_text_simplify_keys udls\">ult</span>s.suffixes)</div>\n<div class=\"highlights fea_regular_expression\" id=\"regular_expression_0\" style=\"display: inline;\">infix_re = re.<span class=\"fea_regular_expression_keys udls\">compile</span>(r'''[-~]''')</div>\ndef customize_tokenizer(nlp):\n    # Adds support to use `-` as the delimiter for tokenization\n    return <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">Token</span>izer(nlp.vocab, prefix_search=prefix_re.search,\n                     suffix_search=suffix_re.search,\n                     infix_finditer=infix_re.finditer,\n                     <span class=\"fea_tokenization_keys udls\">token</span>_match=None\n                     )</div>\n\n\ncustom_nlp.tokenizer = customize_tokenizer(custom_nlp)\ncustom_tokenizer_about_doc = custom_nlp(about_text)\nprint([token.text for token in custom_tokenizer_about_doc])\n</code></pre></div>",
    "thr_22": "<div class=\"codeBlock hljs python\" id=\"thr_22\"><pre id=\"thr_22_code\"><code class=\"python\">def is_token_allowed(token):\n    '''\n        Only allow valid tokens which are not stop words\n        and punctuation symbols.\n    '''\n    if (not token or not token.string.strip() or\n        token.is_stop or token.is_punct):\n        return False\n    return True\n\ndef preprocess_token(token):\n    # Reduce token to its lowercase lemma form\n    return <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">token.<span class=\"fea_lemmatization_keys udls\">lemma</span>_</div>.strip().lower()\n\ncomplete_filtered_tokens = [preprocess_token(token)\n    for token in complete_doc if is_token_allowed(token)]\ncomplete_filtered_tokens</code></pre></div>",
    "thr_26": "<div class=\"codeBlock hljs python\" id=\"thr_26\"><pre id=\"thr_26_code\"><code class=\"python\">import spacy\nnlp = spacy.load('en_core_web_sm')\n\nconference_help_text = ('Gus is helping organize a developer'\n    'conference on Applications of Natural Language'\n    ' Processing. He keeps organizing local Python meetups'\n    ' and several internal talks at his workplace.')\nconference_help_doc = nlp(conference_help_text)\nfor token in conference_help_doc:\n    print (token, <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">token.<span class=\"fea_lemmatization_keys udls\">lemma</span>_</div>)</code></pre></div>",
    "thr_13": "<div class=\"codeBlock hljs python\" id=\"thr_13\"><pre id=\"thr_13_code\"><code class=\"python\"># Construction via add_pipe with default model\ntok2vec = nlp.add_pipe(\"tok2vec\")\n\n# Construction via add_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_tok2vec\"}}\nparser = nlp.add_pipe(\"tok2vec\", config=config)\n\n# Construction from class\n<div class=\"highlights fea_word_vectors\" id=\"word_vectors_0\" style=\"display: inline;\">from spacy.pipeline import Tok2Vec\ntok2vec = Tok2Vec(nlp.vocab, model)</div>\n#https://spacy.io/api/tok2vec</code></pre></div>",
    "thr_5": "<div class=\"codeBlock hljs python\" id=\"thr_5\"><pre id=\"thr_5_code\"><code class=\"python\">from spacy.training import JsonlCorpus\nimport spacy\n\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">corpus = JsonlCorpus(\"./texts.jsonl\")</div>\nnlp = spacy.blank(\"en\")\ndata = corpus(nlp)\n\n#https://spacy.io/api/corpus</code></pre></div>",
    "thr_6": "<div class=\"codeBlock hljs python\" id=\"thr_6\"><pre id=\"thr_6_code\"><code class=\"python\">import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom string import punctuation\nfrom collections import Counter\nfrom heapq import nlargest\n\ndoc =\"\"\"Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to progressively improve their performance on a specific task. Machine learning algorithms build a mathematical model of sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in the applications of email filtering, detection of network intruders, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning.In its application across business problems, machine learning is also referred to as predictive analytics.\"\"\"\n\nnlp = spacy.load('en')\ndoc = nlp(doc)\n\nkeyword = []\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span> = list(<span class=\"fea_nlp_datasets_keys udls\">STOP</span>_<span class=\"fea_nlp_datasets_keys udls\">WORDS</span>)</div>\npos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\nfor token in doc:\n    if(token.text in stopwords or token.text in punctuation):\n        continue\n    if(token.pos_ in pos_tag):\n        keyword.append(token.text)\n\nfreq_word = Counter(keyword)\n\nmax_freq = Counter(keyword).most_common(1)[0][1]\nfor word in freq_word.keys():  \n        freq_word[word] = (freq_word[word]/max_freq)\n\nsent_strength={}\nfor sent in doc.sents:\n    for word in sent:\n        if word.text in freq_word.keys():\n            if sent in sent_strength.keys():\n                sent_strength[sent]+=freq_word[word.text]\n            else:\n                sent_strength[sent]=freq_word[word.text]\nprint(sent_strength)\n\n<div class=\"highlights fea_summarizer\" id=\"summarizer_0\" style=\"display: inline;\"><span class=\"fea_summarizer_keys udls\">summar</span>ized_<span class=\"fea_summarizer_keys udls\">sentence</span>s = nlargest(3, sent_strength, key=sent_strength.get)</div>\n\nfinal_sentences = [ w.text for w in summarized_sentences ]\nsummary = ' '.join(final_sentences)</code></pre></div>",
    "thr_25": "<div class=\"codeBlock hljs python\" id=\"thr_25\"><pre id=\"thr_25_code\"><code class=\"python\">from collections import Counter\ncomplete_text = ('Gus Proto is a Python developer currently'\n    'working for a London-based Fintech company. He is'\n    ' interested in learning Natural Language Processing.'\n    ' There is a developer conference happening on 21 July'\n    ' 2019 in London. It is titled \"Applications of Natural'\n    ' Language Processing\". There is a helpline number '\n    ' available at +1-1234567891. Gus is helping organize it.'\n    ' He keeps organizing local Python meetups and several'\n    ' internal talks at his workplace. Gus is also presenting'\n    ' a talk. The talk will introduce the reader about \"Use'\n    ' cases of Natural Language Processing in Fintech\".'\n    ' Apart from his work, he is very passionate about music.'\n    ' Gus is learning to play the Piano. He has enrolled '\n    ' himself in the weekend batch of Great Piano Academy.'\n    ' Great Piano Academy is situated in Mayfair or the City'\n    ' of London and has world-class piano instructors.')\n\ncomplete_doc = nlp(complete_text)\n# Remove stop words and punctuation symbols\nwords = [token.text for token in complete_doc\n         if not token.is_stop and not token.is_punct]\n<div class=\"highlights fea_word_frequency\" id=\"word_frequency_0\" style=\"display: inline;\"><span class=\"fea_word_frequency_keys udls\">word</span>_<span class=\"fea_word_frequency_keys udls\">freq</span> = Counter(<span class=\"fea_word_frequency_keys udls\">word</span>s)</div>\n# 5 commonly occurring words with their frequencies\ncommon_words = word_freq.most_common(5)\nprint (common_words)\n\n# Unique words\nunique_words = [word for (word, freq) in word_freq.items() if freq == 1]\nprint (unique_words)</code></pre></div>",
    "thr_24": "<div class=\"codeBlock hljs python\" id=\"thr_24\"><pre id=\"thr_24_code\"><code class=\"python\">nouns = []\nadjectives = []\nfor token in about_doc:\n    if <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">token</span>.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_</div> == 'NOUN':\n        nouns.append(token)\n    if <div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">token.pos_ == 'ADJ':</div>\n        adjectives.append(token)\n    print (token, <div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">token.tag_</div>, token.pos_, spacy.explain(token.tag_))</code></pre></div>",
    "thr_11": "<div class=\"codeBlock hljs python\" id=\"thr_11\"><pre id=\"thr_11_code\"><code class=\"python\">from skweak.base import SpanAnnotator\nimport spacy\nimport itertools\nimport json\nfrom spacy.tokens import Doc, Span  # type: ignore\nfrom typing import Tuple, Iterable, List\n\n####################################################################\n# Labelling source based on neural models\n####################################################################\n\n\nclass ModelAnnotator(SpanAnnotator):\n    \"\"\"Annotation based on a spacy NER model\"\"\"\n\n    def __init__(self, name:str, model_path:str, \n                 disabled:List[str]=[\"parser\", \"tagger\", \"lemmatizer\", \"attribute_ruler\"]):\n        \"\"\"Creates a new annotator based on a Spacy model. \"\"\"\n        \n        super(ModelAnnotator, self).__init__(name)\n        self.model = spacy.load(model_path, disable=disabled)\n\n\n    def find_spans(self, doc: Doc) -&gt; Iterable[Tuple[int, int, str]]:\n        \"\"\"Annotates one single document using the Spacy NER model\"\"\"\n\n        # Create a new document (to avoid conflicting annotations)\n        doc2 = self.create_new_doc(doc)\n        # And run the model\n        for _, proc in self.model.pipeline:\n            doc2 = proc(doc2)\n        # Add the annotation\n        for ent in doc2.ents:\n            yield ent.start, ent.end, ent.label_\n\n    def pipe(self, docs: Iterable[Doc]) -&gt; Iterable[Doc]:\n        \"\"\"Annotates the stream of documents based on the Spacy model\"\"\"\n\n        stream1, stream2 = itertools.tee(docs, 2)\n\n        # Remove existing entities from the document\n        stream2 = (self.create_new_doc(d) for d in stream2)\n        \n        # And run the model\n        for _, proc in self.model.pipeline:\n            stream2 = proc.pipe(stream2)\n        \n        for doc, doc_copy in zip(stream1, stream2):\n\n            doc.spans[self.name] = []\n\n            # Add the annotation\n            for ent in doc_copy.ents:\n                doc.spans[self.name].append(Span(doc, ent.start, ent.end, ent.label_))\n\n            yield doc\n\n    def create_new_doc(self, doc: Doc) -&gt; Doc:\n        \"\"\"Create a new, empty Doc (but with the same tokenisation as before)\"\"\"\n\n        return spacy.tokens.Doc(self.model.vocab, [tok.text for tok in doc], #type: ignore\n                               [tok.whitespace_ for tok in doc])\n\n\nclass TruecaseAnnotator(ModelAnnotator):\n    \"\"\"Spacy model annotator that preprocess all texts to convert them to a \n    \"truecased\" representation (see below)\"\"\"\n\n    def __init__(self, name:str, model_path:str, form_frequencies:str,\n                 disabled:List[str]=[\"parser\", \"tagger\", \"lemmatizer\", \"attribute_ruler\"]):\n        \"\"\"Creates a new annotator based on a Spacy model, and a dictionary containing\n        the most common case forms for a given word (to be able to truecase the document).\"\"\"\n        \n        super(TruecaseAnnotator, self).__init__(name, model_path, disabled)\n        with open(form_frequencies) as fd:\n            self.form_frequencies = json.load(fd)\n\n    def create_new_doc(self, doc: Doc, min_prob: float = 0.25) -&gt; Doc:\n        \"\"\"Performs truecasing of the tokens in the spacy document. Based on relative \n        frequencies of word forms, tokens that \n        (1) are made of letters, with a first letter in uppercase\n        (2) and are not sentence start\n        (3) and have a relative frequency below min_prob\n        ... will be replaced by its most likely case (such as lowercase). \"\"\"\n\n  #      print(\"running on\", doc[:10])\n\n        if not self.form_frequencies:\n            raise RuntimeError(\n                \"Cannot truecase without a dictionary of form frequencies\")\n\n        tokens = []\n        spaces = []\n        doctext = doc.text\n        for tok in doc:\n            toktext = tok.text\n\n            # We only change casing for words in Title or UPPER\n            if tok.is_alpha and toktext[0].isupper():\n                cond1 = tok.is_upper and len(toktext) &gt; 2  # word in uppercase\n                cond2 = toktext[0].isupper(\n                ) and not tok.is_sent_start  # titled word\n                if cond1 or cond2:\n                    token_lc = toktext.lower()\n                    if token_lc in self.form_frequencies:\n                        frequencies = self.form_frequencies[token_lc]\n                        if frequencies.get(toktext, 0) &lt; min_prob:\n                            alternative = sorted(\n                                frequencies.keys(), key=lambda x: frequencies[x])[-1]\n\n                            # We do not change from Title to to UPPER\n                            if not tok.is_title or not alternative.isupper():\n                                toktext = alternative\n\n            tokens.append(toktext)\n\n            # Spacy needs to know whether the token is followed by a space\n            if tok.i &lt; len(doc)-1:\n                spaces.append(doctext[tok.idx+len(tok)].isspace())\n            else:\n                spaces.append(False)\n\n        # Creates a new document with the tokenised words and space information\n        doc2 = Doc(self.<div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">model.vocab</div>, words=tokens, spaces=spaces) #type: ignore\n #       print(\"finished with doc\", doc2[:10])\n        return doc2\n        #https://github.com/NorskRegnesentral/skweak/blob/main/skweak/spacy</code></pre></div>",
    "thr_19": "<div class=\"codeBlock hljs python\" id=\"thr_19\"><pre id=\"thr_19_code\"><code class=\"python\">conference_text = ('There is a developer conference'\n    ' happening on 21 July 2019 in London.')\nconference_doc = nlp(conference_text)\n# Extract Noun Phrases\nfor chunk in <div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">conference_doc.<span class=\"fea_n_grams_keys udls\">noun</span>_chunks</div>:\n    print (chunk)</code></pre></div>",
    "thr_8": "<div class=\"codeBlock hljs python\" id=\"thr_8\"><pre id=\"thr_8_code\"><code class=\"python\">import spacy\nimport contextualSpellCheck\n\nnlp = spacy.load('en_core_web_sm')\n<div class=\"highlights fea_spellcheck\" id=\"spellcheck_0\" style=\"display: inline;\">con<span class=\"fea_spellcheck_keys udls\">text</span>ualSpellCheck.add_to_pipe(nlp)</div>\ndoc = nlp('Income was $9.4 milion compared to the prior year of $2.7 milion.')\n\nprint(doc._.performed_spellCheck) #Should be True\nprint(doc._.outcome_spellCheck) #Income was $9.4 million compared to the prior year of $2.7 million.\n#https://spacy.io/universe/project/contextualSpellCheck</code></pre></div>",
    "thr_7": "<div class=\"codeBlock hljs python\" id=\"thr_7\"><pre id=\"thr_7_code\"><code class=\"python\">import spacy\nfrom spacytextblob.spacytextblob import SpacyTextBlob\n\nnlp = spacy.load('en_core_web_sm')\nnlp.add_pipe('spacytextblob')\ntext = 'I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.'\ndoc = nlp(text)\n<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">doc._.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span>      # <span class=\"fea_sentiment_analysis_keys udls\">Polarity</span>: -0.125\ndoc._.subjec<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>ity</div>  # Sujectivity: 0.9\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">doc._.assessm<span class=\"fea_text_scoring_keys udls\">ent</span>s</div>   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n#https://spacy.io/universe/project/spacy-textblob</code></pre></div>",
    "thr_9": "<div class=\"codeBlock hljs python\" id=\"thr_9\"><pre id=\"thr_9_code\"><code class=\"python\">import spacy\nfrom spacy_langdetect import LanguageDetector\nnlp = spacy.load('en')\n<div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">nlp.add_pipe(<span class=\"fea_language_detection_keys udls\">Language</span><span class=\"fea_language_detection_keys udls\">Detect</span>or(), name='<span class=\"fea_language_detection_keys udls\">language</span>_<span class=\"fea_language_detection_keys udls\">detect</span>or', last=True)</div>\ntext = 'This is an english text.'\ndoc = nlp(text)\n# document level language detection. Think of it like average language of the document!\nprint(doc._.language)\n# sentence level language detection\nfor sent in doc.sents:\n   print(sent, sent._.language)\n   #https://spacy.io/universe/project/spacy-langdetect</code></pre></div>",
    "fir_31": "<div class=\"codeBlock hljs python\" id=\"fir_31\"><pre id=\"fir_31_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> copy\n<span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">from</span> collections <span class=\"hljs-keyword\">import</span> defaultdict\n<span class=\"hljs-keyword\">import</span> random\n<span class=\"hljs-keyword\">import</span> cPickle\n<span class=\"hljs-keyword\">import</span> gzip\n<span class=\"hljs-keyword\">import</span> os\n\n<span class=\"hljs-keyword\">import</span> syllables\n\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_1\" class=\"highlights fea_nlp_datasets\">rhyme_entries = nltk.corpus.cmudict.entries()\npronunciation_dictionary = nltk.corpus.cmudict.<span class=\"hljs-built_in\">dict</span>()</div>\n\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_0\" class=\"highlights fea_nlp_datasets\">mega_sentences = ( nltk.corpus.brown.sents() +\n                nltk.corpus.inaugural.sents() +\n                nltk.corpus.reuters.sents() +\n                nltk.corpus.webtext.sents() +\n                nltk.corpus.inaugural.sents() +\n                nltk.corpus.gutenberg.sents(<span class=\"hljs-string\">\"carroll-alice.txt\"</span>) +\n                nltk.corpus.gutenberg.sents(<span class=\"hljs-string\">\"austen-emma.txt\"</span>) +\n                nltk.corpus.gutenberg.sents(<span class=\"hljs-string\">\"austen-sense.txt\"</span>) +\n                nltk.corpus.gutenberg.sents(<span class=\"hljs-string\">\"blake-poems.txt\"</span>) +\n                nltk.corpus.gutenberg.sents(<span class=\"hljs-string\">\"bible-kjv.txt\"</span>) +\n                nltk.corpus.gutenberg.sents(<span class=\"hljs-string\">\"chesterton-ball.txt\"</span>) +\n                nltk.corpus.gutenberg.sents(<span class=\"hljs-string\">\"melville-moby_dick.txt\"</span>) +\n                nltk.corpus.gutenberg.sents(<span class=\"hljs-string\">\"milton-paradise.txt\"</span>) +\n                nltk.corpus.gutenberg.sents(<span class=\"hljs-string\">\"whitman-leaves.txt\"</span>) +\n                nltk.corpus.gutenberg.sents(<span class=\"hljs-string\">\"austen-persuasion.txt\"</span>) +\n                nltk.corpus.gutenberg.sents(<span class=\"hljs-string\">\"shakespeare-hamlet.txt\"</span>) +\n                nltk.corpus.gutenberg.sents(<span class=\"hljs-string\">\"shakespeare-macbeth.txt\"</span>) )</div>\n\n\nlast_word_sentences = defaultdict(<span class=\"hljs-built_in\">list</span>)\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">last_word</span>(<span class=\"hljs-params\"> sentence </span>):</span>\n    ss = [ word <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> sentence <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(word) &gt; <span class=\"hljs-number\">1</span> ]\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(ss) &gt; <span class=\"hljs-number\">0</span>:\n        <span class=\"hljs-keyword\">return</span> ss[-<span class=\"hljs-number\">1</span>]\n    <span class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"\"</span>\n\n<span class=\"hljs-keyword\">if</span> os.path.exists( <span class=\"hljs-string\">\"sentences.gz\"</span> ):\n    <span class=\"hljs-keyword\">with</span> gzip.<span class=\"hljs-built_in\">open</span>( <span class=\"hljs-string\">\"sentences.gz\"</span>, <span class=\"hljs-string\">\"r\"</span> ) <span class=\"hljs-keyword\">as</span> cache_file:\n        last_word_sentences = cPickle.load( cache_file )\n<span class=\"hljs-keyword\">else</span>:\n    <span class=\"hljs-keyword\">for</span> sentence <span class=\"hljs-keyword\">in</span> mega_sentences:\n        lw = last_word(sentence)\n        last_word_sentences[ lw ].append(sentence)\n    <span class=\"hljs-keyword\">with</span> gzip.<span class=\"hljs-built_in\">open</span>( <span class=\"hljs-string\">\"sentences.gz\"</span>, <span class=\"hljs-string\">\"w\"</span>) <span class=\"hljs-keyword\">as</span> cache_file:\n        cPickle.dump(last_word_sentences, cache_file)\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">candidate_sentences</span>(<span class=\"hljs-params\"> word </span>):</span>\n    <span class=\"hljs-string\">\"\"\"\nReturn sentences from corpus that have word as the last word.\n\"\"\"</span>\n    <span class=\"hljs-keyword\">return</span> last_word_sentences[ word.lower() ]\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">rhyme_quality</span>(<span class=\"hljs-params\"> p1, p2 </span>):</span>\n    <span class=\"hljs-string\">\"\"\" Determine a numerical quality of the rhyme between two pronunciation lists.\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span>rhyme_quality( [\"A\", \"B\", \"C\"], [\"A\", \"B\"] )\n0\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span>rhyme_quality( [\"A\", \"B\", \"C\"], [\"B\", \"C\"] )\n2\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span>rhyme_quality( [\"A\", \"B\"], [\"A\", \"B\"] )\n0\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span>rhyme_quality( [\"B\", \"B\", \"C\", \"D\"], [\"A\", \"B\", \"C\", \"D\"] )\n3\n\"\"\"</span>\n    p1 = copy.deepcopy(p1)\n    p2 = copy.deepcopy(p2)\n    p1.reverse()\n    p2.reverse()\n    <span class=\"hljs-keyword\">if</span> p1 == p2:\n        <span class=\"hljs-comment\"># G-Spot rocks the G-Spot</span>\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-number\">0</span>\n    quality = <span class=\"hljs-number\">0</span>\n    <span class=\"hljs-keyword\">for</span> i, p_chunk <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(p1):\n        <span class=\"hljs-keyword\">try</span>:\n            <span class=\"hljs-keyword\">if</span> p_chunk == p2[i]:\n                quality += <span class=\"hljs-number\">1</span>\n            <span class=\"hljs-keyword\">if</span> p_chunk != p2[i]:\n                <span class=\"hljs-keyword\">break</span>\n        <span class=\"hljs-keyword\">except</span> IndexError:\n            <span class=\"hljs-keyword\">break</span>\n    <span class=\"hljs-keyword\">return</span> quality\n    \n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">word_rhyme_candidates</span>(<span class=\"hljs-params\"> word </span>):</span>\n    <span class=\"hljs-string\">\"\"\"\nProduce a list of potential rhyme candidates for a word\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span>print word_rhyme_candidates(\"battalion\")[:5]\n['stallion', 'italian', 'scallion', 'medallion', 'mccallion']\n\"\"\"</span>\n    candidates = []\n    <span class=\"hljs-keyword\">try</span>:\n        pronunciations = pronunciation_dictionary[word]\n    <span class=\"hljs-keyword\">except</span> KeyError:\n        <span class=\"hljs-keyword\">return</span> []\n    <span class=\"hljs-keyword\">if</span> pronunciations == []:\n        <span class=\"hljs-keyword\">return</span> []\n    <span class=\"hljs-keyword\">for</span> pronunciation <span class=\"hljs-keyword\">in</span> pronunciations:\n        <span class=\"hljs-keyword\">for</span> rhyme_word, rhyme_pronunciation <span class=\"hljs-keyword\">in</span> rhyme_entries:\n            quality = rhyme_quality( pronunciation, rhyme_pronunciation )\n            <span class=\"hljs-keyword\">if</span> quality &gt; <span class=\"hljs-number\">0</span>:\n                candidates.append( (quality, rhyme_word) )\n    candidates.sort()\n    candidates.reverse()\n    best_quality = candidates[<span class=\"hljs-number\">0</span>][<span class=\"hljs-number\">0</span>]\n    worst_allowable_quality = best_quality - <span class=\"hljs-number\">1</span>\n    candidates = [ candidate <span class=\"hljs-keyword\">for</span> q, candidate <span class=\"hljs-keyword\">in</span> candidates <span class=\"hljs-keyword\">if</span> q &gt;= worst_allowable_quality ]\n\n    <span class=\"hljs-keyword\">return</span> candidates\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">rhyme</span>(<span class=\"hljs-params\"> sentence </span>):</span>\n    target_syllables = syllables.sentence_syllables( sentence )\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">tokens = nltk.word_tokenize(sentence)</div>\n    rhymes = word_rhyme_candidates(last_word(tokens))\n    cs = []\n            \n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(tokens) == <span class=\"hljs-number\">1</span>:\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\", \"</span>.join(rhymes[:<span class=\"hljs-number\">12</span>])\n\n    <span class=\"hljs-keyword\">for</span> rhyme <span class=\"hljs-keyword\">in</span> rhymes:\n        cs += candidate_sentences( rhyme )\n\n    syllable_sentences = []\n    <span class=\"hljs-keyword\">for</span> sentence <span class=\"hljs-keyword\">in</span> cs:\n        ss = <span class=\"hljs-built_in\">sum</span>( [ syllables.syllables(word) <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> sentence ] )\n        syllable_sentences.append( (ss, <span class=\"hljs-string\">\" \"</span>.join(sentence)) )\n\n    syllable_sentences.sort()\n    syllable_sentences.reverse()\n    \n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>( syllable_sentences ) == <span class=\"hljs-number\">0</span>:\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>( rhymes ) &gt; <span class=\"hljs-number\">0</span>:\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\", \"</span>.join(rhymes[:<span class=\"hljs-number\">12</span>])\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"month, orange, Nantucket\"</span>\n    \n    syllable_numbers = [ n <span class=\"hljs-keyword\">for</span> n, sentence <span class=\"hljs-keyword\">in</span> syllable_sentences ]\n    closest_number = <span class=\"hljs-built_in\">min</span>( syllable_numbers, key=<span class=\"hljs-keyword\">lambda</span> x:<span class=\"hljs-built_in\">abs</span>(x-target_syllables) )\n    \n    closest_sentences = [ sentence <span class=\"hljs-keyword\">for</span> n, sentence <span class=\"hljs-keyword\">in</span> syllable_sentences <span class=\"hljs-keyword\">if</span> n == closest_number ]\n\n    <span class=\"hljs-keyword\">return</span> random.choice(closest_sentences)</code></pre></div>",
    "fir_32": "<div class=\"codeBlock hljs python\" id=\"fir_32\"><pre id=\"fir_32_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">from</span> nltk.tag <span class=\"hljs-keyword\">import</span> TaggerI, untag\n<span class=\"hljs-keyword\">from</span> nltk.chunk <span class=\"hljs-keyword\">import</span> ChunkParserI, tree2conlltags, conlltags2tree\n<span class=\"hljs-keyword\">import</span> pika\n<span class=\"hljs-keyword\">import</span> time\n<span class=\"hljs-keyword\">import</span> logging\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">BigramChunker</span>(<span class=\"hljs-params\"></span><span class=\"hljs-params\"><div style=\"background-color: #f6b73c; display: inline;\" id=\"chunking_0\" class=\"highlights fea_chunking\">nltk.ChunkParserI</div></span>):</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, train_sents</span>):</span> \n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"chunking_1\" class=\"highlights fea_chunking\">train_data = [[(t,c) <span class=\"hljs-keyword\">for</span> _,t,c <span class=\"hljs-keyword\">in</span> tree2conlltags(sent)] <span class=\"hljs-keyword\">for</span> sent <span class=\"hljs-keyword\">in</span> train_sents]</div>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_0\" class=\"highlights fea_tagger\">self.tagger = nltk.BigramTagger(train_data) </div>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">parse</span>(<span class=\"hljs-params\">self, sentence</span>):</span> \n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_0\" class=\"highlights fea_Part_of_Speech\">pos_tags = [pos <span class=\"hljs-keyword\">for</span> (_,pos) <span class=\"hljs-keyword\">in</span> sentence]\n        tagged_pos_tags = self.tagger.tag(pos_tags)</div>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"chunking_2\" class=\"highlights fea_chunking\">chunktags = [chunktag <span class=\"hljs-keyword\">for</span> (pos, chunktag) <span class=\"hljs-keyword\">in</span> tagged_pos_tags]\n        conlltags = [(word, pos, chunktag) <span class=\"hljs-keyword\">for</span> ((word,pos),chunktag)\n                     <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">zip</span>(sentence, chunktags)]\n        <span class=\"hljs-keyword\">return</span> conlltags2tree(conlltags)</div> \n \n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">ConsecutiveNPChunkTagger</span>(<span class=\"hljs-params\">nltk.TaggerI</span>):</span> \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, train_sents</span>):</span>\n        train_set = []\n        <span class=\"hljs-keyword\">for</span> tagged_sent <span class=\"hljs-keyword\">in</span> train_sents:\n            <div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_1\" class=\"highlights fea_tagger\">untagged_sent = nltk.tag.untag(tagged_sent)</div>\n            history = []\n            <span class=\"hljs-keyword\">for</span> i, (_, tag) <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(tagged_sent):\n                featureset = self.npchunk_features(untagged_sent, i, history) \n                train_set.append( (featureset, tag) )\n                history.append(tag)\n        <span class=\"hljs-comment\">#self.classifier = nltk.classify.MaxentClassifier.train(train_set, algorithm='megam', trace=0)</span>\n       <div style=\"background-color: #f6b73c; display: inline;\" id=\"classification_0\" class=\"highlights fea_classification\"> self.classifier = nltk.classify.MaxentClassifier.train(train_set, trace=<span class=\"hljs-number\">0</span>)</div>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">tag</span>(<span class=\"hljs-params\">self, sentence</span>):</span>\n        history = []\n        <span class=\"hljs-keyword\">for</span> i, _ <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(sentence):\n            featureset = self.npchunk_features(sentence, i, history)\n            <div style=\"background-color: #f6b73c; display: inline;\" id=\"classification_1\" class=\"highlights fea_classification\">tag = self.classifier.classify(featureset)</div>\n            history.append(tag)\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">zip</span>(sentence, history)</code></pre></div>",
    "fir_33": "<div class=\"codeBlock hljs python\" id=\"fir_33\"><pre id=\"fir_33_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\">#!/usr/bin/env python</span>\n\n<span class=\"hljs-string\">'''Package some convenient NLTK processing'''</span>\n\n__license__ = <span class=\"hljs-string\">'''Copyright (c) 2009 Chad Colgur\n\nPermission is hereby granted, free of charge, to any person\nobtaining a copy of this software and associated documentation\nfiles (the \"Software\"), to deal in the Software without\nrestriction, including without limitation the rights to use,\ncopy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the\nSoftware is furnished to do so, subject to the following\nconditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nWHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.'''</span>\n\n<span class=\"hljs-comment\"># Imports</span>\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\"><span class=\"hljs-keyword\">from</span> nltk.tokenize.regexp <span class=\"hljs-keyword\">import</span> RegexpTokenizer</div>\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_0\" class=\"highlights fea_nlp_datasets\"><span class=\"hljs-keyword\">from</span> nltk.corpus <span class=\"hljs-keyword\">import</span> stopwords</div>\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"text_frequency_0\" class=\"highlights fea_text_frequency\"><span class=\"hljs-keyword\">from</span> nltk <span class=\"hljs-keyword\">import</span> FreqDist</div>\n\n<span class=\"hljs-comment\"># Global Variables</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_1\" class=\"highlights fea_nlp_datasets\">__ignored_words__ = stopwords.words(<span class=\"hljs-string\">'english'</span>)</div>\n\n<span class=\"hljs-comment\"># Class Declarations</span>\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">ProgWordPunctTokenizer</span>(<span class=\"hljs-params\">RegexpTokenizer</span>):</span>\n   <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span>\n     <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_1\" class=\"highlights fea_tokenization\"> RegexpTokenizer.__init__(self, <span class=\"hljs-string\">r'[\\w+#]+|[^\\w\\s]+'</span>)</div>\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Content</span>:</span>\n   <span class=\"hljs-string\">''''''</span>\n   <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, locale=<span class=\"hljs-string\">'english'</span></span>):</span>\n      <span class=\"hljs-comment\"># <span class=\"hljs-doctag\">TODO:</span> Allow clients to extend Ignored Words but only store the corpus</span>\n      self.locale = locale\n      self.lower_words = {}\n      <div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_2\" class=\"highlights fea_nlp_datasets\">self.ignored_words = stopwords.words(self.locale)</div>\n\n   <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">words</span>(<span class=\"hljs-params\">self, tokens</span>):</span>\n      <span class=\"hljs-string\">''' Use NLTK to drive off stopwords '''</span>\n      all_content = [w <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> tokens <span class=\"hljs-keyword\">if</span> w.lower() <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> unicode(self.ignored_words)]\n      long_content = [w <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> all_content <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(w) &gt; <span class=\"hljs-number\">3</span>]\n\n      <span class=\"hljs-keyword\">return</span> long_content\n\n   <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">custom_stopwords</span>(<span class=\"hljs-params\">self</span>):</span>\n      <span class=\"hljs-string\">'''Expect clients to Extend ignored_words.\n      This method returns that custom list'''</span>\n      custom = <span class=\"hljs-built_in\">set</span>(self.ignored_words)\n      <div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_3\" class=\"highlights fea_nlp_datasets\">default = <span class=\"hljs-built_in\">set</span>(stopwords.words(self.locale))</div>\n      difference = custom.difference(default)\n\n      <span class=\"hljs-keyword\">return</span> [w <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">iter</span>(difference)]\n\n   <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">fraction</span>(<span class=\"hljs-params\">self, tokens</span>):</span>\n      <span class=\"hljs-string\">''' Sample function from NLTK section 2.4.1 '''</span>\n      <span class=\"hljs-keyword\">from</span> decimal <span class=\"hljs-keyword\">import</span> Decimal\n\n      content = self.words(tokens)\n      <span class=\"hljs-keyword\">return</span> Decimal(<span class=\"hljs-built_in\">len</span>(content)) / Decimal(<span class=\"hljs-built_in\">len</span>(tokens))\n\n   <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">top</span>(<span class=\"hljs-params\">self, tokens, lowest_rank=<span class=\"hljs-number\">50</span></span>):</span>\n      <span class=\"hljs-string\">''' A list of the most frequent (non-stopword) tokens '''</span>\n      <span class=\"hljs-keyword\">from</span> operator <span class=\"hljs-keyword\">import</span> itemgetter\n      content = self.words(tokens)\n\n      <div style=\"background-color: #f6b73c; display: inline;\" id=\"text_frequency_1\" class=\"highlights fea_text_frequency\">fdist = FreqDist(content)</div>\n      vocab = <span class=\"hljs-built_in\">iter</span>(fdist.keys())\n\n      <span class=\"hljs-comment\"># Forget all previous ranking</span>\n      self.lower_words = {}\n      <div style=\"background-color: #f6b73c; display: inline;\" id=\"text_frequency_2\" class=\"highlights fea_text_frequency\">frequency = <span class=\"hljs-number\">0</span>\n      <span class=\"hljs-keyword\">while</span> frequency &lt; lowest_rank:\n         <span class=\"hljs-keyword\">try</span>:\n            word = vocab.<span class=\"hljs-built_in\">next</span>()\n         <span class=\"hljs-keyword\">except</span> StopIteration:\n            <span class=\"hljs-keyword\">break</span></div><span class=\"hljs-keyword\"></span>\n\n         <div style=\"background-color: #f6b73c; display: inline;\" id=\"text_frequency_3\" class=\"highlights fea_text_frequency\">word_lower = word.lower()\n         <span class=\"hljs-keyword\">if</span> word_lower <span class=\"hljs-keyword\">in</span> self.lower_words:\n            self.lower_words[word_lower] = self.lower_words[word_lower] + fdist[word]\n         <span class=\"hljs-keyword\">else</span>:\n            self.lower_words[word_lower] = fdist[word]\n\n         frequency = frequency + <span class=\"hljs-number\">1</span></div><span class=\"hljs-number\"></span>\n\n<span class=\"hljs-comment\">#      return sorted(self.lower_words, key=itemgetter(1), reverse=True)</span>\n      <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">map</span>(itemgetter(<span class=\"hljs-number\">0</span>), <span class=\"hljs-built_in\">sorted</span>(self.lower_words.items(), key=itemgetter(<span class=\"hljs-number\">1</span>), reverse=<span class=\"hljs-literal\">True</span>))\n\n   <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">frequency</span>(<span class=\"hljs-params\">self, word</span>):</span>\n      <span class=\"hljs-string\">'''Dictionary accessor'''</span>\n      wordfreq = <span class=\"hljs-number\">0</span>\n      <span class=\"hljs-keyword\">try</span>:\n         wordfreq = self.lower_words[word]\n      <span class=\"hljs-keyword\">except</span> KeyError:\n         <span class=\"hljs-comment\"># Not really an error: word doesn't appear then frequency is 0</span>\n         <span class=\"hljs-keyword\">pass</span>\n      <span class=\"hljs-keyword\">return</span> wordfreq\n\n   <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">collocations</span>(<span class=\"hljs-params\">self, tokens, num=<span class=\"hljs-number\">20</span>, window_size=<span class=\"hljs-number\">2</span></span>):</span>\n      <span class=\"hljs-string\">''' Adopted from nltk.Text method by the same name'''</span>\n      <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"text_frequency_5\" class=\"highlights fea_text_frequency\"><span class=\"hljs-keyword\">from</span> nltk.collocations <span class=\"hljs-keyword\">import</span> BigramCollocationFinder, bigram_measures\n      finder = BigramCollocationFinder.from_words(tokens, window_size)</div>\n      finder.apply_freq_filter(<span class=\"hljs-number\">2</span>) \n      finder.apply_word_filter(<span class=\"hljs-keyword\">lambda</span> w: <span class=\"hljs-built_in\">len</span>(w) &lt; <span class=\"hljs-number\">3</span> <span class=\"hljs-keyword\">or</span> w.lower() <span class=\"hljs-keyword\">in</span> self.ignored_words) \n\n      <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"text_frequency_4\" class=\"highlights fea_text_frequency\"><span class=\"hljs-keyword\">return</span> finder.nbest(bigram_measures.likelihood_ratio, num) </div>\n\n<span class=\"hljs-comment\"># Function Declarations</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">extend_ignored_words</span>(<span class=\"hljs-params\">custom_words</span>):</span>\n   <span class=\"hljs-string\">'''Use this instead of the dictionary '''</span>\n   __ignored_words__.extend(custom_words)\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_content</span>(<span class=\"hljs-params\">tokens</span>):</span>\n   <span class=\"hljs-string\">''' Use NLTK to drive off stopwords '''</span>\n   content = [w <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> tokens <span class=\"hljs-keyword\">if</span> w.lower() <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> unicode(__ignored_words__)]\n   longcontent = [w <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> content <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(w) &gt; <span class=\"hljs-number\">3</span>]\n\n   <span class=\"hljs-keyword\">return</span> longcontent\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">content_fraction</span>(<span class=\"hljs-params\">tokens</span>):</span>\n   <span class=\"hljs-string\">''' Sample function from NLTK section 2.4.1 '''</span>\n   <span class=\"hljs-keyword\">from</span> decimal <span class=\"hljs-keyword\">import</span> Decimal\n\n   content = get_content(tokens)\n   <span class=\"hljs-keyword\">return</span> Decimal(<span class=\"hljs-built_in\">len</span>(content)) / Decimal(<span class=\"hljs-built_in\">len</span>(tokens))\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">collocations</span>(<span class=\"hljs-params\">tokens, num=<span class=\"hljs-number\">20</span>, window_size=<span class=\"hljs-number\">2</span></span>):</span>\n   <span class=\"hljs-string\">''' Adopted from nltk.Text method by the same name'''</span>\n   <span class=\"hljs-keyword\">from</span> nltk.collocations <span class=\"hljs-keyword\">import</span> BigramCollocationFinder, bigram_measures\n   finder = BigramCollocationFinder.from_words(tokens, window_size)\n   finder.apply_freq_filter(<span class=\"hljs-number\">2</span>) \n   finder.apply_word_filter(<span class=\"hljs-keyword\">lambda</span> w: <span class=\"hljs-built_in\">len</span>(w) &lt; <span class=\"hljs-number\">3</span> <span class=\"hljs-keyword\">or</span> w.lower() <span class=\"hljs-keyword\">in</span> __ignored_words__) \n\n   <span class=\"hljs-keyword\">return</span> finder.nbest(bigram_measures.likelihood_ratio, num) \n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">top_content</span>(<span class=\"hljs-params\">tokens</span>):</span>\n   <span class=\"hljs-string\">''' Use NLTK to discover the top ten most frequent terms '''</span>\n   content = getcontent(tokens)\n\n   fdist = nltk.FreqDist(content)\n   vocab = <span class=\"hljs-built_in\">iter</span>(fdist.keys())\n\n   words = {}\n   frequency = <span class=\"hljs-number\">0</span>\n   <span class=\"hljs-keyword\">while</span> frequency &lt; <span class=\"hljs-number\">50</span>:\n      <span class=\"hljs-keyword\">try</span>:\n         word = vocab.<span class=\"hljs-built_in\">next</span>()\n      <span class=\"hljs-keyword\">except</span> StopIteration:\n         <span class=\"hljs-keyword\">break</span>\n\n      word_lower = word.lower()\n      <span class=\"hljs-keyword\">if</span> word_lower <span class=\"hljs-keyword\">in</span> words:\n         words[word_lower] = words[word_lower] + fdist[word]\n      <span class=\"hljs-keyword\">else</span>:\n         words[word_lower] = fdist[word]\n\n      frequency = frequency + <span class=\"hljs-number\">1</span>\n\n   printable_output = []\n   <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> words:\n      output_str = <span class=\"hljs-string\">'%s : %d'</span> % (word, words[word])\n      printable_output.append(output_str)\n\n   <span class=\"hljs-keyword\">return</span> printable_output\n\n<span class=\"hljs-comment\"># \"main\" body</span>\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n   <span class=\"hljs-keyword\">pass</span>\n\nprogwordpunct_tokenize = ProgWordPunctTokenizer().tokenize</code></pre></div>",
    "fir_34": "<div class=\"codeBlock hljs python\" id=\"fir_34\"><pre id=\"fir_34_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> nltk, re, pprint\n<span class=\"hljs-keyword\">from</span> urllib <span class=\"hljs-keyword\">import</span> urlopen\n<span class=\"hljs-keyword\">import</span> csv\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">make_word_tokenizer</span>():</span>\n    word_tokenizer_re = <span class=\"hljs-string\">r'''(?x) \n                            # Numbers\n          \\$?(\\d+(?:[\\.,]\\d+)*)\\s*(?:million|billion|thousand)\n        | (?:\\d+th|\\d+1st|\\d+nd)\\b # cardinal figures\n        | \\$?\\d+(?:,\\d{3})+   # $4,000\n        | \\d+:\\d+(?:\\s*[ap]\\.m\\.)?          # time\n        | \\$?\\d+(?:\\.\\d+)?    # currency amounts, e.g. $12.50 \n        | (?:No\\. )           # abbreviations\n        | (?:[ap]\\.m\\.)       # a.m. / p.m.\n        | (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A. \n        | (?:\\'ve|\\'d|\\'s|\\'ll|\\'m|n\\'t)\\b  # contractions\n        # | [^\\w\\s]+          # sequences of punctuation\n        | (?:\\w+(?:-\\w+)+)     # hyphenated words\n        | (?:\\#\\#[A-Z]+\\#\\#)  # notations\n        | (?:\\w+~\\w+)         # force-joined words with tilde ~\n        | \\w+                 # sequences of 'word' characters \n        '''</span> \n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\"><span class=\"hljs-keyword\">return</span> nltk.tokenize.RegexpTokenizer(word_tokenizer_re).tokenize</div>\n    <span class=\"hljs-comment\"># s=(\"And 4:30 that ends 5.2 million 4.5 tonight.\\n\\nOn October 4,000 2, next $5,000  Thursday, also at 9:00 p.m. Eastern time, \"); word_tokenizer(s)</span>\n\n<span class=\"hljs-keyword\">try</span>:\n    word_tokenizer\n<span class=\"hljs-keyword\">except</span> NameError:\n    word_tokenizer = make_word_tokenizer()\n\n<span class=\"hljs-keyword\">try</span>:\n    sent_tokenizer\n<span class=\"hljs-keyword\">except</span> NameError:\n    sent_tokenizer = nltk.data.load(<span class=\"hljs-string\">'tokenizers/punkt/english.pickle'</span>)\n\n<span class=\"hljs-keyword\">try</span>:\n    tagger\n<span class=\"hljs-keyword\">except</span> NameError:\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_0\" class=\"highlights fea_nlp_datasets\">brown_a = nltk.corpus.brown.tagged_sents(categories=<span class=\"hljs-string\">'a'</span>)</div>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_0\" class=\"highlights fea_tagger\">t0      = nltk.DefaultTagger(<span class=\"hljs-string\">'WTF'</span>) \n    t1      = nltk.UnigramTagger(brown_a, backoff=t0) \n    t2      = nltk.BigramTagger( brown_a, backoff=t1) \n    tagger  = nltk.TrigramTagger(brown_a, backoff=t2)</div>\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">SpeechAnalyzer</span>(<span class=\"hljs-params\"><span class=\"hljs-built_in\">object</span></span>):</span>\n    NOTATIONS = {\n      <span class=\"hljs-string\">r'--'</span>                   : <span class=\"hljs-string\">\"##PAUSE##\"</span>,\n      <span class=\"hljs-string\">r'\\(sic\\)'</span>              : <span class=\"hljs-string\">\"##SIC##\"</span>,\n      <span class=\"hljs-string\">r'\\[mispronunciation\\]'</span> : <span class=\"hljs-string\">'##MISPRONUNCIATION##'</span>,\n      <span class=\"hljs-string\">r'\\.\\.\\.'</span>               : <span class=\"hljs-string\">' ##PAUSE## '</span>\n    }\n    PHRASES = [\n      <span class=\"hljs-string\">\"wall street\"</span>, <span class=\"hljs-string\">\"main street\"</span>, <span class=\"hljs-string\">\"my friends\"</span>, <span class=\"hljs-string\">\"middle class\"</span>, <span class=\"hljs-string\">\"fannie mae\"</span>, <span class=\"hljs-string\">\"freddie mac\"</span>,\n      <span class=\"hljs-string\">\"United States\"</span>, <span class=\"hljs-string\">\"United States of America\"</span>, <span class=\"hljs-string\">'Al Quaeda'</span>, <span class=\"hljs-string\">'Al Qaeda'</span>,\n    ]\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self,text</span>):</span>\n        self.text  = text\n        self.group_by_speaker()\n        self.find_sentences()\n        self.find_words()\n        \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">lowerfirst</span>(<span class=\"hljs-params\">self, words</span>):</span>\n        <span class=\"hljs-keyword\">if</span> (words[<span class=\"hljs-number\">0</span>]) <span class=\"hljs-keyword\">and</span> (<span class=\"hljs-keyword\">not</span> re.match(<span class=\"hljs-string\">r'^(?:I|I\\'(?:m|ve|d)|McCain|Obama|Afghanistan|Russia|America|Jim|China|Al|Iraq|Ireland|Spain|Tom|Osama|John)$'</span>, words[<span class=\"hljs-number\">0</span>])):\n            words[<span class=\"hljs-number\">0</span>] = words[<span class=\"hljs-number\">0</span>].lower()\n        <span class=\"hljs-keyword\">return</span> words\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">group_by_speaker</span>(<span class=\"hljs-params\">self</span>):</span>\n        lines = re.split(<span class=\"hljs-string\">r'(OBAMA|MCCAIN|LEHRER): '</span>, self.text)[<span class=\"hljs-number\">1</span>:]\n        <span class=\"hljs-comment\"># Break out each speaker's lines</span>\n        spkr_lines = <span class=\"hljs-built_in\">dict</span>([[spkr, <span class=\"hljs-string\">\"\"</span>] <span class=\"hljs-keyword\">for</span> spkr <span class=\"hljs-keyword\">in</span> [<span class=\"hljs-string\">'OBAMA'</span>, <span class=\"hljs-string\">'MCCAIN'</span>, <span class=\"hljs-string\">'LEHRER'</span>]])\n        <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">0</span>, <span class=\"hljs-built_in\">len</span>(lines), <span class=\"hljs-number\">2</span>):  <span class=\"hljs-comment\"># in groups of 2</span>\n            spkr_lines[lines[i]] += <span class=\"hljs-string\">\"\\n\\n\"</span> + lines[i+<span class=\"hljs-number\">1</span>]\n        self.spkr_lines = spkr_lines\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">find_sentences</span>(<span class=\"hljs-params\">self</span>):</span>\n        self.spkr_sents = <span class=\"hljs-built_in\">dict</span>((spkr, sent_tokenizer.tokenize(lines)) <span class=\"hljs-keyword\">for</span> spkr, lines <span class=\"hljs-keyword\">in</span> self.spkr_lines.iteritems())\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">normalize</span>(<span class=\"hljs-params\">self, line</span>):</span>\n        <span class=\"hljs-keyword\">for</span> phrase <span class=\"hljs-keyword\">in</span> SpeechAnalyzer.PHRASES:\n            srch = <span class=\"hljs-string\">'(?i)'</span>+re.sub(<span class=\"hljs-string\">r'\\s'</span>, <span class=\"hljs-string\">\"\\\\s\"</span>, phrase)\n            repl = re.sub(<span class=\"hljs-string\">r'\\s'</span>, <span class=\"hljs-string\">\"~\"</span>, phrase)\n            line = re.sub(srch, repl, line)\n        line = re.sub(<span class=\"hljs-string\">r'([a-zA-Z]+)n\\'t'</span>, <span class=\"hljs-string\">r\"\\1 n't\"</span>, line)\n        <span class=\"hljs-keyword\">for</span> srch, repl <span class=\"hljs-keyword\">in</span> SpeechAnalyzer.NOTATIONS.iteritems():\n            line = re.sub(srch, repl, line)\n        <span class=\"hljs-keyword\">return</span> line\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">tokenize</span>(<span class=\"hljs-params\">self, line</span>):</span>\n        line = self.normalize(line)\n        <span class=\"hljs-keyword\">return</span> self.lowerfirst(word_tokenizer(line))\n    \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">find_words</span>(<span class=\"hljs-params\">self</span>):</span>\n        self.spkr_words = <span class=\"hljs-built_in\">dict</span>( (spkr, [self.tokenize(sent) <span class=\"hljs-keyword\">for</span> sent <span class=\"hljs-keyword\">in</span> sents])\n                          <span class=\"hljs-keyword\">for</span> spkr, sents <span class=\"hljs-keyword\">in</span> self.spkr_sents.iteritems())\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">all_words</span>(<span class=\"hljs-params\">self</span>):</span>\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">dict</span>((spkr, <span class=\"hljs-built_in\">sum</span>(sents,[])) <span class=\"hljs-keyword\">for</span> spkr, sents <span class=\"hljs-keyword\">in</span> self.spkr_words.iteritems())\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">dump_csv</span>(<span class=\"hljs-params\">self</span>):</span>\n        writer = csv.writer(<span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">\"temp/foo.csv\"</span>, <span class=\"hljs-string\">\"wb\"</span>))\n        <span class=\"hljs-keyword\">for</span> spkr, words <span class=\"hljs-keyword\">in</span> self.spkr_words.iteritems():\n            writer.writerows(<span class=\"hljs-built_in\">sum</span>(\n                [ [ [<span class=\"hljs-string\">\"%s/%s\"</span> % (wd,tag)] <span class=\"hljs-keyword\">for</span> wd,tag <span class=\"hljs-keyword\">in</span> tagger.tag(sent) <span class=\"hljs-keyword\">if</span> tag == <span class=\"hljs-string\">'WTF'</span>\n                    ] <span class=\"hljs-keyword\">for</span> sent <span class=\"hljs-keyword\">in</span> words ], [])) <span class=\"hljs-comment\"># </span>\n\n\ntext = <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">'rawd/words_debate_20080926-raw.txt'</span>, <span class=\"hljs-string\">'rU'</span>).read()\nsa = SpeechAnalyzer(text)\n\n\n\n<span class=\"hljs-comment\"># phrases = nltk.Text(words).collocations()</span>\n\n<span class=\"hljs-comment\"># United States; Senator Obama; Senator McCain; make sure; General Petraeus; bin Laden; North Korea; health care; Ronald Reagan; sit down; national security; pork barrel; Admiral Mullen; Wall Street; the United; lead question; middle class; parsing words; eight years; nuclear weapons</span>\n\n<span class=\"hljs-comment\"># cfd = nltk.ConditionalFreqDist(nltk.corpus.brown.tagged_words(categories='a'))</span>\n<span class=\"hljs-comment\"># dict((word, cfd[word].max()) for word in most_freq_words)</span></code></pre></div>",
    "fir_35": "<div class=\"codeBlock hljs python\" id=\"fir_35\"><pre id=\"fir_35_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\">#!/usr/bin/env python</span>\n<span class=\"hljs-comment\"># -*- coding: UTF-8 -*-</span>\n\n<span class=\"hljs-keyword\">from</span> __future__ <span class=\"hljs-keyword\">import</span> division\n<span class=\"hljs-keyword\">import</span> cPickle\n<span class=\"hljs-keyword\">import</span> random\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">from</span> corpora <span class=\"hljs-keyword\">import</span> epistemonikos, genia, medpost\n<span class=\"hljs-keyword\">import</span> nltk\n\n\nGENIA_TAG_SUBS = {<span class=\"hljs-string\">'-'</span>: <span class=\"hljs-string\">\":\"</span>,      <span class=\"hljs-comment\"># For a couple of incorrectly tagged dashes.</span>\n                  <span class=\"hljs-string\">'XT'</span>: <span class=\"hljs-string\">'DT'</span>,    <span class=\"hljs-comment\"># Fix 1 incorrect tag for 'a' (DT, not XT).</span>\n                  <span class=\"hljs-string\">'CT'</span>: <span class=\"hljs-string\">'DT'</span>,    <span class=\"hljs-comment\"># Fix 4 incorrect tags for determiners (DT).</span>\n                  <span class=\"hljs-string\">'PP'</span>: <span class=\"hljs-string\">'PRP$'</span>,  <span class=\"hljs-comment\"># Fix 1 incorrect possesive pronoun (PRP$).</span>\n                  <span class=\"hljs-string\">'N'</span>: <span class=\"hljs-string\">'NN'</span>,     <span class=\"hljs-comment\"># Fix 1 incorrect tag for noun (NN).</span>\n                  <span class=\"hljs-string\">'``'</span>: <span class=\"hljs-string\">\"''\"</span>,    <span class=\"hljs-comment\"># Standarize tags for quote symbols.</span>\n                  <span class=\"hljs-string\">'JJ|NN'</span>: <span class=\"hljs-string\">'JJ'</span>,\n                  <span class=\"hljs-string\">'NN|NNS'</span>: <span class=\"hljs-string\">'NN'</span>\n                  }\nMPOST_TAG_SUBS = {<span class=\"hljs-string\">'``'</span>: <span class=\"hljs-string\">\"''\"</span>\n                  }\nTBANK_TAG_SUBS = {<span class=\"hljs-string\">'``'</span>: <span class=\"hljs-string\">\"''\"</span>,\n                  <span class=\"hljs-string\">'$'</span>: <span class=\"hljs-string\">'SYM'</span>,\n                  <span class=\"hljs-string\">'#'</span>: <span class=\"hljs-string\">'SYM'</span>\n                  }\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">train_backoff_tagger</span>(<span class=\"hljs-params\">train_sents, default_tag=<span class=\"hljs-literal\">None</span></span>):</span>\n    <span class=\"hljs-string\">\"\"\"\nThis function returns a tagger composed of a backoff sequence of taggers, some\nof which can be trained on the function input. The input is a list of tagged\nsentences, where each sentence is a list of tuples consisting of token/tag\npairs -- the same data structure that is returned by the .tagged_sents() method\nof NLTK's tagged corpora. It should only be executed once, and the output\nobject --a trained tagger-- should be pickled and saved for later use.\n    \"\"\"</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_0\" class=\"highlights fea_tagger\">default_tagger = nltk.DefaultTagger(default_tag)\n    regexp_patterns = [(<span class=\"hljs-string\">r'\\d+([,.]\\d+)*'</span>, <span class=\"hljs-string\">'CD'</span>)]\n    regexp_tagger = nltk.RegexpTagger(regexp_patterns, backoff=default_tagger)\n    unigram_tagger = nltk.UnigramTagger(train_sents, backoff=regexp_tagger)\n    bigram_tagger = nltk.BigramTagger(train_sents, backoff=unigram_tagger)\n    trigram_tagger = nltk.TrigramTagger(train_sents, backoff=bigram_tagger)\n    <span class=\"hljs-keyword\">return</span> trigram_tagger</div>\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">untagged_tokens_freq</span>(<span class=\"hljs-params\">tagged_sents, none_tag=<span class=\"hljs-literal\">None</span></span>):</span>\n    <span class=\"hljs-string\">\"\"\"\nReturns a dictionary ordered by frequency of tokens that could not be tagged.\n    \"\"\"</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"text_frequency_0\" class=\"highlights fea_text_frequency\">freq_list = []\n    <span class=\"hljs-keyword\">for</span> sent <span class=\"hljs-keyword\">in</span> tagged_sents:\n        <span class=\"hljs-keyword\">for</span> tpl <span class=\"hljs-keyword\">in</span> sent:\n            <span class=\"hljs-keyword\">if</span> tpl[<span class=\"hljs-number\">1</span>] == none_tag:\n                freq_list.append(tpl[<span class=\"hljs-number\">0</span>].lower())\n    <span class=\"hljs-keyword\">return</span> nltk.FreqDist(freq_list)</div>\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">wordnet_tagger</span>(<span class=\"hljs-params\">tagged_sents, none_tag=<span class=\"hljs-literal\">None</span></span>):</span>\n    <span class=\"hljs-string\">\"\"\"\nwordnet_tagger() function docstring.\n    \"\"\"</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_0\" class=\"highlights fea_nlp_datasets\">wnet = nltk.corpus.wordnet</div>\n    first_pass = []\n    <span class=\"hljs-keyword\">for</span> sent <span class=\"hljs-keyword\">in</span> tagged_sents:\n        new_sent = []\n        <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-built_in\">len</span>(sent)):\n            <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> sent[i][<span class=\"hljs-number\">1</span>] == none_tag:\n                new_sent.append(sent[i])\n            <span class=\"hljs-keyword\">else</span>:\n                wnss = wnet.synsets(sent[i][<span class=\"hljs-number\">0</span>].lower())\n                <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> wnss:\n                    new_sent.append(sent[i])\n                <span class=\"hljs-keyword\">else</span>:\n                    pos_tags_set = <span class=\"hljs-built_in\">set</span>(ss.pos <span class=\"hljs-keyword\">for</span> ss <span class=\"hljs-keyword\">in</span> wnss)\n                    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(pos_tags_set) == <span class=\"hljs-number\">1</span>:\n                        <span class=\"hljs-keyword\">if</span> pos_tags_set == <span class=\"hljs-built_in\">set</span>([<span class=\"hljs-string\">'n'</span>]):\n                            <span class=\"hljs-comment\"># <span class=\"hljs-doctag\">TODO:</span> distinguish between NNP, NNS, NNPS</span>\n                            new_sent.append((sent[i][<span class=\"hljs-number\">0</span>], <span class=\"hljs-string\">'NN'</span>),)\n                        <span class=\"hljs-keyword\">elif</span> pos_tags_set == <span class=\"hljs-built_in\">set</span>([<span class=\"hljs-string\">'v'</span>]):\n                            <span class=\"hljs-comment\"># <span class=\"hljs-doctag\">TODO:</span> distinguish between conjugations.</span>\n                            new_sent.append((sent[i][<span class=\"hljs-number\">0</span>], <span class=\"hljs-string\">'VB'</span>),)\n                        <span class=\"hljs-keyword\">elif</span> pos_tags_set == <span class=\"hljs-built_in\">set</span>([<span class=\"hljs-string\">'a'</span>]):\n                            new_sent.append((sent[i][<span class=\"hljs-number\">0</span>], <span class=\"hljs-string\">'JJ'</span>),)\n                        <span class=\"hljs-keyword\">elif</span> pos_tags_set == <span class=\"hljs-built_in\">set</span>([<span class=\"hljs-string\">'s'</span>]):\n                            new_sent.append((sent[i][<span class=\"hljs-number\">0</span>], <span class=\"hljs-string\">'JJ'</span>),)\n                        <span class=\"hljs-keyword\">elif</span> pos_tags_set == <span class=\"hljs-built_in\">set</span>([<span class=\"hljs-string\">'r'</span>]):\n                            new_sent.append((sent[i][<span class=\"hljs-number\">0</span>], <span class=\"hljs-string\">'RB'</span>),)\n                        <span class=\"hljs-keyword\">else</span>:\n                            new_sent.append(sent[i])\n                    <span class=\"hljs-keyword\">elif</span> <span class=\"hljs-built_in\">len</span>(pos_tags_set) == <span class=\"hljs-number\">2</span>:\n                        <span class=\"hljs-keyword\">if</span> pos_tags_set == <span class=\"hljs-built_in\">set</span>([<span class=\"hljs-string\">'a'</span>, <span class=\"hljs-string\">'s'</span>]):\n                            new_sent.append((sent[i][<span class=\"hljs-number\">0</span>], <span class=\"hljs-string\">'JJ'</span>),)\n                        <span class=\"hljs-keyword\">else</span>:\n                            new_sent.append(sent[i])\n                    <span class=\"hljs-keyword\">else</span>:\n                        new_sent.append(sent[i])\n        first_pass.append(new_sent)\n    ret_sents = first_pass\n    <span class=\"hljs-keyword\">return</span> ret_sents\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">load_tagged_corpora</span>(<span class=\"hljs-params\">show_stats=<span class=\"hljs-literal\">True</span></span>):</span>\n    <span class=\"hljs-string\">\"\"\"\nLoads the taggaed corpora (GENIA, MedPost and Treebank), makes a few\nchanges to the tags, and returns them.\n    \"\"\"</span>\n    genia_tagged = genia.corpus_load().tagged_sents()\n    mpost_tagged = medpost.corpus_load().tagged_sents()\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_1\" class=\"highlights fea_nlp_datasets\">tbank_tagged = nltk.corpus.treebank.tagged_sents()</div>\n\n    genia_tagged = transform_tags(genia_tagged, GENIA_TAG_SUBS)\n    genia_accept, genia_reject = \\\n        filter_tagged_sents(genia_tagged, VALID_TAGS)\n\n    mpost_tagged = transform_tags(mpost_tagged, MPOST_TAG_SUBS)\n    mpost_accept, mpost_reject = \\\n        filter_tagged_sents(mpost_tagged, VALID_TAGS)\n\n    tbank_tagged = transform_tags(tbank_tagged, TBANK_TAG_SUBS)\n    tbank_tagged = fix_tbank_brackets(tbank_tagged)\n    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-built_in\">len</span>(tbank_tagged)):    <span class=\"hljs-comment\"># Remove tokens/tags from sentences</span>\n        tbank_tagged[i] = [               <span class=\"hljs-comment\"># when tag == '-NONE-'. These</span>\n            w <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> tbank_tagged[i]    <span class=\"hljs-comment\"># shouldn't be there, it's a bug</span>\n            <span class=\"hljs-keyword\">if</span> w[<span class=\"hljs-number\">1</span>].upper() != <span class=\"hljs-string\">'-NONE-'</span>]  <span class=\"hljs-comment\"># in how NLTK loads the corpus.</span>\n    tbank_accept, tbank_reject = \\\n        filter_tagged_sents(tbank_tagged, VALID_TAGS)\n\n    <span class=\"hljs-keyword\">if</span> show_stats <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">True</span>:\n        <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'\\n\\nGENIA sents: '</span>, <span class=\"hljs-built_in\">len</span>(genia_tagged)\n        <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'GENIA passed:'</span>, <span class=\"hljs-built_in\">len</span>(genia_accept)\n        <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'GENIA reject:'</span>, <span class=\"hljs-built_in\">len</span>(genia_reject)\n        <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'\\nMPOST sents: '</span>, <span class=\"hljs-built_in\">len</span>(mpost_tagged)\n        <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'MPOST passed:'</span>, <span class=\"hljs-built_in\">len</span>(mpost_accept)\n        <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'MPOST reject:'</span>, <span class=\"hljs-built_in\">len</span>(mpost_reject)\n        <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'\\nTBANK sents: '</span>, <span class=\"hljs-built_in\">len</span>(tbank_tagged)\n        <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'TBANK passed:'</span>, <span class=\"hljs-built_in\">len</span>(tbank_accept)\n        <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'TBANK reject:'</span>, <span class=\"hljs-built_in\">len</span>(tbank_reject)\n\n    <span class=\"hljs-keyword\">return</span> genia_accept, mpost_accept, tbank_accept</code></pre></div>",
    "fir_36": "<div class=\"codeBlock hljs python\" id=\"fir_36\"><pre id=\"fir_36_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\">#!/usr/bin/env python</span>\n<span class=\"hljs-string\">\"\"\"Demonstration of using NLTK for Clustering\n\nCluster documents using the NLTK clustering implementations.\n\"\"\"</span>\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_0\" class=\"highlights fea_nlp_datasets\"><span class=\"hljs-keyword\">from</span> nltk.corpus <span class=\"hljs-keyword\">import</span> brown</div>\n<span class=\"hljs-comment\">#from nltk.feature import *</span>\n<span class=\"hljs-comment\">#from nltk.feature.document import *</span>\n<span class=\"hljs-keyword\">from</span> nltk.cluster <span class=\"hljs-keyword\">import</span> *\n<span class=\"hljs-keyword\">from</span> nltk.probability <span class=\"hljs-keyword\">import</span> *\n<span class=\"hljs-comment\">#import numarray</span>\n<span class=\"hljs-keyword\">from</span> numpy <span class=\"hljs-keyword\">import</span> numarray\n<span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">#</span>\n<span class=\"hljs-comment\"># Functions</span>\n<span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">#</span>\ndef document_set(classes, items):\n    <span class=\"hljs-string\">\"\"\"Create a set of documents from the brown corpus.\"\"\"</span>\n\n    ds = []                                  <span class=\"hljs-comment\"># Document set</span>\n    <span class=\"hljs-keyword\">for</span> c <span class=\"hljs-keyword\">in</span> classes:\n        <span class=\"hljs-comment\">#for item in brown.items(group=c)[:items]:</span>\n        <span class=\"hljs-built_in\">print</span> c\n        <span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> brown.fileids(categories=c)[:items]:\n            <span class=\"hljs-built_in\">print</span> item\n            d = brown.raw(item)\n            <span class=\"hljs-comment\">#d = brown.read(item)</span>\n            <span class=\"hljs-comment\">#help(d)</span>\n            <span class=\"hljs-comment\">#d['ITEM'] = item                 # For document based indices</span>\n            <span class=\"hljs-comment\">#d['CLASS'] = c                   # Set the document class</span>\n            <span class=\"hljs-built_in\">print</span> d\n            ds.append(d)\n            \n    <span class=\"hljs-keyword\">return</span> ds\n\ndef encode_documents(ds):\n    <span class=\"hljs-string\">\"\"\"Encode the document in the document sent as word frequency vectors.\n\n    Stores a numarray array object in the FEATURES key of each document\n    token. This format is required by NLTK clusterers.\n    \"\"\"</span>\n    words = set()              <span class=\"hljs-comment\"># All words in the document set</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"text_frequency_0\" class=\"highlights fea_text_frequency\">cf = ConditionalFreqDist()</div>\n    <span class=\"hljs-keyword\">for</span> d <span class=\"hljs-keyword\">in</span> ds:               <span class=\"hljs-comment\"># Loop over all documents</span>\n        <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> d[<span class=\"hljs-string\">'WORDS'</span>]:\n            t = w[<span class=\"hljs-string\">'TEXT'</span>]      <span class=\"hljs-comment\"># Word text instead of token object</span>\n            words.add(t)       <span class=\"hljs-comment\"># Identify all possible words</span>\n            cf[d[<span class=\"hljs-string\">'ITEM'</span>]].inc(t)       <span class=\"hljs-comment\"># Calculate word frequency</span>\n\n    <span class=\"hljs-comment\"># Now encode the word frequency vectors</span>\n    <span class=\"hljs-keyword\">for</span> d <span class=\"hljs-keyword\">in</span> ds:\n        v = [ cf[d[<span class=\"hljs-string\">'ITEM'</span>]].count(w) <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> words ]\n        d[<span class=\"hljs-string\">'FEATURES'</span>] = numarray.array(v)\n\n    <span class=\"hljs-keyword\">return</span> words\n\n<span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">#</span>\n<span class=\"hljs-comment\"># Demonstration, parts taken from nltk.classifier.naivebayes</span>\n<span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">######</span><span class=\"hljs-comment\">#</span>\ndef demo():\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_1\" class=\"highlights fea_nlp_datasets\">classes = brown.categories()</div>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"read document set...\"</span>\n    ds = document_set(classes,<span class=\"hljs-number\">5</span>)       <span class=\"hljs-comment\"># Take 5 documents from each class</span>\n\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"encode the document set...\"</span>\n    words = encode_documents(ds)\n\n    <span class=\"hljs-comment\"># Create a clusterer and cluster the data</span>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"k-means clustering...\"</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"clustering_0\" class=\"highlights fea_clustering\">clusterer = KMeansClusterer(len(classes), euclidean_distance)\n    clusterer.cluster(words, True)</div>\n    <span class=\"hljs-comment\">#clusterer.cluster(ds, True)</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"clustering_1\" class=\"highlights fea_clustering\">means = clusterer.means()   </div>      <span class=\"hljs-comment\"># Starting points for EM clustering</span>\n\n    <span class=\"hljs-comment\"># Print out cluster assignments</span>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"K-Means Cluster assignments\"</span>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"---------------------------\"</span>\n    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> range(len(ds)):\n        <span class=\"hljs-built_in\">print</span> ds[i][<span class=\"hljs-string\">'ITEM'</span>], <span class=\"hljs-string\">\"\\t\"</span>, ds[i][<span class=\"hljs-string\">'CLUSTER'</span>], <span class=\"hljs-string\">\"\\t\"</span>, classes[i / <span class=\"hljs-number\">5</span>]\n\n    <span class=\"hljs-comment\"># Perform agglomerative clustering, uses cosine distance</span>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"GAAC clustering...\"</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"clustering_2\" class=\"highlights fea_clustering\">clusterer = GroupAverageAgglomerativeClusterer(len(classes))\n    clusterer.cluster(ds, True)</div>\n\n    <span class=\"hljs-comment\"># Print out cluster assignments</span>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"GAAC Cluster assignments\"</span>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"------------------------\"</span>\n    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> range(len(ds)):\n        <span class=\"hljs-built_in\">print</span> ds[i][<span class=\"hljs-string\">'ITEM'</span>], <span class=\"hljs-string\">\"\\t\"</span>, ds[i][<span class=\"hljs-string\">'CLUSTER'</span>], <span class=\"hljs-string\">\"\\t\"</span>, classes[i / <span class=\"hljs-number\">5</span>]\n\n    <span class=\"hljs-comment\"># Show the dendrogram, note the NLTK method is mispelled!!!!</span>\n    <span class=\"hljs-comment\">#clusterer.dendogram().show()</span>\n\n    <span class=\"hljs-comment\"># Use EM clustering, performing feature reduction using SVD</span>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"EM clustering...\"</span>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"Identifying initial cluster means...\"</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"clustering_3\" class=\"highlights fea_clustering\">clusterer = KMeansClusterer(len(classes), euclidean_distance, \n            svd_dimensions = <span class=\"hljs-number\">1</span>)\n    clusterer.cluster(ds, False)\n    means = clusterer.means()         <span class=\"hljs-comment\"># Starting points for EM clustering</span>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"Performing EM clustering...\"</span>\n    clusterer = ExpectationMaximizationClusterer(means, svd_dimensions = <span class=\"hljs-number\">1</span>)\n    clusterer.cluster(ds, True)</div>\n\n    <span class=\"hljs-comment\"># Print out cluster assignments</span>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"EM Cluster assignments\"</span>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"----------------------\"</span>\n    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> range(len(ds)):\n        <span class=\"hljs-built_in\">print</span> ds[i][<span class=\"hljs-string\">'ITEM'</span>], <span class=\"hljs-string\">\"\\t\"</span>, ds[i][<span class=\"hljs-string\">'CLUSTER'</span>], <span class=\"hljs-string\">\"\\t\"</span>, classes[i / <span class=\"hljs-number\">5</span>]\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>: demo()\n\n<span class=\"hljs-comment\">#http://webcache.googleusercontent.com/search?q=cache:aVwJt0Bq_5YJ:https://www.mscs.mu.edu/~cstruble/moodle/mod/resource/view.php%3Finpopup%3Dtrue%26id%3D157+def+encode_documents%28ds%29:&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=in</span></code></pre></div>",
    "fir_37": "<div class=\"codeBlock hljs python\" id=\"fir_37\"><pre id=\"fir_37_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">import</span> pika\n<span class=\"hljs-keyword\">import</span> uuid\n<span class=\"hljs-keyword\">import</span> logging\n<span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"lemmatization_0\" class=\"highlights fea_lemmatization\"><span class=\"hljs-keyword\">from</span> nltk.stem.wordnet <span class=\"hljs-keyword\">import</span> WordNetLemmatizer</div>\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Relations</span>():</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span>\n        self.rpc_client = RpcClient()\n        self._logger = logging.getLogger(<span class=\"hljs-string\">\"relations\"</span>)\n    \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">extract_triples</span>(<span class=\"hljs-params\">self, sentence, normalize=<span class=\"hljs-literal\">False</span>, lex_syn_constraints=<span class=\"hljs-literal\">False</span>, allow_unary=<span class=\"hljs-literal\">False</span>, short_rel=<span class=\"hljs-literal\">False</span></span>):</span>\n        np_chunks_tree = self._get_chunks(sentence)\n        reverb = Reverb(sentence, np_chunks_tree, normalize, lex_syn_constraints, allow_unary, short_rel)\n        triples = reverb.extract_triples()\n        <span class=\"hljs-keyword\">return</span> triples\n        \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">_get_chunks</span>(<span class=\"hljs-params\">self, sentence</span>):</span>\n        self._logger.info(<span class=\"hljs-string\">\" [x] Requesting chunking for a sentence\"</span>)\n        response = self.rpc_client.call(sentence)\n        self._logger.info(<span class=\"hljs-string\">\" [.] Got %r\"</span> % (response,))\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tree_0\" class=\"highlights fea_tree\">tree = nltk.Tree(response)</div>\n        <span class=\"hljs-keyword\">return</span> tree\n    \n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Reverb</span>():</span>\n    <span class=\"hljs-string\">\"\"\"\n    Approximately Reverb (regex part of it) translation.\n    \"\"\"</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, sentence, np_chunks_tree, normalize, lex_syn_constraints, allow_unary, short_rel</span>):</span>\n        <span class=\"hljs-string\">\"\"\"\n        :param lex_syn_constraints: Use syntactic and lexical constraints.\n        :param allow_unary: allow unary relations (\"the book is published\").\n        \"\"\"</span>\n        self.sentence = sentence\n        self.np_chunks_tree = np_chunks_tree\n        self.normalize = normalize\n        self.lex_syn_constraints = lex_syn_constraints\n        self.allow_unary = allow_unary\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_0\" class=\"highlights fea_Part_of_Speech\">self.pos_tags = nltk.pos_tag</div>(<div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">nltk.word_tokenize(self.sentence)</div>)\n        self.short_rel = short_rel\n        self._logger = logging.getLogger(<span class=\"hljs-string\">\"relations\"</span>)\n        \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">extract_triples</span>(<span class=\"hljs-params\">self</span>):</span>\n        <span class=\"hljs-string\">\"\"\"\n        Extracts (subject predicate object) triples from the given sentence. Returns a list of triples.\n        \"\"\"</span>\n        <span class=\"hljs-comment\"># verb = optional adverb [modal or other verbs] optional particle/adverb</span>\n        verb = <span class=\"hljs-string\">\"&lt;RB&gt;?&lt;MD|VB|VBD|VBP|VBZ|VBG|VBN&gt;&lt;RP&gt;?&lt;RB&gt;?\"</span>\n        preposition =  <span class=\"hljs-string\">\"&lt;RB&gt;?&lt;IN|TO|RP&gt;&lt;RB&gt;?\"</span>\n        word = <span class=\"hljs-string\">\"&lt;PRP$|CD|DT|JJ|JJS|JJR|NN|NNS|NNP|NNPS|POS|PRP|RB|RBR|RBS|VBN|VBG&gt;\"</span>\n        cp = <span class=\"hljs-literal\">None</span>\n        <span class=\"hljs-keyword\">if</span>  self.short_rel:\n            short_rel_pattern = <span class=\"hljs-string\">\"(%s(%s)?)+\"</span> % (verb, preposition)\n            grammar_short = <span class=\"hljs-string\">\"REL: {%s}\"</span> % short_rel_pattern\n            <div style=\"background-color: #f6b73c; display: inline;\" id=\"parsing_0\" class=\"highlights fea_parsing\">cp = nltk.RegexpParser(grammar_short)</div>\n        <span class=\"hljs-keyword\">else</span>:\n            long_rel_pattern = <span class=\"hljs-string\">\"(%s(%s*(%s)+)?)+\"</span> % (verb, word, preposition)\n            grammar_long = <span class=\"hljs-string\">\"REL: {%s}\"</span> % long_rel_pattern\n            <div style=\"background-color: #f6b73c; display: inline;\" id=\"parsing_1\" class=\"highlights fea_parsing\">cp = nltk.RegexpParser(grammar_long)</div>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"parsing_2\" class=\"highlights fea_parsing\">tree = cp.parse</div>(<div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_1\" class=\"highlights fea_Part_of_Speech\">self.pos_tags</div>)\n        self._logger.info(<span class=\"hljs-string\">\"rel: %s\"</span> % tree)\n        triples = self._get_triples(tree)\n        <span class=\"hljs-keyword\">return</span> triples\n    \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">_get_triples</span>(<span class=\"hljs-params\">self, tree</span>):</span>\n        relation = <span class=\"hljs-string\">\"\"</span>\n        left_part = <span class=\"hljs-string\">\"\"</span>\n        right_part = <span class=\"hljs-string\">\"\"</span>\n        triples = []\n        relations = []\n        normalized_relations = []\n        non_relations = []\n        rel_indices = []\n        curr = <span class=\"hljs-string\">\"\"</span>\n        index = <span class=\"hljs-number\">0</span>\n        <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> tree:\n            <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">type</span>(t) == nltk.Tree <span class=\"hljs-keyword\">and</span> t.node == <span class=\"hljs-string\">\"REL\"</span>:\n                rel_ind = (index, index + <span class=\"hljs-built_in\">len</span>(t.leaves()))\n                rel_indices.append(rel_ind)\n                relation = <span class=\"hljs-string\">\" \"</span>.join(<span class=\"hljs-built_in\">map</span>(<span class=\"hljs-keyword\">lambda</span> x : x[<span class=\"hljs-number\">0</span>], t.leaves()))\n                relations.append(relation)\n                <span class=\"hljs-keyword\">if</span> self.normalize:\n                    norm_rel = self.verb_rel_normalize(rel_ind)\n                <span class=\"hljs-keyword\">else</span>:\n                    norm_rel = relation\n                normalized_relations.append(norm_rel)\n                non_relations.append(curr.strip())\n                curr = <span class=\"hljs-string\">\"\"</span>\n                index += <span class=\"hljs-built_in\">len</span>(t.leaves())\n            <span class=\"hljs-keyword\">else</span>:\n                curr += <span class=\"hljs-string\">\" \"</span> + t[<span class=\"hljs-number\">0</span>]\n                index += <span class=\"hljs-number\">1</span>\n        <span class=\"hljs-keyword\">if</span> curr.strip() != <span class=\"hljs-string\">\"\"</span>: <span class=\"hljs-comment\"># if relation is the last item in the tree drop the following appending </span>\n            non_relations.append(curr.strip())\n        <span class=\"hljs-keyword\">for</span> ind, rel <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(relations):\n            left_part = non_relations[ind]\n            right_part = <span class=\"hljs-string\">\"\"</span>\n            <span class=\"hljs-keyword\">if</span> ind + <span class=\"hljs-number\">1</span> &lt; <span class=\"hljs-built_in\">len</span>(non_relations):\n                right_part = non_relations[ind + <span class=\"hljs-number\">1</span>]\n            <span class=\"hljs-keyword\">else</span>:\n                <span class=\"hljs-keyword\">break</span>\n            left_arg = self._get_left_arg(rel, rel_indices[ind][<span class=\"hljs-number\">0</span>], left_part)\n            right_arg = self._get_right_arg(rel, rel_indices[ind][<span class=\"hljs-number\">1</span>], right_part)\n            <span class=\"hljs-keyword\">if</span> self.lex_syn_constraints:\n                <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> self._is_relation_lex_syn_valid(rel, rel_indices[ind]):\n                    <span class=\"hljs-keyword\">continue</span>\n            <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> self.allow_unary <span class=\"hljs-keyword\">and</span> right_arg == <span class=\"hljs-string\">\"\"</span>:\n                <span class=\"hljs-keyword\">continue</span>\n            <span class=\"hljs-keyword\">if</span> left_arg == <span class=\"hljs-string\">\"\"</span>:\n                <span class=\"hljs-comment\"># todo: try to extract left_arg even the subject is for example before comma like in</span>\n                <span class=\"hljs-comment\"># the following sentence (for \"has\" relation):</span>\n                <span class=\"hljs-comment\"># This chassis supports up to six fans , has a complete black interior</span>\n                <span class=\"hljs-keyword\">continue</span>\n            triples.append((left_arg, normalized_relations[ind], right_arg))\n        <span class=\"hljs-keyword\">return</span> triples</code></pre></div>",
    "fir_38": "<div class=\"codeBlock hljs python\" id=\"fir_38\"><pre id=\"fir_38_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-string\">\"\"\"\nThis code was used to generate a destemmer.\nThe destemmer is a function in the clue parser that is called by a number of solvers.\nEssentially, it takes a word/stem and returns all the possible variations on the word.\n\"\"\"</span>\n\n<span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">from</span> collections <span class=\"hljs-keyword\">import</span> *\n\n<span class=\"hljs-comment\"># use porter stemmer to stem words</span>\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"stemming_2\" class=\"highlights fea_stemming\"><span class=\"hljs-keyword\">from</span> nltk.stem.porter <span class=\"hljs-keyword\">import</span> PorterStemmer</div>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">stem</span>(<span class=\"hljs-params\">stemmer, text</span>):</span>\n\t<span class=\"hljs-string\">\"\"\"Stems the text (obviously)\"\"\"</span>\n\tstemdict = defaultdict(<span class=\"hljs-built_in\">list</span>)\n\t<span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> text:\n\t\ttoken = token.lower()\n\t\t<span class=\"hljs-keyword\">if</span> token.isalpha():\n\t\t\t<span class=\"hljs-keyword\">if</span> token <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> stemdict[stemmer.stem(token)]:\n\t\t\t\tstemdict[stemmer.stem(token)] .append(token)\n\t<span class=\"hljs-keyword\">return</span> stemdict\n\nwords = []\n<span class=\"hljs-comment\"># adding lots of word corpuses--all the ones we could find in nltk</span>\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_0\" class=\"highlights fea_nlp_datasets\"><span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> nltk.corpus.gutenberg.words():\n\twords.append(item)\n<span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> nltk.corpus.brown.words():\n\twords.append(item)\n<span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> nltk.corpus.inaugural.words():\n\twords.append(item)\n<span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> nltk.corpus.reuters.words():\n\twords.append(item)\n<span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> nltk.corpus.switchboard.words():\n\twords.append(item)\n<span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> nltk.corpus.words.words():\n\twords.append(item)\n<span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> nltk.corpus.movie_reviews.words():\n\twords.append(item)</div>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"stemming_0\" class=\"highlights fea_stemming\">stemmer = PorterStemmer()</div>\n<span class=\"hljs-comment\"># stem every word in the corpuses</span>\n<span class=\"hljs-built_in\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"stemming_1\" class=\"highlights fea_stemming\"><span class=\"hljs-built_in\">dict</span> = stem(stemmer, words)</div>\nfile = <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">'stemdata'</span>, <span class=\"hljs-string\">'a'</span>)\n<span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">dict</span>.keys():\n\tstring = item + <span class=\"hljs-string\">'\\t'</span>\n\t<span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">dict</span>[item]:\n\t\tstring += i + <span class=\"hljs-string\">' '</span>\n\tfile.write(string+<span class=\"hljs-string\">'\\n'</span>)\nfile.close()</code></pre></div>",
    "fir_39": "<div class=\"codeBlock hljs python\" id=\"fir_39\"><pre id=\"fir_39_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-string\">\"\"\"\n    Ivy Pi Memorizer\n    Copyright 2008 Ivy Call, LLC &lt;contact AT@AT ivycall.com&gt;\n\n    THIS SOFTWARE IS SUPPLIED WITHOUT WARRANTY OF ANY KIND.\n\n    2008-03-13\n    Released by Author: Joseph Javier Perla\n\n    LICENSE: GPLv3\n\"\"\"</span>\n<span class=\"hljs-keyword\">import</span> nltk\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">grab_leaves</span>(<span class=\"hljs-params\">tree</span>):</span>\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(tree[<div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_0\" class=\"highlights fea_nlp_datasets\">nltk.wordnet.HYPONYM</div>]) == <span class=\"hljs-number\">0</span>:\n        <span class=\"hljs-keyword\">return</span> tree.words\n    <span class=\"hljs-keyword\">else</span>:\n        leaves = []\n        <span class=\"hljs-keyword\">for</span> hyponym <span class=\"hljs-keyword\">in</span> tree[<div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_1\" class=\"highlights fea_nlp_datasets\">nltk.wordnet.HYPONYM</div>]:\n            leaves.extend(grab_leaves(hyponym))\n        <span class=\"hljs-keyword\">return</span> leaves\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_all_physical_entities</span>():</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_2\" class=\"highlights fea_nlp_datasets\">physical_entity = nltk.wordnet.N[<span class=\"hljs-string\">'physical_entity'</span>].synsets()[<span class=\"hljs-number\">0</span>]</div>\n    <span class=\"hljs-keyword\">return</span> grab_leaves(physical_entity)\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">endswith_any</span>(<span class=\"hljs-params\">word, endings</span>):</span>\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">any</span>(<span class=\"hljs-literal\">True</span> <span class=\"hljs-keyword\">for</span> ending <span class=\"hljs-keyword\">in</span> endings <span class=\"hljs-keyword\">if</span> word.endswith(ending))\n\n\nabstract_noun_endings = [<span class=\"hljs-string\">'ness'</span>,<span class=\"hljs-string\">'tion'</span>,<span class=\"hljs-string\">'ly'</span>,<span class=\"hljs-string\">'day'</span>,<span class=\"hljs-string\">'aire'</span>,<span class=\"hljs-string\">'berg'</span>,<span class=\"hljs-string\">'ing'</span>,<span class=\"hljs-string\">'ant'</span>,<span class=\"hljs-string\">'ry'</span>,<span class=\"hljs-string\">'ity'</span>, <span class=\"hljs-string\">'ese'</span>, <span class=\"hljs-string\">'lpn'</span>]\n\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_3\" class=\"highlights fea_nlp_datasets\">all_verbs = <span class=\"hljs-built_in\">set</span>(nltk.wordnet.V.keys())\nall_adjectives = <span class=\"hljs-built_in\">set</span>(nltk.wordnet.ADJ.keys())\nall_adverbs = <span class=\"hljs-built_in\">set</span>(nltk.wordnet.ADV.keys())</div>\nall_non_nouns = all_verbs.union(all_adjectives).union(all_adverbs)\nphysical_entities = <span class=\"hljs-built_in\">set</span>([w.lower() <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> get_all_physical_entities()])\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_4\" class=\"highlights fea_nlp_datasets\">pronounce_words = <span class=\"hljs-built_in\">set</span>([w.lower() <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> nltk.corpus.cmudict.transcriptions()])\ngutenberg_words = <span class=\"hljs-built_in\">set</span>([w.lower() <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> nltk.corpus.gutenberg.words()])\ndictionary_words = <span class=\"hljs-built_in\">set</span>([w.lower() <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> nltk.corpus.words.words()])</div>\nbad_words = <span class=\"hljs-built_in\">set</span>([<span class=\"hljs-string\">'nigger'</span>,<span class=\"hljs-string\">'cunnilingus'</span>,<span class=\"hljs-string\">'fetish'</span>,<span class=\"hljs-string\">'vagina'</span>])\nmanual_abstract_nouns = <span class=\"hljs-built_in\">set</span>([<span class=\"hljs-string\">'life'</span>,<span class=\"hljs-string\">'amaranth'</span>,<span class=\"hljs-string\">'jacob'</span>,<span class=\"hljs-string\">'backup'</span>,<span class=\"hljs-string\">'changer'</span>,<span class=\"hljs-string\">'woof'</span>,<span class=\"hljs-string\">'ounce'</span>,<span class=\"hljs-string\">'baker'</span>,<span class=\"hljs-string\">'phoebe'</span>,<span class=\"hljs-string\">'geneva'</span>,<span class=\"hljs-string\">'fennel'</span>,<span class=\"hljs-string\">'digs'</span>,<span class=\"hljs-string\">'anise'</span>,<span class=\"hljs-string\">'savoy'</span>,<span class=\"hljs-string\">'outtake'</span>,<span class=\"hljs-string\">'osage'</span>,<span class=\"hljs-string\">'epicure'</span>,<span class=\"hljs-string\">'gneiss'</span>,<span class=\"hljs-string\">'nisei'</span>,<span class=\"hljs-string\">'packer'</span>,<span class=\"hljs-string\">'footage'</span>,<span class=\"hljs-string\">'soave'</span>,<span class=\"hljs-string\">'jonah'</span>,<span class=\"hljs-string\">'klutz'</span>,<span class=\"hljs-string\">'minium'</span>,<span class=\"hljs-string\">'swatch'</span>,<span class=\"hljs-string\">'gean'</span>,<span class=\"hljs-string\">'gene'</span>,<span class=\"hljs-string\">'jenny'</span>,<span class=\"hljs-string\">'johnny'</span>,<span class=\"hljs-string\">'jane'</span>,<span class=\"hljs-string\">'fed'</span>,<span class=\"hljs-string\">'backer'</span>,<span class=\"hljs-string\">'haft'</span>,<span class=\"hljs-string\">'shawnee'</span>,<span class=\"hljs-string\">'fission'</span>,<span class=\"hljs-string\">'novice'</span>,<span class=\"hljs-string\">'cob'</span>,<span class=\"hljs-string\">'neve'</span>,<span class=\"hljs-string\">'heifer'</span>,<span class=\"hljs-string\">'chino'</span>])\nnormal_words = pronounce_words - bad_words - manual_abstract_nouns\n\ncommon_nouns = physical_entities.intersection(normal_words) - all_non_nouns\nconcrete_nouns = [noun <span class=\"hljs-keyword\">for</span> noun <span class=\"hljs-keyword\">in</span> common_nouns \\\n                            <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> endswith_any(noun, abstract_noun_endings) \\\n                                <span class=\"hljs-keyword\">and</span> <span class=\"hljs-built_in\">len</span>(noun) &gt;= <span class=\"hljs-number\">3</span>]\n\n\n\n<span class=\"hljs-keyword\">import</span> csv\nw = csv.writer(<span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">'nouns.csv'</span>, <span class=\"hljs-string\">'w'</span>))\n<span class=\"hljs-keyword\">for</span> noun <span class=\"hljs-keyword\">in</span> concrete_nouns:\n    w.writerow([noun.replace(<span class=\"hljs-string\">'_'</span>, <span class=\"hljs-string\">' '</span>)])\n</code></pre></div>",
    "fir_40": "<div class=\"codeBlock hljs python\" id=\"fir_40\"><pre id=\"fir_40_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\">#!/usr/bin/env python</span>\n\n<span class=\"hljs-keyword\">import</span> collections\n<span class=\"hljs-keyword\">import</span> nltk\n\nGRAMMAR = <span class=\"hljs-string\">\"\"\"%start S\nS -&gt; NP VP [1.0]\nNP -&gt; 'Arthur' [1.0]\nVP -&gt; 'is' 'the' 'king' '.' [0.8]\nVP -&gt; 'is' 'awesome' [0.2]\n#V -&gt; 'is' [1.0]\n#VNP -&gt; 'the' [0.5] | 'king' [0.5]\n\"\"\"</span>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">load_terminals</span>():</span>\n    terminals = <span class=\"hljs-built_in\">set</span>()\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">\"allowed_words\"</span>, <span class=\"hljs-string\">\"r\"</span>) <span class=\"hljs-keyword\">as</span> allowed_words:\n        <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> allowed_words:\n            word = word.strip()\n            terminals.add(word)\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">frozenset</span>(terminals)\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">build_grammar</span>():</span>\n    terminals = load_terminals()\n    <span class=\"hljs-keyword\">assert</span> <span class=\"hljs-built_in\">len</span>(terminals) &gt; <span class=\"hljs-number\">0</span>\n    rules = [] <span class=\"hljs-comment\"># Each rule is (count, lhs-nonterminal, [rhs-sequence-of-terminals&amp;nonterminals])</span>\n    lhs_count = collections.defaultdict(<span class=\"hljs-built_in\">int</span>)\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">\"grammar.gr\"</span>, <span class=\"hljs-string\">\"r\"</span>) <span class=\"hljs-keyword\">as</span> raw_grammar:\n        <span class=\"hljs-keyword\">for</span> line <span class=\"hljs-keyword\">in</span> raw_grammar:\n            line = line.strip()\n            <span class=\"hljs-keyword\">if</span> line.startswith(<span class=\"hljs-string\">'#'</span>) <span class=\"hljs-keyword\">or</span> line == <span class=\"hljs-string\">''</span>:\n                <span class=\"hljs-keyword\">continue</span>\n            rule_components = line.split()\n            <span class=\"hljs-keyword\">assert</span> <span class=\"hljs-built_in\">len</span>(rule_components) &gt;= <span class=\"hljs-number\">3</span>, <span class=\"hljs-string\">\"Line %s is not a valid rule\"</span> % line\n            count = <span class=\"hljs-built_in\">int</span>(rule_components[<span class=\"hljs-number\">0</span>])\n            lhs_raw = rule_components[<span class=\"hljs-number\">1</span>]\n            lhs_count[lhs_raw] = lhs_count[lhs_raw] + count\n            lhs = <div style=\"background-color: #f6b73c; display: inline;\" id=\"tree_0\" class=\"highlights fea_tree\">nltk.grammar.Nonterminal</div>(lhs_raw)\n            rhs = []\n            <span class=\"hljs-keyword\">for</span> symbol <span class=\"hljs-keyword\">in</span> rule_components[<span class=\"hljs-number\">2</span>:]:\n                rhs.append(symbol <span class=\"hljs-keyword\">if</span> symbol <span class=\"hljs-keyword\">in</span> terminals\n                    <span class=\"hljs-keyword\">else</span> <div style=\"background-color: #f6b73c; display: inline;\" id=\"tree_1\" class=\"highlights fea_tree\">nltk.grammar.Nonterminal(symbol))</div>\n            <span class=\"hljs-keyword\">assert</span> <span class=\"hljs-built_in\">len</span>(rhs) &gt; <span class=\"hljs-number\">0</span>\n            rules.append((count, lhs, rhs))\n    <span class=\"hljs-keyword\">assert</span> <span class=\"hljs-built_in\">len</span>(rules) &gt; <span class=\"hljs-number\">0</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"tree_3\" class=\"highlights fea_tree\">productions = [nltk.grammar.WeightedProduction(lhs, rhs, prob=(<span class=\"hljs-built_in\">float</span>(count) / lhs_count[lhs.symbol()]))\n        <span class=\"hljs-keyword\">for</span> (count, lhs, rhs) <span class=\"hljs-keyword\">in</span> rules]</div>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"tree_2\" class=\"highlights fea_tree\">start = nltk.grammar.Nonterminal(<span class=\"hljs-string\">\"START\"</span>)\n    grammar = nltk.grammar.WeightedGrammar(start, productions)</div>\n    <span class=\"hljs-keyword\">return</span> grammar\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">build_parser</span>():</span>\n    <span class=\"hljs-comment\">#grammar = nltk.grammar.parse_pcfg(GRAMMAR)</span>\n    grammar = build_grammar()\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"parsing_0\" class=\"highlights fea_parsing\">parser = nltk.parse.pchart.InsideChartParser(grammar)\n    <span class=\"hljs-keyword\">return</span> parser</div>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">parse_sentence</span>(<span class=\"hljs-params\">parser, raw_sentence</span>):</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">sentence = nltk.tokenize.word_tokenize(raw_sentence)</div>\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"parsing_1\" class=\"highlights fea_parsing\"><span class=\"hljs-keyword\">return</span> parser.parse(sentence)</div>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">main</span>():</span>\n    parser = build_parser()\n    is_debug = <span class=\"hljs-literal\">True</span>\n    <span class=\"hljs-keyword\">if</span> is_debug:\n        parse_tree = parse_sentence(parser, <span class=\"hljs-string\">\"Arthur is the king\"</span>)\n        print(parse_tree)\n        print(parse_tree.logprob())\n        <span class=\"hljs-keyword\">return</span>\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">\"examples.sen\"</span>, <span class=\"hljs-string\">\"r\"</span>) <span class=\"hljs-keyword\">as</span> sentences:\n        <span class=\"hljs-keyword\">for</span> sentence <span class=\"hljs-keyword\">in</span> sentences:\n            parse_tree = parse_sentence(parser, sentence)\n            print(<span class=\"hljs-string\">\"log prob = %.2f\\tsentence : %s\"</span> % (parse_tree.logprob(), sentence.strip()))\n\n<span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">\"__main__\"</span> == __name__:\n    main()</code></pre></div>",
    "fir_41": "<div class=\"codeBlock hljs python\" id=\"fir_41\"><pre id=\"fir_41_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\">#!/usr/bin/env python</span>\n\n<span class=\"hljs-string\">'''NLTK learning\n'''</span>\n\n<span class=\"hljs-comment\"># Imports</span>\n<span class=\"hljs-keyword\">import</span> sys, nltk, re, pprint\n\n<span class=\"hljs-comment\"># Global Variables</span>\n\n<span class=\"hljs-comment\"># Class Declarations</span>\n\n<span class=\"hljs-comment\"># Function Declarations</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">getcontent</span>(<span class=\"hljs-params\">tokens</span>):</span>\n   <span class=\"hljs-string\">'''\n   Use NLTK to drive off stopwords\n   '''</span>\n   <div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_0\" class=\"highlights fea_nlp_datasets\">stopwords = nltk.corpus.stopwords.words(<span class=\"hljs-string\">'english'</span>)</div>\n   content = [w <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> tokens <span class=\"hljs-keyword\">if</span> w.lower() <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> unicode(stopwords)]\n   longcontent = [w <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> content <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(w) &gt; <span class=\"hljs-number\">3</span>]\n\n   <span class=\"hljs-keyword\">return</span> longcontent\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">topcontent</span>(<span class=\"hljs-params\">tokens</span>):</span>\n   <span class=\"hljs-string\">'''\n   Use NLTK to discover the top ten most frequent terms\n   '''</span>\n   content = getcontent(tokens)\n\n   <div style=\"background-color: #f6b73c; display: inline;\" id=\"text_frequency_0\" class=\"highlights fea_text_frequency\">fdist = nltk.FreqDist(content)\n   vocab = fdist.keys()</div>\n\n   <span class=\"hljs-keyword\">return</span> vocab[:<span class=\"hljs-number\">50</span>]\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">contentfraction</span>(<span class=\"hljs-params\">tokens</span>):</span>\n   <span class=\"hljs-string\">'''\n   Sample function from NLTK section 2.4.1\n   '''</span>\n   <span class=\"hljs-keyword\">from</span> decimal <span class=\"hljs-keyword\">import</span> Decimal\n\n   content = getcontent(tokens)\n\n   <span class=\"hljs-keyword\">return</span> Decimal(<span class=\"hljs-built_in\">len</span>(content)) / Decimal(<span class=\"hljs-built_in\">len</span>(tokens))\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">tokenize</span>(<span class=\"hljs-params\">feedsetfile</span>):</span>\n   <span class=\"hljs-string\">'''\n   Use NLTK to tokenize titles from Feed Set\n   '''</span>\n   <span class=\"hljs-keyword\">import</span> feedaggregator\n\n   feedset = feedaggregator.copyexistingfeeds(feedsetfile)\n   titletokens = []\n   <span class=\"hljs-keyword\">for</span> (title, link) <span class=\"hljs-keyword\">in</span> feedset:\n      <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">tokens = nltk.word_tokenize(title)</div>\n      titletokens.extend(tokens)\n\n   <span class=\"hljs-keyword\">return</span> titletokens\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">getpicklefile</span>():</span>\n   <span class=\"hljs-string\">'''\n   Parse for the Persistent Feed Set\n   '''</span>\n   <span class=\"hljs-keyword\">from</span> optparse <span class=\"hljs-keyword\">import</span> OptionParser\n   <span class=\"hljs-keyword\">from</span> os.path <span class=\"hljs-keyword\">import</span> exists\n\n   parser = OptionParser()\n   parser.add_option(<span class=\"hljs-string\">\"-f\"</span>, <span class=\"hljs-string\">\"--file\"</span>, dest=<span class=\"hljs-string\">\"filename\"</span>,\n                     <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">\"Persistent Feed Set FILE\"</span>, metavar=<span class=\"hljs-string\">\"FILE\"</span>)\n   (options, args) = parser.parse_args()\n\n   <span class=\"hljs-keyword\">if</span> options.filename == <span class=\"hljs-literal\">None</span>:\n      parser.print_help()\n      sys.exit()\n\n   <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> exists(options.filename):\n      <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'No such file: '</span>, options.filename\n      sys.exit()\n\n   <span class=\"hljs-keyword\">return</span> options.filename\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">main</span>():</span>\n   <span class=\"hljs-string\">''' \n   Entry point for the stand-alone program\n   '''</span>\n   picklefile = getpicklefile()\n   tokens = tokenize(picklefile)\n   fraction = contentfraction(tokens)\n   mostfrequent = topcontent(tokens)\n\n   <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'content fraction: '</span> + <span class=\"hljs-built_in\">str</span>(fraction)\n   <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'top ten: '</span>\n   pprint.pprint(mostfrequent)\n\n<span class=\"hljs-comment\"># \"main\" body</span>\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n    main()</code></pre></div>",
    "fir_42": "<div class=\"codeBlock hljs python\" id=\"fir_42\"><pre id=\"fir_42_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">import</span> numpy\n<span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">from</span> nltk.stem.wordnet <span class=\"hljs-keyword\">import</span> WordNetLemmatizer\n<span class=\"hljs-keyword\">from</span> nltk.corpus <span class=\"hljs-keyword\">import</span> stopwords\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_term_document_matrix</span>(<span class=\"hljs-params\">docs</span>):</span>\n\t<span class=\"hljs-string\">\"\"\"\n\tCompute a term-document matrix such that td_matrix[doc_id][term]\n\treturns a tf-idf score for the term in the document\n\t\"\"\"</span>\n\tentities = [entity[<span class=\"hljs-string\">'content'</span>].lower().split() <span class=\"hljs-keyword\">for</span> entity <span class=\"hljs-keyword\">in</span> docs]\n\ttc = nltk.TextCollection(entities)\n\t<span class=\"hljs-built_in\">print</span> entities\n\ttd_matrix = {}\n\t<span class=\"hljs-keyword\">for</span> idx <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-built_in\">len</span>(entities)):\n\t\tentity = entities[idx]\n\t\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"text_frequency_0\" class=\"highlights fea_text_frequency\">fdist = nltk.FreqDist(entity)</div>\n\t\tdoc_id = docs[idx][<span class=\"hljs-string\">'id'</span>]\n\t\ttd_matrix[doc_id] = {}\n\t\t<span class=\"hljs-keyword\">for</span> term <span class=\"hljs-keyword\">in</span> fdist.iterkeys():\n\t\t\ttd_matrix[doc_id][term] = tc.tf_idf(term, entity)\n\t\t\t<span class=\"hljs-comment\">#print \"%s , %s : %s\" % (doc_id, term, td_matrix[doc_id][term])</span>\n\t<span class=\"hljs-keyword\">return</span> td_matrix\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_distances</span>(<span class=\"hljs-params\">td_matrix</span>):</span>\n\t<span class=\"hljs-string\">\"\"\"\n\tBuild vectors such that term scores are in the same positions...\n\t\"\"\"</span>\n\tdistances = {}\n\t<span class=\"hljs-keyword\">for</span> doc_id <span class=\"hljs-keyword\">in</span> td_matrix.keys():\n\t\tdistances[doc_id] = {}\n\t\t<span class=\"hljs-keyword\">for</span> doc_id2 <span class=\"hljs-keyword\">in</span> td_matrix.keys():\n\t\t\t<span class=\"hljs-comment\"># Take care not to mutate the original data structures</span>\n\t\t\t<span class=\"hljs-comment\"># since we're in a loop and need the originals multiple times</span>\n\t\t\tterms1 = td_matrix[doc_id].copy()\n\t\t\tterms2 = td_matrix[doc_id2].copy()\n\t\t\t<span class=\"hljs-comment\"># Fill in \"gaps\" in each map so vectors of the same length can be computed</span>\n\t\t\t<span class=\"hljs-keyword\">for</span> term1 <span class=\"hljs-keyword\">in</span> terms1:\n\t\t\t\t<span class=\"hljs-keyword\">if</span> term1 <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> terms2:\n\t\t\t\t\tterms2[term1] = <span class=\"hljs-number\">0</span>\n\t\t\t<span class=\"hljs-keyword\">for</span> term2 <span class=\"hljs-keyword\">in</span> terms2:\n\t\t\t\t<span class=\"hljs-keyword\">if</span> term2 <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> terms1:\n\t\t\t\t\tterms1[term2] = <span class=\"hljs-number\">0</span>\n\t\t\t<span class=\"hljs-comment\"># Create vectors from term maps</span>\n\t\t\tv1 = [score <span class=\"hljs-keyword\">for</span> (_, score) <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">sorted</span>(terms1.items())]\n\t\t\tv2 = [score <span class=\"hljs-keyword\">for</span> (_, score) <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">sorted</span>(terms2.items())]\n\t\t\t<span class=\"hljs-comment\"># Compute similarity amongst documents</span>\n\t\t\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"text_similarity_0\" class=\"highlights fea_text_similarity\">distance = nltk.cluster.util.cosine_distance(v1, v2)</div>\n\t\t\tdistances[doc_id][doc_id2] = distance\n\t<span class=\"hljs-keyword\">return</span> distances\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_standard_deviation</span>(<span class=\"hljs-params\">distances</span>):</span>\n\t<span class=\"hljs-string\">\"\"\"\n\tCompute the standard deviation for the distances as a basis of automated thresholding.\n\t\"\"\"</span>\n\tstd = numpy.std([distances[k1][k2] <span class=\"hljs-keyword\">for</span> k1 <span class=\"hljs-keyword\">in</span> distances <span class=\"hljs-keyword\">for</span> k2 <span class=\"hljs-keyword\">in</span> distances[k1]])\n\t<span class=\"hljs-keyword\">return</span> std\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">clean_tweet</span>(<span class=\"hljs-params\">tweet, lang=<span class=\"hljs-string\">\"english\"</span></span>):</span>\n\ttweet = extract_urls(tweet)\n\ttweet = tweet.lower()\n\t<span class=\"hljs-comment\">#tokens = nltk.word_tokenize(tweet)</span>\n\ttokens = re.findall(<span class=\"hljs-string\">\"\\w+\"</span>, tweet, re.UNICODE)\n\t<span class=\"hljs-built_in\">print</span> tokens\n\tswords = stopwords.words(lang)\t\t\n\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"lemmatization_0\" class=\"highlights fea_lemmatization\">lmtzr = WordNetLemmatizer()</div>\n\tnew_text = <span class=\"hljs-string\">\"\"</span>\n\t<span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> tokens:\n\t\t<span class=\"hljs-keyword\">if</span> token <span class=\"hljs-keyword\">in</span> swords:\n\t\t\t<span class=\"hljs-keyword\">continue</span>\n\t\tlword = token\n\t\t<span class=\"hljs-keyword\">if</span> lang == <span class=\"hljs-string\">\"english\"</span>:\n\t\t\tlword = lmtzr.lemmatize(token)\n\t\t<span class=\"hljs-comment\">#print token, lword</span>\n\t\tnew_text += <span class=\"hljs-string\">\" \"</span> + lword\n\tnew_text = new_text.strip()\n\t<span class=\"hljs-built_in\">print</span> new_text\n\t<span class=\"hljs-keyword\">return</span> new_text\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">extract_relevant_info</span>(<span class=\"hljs-params\">tweet</span>):</span>\n\ttweet = extract_urls(tweet)\n\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">tokens = nltk.word_tokenize(tweet)</div>\n\t<span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> tokens:\n\t\t<span class=\"hljs-built_in\">print</span> token\n\t<span class=\"hljs-comment\">#todo</span>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">extract_urls</span>(<span class=\"hljs-params\">text</span>):</span>\n\turls = re.findall(<span class=\"hljs-string\">\"http://.*?(?:\\s|$)\"</span>, text)\n\t<span class=\"hljs-keyword\">for</span> url <span class=\"hljs-keyword\">in</span> urls:\n\t\tstart = text.find(url)\n\t\tstop = start + <span class=\"hljs-built_in\">len</span>(url.strip())\n\t\ttext = text[<span class=\"hljs-number\">0</span>:start] + text[stop:]\n\t<span class=\"hljs-keyword\">return</span> text\n\n\n</code></pre></div>",
    "fir_43": "<div class=\"codeBlock hljs python\" id=\"fir_43\"><pre id=\"fir_43_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\"># coding=utf-8</span>\n<span class=\"hljs-keyword\">import</span> re, nltk, translator, copy\n<span class=\"hljs-keyword\">from</span> abuser <span class=\"hljs-keyword\">import</span> abuser\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_2\" class=\"highlights fea_nlp_datasets\"><span class=\"hljs-keyword\">from</span> nltk.corpus <span class=\"hljs-keyword\">import</span> wordnet <span class=\"hljs-keyword\">as</span> wn</div>\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Calculator</span>:</span>\n    <span class=\"hljs-string\">'''\n    Calculators' base class\n    '''</span>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">tokenize</span>(<span class=\"hljs-params\">self, string</span>):</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">tokenizer = nltk.tokenize.RegexpTokenizer(<span class=\"hljs-string\">u'\\s+'</span>, gaps=<span class=\"hljs-literal\">True</span>)\n        <span class=\"hljs-keyword\">return</span> tokenizer.tokenize(unicode(string))</div>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">stem</span>(<span class=\"hljs-params\">self, word, lang=<span class=\"hljs-string\">u'en'</span></span>):</span>\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"stemming_0\" class=\"highlights fea_stemming\"><span class=\"hljs-keyword\">if</span> lang == <span class=\"hljs-string\">u'en'</span>:\n            <span class=\"hljs-keyword\">return</span> nltk.stem.PorterStemmer().stem(word)</div>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">remove_stopwords</span>(<span class=\"hljs-params\">self, word_list, lang=<span class=\"hljs-string\">u'en'</span></span>):</span>\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_0\" class=\"highlights fea_nlp_datasets\"><span class=\"hljs-keyword\">if</span> lang == <span class=\"hljs-string\">'en'</span>:\n            stopwords = <span class=\"hljs-built_in\">set</span>(nltk.corpus.stopwords.words(<span class=\"hljs-string\">'english'</span>))\n            <span class=\"hljs-keyword\">return</span> [word <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> word_list <span class=\"hljs-keyword\">if</span> word <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> stopwords]</div>\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">None</span>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">normalize_sentence</span>(<span class=\"hljs-params\">self, sentence, lang=<span class=\"hljs-string\">'en'</span>, filter_stopwords=<span class=\"hljs-literal\">True</span></span>):</span>\n        <span class=\"hljs-string\">\"\"\"\n        эта функция выполняет разбиение на токены, удаление стоп-слова и стемминг\n        \"\"\"</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_1\" class=\"highlights fea_tokenization\">tokens = self.tokenize(sentence)\n        <span class=\"hljs-keyword\">if</span> filter_stopwords == <span class=\"hljs-literal\">False</span>:\n            <span class=\"hljs-keyword\">return</span> [self.stem(token).lower() <span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> tokens]\n        <span class=\"hljs-keyword\">else</span>:\n            r_tokens = self.remove_stopwords(tokens, lang)\n            <span class=\"hljs-keyword\">return</span> [self.stem(token).lower() <span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> r_tokens]</div>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">perform_calc</span>(<span class=\"hljs-params\">self, translation_unit</span>):</span>\n        <span class=\"hljs-keyword\">raise</span> NotImplementedError\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Calculator_with_translator</span>(<span class=\"hljs-params\">Calculator</span>):</span>\n    <span class=\"hljs-string\">'''\n    расширение класса с использование машинного переводчика\n    '''</span>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">translate</span>(<span class=\"hljs-params\">self, text, or_lang=<span class=\"hljs-string\">'ru'</span>, tar_lang=<span class=\"hljs-string\">'en'</span></span>):</span>\n        <span class=\"hljs-keyword\">return</span> translator.Translator().translate(text, or_lang, tar_lang)\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">normalize_in_english</span>(<span class=\"hljs-params\">self, sentence</span>):</span>\n        <span class=\"hljs-string\">'''\n        этот метод приводит предложение к набору лемм на английском языке\n        '''</span>\n        <span class=\"hljs-keyword\">if</span> sentence.lang != <span class=\"hljs-string\">u'en'</span>:\n            <span class=\"hljs-keyword\">return</span> self.normalize_sentence(self.translate(sentence.text, sentence.lang, <span class=\"hljs-string\">'en'</span>))\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-keyword\">return</span> self.normalize_sentence(sentence.text)\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">normalize_with_wnmorphy</span>(<span class=\"hljs-params\">self, sentence</span>):</span>\n        <span class=\"hljs-comment\">#этот метод нормализует слова с помощьюю встроенного в wordnet функкионала</span>\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_1\" class=\"highlights fea_nlp_datasets\"><span class=\"hljs-keyword\">if</span> sentence.lang != <span class=\"hljs-string\">u'en'</span>:\n            <span class=\"hljs-keyword\">return</span> [wn.morphy(t) <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> Calculator().tokenize(self.translate(sentence.text, sentence.lang, <span class=\"hljs-string\">'en'</span>)) <span class=\"hljs-keyword\">if</span>\n                    wn.morphy(t)]\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-keyword\">return</span> [wn.morphy(t) <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> Calculator().tokenize(sentence.text) <span class=\"hljs-keyword\">if</span> wn.morphy(t)]</div>\n</code></pre></div>",
    "fir_44": "<div class=\"codeBlock hljs python\" id=\"fir_44\"><pre id=\"fir_44_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">from</span> makedb.makedb <span class=\"hljs-keyword\">import</span> unescape\n<span class=\"hljs-keyword\">from</span> nlptools.urlcache <span class=\"hljs-keyword\">import</span> get_cached_url\n<span class=\"hljs-keyword\">from</span> nlptools.html_to_text <span class=\"hljs-keyword\">import</span> html_to_segments, html_to_text\n<span class=\"hljs-keyword\">from</span> nlptools <span class=\"hljs-keyword\">import</span> trim_to_words\n<span class=\"hljs-keyword\">import</span> websearch.rareword_match <span class=\"hljs-keyword\">as</span> rm\n<span class=\"hljs-keyword\">import</span> websearch.compute_rarewords <span class=\"hljs-keyword\">as</span> cr\n<span class=\"hljs-keyword\">import</span> math\n<span class=\"hljs-keyword\">from</span> vector <span class=\"hljs-keyword\">import</span> text_similarity, tfidf, idf\n<span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">import</span> patterns.regexpatterns <span class=\"hljs-keyword\">as</span> rp\n<span class=\"hljs-keyword\">import</span> operator <span class=\"hljs-keyword\">as</span> op\n<span class=\"hljs-keyword\">import</span> settings\n<span class=\"hljs-keyword\">import</span> pickle\n<span class=\"hljs-keyword\">from</span> urlcheck.models <span class=\"hljs-keyword\">import</span> MatchVote\n<span class=\"hljs-keyword\">import</span> svmutil\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">tokenize</span>(<span class=\"hljs-params\">claim</span>):</span> <span class=\"hljs-keyword\">return</span> re.split(<span class=\"hljs-string\">\"[\\W=]+\"</span>,claim)\n\nfilename = <span class=\"hljs-string\">\"/home/rob/git/thinklink/output/training/rawinput.csv\"</span>\n\n\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_0\" class=\"highlights fea_nlp_datasets\">stopwords = <span class=\"hljs-built_in\">set</span>(nltk.corpus.stopwords.words(<span class=\"hljs-string\">'english'</span>))</div>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">add_shared_props</span>(<span class=\"hljs-params\">item</span>):</span>\n\titem[<span class=\"hljs-string\">'claimwords'</span>] = tokenize(item[<span class=\"hljs-string\">'claimtext'</span>].lower())\n\titem[<span class=\"hljs-string\">'matchwords'</span>] = get_trimmed_match(item)\t\n\titem[<span class=\"hljs-string\">'trimmedmatch'</span>] = <span class=\"hljs-string\">\" \"</span>.join(item[<span class=\"hljs-string\">'matchwords'</span>])\t\n\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_0\" class=\"highlights fea_Part_of_Speech\">item[<span class=\"hljs-string\">'claimtags'</span>] = nltk.pos_tag(item[<span class=\"hljs-string\">'claimwords'</span>])</div>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">bigram_overlap</span>(<span class=\"hljs-params\">item</span>):</span>\n\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"n_grams_0\" class=\"highlights fea_n_grams\">claim_bigrams = <span class=\"hljs-built_in\">set</span>(nltk.bigrams(item[<span class=\"hljs-string\">'claimwords'</span>]))\n\tmatch_bigrams = <span class=\"hljs-built_in\">set</span>(nltk.bigrams(item[<span class=\"hljs-string\">'matchwords'</span>]))\n\tboth_bigrams = claim_bigrams &amp; match_bigrams\n\teither_bigrams = claim_bigrams | match_bigrams\n\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">float</span>(<span class=\"hljs-built_in\">len</span>(both_bigrams))/<span class=\"hljs-built_in\">max</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-built_in\">len</span>(either_bigrams))</div>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">trigram_overlap</span>(<span class=\"hljs-params\">item</span>):</span>\n\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"n_grams_1\" class=\"highlights fea_n_grams\">claim_trigrams = <span class=\"hljs-built_in\">set</span>(nltk.trigrams(item[<span class=\"hljs-string\">'claimwords'</span>]))\n\tmatch_trigrams = <span class=\"hljs-built_in\">set</span>(nltk.trigrams(item[<span class=\"hljs-string\">'matchwords'</span>]))\n\tboth_trigrams = claim_trigrams &amp; match_trigrams\n\teither_trigrams = claim_trigrams | match_trigrams</div>\n\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">float</span>(<span class=\"hljs-built_in\">len</span>(both_trigrams))/<span class=\"hljs-built_in\">max</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-built_in\">len</span>(either_trigrams))\n\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_1\" class=\"highlights fea_Part_of_Speech\">item[<span class=\"hljs-string\">'matchtags'</span>] = nltk.pos_tag(item[<span class=\"hljs-string\">'matchwords'</span>])</div>\t</code></pre></div>",
    "fir_45": "<div class=\"codeBlock hljs python\" id=\"fir_45\"><pre id=\"fir_45_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\">#!/usr/bin/env python</span>\n<span class=\"hljs-comment\"># encoding: utf-8</span>\n\n<span class=\"hljs-string\">\"\"\"\nUse a classifier to drop sentences that aren't actually making a disputed claim.\n\nWe use a simple statistical classifier to do this, based on hand-chosen features.\n\"\"\"</span>\n\n<span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">import</span> claimfinder <span class=\"hljs-keyword\">as</span> cf\n<span class=\"hljs-keyword\">import</span> accuracy_stats <span class=\"hljs-keyword\">as</span> a\n<span class=\"hljs-keyword\">import</span> random\n<span class=\"hljs-keyword\">import</span> os\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">noun_verb_noun</span>(<span class=\"hljs-params\">tags</span>):</span>\n\tnoun1 = <span class=\"hljs-literal\">False</span>\n\tverb = <span class=\"hljs-literal\">False</span>\n\tthing2 = <span class=\"hljs-literal\">False</span>\n\t<span class=\"hljs-keyword\">for</span> tag <span class=\"hljs-keyword\">in</span> tags:\n\t\t<span class=\"hljs-keyword\">if</span> tag.startswith(<span class=\"hljs-string\">\"NN\"</span>) <span class=\"hljs-keyword\">and</span> <span class=\"hljs-keyword\">not</span> noun1:\n\t\t\tnoun1 = <span class=\"hljs-literal\">True</span>\n\t\t<span class=\"hljs-keyword\">if</span> tag.startswith(<span class=\"hljs-string\">\"VB\"</span>) <span class=\"hljs-keyword\">and</span> noun1:\n\t\t\tverb = <span class=\"hljs-literal\">True</span>\n\t\t<span class=\"hljs-keyword\">if</span> tag.startswith(<span class=\"hljs-string\">\"NN\"</span>) <span class=\"hljs-keyword\">and</span> verb:\n\t\t\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">True</span>\n\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">False</span>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">noun_verb_adj</span>(<span class=\"hljs-params\">tags</span>):</span>\n\tnoun1 = <span class=\"hljs-literal\">False</span>\n\tverb = <span class=\"hljs-literal\">False</span>\n\tthing2 = <span class=\"hljs-literal\">False</span>\n\t<span class=\"hljs-keyword\">for</span> tag <span class=\"hljs-keyword\">in</span> tags:\n\t\t<span class=\"hljs-keyword\">if</span> tag.startswith(<span class=\"hljs-string\">\"NN\"</span>) <span class=\"hljs-keyword\">and</span> <span class=\"hljs-keyword\">not</span> noun1:\n\t\t\tnoun1 = <span class=\"hljs-literal\">True</span>\n\t\t<span class=\"hljs-keyword\">if</span> tag.startswith(<span class=\"hljs-string\">\"VB\"</span>) <span class=\"hljs-keyword\">and</span> noun1:\n\t\t\tverb = <span class=\"hljs-literal\">True</span>\n\t\t<span class=\"hljs-keyword\">if</span> tag.startswith(<span class=\"hljs-string\">\"JJ\"</span>) <span class=\"hljs-keyword\">and</span> verb:\n\t\t\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">True</span>\n\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">False</span>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">noun_verb_verb</span>(<span class=\"hljs-params\">tags</span>):</span>\n\tnoun1 = <span class=\"hljs-literal\">False</span>\n\tverb = <span class=\"hljs-literal\">False</span>\n\tthing2 = <span class=\"hljs-literal\">False</span>\n\t<span class=\"hljs-keyword\">for</span> tag <span class=\"hljs-keyword\">in</span> tags:\n\t\t<span class=\"hljs-keyword\">if</span> tag.startswith(<span class=\"hljs-string\">\"NN\"</span>) <span class=\"hljs-keyword\">and</span> <span class=\"hljs-keyword\">not</span> noun1:\n\t\t\tnoun1 = <span class=\"hljs-literal\">True</span>\n\t\t<span class=\"hljs-keyword\">if</span> tag.startswith(<span class=\"hljs-string\">\"VB\"</span>) <span class=\"hljs-keyword\">and</span> noun1:\n\t\t\tverb = <span class=\"hljs-literal\">True</span>\n\t\t<span class=\"hljs-keyword\">if</span> tag == <span class=\"hljs-string\">\"VBG\"</span> <span class=\"hljs-keyword\">and</span> verb:\n\t\t\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">True</span>\n\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">False</span>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">features</span>(<span class=\"hljs-params\">claim</span>):</span>\n\tfeatures = {}\n\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">words = nltk.word_tokenize(claim)</div>\n\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_0\" class=\"highlights fea_Part_of_Speech\">taggedwords = nltk.pos_tag(words)</div>\n\ttags = [tword[<span class=\"hljs-number\">1</span>] <span class=\"hljs-keyword\">for</span> tword <span class=\"hljs-keyword\">in</span> taggedwords]\n\t<span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> words: features[<span class=\"hljs-string\">\"has-\"</span>+word] = <span class=\"hljs-literal\">True</span>\n\t<span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> words[<span class=\"hljs-number\">0</span>:<span class=\"hljs-number\">5</span>]: features[<span class=\"hljs-string\">\"early-has-\"</span>+word] = <span class=\"hljs-literal\">True</span>\n\t<span class=\"hljs-keyword\">for</span> tag <span class=\"hljs-keyword\">in</span> tags: features[<span class=\"hljs-string\">\"tag-\"</span>+tag] = <span class=\"hljs-literal\">True</span>\n\t<span class=\"hljs-keyword\">for</span> tag <span class=\"hljs-keyword\">in</span> tags[<span class=\"hljs-number\">0</span>:<span class=\"hljs-number\">5</span>]: features[<span class=\"hljs-string\">\"early-tag-\"</span>+tag] = <span class=\"hljs-literal\">True</span>\n\t<span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(words) &gt; <span class=\"hljs-number\">0</span>:\n\t\tfeatures[<span class=\"hljs-string\">\"first-word\"</span>] = words[<span class=\"hljs-number\">0</span>]\n\t<span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(words) &gt; <span class=\"hljs-number\">1</span>:\n\t\tfeatures[<span class=\"hljs-string\">\"second-word\"</span>] = words[<span class=\"hljs-number\">1</span>]\n\t<span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(words) &gt; <span class=\"hljs-number\">0</span>:\n\t\tfeatures[<span class=\"hljs-string\">\"first-tag\"</span>] = tags[<span class=\"hljs-number\">0</span>]\n\t<span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(words) &gt; <span class=\"hljs-number\">1</span>:\n\t\tfeatures[<span class=\"hljs-string\">\"second-tag\"</span>] = tags[<span class=\"hljs-number\">1</span>]\n\tfeatures[<span class=\"hljs-string\">\"noun-verb-noun\"</span>] = noun_verb_noun(tags)\n\tfeatures[<span class=\"hljs-string\">\"noun-verb-adj\"</span>] = noun_verb_adj(tags)\n\tfeatures[<span class=\"hljs-string\">\"noun-verb-verb\"</span>] = noun_verb_adj(tags)\n\tfeatures[<span class=\"hljs-string\">\"length\"</span>] = <span class=\"hljs-built_in\">len</span>(words)\t\n\t<span class=\"hljs-keyword\">return</span> features\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">features_words</span>(<span class=\"hljs-params\">claim</span>):</span>\n\tfeatures = {}\n\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_1\" class=\"highlights fea_tokenization\">words = nltk.word_tokenize(claim)</div>\n\t<span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> words: features[<span class=\"hljs-string\">\"has-\"</span>+word] = <span class=\"hljs-literal\">True</span>\n\t<span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(words) &gt; <span class=\"hljs-number\">0</span>:\n\t\tfeatures[<span class=\"hljs-string\">\"first\"</span>] = words[<span class=\"hljs-number\">0</span>]\n\t<span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(words) &gt; <span class=\"hljs-number\">1</span>:\n\t\tfeatures[<span class=\"hljs-string\">\"second\"</span>] = words[<span class=\"hljs-number\">1</span>]\n\tfeatures[<span class=\"hljs-string\">\"length\"</span>] = <span class=\"hljs-built_in\">len</span>(words)\n\t<span class=\"hljs-keyword\">return</span> features\n\n\t\t\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_training_data</span>(<span class=\"hljs-params\">phrases</span>):</span>\n\thuman_marked = []\n\t<span class=\"hljs-keyword\">for</span> phrase <span class=\"hljs-keyword\">in</span> phrases:\n\t\tbasename = <span class=\"hljs-string\">\"../training/\"</span>+phrase.replace(<span class=\"hljs-string\">\" \"</span>,<span class=\"hljs-string\">\"_\"</span>)\n\t\thumangood = basename+<span class=\"hljs-string\">\".manual_good\"</span>\n\t\tallfile = basename+<span class=\"hljs-string\">\".pickedclaims\"</span>\n\t\t<span class=\"hljs-keyword\">if</span> os.path.exists(humangood):\n\t\t\t(good,bad,<span class=\"hljs-built_in\">all</span>) = a.get_human_sets(humangood,allfile)\t\t\n\t\t\t<span class=\"hljs-keyword\">for</span> claim <span class=\"hljs-keyword\">in</span> good:\n\t\t\t\thuman_marked.append((claim,<span class=\"hljs-literal\">True</span>))\n\t\t\t<span class=\"hljs-keyword\">for</span> claim <span class=\"hljs-keyword\">in</span> bad:\n\t\t\t\thuman_marked.append((claim,<span class=\"hljs-literal\">False</span>))\n\trandom.shuffle(human_marked)\n\t<span class=\"hljs-keyword\">return</span> human_marked\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_featured_data</span>(<span class=\"hljs-params\">phrases</span>):</span>\n\tannotated = get_training_data(phrases)\n\tfeaturesets = [(features(claim),g) <span class=\"hljs-keyword\">for</span> (claim,g) <span class=\"hljs-keyword\">in</span> annotated]\n\t<span class=\"hljs-keyword\">return</span> featuresets\n\t\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_featured_data_split</span>(<span class=\"hljs-params\">phrases</span>):</span>\n\tfeaturesets = get_featured_data(phrases)\n\tlength = <span class=\"hljs-built_in\">len</span>(featuresets)\n\tsplitpoint = <span class=\"hljs-built_in\">int</span>(length * <span class=\"hljs-number\">0.8</span>)\n\ttrain_set,test_set = featuresets[:splitpoint],featuresets[splitpoint:]\n\t<span class=\"hljs-keyword\">return</span> (train_set,test_set)\n\t\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">real_phrases</span>(<span class=\"hljs-params\">phrases</span>):</span>\n\t<span class=\"hljs-keyword\">return</span> [phrase <span class=\"hljs-keyword\">for</span> phrase <span class=\"hljs-keyword\">in</span> phrases <span class=\"hljs-keyword\">if</span> os.path.exists(<span class=\"hljs-string\">\"../training/\"</span>+phrase.replace(<span class=\"hljs-string\">\" \"</span>,<span class=\"hljs-string\">\"_\"</span>)+<span class=\"hljs-string\">\".manual_good\"</span>)]\n\t\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">measure_phrases</span>(<span class=\"hljs-params\">phrases</span>):</span>\n\tphrases\t= real_phrases(phrases)\n\tfeatures = {}\n\t<span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\" --- accuracy --- \"</span>\n\t<span class=\"hljs-keyword\">for</span> phrase <span class=\"hljs-keyword\">in</span> phrases:\n\t\t<span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"features: \"</span>+phrase\n\t\tfeatures[phrase] = get_featured_data([phrase])\t\n\t<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"classification_0\" class=\"highlights fea_classification\"><span class=\"hljs-keyword\">for</span> phrase <span class=\"hljs-keyword\">in</span> phrases:\n\t\t<span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"--\"</span>,phrase,<span class=\"hljs-string\">\"--\"</span>\n\t\ttest_set = features[phrase]\n\t\ttrain_set = []\n\t\t<span class=\"hljs-keyword\">for</span> otherphrase <span class=\"hljs-keyword\">in</span> phrases:\n\t\t\t<span class=\"hljs-keyword\">if</span> otherphrase != phrase:\n\t\t\t\ttrain_set = train_set + features[otherphrase] \n\t\tclassifier = nltk.NaiveBayesClassifier.train(train_set)\t\t\n\t\taccuracy = nltk.classify.accuracy(classifier,test_set)\n\t\t<span class=\"hljs-built_in\">print</span> phrase,<span class=\"hljs-string\">\"&amp;\"</span>,<span class=\"hljs-built_in\">str</span>(<span class=\"hljs-built_in\">int</span>(<span class=\"hljs-number\">100</span>*accuracy)) +<span class=\"hljs-string\">\"\\%\"</span>\t</div>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_classifier_single</span>(<span class=\"hljs-params\">phrase</span>):</span>\n\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"classification_1\" class=\"highlights fea_classification\">featureset = get_featured_data([phrase])\n\tright = <span class=\"hljs-number\">0</span>\n\t<span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">0</span>,<span class=\"hljs-built_in\">len</span>(featureset)):\n\t\ttrain_set = featureset[<span class=\"hljs-number\">0</span>:i] + featureset[i+<span class=\"hljs-number\">1</span>:]\n\t\ttest_features = featureset[i][<span class=\"hljs-number\">0</span>]\n\t\tclassifier = nltk.NaiveBayesClassifier.train(train_set)\n\t\tjudgement = classifier.classify(test_features)\n\t\t<span class=\"hljs-keyword\">if</span> judgement == featureset[i][<span class=\"hljs-number\">1</span>]:\n\t\t\tright +=<span class=\"hljs-number\">1</span>\n\t<span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"accuracy =\"</span>,(right*<span class=\"hljs-number\">100</span>)/<span class=\"hljs-built_in\">len</span>(featureset)</div>\n\t\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_classifier</span>(<span class=\"hljs-params\">phrases</span>):</span>\n\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"classification_2\" class=\"highlights fea_classification\">(train_set,test_set) = get_featured_data_split(phrases)\n\tclassifier = nltk.NaiveBayesClassifier.train(train_set)\t\n\t<span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"accuracy = \"</span>+nltk.classify.accuracy(classifier,test_set)</div>\n\t\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_classifier_all</span>():</span>\n\ttest_classifier(cf.phrases)\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_classifier_good</span>():</span>\n\ttest_classifier(cf.goodphrases)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n\tmeasure_phrases(cf.bad_phrases)\n</code></pre></div>",
    "fir_46": "<div class=\"codeBlock hljs python\" id=\"fir_46\"><pre id=\"fir_46_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> collections\n<span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">import</span> datetime\n<span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">from</span> dashboard.models <span class=\"hljs-keyword\">import</span> Discurso\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_words_in_sentences</span>(<span class=\"hljs-params\">sentences</span>):</span>\n    all_words = []\n    <span class=\"hljs-keyword\">for</span> (words, category) <span class=\"hljs-keyword\">in</span> sentences:\n      all_words.extend(words)\n    <span class=\"hljs-keyword\">return</span> all_words\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_word_features</span>(<span class=\"hljs-params\">wordlist</span>):</span>\n    wordlist = nltk.FreqDist(wordlist)\n    word_features = wordlist.keys()\n    <span class=\"hljs-keyword\">return</span> word_features\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">extract_features</span>(<span class=\"hljs-params\">document</span>):</span>\n    document_words = <span class=\"hljs-built_in\">set</span>(document)\n    features = {}\n    <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> word_features:\n      features[<span class=\"hljs-string\">'contains(%s)'</span> % word] = (word <span class=\"hljs-keyword\">in</span> document_words)\n    <span class=\"hljs-keyword\">return</span> features\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">collect_sentences</span>(<span class=\"hljs-params\">rawtuples</span>):</span>\n    sentences = []\n    <span class=\"hljs-keyword\">for</span> (words, category) <span class=\"hljs-keyword\">in</span> rawtuples:\n      words_filtered = [e.lower() <span class=\"hljs-keyword\">for</span> e <span class=\"hljs-keyword\">in</span> words.split() <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(e) &gt;= <span class=\"hljs-number\">3</span>]\n      sentences.append((words_filtered, category))\n    <span class=\"hljs-keyword\">return</span> sentences\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">tuple_from_queryset</span>(<span class=\"hljs-params\">queryset, parameter</span>):</span>\n    punctuation = re.<span class=\"hljs-built_in\">compile</span>(<span class=\"hljs-string\">r'[-.?!,\":;()|0-9]'</span>) \n    sentence_tuple = []\n    <span class=\"hljs-keyword\">for</span> q <span class=\"hljs-keyword\">in</span> queryset:\n        sentence_tuple.append((punctuation.sub(<span class=\"hljs-string\">\"\"</span>, q.sumario.lower().encode(<span class=\"hljs-string\">'utf-8'</span>)), q.partido.sigla.lower()))\n    <span class=\"hljs-keyword\">return</span> sentence_tuple\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">precision_recall</span>(<span class=\"hljs-params\">classifier, testfeats</span>):</span>\n    refsets = collections.defaultdict(<span class=\"hljs-built_in\">set</span>)\n    testsets = collections.defaultdict(<span class=\"hljs-built_in\">set</span>)\n    <span class=\"hljs-keyword\">for</span> i, (feats, label) <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(testfeats):\n        refsets[label].add(i)\n        observed = classifier.classify(feats)\n        testsets[observed].add(i)\n    precisions = {}\n    recalls = {}\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"classification_2\" class=\"highlights fea_classification\"><span class=\"hljs-keyword\">for</span> label <span class=\"hljs-keyword\">in</span> classifier.labels():\n        precisions[label] = nltk.metrics.precision(refsets[label],\n        testsets[label])\n        recalls[label] = nltk.metrics.recall(refsets[label], testsets[label])\n    <span class=\"hljs-keyword\">return</span> precisions, recalls</div>\n\n\narena = Discurso.objects.<span class=\"hljs-built_in\">filter</span>(data__gt=datetime.date(<span class=\"hljs-number\">1968</span>, <span class=\"hljs-number\">10</span>, 01), partido__sigla=<span class=\"hljs-string\">\"ARENA\"</span>)\narena_sentences = tuple_from_queryset(arena, <span class=\"hljs-string\">'arena'</span>)\n\nmdb = Discurso.objects.<span class=\"hljs-built_in\">filter</span>(data__gt=datetime.date(<span class=\"hljs-number\">1968</span>, <span class=\"hljs-number\">10</span>, 01) , partido__sigla=<span class=\"hljs-string\">\"MDB\"</span>)\nmdb_sentences = tuple_from_queryset(mdb, <span class=\"hljs-string\">'mdb'</span>)\n\ntraining_sentences = collect_sentences(arena_sentences + mdb_sentences)\n\nword_features = get_word_features(get_words_in_sentences(training_sentences))\n\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"classification_0\" class=\"highlights fea_classification\">training_set = nltk.classify.apply_features(extract_features, training_sentences)\n\nclassifier = nltk.NaiveBayesClassifier.train(training_set)</div>\n\n<span class=\"hljs-comment\">#--</span>\n\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"classification_1\" class=\"highlights fea_classification\">check_sentences = tuple_from_queryset(Discurso.objects.<span class=\"hljs-built_in\">filter</span>(estado__sigla=<span class=\"hljs-string\">'CE'</span>), <span class=\"hljs-string\">'arena'</span>)\ncheck_set = nltk.classify.apply_features(extract_features, collect_sentences(check_sentences))</div>\n\n<span class=\"hljs-comment\">#classifier.show_most_informative_features(32) p/ visualizar diferencas</span>\n<span class=\"hljs-comment\">#classifier.classify(extract_features(sentence.split())) p/ classificar um resultado</span>\n<span class=\"hljs-comment\">#nltk.classify.util.accuracy(classifier, training_set) p/ testar accuracy</span>\n\n</code></pre></div>",
    "fir_47": "<div class=\"codeBlock hljs python\" id=\"fir_47\"><pre id=\"fir_47_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\">#!python</span>\n\n<span class=\"hljs-comment\"># http://nlpb.blogspot.com/2011/01/experiments-with-statistical-language.html</span>\n\n<span class=\"hljs-comment\"># Import the corpus and functions used from nltk library</span>\n<span class=\"hljs-keyword\">import</span> __future__\n\n<span class=\"hljs-keyword\">import</span> nltk.data\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"text_frequency_1\" class=\"highlights fea_text_frequency\"><span class=\"hljs-keyword\">from</span> nltk.probability <span class=\"hljs-keyword\">import</span> LidstoneProbDist</div>\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"n_grams_1\" class=\"highlights fea_n_grams\"><span class=\"hljs-keyword\">from</span> nltk.model <span class=\"hljs-keyword\">import</span> NgramModel</div>\n<span class=\"hljs-comment\">#from nltk.corpus import reuters</span>\n<span class=\"hljs-comment\">#from nltk.corpus import genesis</span>\n\n<span class=\"hljs-keyword\">import</span> argparse\n<span class=\"hljs-keyword\">import</span> sys\n<span class=\"hljs-keyword\">import</span> random\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">build_cmdline_parser</span>():</span>\n    parser = argparse.ArgumentParser(\n        <span class=\"hljs-string\">'Generate random text based on the specified corpus using an n-gram model'</span>\n        )\n\n    parser.add_argument(\n        <span class=\"hljs-string\">'--n-gram-n'</span>, <span class=\"hljs-string\">'-n'</span>,\n        <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">int</span>,\n        default=<span class=\"hljs-number\">3</span>,\n        <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">'The size (&lt;n&gt;) of the n-gram'</span>,\n        )\n\n    parser.add_argument(\n        <span class=\"hljs-string\">'--text'</span>, <span class=\"hljs-string\">'-t'</span>,\n        <span class=\"hljs-built_in\">type</span>=argparse.FileType(<span class=\"hljs-string\">'r'</span>),\n        default=sys.stdin,\n        <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">'The corpus (file) to use. sys.stdin is used if none is specified.'</span>,\n        )\n\n    parser.add_argument(\n        <span class=\"hljs-string\">'--sentences'</span>, <span class=\"hljs-string\">'-s'</span>,\n        <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">int</span>,\n        default=<span class=\"hljs-number\">30</span>,\n        <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">'The number of \"sentences\" to generate'</span>,\n        )\n\n    parser.add_argument(\n        <span class=\"hljs-string\">'--length'</span>, <span class=\"hljs-string\">'-l'</span>,\n        <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">int</span>,\n        default=<span class=\"hljs-number\">50</span>,\n        <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">'The number of \"words\" in a generated \"sentence\"'</span>,\n        )\n\n    <span class=\"hljs-keyword\">return</span> parser\n\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n    cmdline = build_cmdline_parser().parse_args()\n\n    nltk.data.path.append(<span class=\"hljs-string\">'C:/Python/nltk_data'</span>)\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">tokenizer = nltk.tokenize.RegexpTokenizer(<span class=\"hljs-string\">r'\\w+|[^\\w\\s]+'</span>)</div>\n\n    tokens = tokenizer.tokenize(cmdline.text.read())\n    <span class=\"hljs-comment\"># Tokens contains the words for Genesis and Reuters Trade</span>\n    <span class=\"hljs-comment\">#tokens = list(genesis.words('english-kjv.txt'))</span>\n    <span class=\"hljs-comment\">#tokens.extend(list(reuters.words(categories = 'trade')))</span>\n\n    <span class=\"hljs-comment\"># estimator for smoothing the N-gram model</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"text_frequency_0\" class=\"highlights fea_text_frequency\">estimator = <span class=\"hljs-keyword\">lambda</span> fdist, bins: LidstoneProbDist(fdist, <span class=\"hljs-number\">0.2</span>)</div>\n\n    <span class=\"hljs-comment\"># N-gram language model with 3-grams</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"n_grams_0\" class=\"highlights fea_n_grams\">model = NgramModel(cmdline.n_gram_n, tokens, estimator=estimator)</div>\n\n    <span class=\"hljs-comment\"># Apply the language model to generate 50 words in sequence</span>\n    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(cmdline.sentences):\n        <span class=\"hljs-comment\"># Generate some random words, expecting those at the end to be</span>\n        <span class=\"hljs-comment\">#   more generally statistically likely phrases.</span>\n        starting_words = model.generate(cmdline.n_gram_n*<span class=\"hljs-number\">5</span>,\n                                        random.sample(tokens,\n                                                      cmdline.n_gram_n-<span class=\"hljs-number\">1</span>))\n\n        text_words = model.generate(cmdline.length,starting_words)\n\n        <span class=\"hljs-comment\"># Concatenate all words generated in a string separating them by a space.</span>\n        text = <span class=\"hljs-string\">' '</span>.join([word <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> text_words])\n\n        <span class=\"hljs-comment\"># print the text</span>\n        print(text)\n        print(<span class=\"hljs-string\">''</span>)</code></pre></div>",
    "fir_48": "<div class=\"codeBlock hljs python\" id=\"fir_48\"><pre id=\"fir_48_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> os, os.path\n<span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">from</span> nltk <span class=\"hljs-keyword\">import</span> pos_tag, word_tokenize\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_1\" class=\"highlights fea_nlp_datasets\"><span class=\"hljs-keyword\">from</span> nltk.corpus <span class=\"hljs-keyword\">import</span> senseval\n</div>\npath = os.path.relpath(<span class=\"hljs-string\">'nltk_data'</span>)\nnltk.data.path[<span class=\"hljs-number\">0</span>]=path\n\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_0\" class=\"highlights fea_nlp_datasets\">items = senseval.fileids()</div>\nitems = items[:<span class=\"hljs-number\">1</span>]\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">colocation</span>(<span class=\"hljs-params\">windowSize, pos, context,dictionary</span>):</span>\n    <span class=\"hljs-keyword\">if</span> windowSize&lt;=<span class=\"hljs-number\">0</span>:\n        <span class=\"hljs-keyword\">return</span> dictionary\n    <span class=\"hljs-comment\">#going forward</span>\n    forward= context[:(pos)]\n    f= forward[(-windowSize/<span class=\"hljs-number\">2</span>):]\n    <span class=\"hljs-comment\">#going backward    </span>\n    backward= context[pos+<span class=\"hljs-number\">1</span>:]\n    b= backward[:windowSize/<span class=\"hljs-number\">2</span>]\n    <span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> f:\n        key= <span class=\"hljs-string\">\"pre\"</span>+<span class=\"hljs-built_in\">str</span>(<span class=\"hljs-built_in\">len</span>(f)-f.index(item))+<span class=\"hljs-string\">\"-word\"</span>\n        value= item\n        dictionary[key]=value\n        key= <span class=\"hljs-string\">\"pre\"</span>+<span class=\"hljs-built_in\">str</span>(<span class=\"hljs-built_in\">len</span>(f)-f.index(item))+<span class=\"hljs-string\">\"-pos\"</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">text = nltk.word_tokenize(item)</div>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_0\" class=\"highlights fea_Part_of_Speech\">value= nltk.pos_tag(text)[<span class=\"hljs-number\">0</span>][<span class=\"hljs-number\">1</span>]</div>\n        dictionary[key]=value\n    <span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> b:\n        key= <span class=\"hljs-string\">\"fol\"</span>+<span class=\"hljs-built_in\">str</span>(b.index(item)+<span class=\"hljs-number\">1</span>)+<span class=\"hljs-string\">\"-word\"</span>\n        value= item\n        dictionary[key]=value\n        key= <span class=\"hljs-string\">\"fol\"</span>+<span class=\"hljs-built_in\">str</span>(b.index(item)+<span class=\"hljs-number\">1</span>)+<span class=\"hljs-string\">\"-pos\"</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_1\" class=\"highlights fea_tokenization\">text = nltk.word_tokenize(item)</div>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_1\" class=\"highlights fea_Part_of_Speech\">value= nltk.pos_tag(text)[<span class=\"hljs-number\">0</span>][<span class=\"hljs-number\">1</span>]</div>\n        dictionary[key]=value\n    <span class=\"hljs-keyword\">return</span> dictionary\n        \n<span class=\"hljs-keyword\">if</span> __name__==<span class=\"hljs-string\">\"__main__\"</span>:\n    <span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> items:\n        totalResult= []\n        windowSize=<span class=\"hljs-number\">4</span>\n        dictionary={}\n        <span class=\"hljs-keyword\">for</span> instance <span class=\"hljs-keyword\">in</span> senseval.instances(item)[:<span class=\"hljs-number\">10</span>]:\n                pos = instance.position\n                context = instance.context\n                senses = instance.senses\n                <span class=\"hljs-comment\">#print context</span>\n                <span class=\"hljs-comment\">#print context[pos]</span>\n                d= colocation(windowSize, pos, context,dictionary)\n                <span class=\"hljs-built_in\">print</span> d</code></pre></div>",
    "fir_49": "<div class=\"codeBlock hljs python\" id=\"fir_49\"><pre id=\"fir_49_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> nltk.chunk\n<span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">from</span> nltk.corpus <span class=\"hljs-keyword\">import</span> conll2000\n<span class=\"hljs-keyword\">import</span> itertools\n<span class=\"hljs-keyword\">from</span> nltk <span class=\"hljs-keyword\">import</span> pos_tag, word_tokenize\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">UnigramChunker</span>(<span class=\"hljs-params\">nltk.ChunkParserI</span>):</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, train_sents</span>):</span> \n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"chunking_0\" class=\"highlights fea_chunking\">train_data = [[(t,c) <span class=\"hljs-keyword\">for</span> w,t,c <span class=\"hljs-keyword\">in</span> nltk.chunk.tree2conlltags(sent)]\n                      <span class=\"hljs-keyword\">for</span> sent <span class=\"hljs-keyword\">in</span> train_sents]</div>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_0\" class=\"highlights fea_tagger\">self.tagger = nltk.UnigramTagger(train_data) </div>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">parse2</span>(<span class=\"hljs-params\">self, tokens</span>):</span>\n        <span class=\"hljs-comment\"># split words and part of speech tags</span>\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(tokens) &gt; <span class=\"hljs-number\">0</span>:\n           (words, tags) = <span class=\"hljs-built_in\">zip</span>(*tokens)\n           <span class=\"hljs-comment\"># get IOB chunk tags</span>\n           <div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_1\" class=\"highlights fea_tagger\">chunks = self.tagger.tag(tags)</div>\n           <span class=\"hljs-comment\"># join words with chunk tags</span>\n           wtc = itertools.izip(words, chunks)\n           <span class=\"hljs-comment\"># w = word, t = part-of-speech tag, c = chunk tag</span>\n           lines = [<span class=\"hljs-string\">' '</span>.join([w, t, c]) <span class=\"hljs-keyword\">for</span> (w, (t, c)) <span class=\"hljs-keyword\">in</span> wtc <span class=\"hljs-keyword\">if</span> c]\n           <span class=\"hljs-comment\"># create tree from conll formatted chunk lines</span>\n        <span class=\"hljs-keyword\">else</span>:\n           lines = []\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"chunking_1\" class=\"highlights fea_chunking\"><span class=\"hljs-keyword\">return</span> nltk.chunk.conllstr2tree(<span class=\"hljs-string\">'\\n'</span>.join(lines))</div>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">clean_punctuation</span>(<span class=\"hljs-params\">text</span>):</span>\n    text = text.replace(<span class=\"hljs-string\">\".\"</span>,<span class=\"hljs-string\">\" .\"</span>)\n    text = text.replace(<span class=\"hljs-string\">\",\"</span>,<span class=\"hljs-string\">\" ,\"</span>)\n    text = text.replace(<span class=\"hljs-string\">\"!\"</span>,<span class=\"hljs-string\">\" !\"</span>)\n    text = text.replace(<span class=\"hljs-string\">\":\"</span>,<span class=\"hljs-string\">\" :\"</span>)\n    text = text.replace(<span class=\"hljs-string\">\"\\\"\"</span>,<span class=\"hljs-string\">\" \\\"\"</span>)\n    <span class=\"hljs-keyword\">return</span> text\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">run</span>(<span class=\"hljs-params\">q_id</span>):</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_0\" class=\"highlights fea_nlp_datasets\">train_sents = conll2000.chunked_sents(<span class=\"hljs-string\">'train.txt'</span>)</div>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"chunking_2\" class=\"highlights fea_chunking\">unigram_chunker = UnigramChunker(train_sents)</div>\n\n    <span class=\"hljs-keyword\">import</span> init\n    <span class=\"hljs-comment\">#get document here and tag; put into this format:</span>\n    <span class=\"hljs-comment\">#tagged = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),(\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\"),(\".\", \".\")]</span>\n    topdoc = init.get_corpus(q_id)\n    doc_nums = topdoc.keys()\n    answers= [];\n    <span class=\"hljs-keyword\">for</span> key <span class=\"hljs-keyword\">in</span> doc_nums:\n        doc_text = topdoc[key]\n        docnum= key\n        <span class=\"hljs-comment\">#print docnum</span>\n        doc_text = clean_punctuation(doc_text)\n        <span class=\"hljs-comment\">#print doc_text</span>\n        doc_text= doc_text.split()\n        tagged=pos_tag(doc_text)\n\n    \n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"chunking_3\" class=\"highlights fea_chunking\">chunked=unigram_chunker.parse2(tagged)</div>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_0\" class=\"highlights fea_Part_of_Speech\">flatten= chunked.pos()</div>\n        <span class=\"hljs-comment\">#print flatten</span>\n        numbered= <span class=\"hljs-built_in\">enumerate</span>(flatten)\n        currentTag=<span class=\"hljs-string\">''</span>\n        words=[]\n        <span class=\"hljs-keyword\">for</span> i,v <span class=\"hljs-keyword\">in</span> numbered:\n            <span class=\"hljs-comment\">#print i,v</span>\n            ((word,tag),phrasetag)=v\n            <span class=\"hljs-keyword\">if</span> currentTag==<span class=\"hljs-string\">''</span>:\n                currentTag=phrasetag\n            <span class=\"hljs-keyword\">if</span> currentTag==phrasetag:\n                words.append(word)\n            <span class=\"hljs-keyword\">else</span>:\n                answers.append((<span class=\"hljs-string\">' '</span>.join(words),docnum,i-<span class=\"hljs-built_in\">len</span>(words),currentTag,q_id))\n                currentTag= phrasetag\n                words= [word]\n        answers.append((<span class=\"hljs-string\">' '</span>.join(words),docnum,i-<span class=\"hljs-built_in\">len</span>(words),currentTag,q_id))\n        <span class=\"hljs-comment\">#print answers</span>\n          \n    <span class=\"hljs-keyword\">return</span> answers\n\n\n<span class=\"hljs-keyword\">if</span> __name__==<span class=\"hljs-string\">\"__main__\"</span>:\n    <span class=\"hljs-built_in\">print</span> run(<span class=\"hljs-number\">213</span>)</code></pre></div>",
    "fir_50": "<div class=\"codeBlock hljs python\" id=\"fir_50\"><pre id=\"fir_50_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">main</span>():</span>\n    <span class=\"hljs-keyword\">import</span> optparse, random, nltk\n\n    parser = optparse.OptionParser()\n    parser.add_option(<span class=\"hljs-string\">\"-c\"</span>, dest=<span class=\"hljs-string\">\"corpus\"</span>, <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">\"corpus module name under nltk.corpus (e.g. brown, reuters)\"</span>, default=<span class=\"hljs-string\">'brown'</span>)\n    parser.add_option(<span class=\"hljs-string\">\"-r\"</span>, dest=<span class=\"hljs-string\">\"testrate\"</span>, <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-string\">\"float\"</span>, <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">\"rate of test dataset in corpus\"</span>, default=<span class=\"hljs-number\">0.1</span>)\n    parser.add_option(<span class=\"hljs-string\">\"--seed\"</span>, dest=<span class=\"hljs-string\">\"seed\"</span>, <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-string\">\"int\"</span>, <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\">\"random seed\"</span>)\n    (opt, args) = parser.parse_args()\n\n    random.seed(opt.seed)\n\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_0\" class=\"highlights fea_nlp_datasets\">m = <span class=\"hljs-built_in\">__import__</span>(<span class=\"hljs-string\">'nltk.corpus'</span>, <span class=\"hljs-built_in\">globals</span>(), <span class=\"hljs-built_in\">locals</span>(), [opt.corpus], -<span class=\"hljs-number\">1</span>)</div>\n    corpus = <span class=\"hljs-built_in\">getattr</span>(m, opt.corpus)\n    ids = corpus.fileids()\n    D = <span class=\"hljs-built_in\">len</span>(ids)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"found corpus : %s (D=%d)\"</span> % (opt.corpus, D)\n\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"nlp_datasets_1\" class=\"highlights fea_nlp_datasets\">testids = <span class=\"hljs-built_in\">set</span>(random.sample(ids, <span class=\"hljs-built_in\">int</span>(D * opt.testrate)))\n    trainids = [<span class=\"hljs-built_in\">id</span> <span class=\"hljs-keyword\">for</span> <span class=\"hljs-built_in\">id</span> <span class=\"hljs-keyword\">in</span> ids <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">id</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> testids]\n    trainwords = [w.lower() <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> corpus.words(trainids)]</div>\n\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"text_frequency_0\" class=\"highlights fea_text_frequency\">freq1 = nltk.FreqDist(trainwords)\n    gram1 = Distribution()</div>\n    <span class=\"hljs-keyword\">for</span> w, c <span class=\"hljs-keyword\">in</span> freq1.iteritems():\n        gram1[(w,)] = c\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"# of terms=%d, vocabulary size=%d\"</span> % (gram1.n, <span class=\"hljs-built_in\">len</span>(gram1))\n\n    gram2 = Distribution()\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"text_frequency_1\" class=\"highlights fea_text_frequency\"><span class=\"hljs-keyword\">for</span> w, c <span class=\"hljs-keyword\">in</span> nltk.FreqDist(nltk.bigrams(trainwords)).iteritems():\n        gram2[w] = c\n    gram3 = Distribution()\n    <span class=\"hljs-keyword\">for</span> w, c <span class=\"hljs-keyword\">in</span> nltk.FreqDist(nltk.trigrams(trainwords)).iteritems():\n        gram3[w] = c</div>\n\n    testset = []\n    voca = <span class=\"hljs-built_in\">set</span>(freq1.iterkeys())\n    <span class=\"hljs-keyword\">for</span> <span class=\"hljs-built_in\">id</span> <span class=\"hljs-keyword\">in</span> testids:\n        f = corpus.words(<span class=\"hljs-built_in\">id</span>)\n        doc = [w.lower() <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> f]\n        f.close()\n\n        testset.append(doc)\n        <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> doc:\n            voca.add(w)\n    V = <span class=\"hljs-built_in\">len</span>(voca)\n\n    D1 = gram1.n1 / <span class=\"hljs-built_in\">float</span>(gram1.n1 + <span class=\"hljs-number\">2</span> * gram1.n2)\n    D2 = gram2.n1 / <span class=\"hljs-built_in\">float</span>(gram2.n1 + <span class=\"hljs-number\">2</span> * gram2.n2)\n    D3 = gram3.n1 / <span class=\"hljs-built_in\">float</span>(gram3.n1 + <span class=\"hljs-number\">2</span> * gram3.n2)\n\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"\\nUNIGRAM:\"</span>\n    alpha1, minppl = golden_section_search(<span class=\"hljs-keyword\">lambda</span> a:unigram_perplexity(gram1, testset, V, a), <span class=\"hljs-number\">0.0001</span>, <span class=\"hljs-number\">1.0</span>)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"additive smoother: alpha1=%.4f, perplexity=%.3f\"</span> % (alpha1, minppl)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"Kneser-Ney: heuristic D=%.3f, perplexity=%.3f\"</span> % (D1, kn1_perplexity(gram1, testset, V, D1))\n    D1min, minppl = golden_section_search(<span class=\"hljs-keyword\">lambda</span> d:kn1_perplexity(gram1, testset, V, d), <span class=\"hljs-number\">0.0001</span>, <span class=\"hljs-number\">0.9999</span>)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"Kneser-Ney: minimum D=%.3f, perplexity=%.3f\"</span> % (D1min, minppl)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"modified Kneser-Ney: perplexity=%.3f\"</span> % mkn1_perplexity(gram1, testset, V)\n\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"\\nBIGRAM:\"</span>\n    alpha2, minppl = golden_section_search(<span class=\"hljs-keyword\">lambda</span> a:bigram_perplexity(gram1, gram2, testset, V, alpha1, a), <span class=\"hljs-number\">0.0001</span>, <span class=\"hljs-number\">1.0</span>)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"additive smoother: alpha2=%.4f, perplexity=%.3f\"</span> % (alpha2, minppl)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"Kneser-Ney: heuristic D=%.3f, perplexity=%.3f\"</span> % (D2, kn2_perplexity(gram1, gram2, testset, V, D1, D2))\n    D2min, minppl = golden_section_search(<span class=\"hljs-keyword\">lambda</span> a:kn2_perplexity(gram1, gram2, testset, V, D1, a), <span class=\"hljs-number\">0.0001</span>, <span class=\"hljs-number\">0.9999</span>)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"Kneser-Ney: minimum D=%.3f, perplexity=%.3f\"</span> % (D2min, minppl)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"modified Kneser-Ney: perplexity=%.3f\"</span> % mkn2_perplexity(gram1, gram2, testset, V)\n\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"\\nTRIGRAM:\"</span>\n    alpha3, minppl = golden_section_search(<span class=\"hljs-keyword\">lambda</span> a:trigram_perplexity(gram1, gram2, gram3, testset, V, alpha1, alpha2, a), <span class=\"hljs-number\">0.0001</span>, <span class=\"hljs-number\">1.0</span>)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"additive smoother: alpha3=%.4f, perplexity=%.3f\"</span> % (alpha3, minppl)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"Kneser-Ney: heuristic D=%.3f, perplexity=%.3f\"</span> % (D3, kn3_perplexity(gram1, gram2, gram3, testset, V, D1, D2, D3))\n    D3min, minppl = golden_section_search(<span class=\"hljs-keyword\">lambda</span> a:kn3_perplexity(gram1, gram2, gram3, testset, V, D1, D2, a), <span class=\"hljs-number\">0.0001</span>, <span class=\"hljs-number\">0.9999</span>)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"Kneser-Ney: minimum D=%.3f, perplexity=%.3f\"</span> % (D3min, minppl)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">\"modified Kneser-Ney: perplexity=%.3f\"</span> % mkn3_perplexity(gram1, gram2, gram3, testset, V)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    main()</code></pre></div>",
    "sec_31": "<div class=\"codeBlock hljs python\" id=\"sec_31\"><pre id=\"sec_31_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n<span class=\"hljs-keyword\">import</span> scipy <span class=\"hljs-keyword\">as</span> sp\n<span class=\"hljs-keyword\">from</span> sklearn.cross_validation <span class=\"hljs-keyword\">import</span> train_test_split\n<span class=\"hljs-keyword\">from</span> sklearn.feature_extraction.text <span class=\"hljs-keyword\">import</span> CountVectorizer, TfidfVectorizer\n<span class=\"hljs-keyword\">from</span> sklearn.naive_bayes <span class=\"hljs-keyword\">import</span> MultinomialNB\n<span class=\"hljs-keyword\">from</span> sklearn.linear_model <span class=\"hljs-keyword\">import</span> LogisticRegression\n<span class=\"hljs-keyword\">from</span> sklearn <span class=\"hljs-keyword\">import</span> metrics\n<span class=\"hljs-keyword\">from</span> textblob <span class=\"hljs-keyword\">import</span> TextBlob, Word\n<span class=\"hljs-keyword\">from</span> nltk.stem.snowball <span class=\"hljs-keyword\">import</span> SnowballStemmer\n\n<span class=\"hljs-comment\"># read yelp.csv into a DataFrame</span>\nurl = <span class=\"hljs-string\">'https://raw.githubusercontent.com/justmarkham/DAT7/master/data/yelp.csv'</span>\nyelp = pd.read_csv(url)\n\n<span class=\"hljs-comment\"># create a new DataFrame that only contains the 5-star and 1-star reviews</span>\nyelp_best_worst = yelp[(yelp.stars==<span class=\"hljs-number\">5</span>) | (yelp.stars==<span class=\"hljs-number\">1</span>)]\n\n<span class=\"hljs-comment\"># split the new DataFrame into training and testing sets</span>\nX_train, X_test, y_train, y_test = train_test_split(yelp_best_worst.text, yelp_best_worst.stars, random_state=<span class=\"hljs-number\">1</span>)\n\n\n<span class=\"hljs-comment\"># ## Part 3: Tokenization</span>\n<span class=\"hljs-comment\"># - **What:** Separate text into units such as sentences or words</span>\n<span class=\"hljs-comment\"># - **Why:** Gives structure to previously unstructured text</span>\n<span class=\"hljs-comment\"># - **Notes:** Relatively easy with English language text, not easy with some languages</span>\n\n<span class=\"hljs-comment\"># use CountVectorizer to create document-term matrices from X_train and X_test</span>\nvect = CountVectorizer()\ntrain_dtm = vect.fit_transform(X_train)\ntest_dtm = vect.transform(X_test)\n\n<span class=\"hljs-comment\"># rows are documents, columns are terms (aka \"tokens\" or \"features\")</span>\ntrain_dtm.shape\n\n<span class=\"hljs-comment\"># last 50 features</span>\n<span class=\"hljs-built_in\">print</span> vect.get_feature_names()[-<span class=\"hljs-number\">50</span>:]\n\n<span class=\"hljs-comment\"># show vectorizer options</span>\nvect\n\n<span class=\"hljs-comment\"># [CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)</span>\n\n<span class=\"hljs-comment\"># - **lowercase:** boolean, True by default</span>\n<span class=\"hljs-comment\"># - Convert all characters to lowercase before tokenizing.</span>\n\n<span class=\"hljs-comment\"># don't convert to lowercase</span>\nvect = CountVectorizer(lowercase=<span class=\"hljs-literal\">False</span>)\ntrain_dtm = vect.fit_transform(X_train)\ntrain_dtm.shape\n\n<span class=\"hljs-comment\"># - **token_pattern:** string</span>\n<span class=\"hljs-comment\"># - Regular expression denoting what constitutes a \"token\". The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).</span>\n\n<span class=\"hljs-comment\"># allow tokens of one character</span>\nvect = CountVectorizer(token_pattern=<span class=\"hljs-string\">r'(?u)\\b\\w+\\b'</span>)\ntrain_dtm = vect.fit_transform(X_train)\ntrain_dtm.shape\n\n<span class=\"hljs-comment\"># - **ngram_range:** tuple (min_n, max_n)</span>\n<span class=\"hljs-comment\"># - The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n will be used.</span>\n\n<span class=\"hljs-comment\"># include 1-grams and 2-grams</span>\nvect = CountVectorizer(ngram_range=(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>))\ntrain_dtm = vect.fit_transform(X_train)\ntrain_dtm.shape\n\n<span class=\"hljs-comment\"># last 50 features</span>\n<span class=\"hljs-built_in\">print</span> vect.get_feature_names()[-<span class=\"hljs-number\">50</span>:]\n\n<span class=\"hljs-comment\"># **Predicting the star rating:**</span>\n\n<span class=\"hljs-comment\"># use default options for CountVectorizer</span>\nvect = CountVectorizer()\n\n<span class=\"hljs-comment\"># create document-term matrices</span>\ntrain_dtm = vect.fit_transform(X_train)\ntest_dtm = vect.transform(X_test)\n\n<span class=\"hljs-comment\"># use Naive Bayes to predict the star rating</span>\nnb = MultinomialNB()\nnb.fit(train_dtm, y_train)\ny_pred_class = nb.predict(test_dtm)\n\n<span class=\"hljs-comment\"># calculate accuracy</span>\n<span class=\"hljs-built_in\">print</span> metrics.accuracy_score(y_test, y_pred_class)\n\n<span class=\"hljs-comment\"># calculate null accuracy</span>\ny_test_binary = np.where(y_test==<span class=\"hljs-number\">5</span>, <span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">0</span>)\ny_test_binary.mean()\n\n<span class=\"hljs-comment\"># define a function that accepts a vectorizer and returns the accuracy</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">tokenize_test</span>(<span class=\"hljs-params\">vect</span>):</span>\n    train_dtm = vect.fit_transform(X_train)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'Features: '</span>, train_dtm.shape[<span class=\"hljs-number\">1</span>]\n    test_dtm = vect.transform(X_test)\n    nb = MultinomialNB()\n    nb.fit(train_dtm, y_train)\n    y_pred_class = nb.predict(test_dtm)\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'Accuracy: '</span>, metrics.accuracy_score(y_test, y_pred_class)\n\n<span class=\"hljs-comment\"># include 1-grams and 2-grams</span>\nvect = CountVectorizer(ngram_range=(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>))\ntokenize_test(vect)\n\n\n<span class=\"hljs-comment\"># ## Part 4: Stopword Removal</span>\n<span class=\"hljs-comment\"># - **What:** Remove common words that will likely appear in any text</span>\n<span class=\"hljs-comment\"># - **Why:** They don't tell you much about your text</span>\n\n<span class=\"hljs-comment\"># show vectorizer options</span>\nvect\n\n<span class=\"hljs-comment\"># - **stop_words:** string {'english'}, list, or None (default)</span>\n<span class=\"hljs-comment\"># - If 'english', a built-in stop word list for English is used.</span>\n<span class=\"hljs-comment\"># - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.</span>\n<span class=\"hljs-comment\"># - If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.</span>\n\n<span class=\"hljs-comment\"># remove English stop words</span>\nvect = CountVectorizer(stop_words=<span class=\"hljs-string\">'english'</span>)\ntokenize_test(vect)\n\n<span class=\"hljs-comment\"># set of stop words</span>\n<span class=\"hljs-built_in\">print</span> vect.get_stop_words()\n\n\n<span class=\"hljs-comment\"># ## Part 5: Other CountVectorizer Options</span>\n<span class=\"hljs-comment\"># - **max_features:** int or None, default=None</span>\n<span class=\"hljs-comment\"># - If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.</span>\n\n<span class=\"hljs-comment\"># remove English stop words and only keep 100 features</span>\nvect = CountVectorizer(stop_words=<span class=\"hljs-string\">'english'</span>, max_features=<span class=\"hljs-number\">100</span>)\ntokenize_test(vect)\n\n<span class=\"hljs-comment\"># all 100 features</span>\n<span class=\"hljs-built_in\">print</span> vect.get_feature_names()\n\n<span class=\"hljs-comment\"># include 1-grams and 2-grams, and limit the number of features</span>\nvect = CountVectorizer(ngram_range=(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>), max_features=<span class=\"hljs-number\">100000</span>)\ntokenize_test(vect)\n\n<span class=\"hljs-comment\"># - **min_df:** float in range [0.0, 1.0] or int, default=1</span>\n<span class=\"hljs-comment\"># - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts.</span>\n\n<span class=\"hljs-comment\"># include 1-grams and 2-grams, and only include terms that appear at least 2 times</span>\nvect = CountVectorizer(ngram_range=(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>), min_df=<span class=\"hljs-number\">2</span>)\ntokenize_test(vect)\n\n\n<span class=\"hljs-comment\"># ## Part 6: Introduction to TextBlob</span>\n<span class=\"hljs-comment\"># TextBlob: \"Simplified Text Processing\"</span>\n\n<span class=\"hljs-comment\"># print the first review</span>\n<span class=\"hljs-built_in\">print</span> yelp_best_worst.text[<span class=\"hljs-number\">0</span>]\n\n<span class=\"hljs-comment\"># save it as a TextBlob object</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"text_segmentation_0\" class=\"highlights fea_text_segmentation\">review = TextBlob(yelp_best_worst.text[<span class=\"hljs-number\">0</span>])\n\n<span class=\"hljs-comment\"># list the words</span>\nreview.words\n\n<span class=\"hljs-comment\"># list the sentences</span>\nreview.sentences\n\n<span class=\"hljs-comment\"># some string methods are available</span>\nreview.lower()</div>\n\n\n<span class=\"hljs-comment\"># ## Part 7: Stemming and Lemmatization</span>\n\n<span class=\"hljs-comment\"># **Stemming:**</span>\n<span class=\"hljs-comment\"># - **What:** Reduce a word to its base/stem/root form</span>\n<span class=\"hljs-comment\"># - **Why:** Often makes sense to treat related words the same way</span>\n<span class=\"hljs-comment\"># - **Notes:**</span>\n<span class=\"hljs-comment\">#     - Uses a \"simple\" and fast rule-based approach</span>\n<span class=\"hljs-comment\">#     - Stemmed words are usually not shown to users (used for analysis/indexing)</span>\n<span class=\"hljs-comment\">#     - Some search engines treat words with the same stem as synonyms</span>\n\n<span class=\"hljs-comment\"># initialize stemmer</span>\nstemmer = SnowballStemmer(<span class=\"hljs-string\">'english'</span>)\n\n<span class=\"hljs-comment\"># stem each word</span>\n<span class=\"hljs-built_in\">print</span> [stemmer.stem(word) <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> review.words]\n\n<span class=\"hljs-comment\"># **Lemmatization**</span>\n<span class=\"hljs-comment\"># - **What:** Derive the canonical form ('lemma') of a word</span>\n<span class=\"hljs-comment\"># - **Why:** Can be better than stemming</span>\n<span class=\"hljs-comment\"># - **Notes:** Uses a dictionary-based approach (slower than stemming)</span>\n\n<span class=\"hljs-comment\"># assume every word is a noun</span>\n<span class=\"hljs-built_in\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"lemmatization_0\" class=\"highlights fea_lemmatization\"><span class=\"hljs-built_in\">print</span> [word.lemmatize() <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> review.words]</div>\n\n<span class=\"hljs-comment\"># assume every word is a verb</span>\n<span class=\"hljs-built_in\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"lemmatization_1\" class=\"highlights fea_lemmatization\"><span class=\"hljs-built_in\">print</span> [word.lemmatize(pos=<span class=\"hljs-string\">'v'</span>) <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> review.words]</div>\n\n<span class=\"hljs-comment\"># define a function that accepts text and returns a list of lemmas</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">split_into_lemmas</span>(<span class=\"hljs-params\">text</span>):</span>\n    text = unicode(text, <span class=\"hljs-string\">'utf-8'</span>).lower()\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"lemmatization_2\" class=\"highlights fea_lemmatization\">words = TextBlob(text).words\n    <span class=\"hljs-keyword\">return</span> [word.lemmatize() <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> words]</div>\n\n<span class=\"hljs-comment\"># use split_into_lemmas as the feature extraction function</span>\nvect = CountVectorizer(analyzer=split_into_lemmas)\ntokenize_test(vect)\n\n<span class=\"hljs-comment\"># last 50 features</span>\n<span class=\"hljs-built_in\">print</span> vect.get_feature_names()[-<span class=\"hljs-number\">50</span>:]\n\n\n<span class=\"hljs-comment\"># ## Part 8: Term Frequency - Inverse Document Frequency (TF-IDF)</span>\n<span class=\"hljs-comment\"># - **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents</span>\n<span class=\"hljs-comment\"># - **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents)</span>\n<span class=\"hljs-comment\"># - **Notes:** Used for search engine scoring, text summarization, document clustering</span>\n\n<span class=\"hljs-comment\"># example documents</span>\ntrain_simple = [<span class=\"hljs-string\">'call you tonight'</span>,\n                <span class=\"hljs-string\">'Call me a cab'</span>,\n                <span class=\"hljs-string\">'please call me... PLEASE!'</span>]\n\n<span class=\"hljs-comment\"># CountVectorizer</span>\nvect = CountVectorizer()\npd.DataFrame(vect.fit_transform(train_simple).toarray(), columns=vect.get_feature_names())\n\n<span class=\"hljs-comment\"># TfidfVectorizer</span>\nvect = TfidfVectorizer()\npd.DataFrame(vect.fit_transform(train_simple).toarray(), columns=vect.get_feature_names())\n\n\n<span class=\"hljs-comment\"># ## Part 9: Using TF-IDF to Summarize a Yelp Review</span>\n\n<span class=\"hljs-comment\"># create a document-term matrix using TF-IDF</span>\nvect = TfidfVectorizer(stop_words=<span class=\"hljs-string\">'english'</span>)\ndtm = vect.fit_transform(yelp.text)\nfeatures = vect.get_feature_names()\ndtm.shape\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">summarize</span>():</span>\n    \n    <span class=\"hljs-comment\"># choose a random review that is at least 300 characters</span>\n    review_length = <span class=\"hljs-number\">0</span>\n    <span class=\"hljs-keyword\">while</span> review_length &lt; <span class=\"hljs-number\">300</span>:\n        review_id = np.random.randint(<span class=\"hljs-number\">0</span>, <span class=\"hljs-built_in\">len</span>(yelp))\n        review_text = unicode(yelp.text[review_id], <span class=\"hljs-string\">'utf-8'</span>)\n        review_length = <span class=\"hljs-built_in\">len</span>(review_text)\n    \n    <span class=\"hljs-comment\"># create a dictionary of words and their TF-IDF scores</span>\n    word_scores = {}\n    <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> TextBlob(review_text).words:\n        word = word.lower()\n        <span class=\"hljs-keyword\">if</span> word <span class=\"hljs-keyword\">in</span> features:\n            word_scores[word] = dtm[review_id, features.index(word)]\n    \n    <span class=\"hljs-comment\"># print words with the top 5 TF-IDF scores</span>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'TOP SCORING WORDS:'</span>\n    top_scores = <span class=\"hljs-built_in\">sorted</span>(word_scores.items(), key=<span class=\"hljs-keyword\">lambda</span> x: x[<span class=\"hljs-number\">1</span>], reverse=<span class=\"hljs-literal\">True</span>)[:<span class=\"hljs-number\">5</span>]\n    <span class=\"hljs-keyword\">for</span> word, score <span class=\"hljs-keyword\">in</span> top_scores:\n        <span class=\"hljs-built_in\">print</span> word\n    \n    <span class=\"hljs-comment\"># print 5 random words</span>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'\\n'</span> + <span class=\"hljs-string\">'RANDOM WORDS:'</span>\n    random_words = np.random.choice(word_scores.keys(), size=<span class=\"hljs-number\">5</span>, replace=<span class=\"hljs-literal\">False</span>)\n    <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> random_words:\n        <span class=\"hljs-built_in\">print</span> word\n    \n    <span class=\"hljs-comment\"># print the review</span>\n    <span class=\"hljs-built_in\">print</span> <span class=\"hljs-string\">'\\n'</span> + review_text\n\nsummarize()\n\n\n<span class=\"hljs-comment\"># ## Part 10: Sentiment Analysis</span>\n\n<span class=\"hljs-built_in\">print</span> review\n\n<span class=\"hljs-comment\"># polarity ranges from -1 (most negative) to 1 (most positive)</span>\nreview.sentiment.polarity\n\n<span class=\"hljs-comment\"># understanding the apply method</span>\nyelp[<span class=\"hljs-string\">'length'</span>] = yelp.text.apply(<span class=\"hljs-built_in\">len</span>)\n\n<span class=\"hljs-comment\"># define a function that accepts text and returns the polarity</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">detect_sentiment</span>(<span class=\"hljs-params\">text</span>):</span>\n    <span class=\"hljs-keyword\">return</span> TextBlob(text.decode(<span class=\"hljs-string\">'utf-8'</span>)).sentiment.polarity\n\n<span class=\"hljs-comment\"># create a new DataFrame column for sentiment</span>\nyelp[<span class=\"hljs-string\">'sentiment'</span>] = yelp.text.apply(detect_sentiment)\n\n<span class=\"hljs-comment\"># boxplot of sentiment grouped by stars</span>\nyelp.boxplot(column=<span class=\"hljs-string\">'sentiment'</span>, by=<span class=\"hljs-string\">'stars'</span>)\n\n<span class=\"hljs-comment\"># reviews with most positive sentiment</span>\nyelp[yelp.sentiment == <span class=\"hljs-number\">1</span>].text.head()\n\n<span class=\"hljs-comment\"># reviews with most negative sentiment</span>\nyelp[yelp.sentiment == -<span class=\"hljs-number\">1</span>].text.head()\n\n<span class=\"hljs-comment\"># widen the column display</span>\npd.set_option(<span class=\"hljs-string\">'max_colwidth'</span>, <span class=\"hljs-number\">500</span>)\n\n<span class=\"hljs-comment\"># negative sentiment in a 5-star review</span>\nyelp[(yelp.stars == <span class=\"hljs-number\">5</span>) &amp; (yelp.sentiment &lt; -<span class=\"hljs-number\">0.3</span>)].head()\n\n<span class=\"hljs-comment\"># positive sentiment in a 1-star review</span>\nyelp[(yelp.stars == <span class=\"hljs-number\">1</span>) &amp; (yelp.sentiment &gt; <span class=\"hljs-number\">0.5</span>)].head()\n\n<span class=\"hljs-comment\"># reset the column display width</span>\npd.reset_option(<span class=\"hljs-string\">'max_colwidth'</span>)\n\n\n<span class=\"hljs-comment\"># ## Part 11: Adding Features to a Document-Term Matrix</span>\n\n<span class=\"hljs-comment\"># create a new DataFrame that only contains the 5-star and 1-star reviews</span>\nyelp_best_worst = yelp[(yelp.stars==<span class=\"hljs-number\">5</span>) | (yelp.stars==<span class=\"hljs-number\">1</span>)]\n\n<span class=\"hljs-comment\"># split the new DataFrame into training and testing sets</span>\nfeature_cols = [<span class=\"hljs-string\">'text'</span>, <span class=\"hljs-string\">'sentiment'</span>, <span class=\"hljs-string\">'cool'</span>, <span class=\"hljs-string\">'useful'</span>, <span class=\"hljs-string\">'funny'</span>]\nX = yelp_best_worst[feature_cols]\ny = yelp_best_worst.stars\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class=\"hljs-number\">1</span>)\n\n<span class=\"hljs-comment\"># use CountVectorizer with text column only</span>\nvect = CountVectorizer()\ntrain_dtm = vect.fit_transform(X_train[:, <span class=\"hljs-number\">0</span>])\ntest_dtm = vect.transform(X_test[:, <span class=\"hljs-number\">0</span>])\n<span class=\"hljs-built_in\">print</span> train_dtm.shape\n<span class=\"hljs-built_in\">print</span> test_dtm.shape\n\n<span class=\"hljs-comment\"># shape of other four feature columns</span>\nX_train[:, <span class=\"hljs-number\">1</span>:].shape\n\n<span class=\"hljs-comment\"># cast other feature columns to float and convert to a sparse matrix</span>\nextra = sp.sparse.csr_matrix(X_train[:, <span class=\"hljs-number\">1</span>:].astype(<span class=\"hljs-built_in\">float</span>))\nextra.shape\n\n<span class=\"hljs-comment\"># combine sparse matrices</span>\ntrain_dtm_extra = sp.sparse.hstack((train_dtm, extra))\ntrain_dtm_extra.shape\n\n<span class=\"hljs-comment\"># repeat for testing set</span>\nextra = sp.sparse.csr_matrix(X_test[:, <span class=\"hljs-number\">1</span>:].astype(<span class=\"hljs-built_in\">float</span>))\ntest_dtm_extra = sp.sparse.hstack((test_dtm, extra))\ntest_dtm_extra.shape\n\n<span class=\"hljs-comment\"># use logistic regression with text column only</span>\nlogreg = LogisticRegression(C=<span class=\"hljs-number\">1e9</span>)\nlogreg.fit(train_dtm, y_train)\ny_pred_class = logreg.predict(test_dtm)\n<span class=\"hljs-built_in\">print</span> metrics.accuracy_score(y_test, y_pred_class)\n\n<span class=\"hljs-comment\"># use logistic regression with all features</span>\nlogreg = LogisticRegression(C=<span class=\"hljs-number\">1e9</span>)\nlogreg.fit(train_dtm_extra, y_train)\ny_pred_class = logreg.predict(test_dtm_extra)\n<span class=\"hljs-built_in\">print</span> metrics.accuracy_score(y_test, y_pred_class)\n\n\n<span class=\"hljs-comment\"># ## Part 12: Fun TextBlob Features</span>\n\n<span class=\"hljs-comment\"># spelling correction</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"spellcheck_1\" class=\"highlights fea_spellcheck\">TextBlob(<span class=\"hljs-string\">'15 minuets late'</span>).correct()</div>\n\n<span class=\"hljs-comment\"># spellcheck</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"spellcheck_0\" class=\"highlights fea_spellcheck\">Word(<span class=\"hljs-string\">'parot'</span>).spellcheck()</div>\n\n<span class=\"hljs-comment\"># definitions</span>\nWord(<span class=\"hljs-string\">'bank'</span>).define(<span class=\"hljs-string\">'v'</span>)\n\n<span class=\"hljs-comment\"># language identification</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"language_detection_0\" class=\"highlights fea_language_detection\">TextBlob(<span class=\"hljs-string\">'Hola amigos'</span>).detect_language()</div></code></pre></div>",
    "sec_32": "<div class=\"codeBlock hljs python\" id=\"sec_32\"><pre id=\"sec_32_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">from</span> textblob <span class=\"hljs-keyword\">import</span> TextBlob\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">clean_text</span>(<span class=\"hljs-params\">test</span>):</span>\n        <span class=\"hljs-string\">'''\n        Utility function to clean text by removing links, special characters\n        using simple regex statements.\n        '''</span>\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">' '</span>.join(re.sub(<span class=\"hljs-string\">\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\"</span>, <span class=\"hljs-string\">\" \"</span>, test).split())\n        <span class=\"hljs-comment\">#return</span>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">sentiment</span>(<span class=\"hljs-params\">test</span>):</span>\n        <span class=\"hljs-string\">'''\n        Utility function to classify sentiment of passed text\n        using textblob's sentiment method\n        '''</span>\n        <span class=\"hljs-comment\"># create TextBlob object of passed text</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_0\" class=\"highlights fea_sentiment_analysis\">analysis = TextBlob(clean_text(test))</div>\n        <span class=\"hljs-keyword\">return</span> analysis\n<span class=\"hljs-comment\">#print (text_use)</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_1\" class=\"highlights fea_sentiment_analysis\">sent = sent + <span class=\"hljs-string\">\"Overall sentiment Analysis for the test is \"</span> + <span class=\"hljs-built_in\">str</span>(<span class=\"hljs-built_in\">float</span>(sentiment(text_use).sentiment.polarity)*<span class=\"hljs-number\">100</span>) + <span class=\"hljs-string\">\"%&lt;br&gt;\"</span></div><span class=\"hljs-string\"></span>\n<span class=\"hljs-comment\">#print (sentiment(text_use).sentiment.polarity)</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_2\" class=\"highlights fea_sentiment_analysis\">sent = sent + <span class=\"hljs-string\">\"Overall subjectivity Analysis for the test is \"</span> + <span class=\"hljs-built_in\">str</span>(<span class=\"hljs-built_in\">float</span>(sentiment(text_use).sentiment.subjectivity)*<span class=\"hljs-number\">100</span>) + <span class=\"hljs-string\">\"%&lt;br&gt;\"</span>\nsent = sent + <span class=\"hljs-string\">\"&lt;small&gt;Note: The subjectivity Analysis is a number within the range [00, 100] where 0.0 is very objective and 100.0 is very subjective.&lt;/small&gt;&lt;br&gt;\"</span></div><span class=\"hljs-string\"></span>\n<span class=\"hljs-comment\">#print (sentiment(text_use).sentiment.subjectivity)</span>\n<span class=\"hljs-comment\">#print (sentiment(text_use).tags)</span>\n<span class=\"hljs-comment\">#print (sentiment(text_use).correct())</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"language_detection_0\" class=\"highlights fea_language_detection\">sent = sent + <span class=\"hljs-string\">\"Language is \"</span> + <span class=\"hljs-built_in\">str</span>(sentiment(text_use).detect_language()) + <span class=\"hljs-string\">\"&lt;br&gt;\"</span></div><span class=\"hljs-string\"></span>\n<span class=\"hljs-comment\">#print (sentiment(text_use).detect_language())</span>\n\n\n<span class=\"hljs-comment\"># In[11]:</span>\n\n\n<span class=\"hljs-comment\">#import re</span>\n<span class=\"hljs-comment\">#from textblob import TextBlob</span>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">clean_text</span>(<span class=\"hljs-params\">test</span>):</span>\n        <span class=\"hljs-string\">'''\n        Utility function to clean text by removing links, special characters\n        using simple regex statements.\n        '''</span>\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">' '</span>.join(re.sub(<span class=\"hljs-string\">\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\"</span>, <span class=\"hljs-string\">\" \"</span>, test).split())\n        <span class=\"hljs-comment\">#return</span>\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">sentiment</span>(<span class=\"hljs-params\">test,subject</span>):</span>\n        <span class=\"hljs-string\">'''\n        Utility function to classify sentiment of passed text\n        using textblob's sentiment method\n        '''</span>\n        <span class=\"hljs-comment\"># create TextBlob object of passed text</span>\n        <span class=\"hljs-comment\">#analysis = TextBlob(clean_text(test))</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"text_simplify_0\" class=\"highlights fea_text_simplify\">analysis = TextBlob(test)\n        s = analysis.sentences\n        asn = <span class=\"hljs-string\">''</span>\n        <span class=\"hljs-keyword\">for</span> sn <span class=\"hljs-keyword\">in</span> s:\n            tb = TextBlob(<span class=\"hljs-built_in\">str</span>(sn))\n            <span class=\"hljs-keyword\">if</span> subject.lower() <span class=\"hljs-keyword\">in</span> tb.words.lower():\n                asn = asn + <span class=\"hljs-string\">' '</span> + <span class=\"hljs-built_in\">str</span>(sn)\n        <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-built_in\">len</span>(asn.strip())&gt;<span class=\"hljs-number\">4</span>):\n            analysis = TextBlob(asn)\n            <span class=\"hljs-keyword\">return</span> analysis\n        <span class=\"hljs-keyword\">else</span>:\n            asn = <span class=\"hljs-string\">\"NONE\"</span>\n            analysis = TextBlob(asn)\n            <span class=\"hljs-keyword\">return</span> analysis</div>\nsubjects = subjects.split(<span class=\"hljs-string\">','</span>)\n<span class=\"hljs-keyword\">for</span> sub <span class=\"hljs-keyword\">in</span> subjects:\n    subject = sub.strip()\n    <span class=\"hljs-comment\">#print (text_use)</span>\n    <span class=\"hljs-comment\">#print (subject)</span>\n    sent = sent + <span class=\"hljs-string\">\"Sentiment for tag \"</span> + subject + <span class=\"hljs-string\">\" is \"</span>\n    <span class=\"hljs-comment\">#print (sentiment(text_use,subject))</span>\n    <span class=\"hljs-comment\">#print (sentiment(text_use,subject).sentiment.polarity)</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_3\" class=\"highlights fea_sentiment_analysis\">sent = sent + <span class=\"hljs-string\">\" \"</span> + <span class=\"hljs-built_in\">str</span>(<span class=\"hljs-built_in\">float</span>(sentiment(text_use,subject).sentiment.polarity)*<span class=\"hljs-number\">100</span>) + <span class=\"hljs-string\">\"%&lt;br&gt;\"</span></div><span class=\"hljs-string\"></span>\n    <span class=\"hljs-comment\">#print (sentiment(text_use,subject).sentiment.subjectivity)</span>\n    <span class=\"hljs-comment\">#print (sentiment(text_use,subject).tags)</span>\n    <span class=\"hljs-comment\">#print (sentiment(text_use,subject).correct())</span>\n    <span class=\"hljs-comment\">#print (sentiment(text_use,subject).detect_language())</span>\nsent  = sent.replace(<span class=\"hljs-string\">\"&lt;br&gt;\"</span>,<span class=\"hljs-string\">\".  \"</span>)\nsent = sent.replace(<span class=\"hljs-string\">\"&lt;small&gt;\"</span>, <span class=\"hljs-string\">\"\"</span>)\nsent = sent.replace(<span class=\"hljs-string\">\"&lt;/small&gt;\"</span>, <span class=\"hljs-string\">\"\"</span>)</code></pre></div>",
    "sec_33": "<div class=\"codeBlock hljs python\" id=\"sec_33\"><pre id=\"sec_33_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">from</span> textblob <span class=\"hljs-keyword\">import</span> TextBlob\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">tweetSentiment</span>(<span class=\"hljs-params\">tweet, verbose=<span class=\"hljs-literal\">False</span></span>):</span>\n    <span class=\"hljs-string\">\"\"\"\n    This function analyzes a tweet which it takes in as an input as either a\n    tweet object or a string. It breaks down the tweet into its sentances and then analyzes the\n    sentiment of each sentance on a sliding scale from -1 to 1.\n\n    Currently the function then blindly averages the sentiments of each sentance\n    and returns a float which corresponds to how \"positive\" or \"negative\" the tweet is.\n\n    INPUTS\n    ------------------\n    tweet       str, textblob.blob.TextBlob, or tweet object\n                The tweet which we are to analyze the sentient of\n\n    verbose     boolean\n                Should the funciton print out a bunch of infomation about\n                what it is up to?\n\n    RETURNS\n    ------------------\n    avg_sent    float\n                What is the sentient of all included sentances averaged together?\n                a negative number equates to a negative sentiment while a positive number\n                equates to positive sentiment.\n\n    TODO: add logic which ensures that the sentiment analysis is being performed in regard\n    to our key_terms\n    \"\"\"</span>\n    text = translateTweets(tweet).text\n    text = filterTweetText(text)\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_0\" class=\"highlights fea_sentiment_analysis\">blob = TextBlob(text)</div>\n    <span class=\"hljs-comment\"># lblob = TextBlob(lemmatizeTweet(text))</span>\n    <span class=\"hljs-comment\"># print(blob.sentences[0].polarity, \"/\",lblob.sentences[0].polarity)</span>\n\n    <span class=\"hljs-comment\"># this just allows for debug printing</span>\n    <span class=\"hljs-keyword\">if</span> verbose <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">True</span>:\n        print(blob.tags)\n        print(<span class=\"hljs-string\">\"------------------------------\"</span>)\n        print(blob.noun_phrases)\n        print(<span class=\"hljs-string\">\"------------------------------\"</span>)\n        print(<span class=\"hljs-built_in\">len</span>(blob.sentences), blob.sentences)\n        print(<span class=\"hljs-string\">\"------------------------------\"</span>)\n        <span class=\"hljs-keyword\">for</span> sent <span class=\"hljs-keyword\">in</span> blob.sentences:\n            print(sent.sentiment.polarity)\n        print(<span class=\"hljs-string\">\"------------------------------\"</span>)\n\n    <span class=\"hljs-comment\"># we need to make sure that we double check the subject of each sentance</span>\n\n    <span class=\"hljs-comment\"># we also need to figure out how to deal with multiple sentences</span>\n    <span class=\"hljs-comment\"># right now the function is niavely adding them together</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_1\" class=\"highlights fea_sentiment_analysis\">avg_sent = <span class=\"hljs-number\">0</span>\n    <span class=\"hljs-keyword\">for</span> sent <span class=\"hljs-keyword\">in</span> blob.sentences:\n        avg_sent += sent.sentiment.polarity\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(blob.sentences) &gt; <span class=\"hljs-number\">0</span>:\n        avg_sent /= <span class=\"hljs-built_in\">len</span>(blob.sentences)\n    <span class=\"hljs-keyword\">else</span>:\n        avg_sent = <span class=\"hljs-number\">0</span></div><span class=\"hljs-number\"></span>\n\n    <span class=\"hljs-keyword\">if</span> verbose <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">True</span>:\n        print(<span class=\"hljs-string\">\"Returned sent : \"</span>, avg_sent)\n        print(<span class=\"hljs-string\">\"||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\"</span>)\n\n    <span class=\"hljs-keyword\">return</span> avg_sent\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">translateTweets</span>(<span class=\"hljs-params\">tweets, verbose=<span class=\"hljs-literal\">False</span></span>):</span>\n    <span class=\"hljs-string\">\"\"\"\n    Takes in a list of tweets and then translates the ones which are not english into english\n    Note that this function can now take in either a single tweet object or a list of\n    tweet objects\n\n    INPUTS\n    --------------\n    tweet       str, textblob.blob.TextBlob, or tweet object\n                The tweet which we are to analyze the sentient of\n\n    verbose     boolean\n                Should the funciton print out a bunch of infomation about\n                what it is up to?\n\n    RETURNS\n    -------------\n    tweets      tweet obects\n                The list of tweets passed into the function with translation applied\n    \"\"\"</span>\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"translation_0\" class=\"highlights fea_translation\"><span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">type</span>(tweets) <span class=\"hljs-keyword\">is</span> <span class=\"hljs-built_in\">list</span>:\n        <span class=\"hljs-keyword\">for</span> tweet <span class=\"hljs-keyword\">in</span> tweets:\n            <span class=\"hljs-keyword\"></span><span class=\"hljs-keyword\">if</span> tweet.lang != <span class=\"hljs-string\">'en'</span>:\n                blob = TextBlob(tweet.text)\n                <span class=\"hljs-keyword\">try</span>:\n                    tweet.text = blob.translate(to=<span class=\"hljs-string\">'en'</span>)\n                    tweet.lang = <span class=\"hljs-string\">'en'</span>\n                    <span class=\"hljs-keyword\">if</span> verbose == <span class=\"hljs-literal\">True</span>:\n                        print(tweet.text)\n                <span class=\"hljs-keyword\">except</span> Exception <span class=\"hljs-keyword\">as</span> NotTranslated:\n                    <span class=\"hljs-keyword\">if</span> verbose == <span class=\"hljs-literal\">True</span>:\n                        print(<span class=\"hljs-string\">\"Could not translate! \"</span>, tweet.text)\n    <span class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-keyword\">if</span> tweets.lang != <span class=\"hljs-string\">'en'</span>:\n            blob = TextBlob(tweets.text)\n            <span class=\"hljs-keyword\">try</span>:\n                tweets.text = blob.translate(to=<span class=\"hljs-string\">'en'</span>)\n                tweets.lang = <span class=\"hljs-string\">'en'</span>\n                <span class=\"hljs-keyword\">if</span> verbose == <span class=\"hljs-literal\">True</span>:\n                    print(tweets.text)\n            <span class=\"hljs-keyword\">except</span> Exception <span class=\"hljs-keyword\">as</span> NotTranslated:\n                <span class=\"hljs-keyword\">if</span> verbose == <span class=\"hljs-literal\">True</span>:\n                    print(<span class=\"hljs-string\">\"Could not translate! \"</span>, tweets.text)</div>\n\n    <span class=\"hljs-keyword\">return</span> tweets\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">lemmatizeTweet</span>(<span class=\"hljs-params\">tweet</span>):</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"lemmatization_0\" class=\"highlights fea_lemmatization\">normalized_tweet = <span class=\"hljs-string\">\"\"</span>\n    text = TextBlob(tweet.lower())\n    <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> text.words:\n        normalized_tweet += word.lemmatize() + <span class=\"hljs-string\">\" \"</span>\n    <span class=\"hljs-keyword\">return</span> normalized_tweet</div>\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">removeUnwantedWords</span>(<span class=\"hljs-params\">text</span>):</span>\n    <span class=\"hljs-string\">\"\"\"\n    Also removes links\n    \"\"\"</span>\n    new = <span class=\"hljs-string\">\"\"</span>\n    <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> text.split(<span class=\"hljs-string\">\" \"</span>):\n        word = word.replace(<span class=\"hljs-string\">'\\n'</span>, <span class=\"hljs-string\">' '</span>).strip(<span class=\"hljs-string\">'\\n'</span>)\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">\"\\n\"</span> <span class=\"hljs-keyword\">in</span> word:\n            print(<span class=\"hljs-string\">\"newline found! : \"</span>, word)\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> word.startswith(<span class=\"hljs-string\">\"@\"</span>) <span class=\"hljs-keyword\">and</span> <span class=\"hljs-keyword\">not</span> word.startswith(<span class=\"hljs-string\">\"http\"</span>):\n            new = new + word.rstrip() + <span class=\"hljs-string\">\" \"</span>\n    <span class=\"hljs-keyword\">return</span> new\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">filterTweetText</span>(<span class=\"hljs-params\">text</span>):</span>\n    <span class=\"hljs-string\">\"\"\"\n    This function is intended to perform NL processing that\n    is specific to tweet texts.\n\n    This includes removing RT from the start of tweets and the removal of '@'\n    \"\"\"</span>\n    new = text\n    <span class=\"hljs-keyword\">if</span> new.startswith(<span class=\"hljs-string\">\"RT \"</span>):\n        new = text[<span class=\"hljs-number\">3</span>:]\n    new = removeUnwantedWords(new)\n    <span class=\"hljs-keyword\">return</span> new\n\n</code></pre></div>",
    "sec_34": "<div class=\"codeBlock hljs python\" id=\"sec_34\"><pre id=\"sec_34_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\"># -*- coding: utf-8 -*-</span>\n<span class=\"hljs-string\">\"\"\"Parts-of-speech tagger implementations.\"\"\"</span>\n<span class=\"hljs-keyword\">from</span> __future__ <span class=\"hljs-keyword\">import</span> absolute_import\n\n<span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">import</span> textblob.compat\n\n<span class=\"hljs-keyword\">import</span> textblob <span class=\"hljs-keyword\">as</span> tb\n<span class=\"hljs-keyword\">from</span> textblob.en <span class=\"hljs-keyword\">import</span> tag <span class=\"hljs-keyword\">as</span> pattern_tag\n<span class=\"hljs-keyword\">from</span> textblob.decorators <span class=\"hljs-keyword\">import</span> requires_nltk_corpus\n<span class=\"hljs-keyword\">from</span> textblob.base <span class=\"hljs-keyword\">import</span> BaseTagger\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">PatternTagger</span>(<span class=\"hljs-params\">BaseTagger</span>):</span>\n    <span class=\"hljs-string\">\"\"\"Tagger that uses the implementation in\n    Tom de Smedt's pattern library\n    (http://www.clips.ua.ac.be/pattern).\n    \"\"\"</span>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">tag</span>(<span class=\"hljs-params\">self, text, tokenize=<span class=\"hljs-literal\">True</span></span>):</span>\n        <span class=\"hljs-string\">\"\"\"Tag a string or BaseBlob.\"\"\"</span>\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_0\" class=\"highlights fea_tagger\"><span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-built_in\">isinstance</span>(text, textblob.compat.text_type):\n            text = text.raw\n        <span class=\"hljs-keyword\">return</span> pattern_tag(text, tokenize)</div>\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">NLTKTagger</span>(<span class=\"hljs-params\">BaseTagger</span>):</span>\n    <span class=\"hljs-string\">\"\"\"Tagger that uses NLTK's standard TreeBank tagger.\n    NOTE: Requires numpy. Not yet supported with PyPy.\n    \"\"\"</span>\n\n<span class=\"hljs-meta\">    @requires_nltk_corpus</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">tag</span>(<span class=\"hljs-params\">self, text</span>):</span>\n        <span class=\"hljs-string\">\"\"\"Tag a string or BaseBlob.\"\"\"</span>\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\"><span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">isinstance</span>(text, textblob.compat.text_type):\n            text = tb.TextBlob(text)\n\n        <span class=\"hljs-keyword\">return</span> nltk.tag.pos_tag(text.tokens)</div></code></pre></div>",
    "sec_35": "<div class=\"codeBlock hljs python\" id=\"sec_35\"><pre id=\"sec_35_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\"># -*- coding: utf-8 -*-</span>\n<span class=\"hljs-keyword\">from</span> __future__ <span class=\"hljs-keyword\">import</span> unicode_literals\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">import</span> unittest\n<span class=\"hljs-keyword\">from</span> nose.tools <span class=\"hljs-keyword\">import</span> *  <span class=\"hljs-comment\"># PEP8 asserts</span>\n<span class=\"hljs-keyword\">from</span> nose.plugins.attrib <span class=\"hljs-keyword\">import</span> attr\n\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_0\" class=\"highlights fea_tagger\"><span class=\"hljs-keyword\">from</span> textblob.base <span class=\"hljs-keyword\">import</span> BaseTagger\n<span class=\"hljs-keyword\">import</span> textblob.taggers</div>\n\nHERE = os.path.abspath(os.path.dirname(__file__))\nAP_MODEL_LOC = os.path.join(HERE, <span class=\"hljs-string\">'trontagger.pickle'</span>)\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">TestPatternTagger</span>(<span class=\"hljs-params\">unittest.TestCase</span>):</span>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">setUp</span>(<span class=\"hljs-params\">self</span>):</span>\n        self.text = (<span class=\"hljs-string\">\"Simple is better than complex. \"</span>\n                    <span class=\"hljs-string\">\"Complex is better than complicated.\"</span>)\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_1\" class=\"highlights fea_tagger\">self.tagger = textblob.taggers.PatternTagger()</div>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_init</span>(<span class=\"hljs-params\">self</span>):</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_2\" class=\"highlights fea_tagger\">tagger = textblob.taggers.PatternTagger()\n        assert_true(<span class=\"hljs-built_in\">isinstance</span>(tagger, textblob.taggers.BaseTagger))</div>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_tag</span>(<span class=\"hljs-params\">self</span>):</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_3\" class=\"highlights fea_tagger\">tags = self.tagger.tag(self.text)\n        assert_equal(tags,\n            [(<span class=\"hljs-string\">'Simple'</span>, <span class=\"hljs-string\">'JJ'</span>), (<span class=\"hljs-string\">'is'</span>, <span class=\"hljs-string\">'VBZ'</span>), (<span class=\"hljs-string\">'better'</span>, <span class=\"hljs-string\">'JJR'</span>),\n            (<span class=\"hljs-string\">'than'</span>, <span class=\"hljs-string\">'IN'</span>), (<span class=\"hljs-string\">'complex'</span>, <span class=\"hljs-string\">'JJ'</span>), (<span class=\"hljs-string\">'.'</span>, <span class=\"hljs-string\">'.'</span>),\n            (<span class=\"hljs-string\">'Complex'</span>, <span class=\"hljs-string\">'NNP'</span>), (<span class=\"hljs-string\">'is'</span>, <span class=\"hljs-string\">'VBZ'</span>), (<span class=\"hljs-string\">'better'</span>, <span class=\"hljs-string\">'JJR'</span>),\n            (<span class=\"hljs-string\">'than'</span>, <span class=\"hljs-string\">'IN'</span>), (<span class=\"hljs-string\">'complicated'</span>, <span class=\"hljs-string\">'VBN'</span>), (<span class=\"hljs-string\">'.'</span>, <span class=\"hljs-string\">'.'</span>)])</div>\n\n\n<span class=\"hljs-meta\">@attr(<span class=\"hljs-params\"><span class=\"hljs-string\">\"slow\"</span></span>)</span>\n<span class=\"hljs-meta\">@attr(<span class=\"hljs-params\"><span class=\"hljs-string\">\"no_pypy\"</span></span>)</span>\n<span class=\"hljs-meta\">@attr(<span class=\"hljs-params\"><span class=\"hljs-string\">\"requires_numpy\"</span></span>)</span>\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">TestNLTKTagger</span>(<span class=\"hljs-params\">unittest.TestCase</span>):</span>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">setUp</span>(<span class=\"hljs-params\">self</span>):</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_4\" class=\"highlights fea_tagger\">self.text = (<span class=\"hljs-string\">\"Simple is better than complex. \"</span>\n                    <span class=\"hljs-string\">\"Complex is better than complicated.\"</span>)\n        self.tagger = textblob.taggers.NLTKTagger()</div>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_tag</span>(<span class=\"hljs-params\">self</span>):</span>\n        tags = self.tagger.tag(self.text)\n        assert_equal(tags,\n            [(<span class=\"hljs-string\">'Simple'</span>, <span class=\"hljs-string\">'NNP'</span>), (<span class=\"hljs-string\">'is'</span>, <span class=\"hljs-string\">'VBZ'</span>),\n            (<span class=\"hljs-string\">'better'</span>, <span class=\"hljs-string\">'JJR'</span>), (<span class=\"hljs-string\">'than'</span>, <span class=\"hljs-string\">'IN'</span>),\n            (<span class=\"hljs-string\">'complex'</span>, <span class=\"hljs-string\">'JJ'</span>), (<span class=\"hljs-string\">'.'</span>, <span class=\"hljs-string\">'.'</span>), (<span class=\"hljs-string\">'Complex'</span>, <span class=\"hljs-string\">'NNP'</span>),\n            (<span class=\"hljs-string\">'is'</span>, <span class=\"hljs-string\">'VBZ'</span>), (<span class=\"hljs-string\">'better'</span>, <span class=\"hljs-string\">'JJR'</span>),\n            (<span class=\"hljs-string\">'than'</span>, <span class=\"hljs-string\">'IN'</span>), (<span class=\"hljs-string\">'complicated'</span>, <span class=\"hljs-string\">'JJ'</span>), (<span class=\"hljs-string\">'.'</span>, <span class=\"hljs-string\">'.'</span>)])\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_cannot_instantiate_incomplete_tagger</span>():</span>\n    <span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">BadTagger</span>(<span class=\"hljs-params\">BaseTagger</span>):</span>\n        <span class=\"hljs-string\">'''A tagger without a tag method. How useless.'''</span>\n        <span class=\"hljs-keyword\">pass</span>\n    assert_raises(TypeError, <span class=\"hljs-keyword\">lambda</span>: BadTagger())\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n    unittest.main()</code></pre></div>",
    "sec_36": "<div class=\"codeBlock hljs python\" id=\"sec_36\"><pre id=\"sec_36_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\">#pip install -U textblob nltk</span>\n<span class=\"hljs-comment\">#&gt;python -m textblob.download_corpora</span>\n<span class=\"hljs-comment\">#https://stevenloria.com/simple-text-classification/</span>\n<span class=\"hljs-keyword\">from</span> textblob.classifiers <span class=\"hljs-keyword\">import</span> NaiveBayesClassifier\n<span class=\"hljs-keyword\">from</span> textblob <span class=\"hljs-keyword\">import</span> TextBlob\n<span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">import</span> glob\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">from</span> random <span class=\"hljs-keyword\">import</span> shuffle\n\n<span class=\"hljs-keyword\">import</span> nltk\nnltk.download(<span class=\"hljs-string\">'stopwords'</span>)\n\nratings = [<span class=\"hljs-string\">'one_star'</span>, <span class=\"hljs-string\">'two_star'</span>, <span class=\"hljs-string\">'three_star'</span>, <span class=\"hljs-string\">'four_star'</span>, <span class=\"hljs-string\">'five_star'</span>]\ncwd = os.getcwd()\ntraining = []\ntesting = []\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">filter</span>(<span class=\"hljs-params\">text</span>):</span>\n    <span class=\"hljs-keyword\">from</span> nltk.tokenize <span class=\"hljs-keyword\">import</span> word_tokenize\n    tokens = word_tokenize(text)\n    <span class=\"hljs-comment\"># convert to lower case</span>\n    tokens = [w.lower() <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> tokens]\n    <span class=\"hljs-comment\"># remove punctuation from each word</span>\n    <span class=\"hljs-keyword\">import</span> string\n    table = <span class=\"hljs-built_in\">str</span>.maketrans(<span class=\"hljs-string\">''</span>, <span class=\"hljs-string\">''</span>, string.punctuation)\n    stripped = [w.translate(table) <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> tokens]\n    <span class=\"hljs-comment\"># remove remaining tokens that are not alphabetic</span>\n    words = [word <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> stripped <span class=\"hljs-keyword\">if</span> word.isalpha()]\n    <span class=\"hljs-comment\"># filter out stop words</span>\n    <span class=\"hljs-keyword\">from</span> nltk.corpus <span class=\"hljs-keyword\">import</span> stopwords\n    stop_words = <span class=\"hljs-built_in\">set</span>(stopwords.words(<span class=\"hljs-string\">'english'</span>))\n    words = [w <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> words <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> w <span class=\"hljs-keyword\">in</span> stop_words]\n    text = <span class=\"hljs-string\">''</span>\n    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> words:\n        text += i +<span class=\"hljs-string\">' '</span>\n    <span class=\"hljs-keyword\">return</span> text\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">analyzeSample</span>(<span class=\"hljs-params\">filename, rating</span>):</span>\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(filename, encoding=<span class=\"hljs-string\">'utf8'</span>, errors=<span class=\"hljs-string\">'ignore'</span>) <span class=\"hljs-keyword\">as</span> f:\n        text = f.read()\n        filtered = <span class=\"hljs-built_in\">filter</span>(text)\n        training.extend([(filtered, rating)])\n    <span class=\"hljs-keyword\">return</span> text\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">analyzeAll</span>():</span>\n    total_files = <span class=\"hljs-number\">0</span>\n    correct_classifications = <span class=\"hljs-number\">0</span>\n\n    cwd = os.getcwd()\n    <span class=\"hljs-keyword\">for</span> rating <span class=\"hljs-keyword\">in</span> ratings:\n        path = cwd + <span class=\"hljs-string\">'\\\\training\\\\'</span> + rating\n        print(path)\n        <span class=\"hljs-keyword\">for</span> filename <span class=\"hljs-keyword\">in</span> glob.glob(os.path.join(path, <span class=\"hljs-string\">'*.txt'</span>)):\n             analyzeSample(filename, rating)\n\n    print(<span class=\"hljs-string\">'reading done'</span>)\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">analyzeAllTesting</span>():</span>\n    total_files = <span class=\"hljs-number\">0</span>\n    correct_classifications = <span class=\"hljs-number\">0</span>\n\n    cwd = os.getcwd()\n    <span class=\"hljs-keyword\">for</span> rating <span class=\"hljs-keyword\">in</span> ratings:\n        path = cwd + <span class=\"hljs-string\">'\\\\data\\\\'</span> + rating\n        print(path)\n        <span class=\"hljs-keyword\">for</span> filename <span class=\"hljs-keyword\">in</span> glob.glob(os.path.join(path, <span class=\"hljs-string\">'*.txt'</span>)):\n             analyzeSampleTesting(filename, rating)\n\n    print(<span class=\"hljs-string\">'reading done'</span>)\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">analyzeSampleTesting</span>(<span class=\"hljs-params\">filename, rating</span>):</span>\n\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(filename, encoding=<span class=\"hljs-string\">'utf8'</span>, errors=<span class=\"hljs-string\">'ignore'</span>) <span class=\"hljs-keyword\">as</span> f:\n        text = f.read()\n        testing.extend([(text, rating)])\n    <span class=\"hljs-keyword\">return</span> text\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test</span>(<span class=\"hljs-params\">maxtesting</span>):</span>\n\n    print(<span class=\"hljs-built_in\">len</span>(testing))\n    <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-built_in\">len</span>(testing) &lt; maxtesting):\n        maxtesting = <span class=\"hljs-built_in\">len</span>(testing)\n    print(<span class=\"hljs-string\">\"Accuracy: {0}\"</span>.<span class=\"hljs-built_in\">format</span>(cl.accuracy(testing[:maxtesting])))\n    print(<span class=\"hljs-string\">'testing_done'</span>)\n\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">check</span>(<span class=\"hljs-params\"><span class=\"hljs-built_in\">max</span></span>):</span>\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"classification_0\" class=\"highlights fea_classification\"><span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> testing[:<span class=\"hljs-built_in\">max</span>]:\n        textInput = i[<span class=\"hljs-number\">0</span>]\n        blob = TextBlob(textInput, classifier=cl)\n        print(blob)\n        print(blob.classify())</div>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">data</span>():</span>\n    analyzeAll()\n    analyzeAllTesting()\n    <span class=\"hljs-comment\">#mix data</span>\n    shuffle(training)\n    shuffle(testing)\ndata()\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"classification_1\" class=\"highlights fea_classification\">maxtraining = <span class=\"hljs-number\">2000</span>\n\nprint(<span class=\"hljs-built_in\">len</span>(training))\n<span class=\"hljs-keyword\">if</span> (<span class=\"hljs-built_in\">len</span>(training) &lt; maxtraining):\n    maxtraining = <span class=\"hljs-built_in\">len</span>(training)\ncl = NaiveBayesClassifier(training[:maxtraining])</div>\nprint(<span class=\"hljs-string\">'training_done'</span>)\ntest(<span class=\"hljs-number\">500</span>)\ncheck(<span class=\"hljs-number\">20</span>)</code></pre></div>",
    "sec_37": "<div class=\"codeBlock hljs python\" id=\"sec_37\"><pre id=\"sec_37_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\"># -*- coding: utf-8 -*-</span>\n<span class=\"hljs-string\">\"\"\"Sentiment analysis implementations.\n\n.. versionadded:: 0.5.0\n\"\"\"</span>\n<span class=\"hljs-keyword\">from</span> __future__ <span class=\"hljs-keyword\">import</span> absolute_import\n<span class=\"hljs-keyword\">from</span> collections <span class=\"hljs-keyword\">import</span> namedtuple\n\n<span class=\"hljs-keyword\">from</span> textblob.packages <span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_2\" class=\"highlights fea_sentiment_analysis\"><span class=\"hljs-keyword\">from</span> textblob.en <span class=\"hljs-keyword\">import</span> sentiment <span class=\"hljs-keyword\">as</span> pattern_sentiment</div>\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_1\" class=\"highlights fea_tokenization\"><span class=\"hljs-keyword\">from</span> textblob.tokenizers <span class=\"hljs-keyword\">import</span> word_tokenize</div>\n<span class=\"hljs-keyword\">from</span> textblob.decorators <span class=\"hljs-keyword\">import</span> requires_nltk_corpus\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_1\" class=\"highlights fea_sentiment_analysis\"><span class=\"hljs-keyword\">from</span> textblob.base <span class=\"hljs-keyword\">import</span> BaseSentimentAnalyzer</div>, DISCRETE, CONTINUOUS\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">PatternAnalyzer</span>(<span class=\"hljs-params\">BaseSentimentAnalyzer</span>):</span>\n\n    <span class=\"hljs-string\">'''Sentiment analyzer that uses the same implementation as the\n    pattern library. Returns results as a named tuple of the form:\n\n    ``Sentiment(polarity, subjectivity)``\n    '''</span>\n    kind = CONTINUOUS\n    <span class=\"hljs-comment\">#: Return type declaration</span>\n    RETURN_TYPE = namedtuple(<span class=\"hljs-string\">'Sentiment'</span>, [<span class=\"hljs-string\">'polarity'</span>, <span class=\"hljs-string\">'subjectivity'</span>])\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">analyze</span>(<span class=\"hljs-params\">self, text</span>):</span>\n        <span class=\"hljs-string\">\"\"\"Return the sentiment as a named tuple of the form:\n        ``Sentiment(polarity, subjectivity)``.\n        \"\"\"</span>\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_3\" class=\"highlights fea_sentiment_analysis\"><span class=\"hljs-keyword\">return</span> self.RETURN_TYPE(*pattern_sentiment(text))</div>\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">NaiveBayesAnalyzer</span>(<span class=\"hljs-params\"><div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_0\" class=\"highlights fea_sentiment_analysis\">BaseSentimentAnalyzer</div></span>):</span>\n\n    <span class=\"hljs-string\">'''Naive Bayes analyzer that is trained on a dataset of movie reviews.\n    Returns results as a named tuple of the form:\n    ``Sentiment(classification, p_pos, p_neg)``\n    '''</span>\n\n    kind = DISCRETE\n    <span class=\"hljs-comment\">#: Return type declaration</span>\n    RETURN_TYPE = namedtuple(<span class=\"hljs-string\">'Sentiment'</span>, [<span class=\"hljs-string\">'classification'</span>, <span class=\"hljs-string\">'p_pos'</span>, <span class=\"hljs-string\">'p_neg'</span>])\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span>\n        <span class=\"hljs-built_in\">super</span>(NaiveBayesAnalyzer, self).__init__()\n        self._classifier = <span class=\"hljs-literal\">None</span>\n\n<span class=\"hljs-meta\">    @requires_nltk_corpus</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">train</span>(<span class=\"hljs-params\">self</span>):</span>\n        <span class=\"hljs-string\">'''Train the Naive Bayes classifier on the movie review corpus.'''</span>\n        <span class=\"hljs-built_in\">super</span>(NaiveBayesAnalyzer, self).train()\n        neg_ids = nltk.corpus.movie_reviews.fileids(<span class=\"hljs-string\">'neg'</span>)\n        pos_ids = nltk.corpus.movie_reviews.fileids(<span class=\"hljs-string\">'pos'</span>)\n        neg_feats = [(self._extract_feats(\n            nltk.corpus.movie_reviews.words(fileids=[f])), <span class=\"hljs-string\">'neg'</span>) <span class=\"hljs-keyword\">for</span> f <span class=\"hljs-keyword\">in</span> neg_ids]\n        pos_feats = [(self._extract_feats(\n            nltk.corpus.movie_reviews.words(fileids=[f])), <span class=\"hljs-string\">'pos'</span>) <span class=\"hljs-keyword\">for</span> f <span class=\"hljs-keyword\">in</span> pos_ids]\n        train_data = neg_feats + pos_feats\n        self._classifier = nltk.classify.NaiveBayesClassifier.train(train_data)\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">_extract_feats</span>(<span class=\"hljs-params\">self, words</span>):</span>\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">dict</span>([(word, <span class=\"hljs-literal\">True</span>) <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> words])\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">analyze</span>(<span class=\"hljs-params\">self, text</span>):</span>\n        <span class=\"hljs-string\">\"\"\"Return the sentiment as a named tuple of the form:\n        ``Sentiment(classification, p_pos, p_neg)``\n        \"\"\"</span>\n        <span class=\"hljs-comment\"># Lazily train the classifier</span>\n        <span class=\"hljs-built_in\">super</span>(NaiveBayesAnalyzer, self).analyze(text)\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">tokens = word_tokenize(text, include_punc=<span class=\"hljs-literal\">False</span>)</div>\n        filtered = (t.lower() <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> tokens <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(t) &gt;= <span class=\"hljs-number\">3</span>)\n        feats = self._extract_feats(filtered)\n        prob_dist = self._classifier.prob_classify(feats)\n        <span class=\"hljs-keyword\">return</span> self.RETURN_TYPE(\n            classification=prob_dist.<span class=\"hljs-built_in\">max</span>(),\n            p_pos=prob_dist.prob(<span class=\"hljs-string\">'pos'</span>),\n            p_neg=prob_dist.prob(<span class=\"hljs-string\">\"neg\"</span>)\n        )</code></pre></div>",
    "sec_38": "<div class=\"codeBlock hljs python\" id=\"sec_38\"><pre id=\"sec_38_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\"># -*- coding: utf-8 -*-</span>\n<span class=\"hljs-string\">'''Various noun phrase extractors.'''</span>\n<span class=\"hljs-keyword\">from</span> __future__ <span class=\"hljs-keyword\">import</span> unicode_literals, absolute_import\n<span class=\"hljs-keyword\">from</span> textblob.packages <span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_0\" class=\"highlights fea_tagger\"><span class=\"hljs-keyword\">from</span> textblob.taggers <span class=\"hljs-keyword\">import</span> PatternTagger</div>\n<span class=\"hljs-keyword\">from</span> textblob.decorators <span class=\"hljs-keyword\">import</span> requires_nltk_corpus\n<span class=\"hljs-keyword\">from</span> textblob.utils <span class=\"hljs-keyword\">import</span> tree2str, filter_insignificant\n<span class=\"hljs-keyword\">from</span> textblob.base <span class=\"hljs-keyword\">import</span> BaseNPExtractor\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">ChunkParser</span>(<span class=\"hljs-params\">nltk.ChunkParserI</span>):</span>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span>\n        self._trained = <span class=\"hljs-literal\">False</span>\n\n<span class=\"hljs-meta\">    @requires_nltk_corpus</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">train</span>(<span class=\"hljs-params\">self</span>):</span>\n        <span class=\"hljs-string\">'''Train the Chunker on the ConLL-2000 corpus.'''</span>\n        train_data = [[(t, c) <span class=\"hljs-keyword\">for</span> _, t, c <span class=\"hljs-keyword\">in</span> nltk.chunk.tree2conlltags(sent)]\n                      <span class=\"hljs-keyword\">for</span> sent <span class=\"hljs-keyword\">in</span>\n                      nltk.corpus.conll2000.chunked_sents(<span class=\"hljs-string\">'train.txt'</span>,\n                                                    chunk_types=[<span class=\"hljs-string\">'NP'</span>])]\n        unigram_tagger = nltk.UnigramTagger(train_data)\n        self.tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n        self._trained = <span class=\"hljs-literal\">True</span>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">parse</span>(<span class=\"hljs-params\">self, sentence</span>):</span>\n        <span class=\"hljs-string\">'''Return the parse tree for the sentence.'''</span>\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> self._trained:\n            self.train()\n        pos_tags = [pos <span class=\"hljs-keyword\">for</span> (word, pos) <span class=\"hljs-keyword\">in</span> sentence]\n        tagged_pos_tags = self.tagger.tag(pos_tags)\n        chunktags = [chunktag <span class=\"hljs-keyword\">for</span> (pos, chunktag) <span class=\"hljs-keyword\">in</span> tagged_pos_tags]\n        conlltags = [(word, pos, chunktag) <span class=\"hljs-keyword\">for</span> ((word, pos), chunktag) <span class=\"hljs-keyword\">in</span>\n                     <span class=\"hljs-built_in\">zip</span>(sentence, chunktags)]\n        <span class=\"hljs-keyword\">return</span> nltk.chunk.util.conlltags2tree(conlltags)\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">ConllExtractor</span>(<span class=\"hljs-params\">BaseNPExtractor</span>):</span>\n\n    <span class=\"hljs-string\">'''A noun phrase extractor that uses chunk parsing trained with the\n    ConLL-2000 training corpus.\n    '''</span>\n\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_1\" class=\"highlights fea_tagger\">POS_TAGGER = PatternTagger()</div>\n\n    <span class=\"hljs-comment\"># The context-free grammar with which to filter the noun phrases</span>\n    CFG = {\n        (<span class=\"hljs-string\">'NNP'</span>, <span class=\"hljs-string\">'NNP'</span>): <span class=\"hljs-string\">'NNP'</span>,\n        (<span class=\"hljs-string\">'NN'</span>, <span class=\"hljs-string\">'NN'</span>): <span class=\"hljs-string\">'NNI'</span>,\n        (<span class=\"hljs-string\">'NNI'</span>, <span class=\"hljs-string\">'NN'</span>): <span class=\"hljs-string\">'NNI'</span>,\n        (<span class=\"hljs-string\">'JJ'</span>, <span class=\"hljs-string\">'JJ'</span>): <span class=\"hljs-string\">'JJ'</span>,\n        (<span class=\"hljs-string\">'JJ'</span>, <span class=\"hljs-string\">'NN'</span>): <span class=\"hljs-string\">'NNI'</span>,\n        }\n\n    <span class=\"hljs-comment\"># POS suffixes that will be ignored</span>\n    INSIGNIFICANT_SUFFIXES = [<span class=\"hljs-string\">'DT'</span>, <span class=\"hljs-string\">'CC'</span>, <span class=\"hljs-string\">'PRP$'</span>, <span class=\"hljs-string\">'PRP'</span>]\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, parser=<span class=\"hljs-literal\">None</span></span>):</span>\n        self.parser = ChunkParser() <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> parser <span class=\"hljs-keyword\">else</span> parser\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">extract</span>(<span class=\"hljs-params\">self, text</span>):</span>\n        <span class=\"hljs-string\">'''Return a list of noun phrases (strings) for body of text.'''</span>\n        sentences = nltk.tokenize.sent_tokenize(text)\n        noun_phrases = []\n        <span class=\"hljs-keyword\">for</span> sentence <span class=\"hljs-keyword\">in</span> sentences:\n            parsed = self._parse_sentence(sentence)\n            <span class=\"hljs-comment\"># Get the string representation of each subtree that is a</span>\n            <span class=\"hljs-comment\"># noun phrase tree</span>\n            phrases = [_normalize_tags(filter_insignificant(each,\n                       self.INSIGNIFICANT_SUFFIXES)) <span class=\"hljs-keyword\">for</span> each <span class=\"hljs-keyword\">in</span> parsed\n                       <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">isinstance</span>(each, nltk.tree.Tree) <span class=\"hljs-keyword\">and</span> each.node\n                       == <span class=\"hljs-string\">'NP'</span> <span class=\"hljs-keyword\">and</span> <span class=\"hljs-built_in\">len</span>(filter_insignificant(each)) &gt;= <span class=\"hljs-number\">1</span>\n                       <span class=\"hljs-keyword\">and</span> _is_match(each, cfg=self.CFG)]\n            nps = [tree2str(phrase) <span class=\"hljs-keyword\">for</span> phrase <span class=\"hljs-keyword\">in</span> phrases]\n            noun_phrases.extend(nps)\n        <span class=\"hljs-keyword\">return</span> noun_phrases\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">_parse_sentence</span>(<span class=\"hljs-params\">self, sentence</span>):</span>\n        <span class=\"hljs-string\">'''Tag and parse a sentence (a plain, untagged string).'''</span>\n        tagged = self.POS_TAGGER.tag(sentence)\n        <span class=\"hljs-keyword\">return</span> self.parser.parse(tagged)</code></pre></div>",
    "sec_39": "<div class=\"codeBlock hljs python\" id=\"sec_39\"><pre id=\"sec_39_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\"># -*- coding: utf-8 -*-</span>\n<span class=\"hljs-string\">'''Parts-of-speech tagger implementations.'''</span>\n<span class=\"hljs-keyword\">from</span> __future__ <span class=\"hljs-keyword\">import</span> absolute_import\n\n<span class=\"hljs-keyword\">from</span> textblob.packages <span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">from</span> textblob.en <span class=\"hljs-keyword\">import</span> tag <span class=\"hljs-keyword\">as</span> pattern_tag\n<span class=\"hljs-keyword\">from</span> textblob.decorators <span class=\"hljs-keyword\">import</span> requires_nltk_corpus\n<span class=\"hljs-keyword\">from</span> textblob.tokenizers <span class=\"hljs-keyword\">import</span> word_tokenize\n<span class=\"hljs-keyword\">from</span> textblob.base <span class=\"hljs-keyword\">import</span> BaseTagger\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">PatternTagger</span>(<span class=\"hljs-params\">BaseTagger</span>):</span>\n\n    <span class=\"hljs-string\">'''Tagger that uses the implementation in\n    Tom de Smedt's pattern library\n    (http://www.clips.ua.ac.be/pattern).\n    '''</span>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">tag</span>(<span class=\"hljs-params\">self, text, tokenize=<span class=\"hljs-literal\">True</span></span>):</span>\n        <span class=\"hljs-string\">'''Tag a string `text`.'''</span>\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_0\" class=\"highlights fea_tagger\"><span class=\"hljs-keyword\">return</span> pattern_tag(text, tokenize)</div>\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">NLTKTagger</span>(<span class=\"hljs-params\">BaseTagger</span>):</span>\n\n    <span class=\"hljs-string\">'''Tagger that uses NLTK's standard TreeBank tagger.\n    NOTE: Requires numpy. Not yet supported with PyPy.\n    '''</span>\n\n<span class=\"hljs-meta\">    @requires_nltk_corpus</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">tag</span>(<span class=\"hljs-params\">self, text, tokenize=<span class=\"hljs-literal\">True</span></span>):</span>\n        <span class=\"hljs-string\">'''Tag a string `text`.'''</span>\n        <span class=\"hljs-keyword\">if</span> tokenize:\n           <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\"> text = <span class=\"hljs-built_in\">list</span>(word_tokenize(text))</div>\n        tagged = nltk.tag.pos_tag(text)\n        <span class=\"hljs-keyword\">return</span> tagged</code></pre></div>",
    "sec_40": "<div class=\"codeBlock hljs python\" id=\"sec_40\"><pre id=\"sec_40_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">from</span> textblob <span class=\"hljs-keyword\">import</span> TextBlob\n<span class=\"hljs-keyword\">import</span> random\n<span class=\"hljs-keyword\">import</span> time\n<span class=\"hljs-keyword\">import</span> twython\n<span class=\"hljs-keyword\">from</span> twython <span class=\"hljs-keyword\">import</span> TwythonError\n<span class=\"hljs-keyword\">import</span> sys\n\n\napi_key, api_secret, access_token, token_secret = sys.argv[<span class=\"hljs-number\">1</span>:]\ntwitter = twython.Twython(api_key, api_secret, access_token, token_secret)\n\nnouns = []\nadjective = []\nmarriageText = <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">'marriage.txt'</span>, <span class=\"hljs-string\">'r'</span>)\ntaxText = <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">'taxidermy.txt'</span>, <span class=\"hljs-string\">'r'</span>)\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">textGen</span>(<span class=\"hljs-params\"><span class=\"hljs-built_in\">object</span></span>):</span>\n\t<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, text1, text2</span>):</span>\n\t\tself.text1 = text1\n\t\tself.text2 = text2\n\n\n\t<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">strip</span>(<span class=\"hljs-params\">self</span>):</span>\n\t\t<span class=\"hljs-keyword\">for</span> line <span class=\"hljs-keyword\">in</span> self.text1:\n\t\t\tline = line.decode(<span class=\"hljs-string\">'utf-8'</span>).strip()\n\t\t\tblob = TextBlob(line)\n\n\t\t\t<span class=\"hljs-keyword\">for</span> word, tag <span class=\"hljs-keyword\">in</span> blob.tags:\n\t\t\t\t<span class=\"hljs-keyword\">if</span> tag == <span class=\"hljs-string\">\"NN\"</span>:\n\t\t\t\t\tnouns.append(word)\n\n\n\t<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">wordSwap</span>(<span class=\"hljs-params\">self</span>):</span>\n\t\ttextIn = <span class=\"hljs-string\">\"\"</span>\n\t\tplaceholder = []\n\n\t\t<span class=\"hljs-keyword\">for</span> line <span class=\"hljs-keyword\">in</span> self.text2:\n\t\t\tline = line.strip()\n\t\t\ttextIn += line + <span class=\"hljs-string\">\" \"</span>\n\t\ttextIn.split()\n\t\ttextIn = textIn.decode(<span class=\"hljs-string\">'utf-8'</span>)\n\t\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_0\" class=\"highlights fea_tagger\">result = TextBlob(textIn)\n\n\t\t<span class=\"hljs-keyword\">for</span> sentence <span class=\"hljs-keyword\">in</span> result.sentences:\n\t\t\t<span class=\"hljs-keyword\">for</span> word, tag <span class=\"hljs-keyword\">in</span> sentence.tags:</div>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_1\" class=\"highlights fea_tagger\">\t\t\t\t<span class=\"hljs-keyword\"></span><span class=\"hljs-keyword\">if</span> tag == <span class=\"hljs-string\">\"NN\"</span>:\n\t\t\t\t\tsentence = sentence.replace(word, random.choice(nouns))\n\t\t\tplaceholder.append(sentence)</div>\n\n\t\tstringTest = <span class=\"hljs-built_in\">str</span>(placeholder)\n\t\t\n\t\tstringTest.split()\n\t\tstringTest = stringTest.decode(<span class=\"hljs-string\">'utf-8'</span>)\n\t\tcorpus = TextBlob(stringTest)\n\t\tswapOutput = random.choice(corpus.sentences)\n\t\t<span class=\"hljs-keyword\">return</span> swapOutput\n\t\t\n\ngenerate = textGen(taxText, marriageText)\noutput1 = textGen.strip(generate)\noutput2 = <span class=\"hljs-built_in\">str</span>(textGen.wordSwap(generate))\nfixer = output2[<span class=\"hljs-number\">14</span>:]\nadvice = (fixer[:<span class=\"hljs-number\">120</span>]) <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(fixer) &gt; <span class=\"hljs-number\">120</span> <span class=\"hljs-keyword\">else</span> fixer\n<span class=\"hljs-built_in\">print</span> advice\n\nresponse = twitter.search(q=<span class=\"hljs-string\">\"marriage\"</span>, result_type=<span class=\"hljs-string\">'recent'</span>, lang = <span class=\"hljs-string\">\"en\"</span>, count=<span class=\"hljs-number\">1</span>)\n<span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(response[<span class=\"hljs-string\">'statuses'</span>]) &gt; <span class=\"hljs-number\">0</span>:\n\tfirst_tweet = response[<span class=\"hljs-string\">'statuses'</span>][<span class=\"hljs-number\">0</span>]\n\ttarget = first_tweet[<span class=\"hljs-string\">'user'</span>][<span class=\"hljs-string\">'screen_name'</span>]\n\ttargetID = first_tweet[<span class=\"hljs-string\">u'id_str'</span>]\n\t\n<span class=\"hljs-keyword\">else</span>:\n\t\n\t<span class=\"hljs-built_in\">print</span> line\ntime.sleep(<span class=\"hljs-number\">0.5</span>)\n\n\n<span class=\"hljs-keyword\">try</span>:\n\ttwitter.update_status(status= <span class=\"hljs-string\">\"@\"</span> + target + <span class=\"hljs-string\">\" \"</span> + advice, in_reply_to_status_id=targetID)\n<span class=\"hljs-keyword\">except</span> TwythonError <span class=\"hljs-keyword\">as</span> e:\n\t<span class=\"hljs-built_in\">print</span> e</code></pre></div>",
    "sec_41": "<div class=\"codeBlock hljs python\" id=\"sec_41\"><pre id=\"sec_41_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\"># -*- coding: utf-8 -*-</span>\n<span class=\"hljs-keyword\">from</span> __future__ <span class=\"hljs-keyword\">import</span> unicode_literals\n<span class=\"hljs-keyword\">import</span> unittest\n<span class=\"hljs-keyword\">from</span> nose.tools <span class=\"hljs-keyword\">import</span> *  <span class=\"hljs-comment\"># PEP8 asserts</span>\n<span class=\"hljs-keyword\">from</span> nose.plugins.attrib <span class=\"hljs-keyword\">import</span> attr\n\n<span class=\"hljs-keyword\">from</span> textblob.base <span class=\"hljs-keyword\">import</span> BaseTagger\n<span class=\"hljs-keyword\">from</span> textblob.blob <span class=\"hljs-keyword\">import</span> TextBlob\n<span class=\"hljs-keyword\">from</span> textblob.exceptions <span class=\"hljs-keyword\">import</span> MissingCorpusException\n<span class=\"hljs-keyword\">from</span> textblob_aptagger <span class=\"hljs-keyword\">import</span> PerceptronTagger\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">TestPerceptronTagger</span>(<span class=\"hljs-params\">unittest.TestCase</span>):</span>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">setUp</span>(<span class=\"hljs-params\">self</span>):</span>\n        self.text = (<span class=\"hljs-string\">\"Simple is better than complex. \"</span>\n                     <span class=\"hljs-string\">\"Complex is better than complicated.\"</span>)\n        self.tagger = PerceptronTagger(load=<span class=\"hljs-literal\">False</span>)\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_init</span>(<span class=\"hljs-params\">self</span>):</span>\n        tagger = PerceptronTagger(load=<span class=\"hljs-literal\">False</span>)\n        assert_true(<span class=\"hljs-built_in\">isinstance</span>(tagger, BaseTagger))\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_train</span>(<span class=\"hljs-params\">self</span>):</span>\n        sentences = _read_tagged(_wsj_train)\n        nr_iter = <span class=\"hljs-number\">5</span>\n        self.tagger.train(sentences, nr_iter=nr_iter)\n        nr_words = <span class=\"hljs-built_in\">sum</span>(<span class=\"hljs-built_in\">len</span>(words) <span class=\"hljs-keyword\">for</span> words, tags <span class=\"hljs-keyword\">in</span> sentences)\n        <span class=\"hljs-comment\"># Check that the model has 'ticked over' once per instance</span>\n        assert_equal(nr_words * nr_iter, self.tagger.model.i)\n        <span class=\"hljs-comment\"># Check that the tagger has a class for every seen tag</span>\n        tag_set = <span class=\"hljs-built_in\">set</span>()\n        <span class=\"hljs-keyword\">for</span> _, tags <span class=\"hljs-keyword\">in</span> sentences:\n            tag_set.update(tags)\n        assert_equal(<span class=\"hljs-built_in\">len</span>(tag_set), <span class=\"hljs-built_in\">len</span>(self.tagger.model.classes))\n        <span class=\"hljs-keyword\">for</span> tag <span class=\"hljs-keyword\">in</span> tag_set:\n            assert_true(tag <span class=\"hljs-keyword\">in</span> self.tagger.model.classes)\n\n<span class=\"hljs-meta\">    @attr(<span class=\"hljs-params\"><span class=\"hljs-string\">\"slow\"</span></span>)</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_tag</span>(<span class=\"hljs-params\">self</span>):</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_0\" class=\"highlights fea_tagger\">trained_tagger = PerceptronTagger()\n        tokens = trained_tagger.tag(self.text)\n        assert_equal([w <span class=\"hljs-keyword\">for</span> w, t <span class=\"hljs-keyword\">in</span> tokens],\n            [<span class=\"hljs-string\">'Simple'</span>, <span class=\"hljs-string\">'is'</span>, <span class=\"hljs-string\">'better'</span>, <span class=\"hljs-string\">'than'</span>, <span class=\"hljs-string\">'complex'</span>, <span class=\"hljs-string\">'.'</span>, <span class=\"hljs-string\">'Complex'</span>, <span class=\"hljs-string\">'is'</span>,\n             <span class=\"hljs-string\">'better'</span>, <span class=\"hljs-string\">'than'</span>, <span class=\"hljs-string\">'complicated'</span>, <span class=\"hljs-string\">'.'</span>])</div>\n\n<span class=\"hljs-meta\">    @attr(<span class=\"hljs-params\"><span class=\"hljs-string\">\"slow\"</span></span>)</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_tag_textblob</span>(<span class=\"hljs-params\">self</span>):</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_1\" class=\"highlights fea_tagger\">trained_tagger = PerceptronTagger()\n        blob = TextBlob(self.text, pos_tagger=trained_tagger)\n        <span class=\"hljs-comment\"># Punctuation is excluded</span>\n        assert_equal([w <span class=\"hljs-keyword\">for</span> w, t <span class=\"hljs-keyword\">in</span> blob.tags],\n            [<span class=\"hljs-string\">'Simple'</span>, <span class=\"hljs-string\">'is'</span>, <span class=\"hljs-string\">'better'</span>, <span class=\"hljs-string\">'than'</span>, <span class=\"hljs-string\">'complex'</span>, <span class=\"hljs-string\">'Complex'</span>, <span class=\"hljs-string\">'is'</span>,\n             <span class=\"hljs-string\">'better'</span>, <span class=\"hljs-string\">'than'</span>, <span class=\"hljs-string\">'complicated'</span>])</div>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_loading_missing_file_raises_missing_corpus_exception</span>(<span class=\"hljs-params\">self</span>):</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_2\" class=\"highlights fea_tagger\">tagger = PerceptronTagger(load=<span class=\"hljs-literal\">False</span>)\n        assert_raises(MissingCorpusException, tagger.load, <span class=\"hljs-string\">'missing.pickle'</span>)</div>\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">_read_tagged</span>(<span class=\"hljs-params\">text, sep=<span class=\"hljs-string\">'|'</span></span>):</span>\n    sentences = []\n    <span class=\"hljs-keyword\">for</span> sent <span class=\"hljs-keyword\">in</span> text.split(<span class=\"hljs-string\">'\\n'</span>):\n        tokens = []\n        tags = []\n        <span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> sent.split():\n            word, pos = token.split(sep)\n            tokens.append(word)\n            tags.append(pos)\n        sentences.append((tokens, tags))\n    <span class=\"hljs-keyword\">return</span> sentences\n\n_wsj_train = (<span class=\"hljs-string\">\"Pierre|NNP Vinken|NNP ,|, 61|CD years|NNS old|JJ ,|, will|MD \"</span>\n              <span class=\"hljs-string\">\"join|VB the|DT board|NN as|IN a|DT nonexecutive|JJ director|NN \"</span>\n              <span class=\"hljs-string\">\"Nov.|NNP 29|CD .|.\\nMr.|NNP Vinken|NNP is|VBZ chairman|NN of|IN \"</span>\n              <span class=\"hljs-string\">\"Elsevier|NNP N.V.|NNP ,|, the|DT Dutch|NNP publishing|VBG \"</span>\n              <span class=\"hljs-string\">\"group|NN .|. Rudolph|NNP Agnew|NNP ,|, 55|CD years|NNS old|JJ \"</span>\n              <span class=\"hljs-string\">\"and|CC former|JJ chairman|NN of|IN Consolidated|NNP Gold|NNP \"</span>\n              <span class=\"hljs-string\">\"Fields|NNP PLC|NNP ,|, was|VBD named|VBN a|DT nonexecutive|JJ \"</span>\n              <span class=\"hljs-string\">\"director|NN of|IN this|DT British|JJ industrial|JJ conglomerate|NN \"</span>\n              <span class=\"hljs-string\">\".|.\\nA|DT form|NN of|IN asbestos|NN once|RB used|VBN to|TO make|VB \"</span>\n              <span class=\"hljs-string\">\"Kent|NNP cigarette|NN filters|NNS has|VBZ caused|VBN a|DT high|JJ \"</span>\n              <span class=\"hljs-string\">\"percentage|NN of|IN cancer|NN deaths|NNS among|IN a|DT group|NN \"</span>\n              <span class=\"hljs-string\">\"of|IN workers|NNS exposed|VBN to|TO it|PRP more|RBR than|IN \"</span>\n              <span class=\"hljs-string\">\"30|CD years|NNS ago|IN ,|, researchers|NNS reported|VBD .|.\"</span>)\n\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n    unittest.main()</code></pre></div>",
    "sec_42": "<div class=\"codeBlock hljs python\" id=\"sec_42\"><pre id=\"sec_42_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> urllib2\n<span class=\"hljs-keyword\">from</span> bs4 <span class=\"hljs-keyword\">import</span> BeautifulSoup\n<span class=\"hljs-keyword\">from</span> textblob <span class=\"hljs-keyword\">import</span> TextBlob\n<span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">import</span> json\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">G2GAnalysis</span>():</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, url</span>):</span>\n        self.home=url\n        self.path=url[:(url.find(<span class=\"hljs-string\">'questions'</span>))]\n        self.page = urllib2.urlopen(self.home)\n        self.Users={} <span class=\"hljs-comment\">#dict holding the users</span>\n        self.Questions={} <span class=\"hljs-comment\"># dict holding the questions</span>\n        self.flipped=<span class=\"hljs-number\">0</span> <span class=\"hljs-comment\"># number of questions flipped through</span>\n            \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">runScrape</span>(<span class=\"hljs-params\">self, nQuestions</span>):</span>\n        <span class=\"hljs-string\">'''\n        Run through a bunch of pages of g2g questions\n        Parse them for the users, the users comments\n        and the user stats.\n        Fill a dictionary of results.\n        '''</span>\n        \n        tick=<span class=\"hljs-number\">0</span>\n        self.soup = BeautifulSoup(self.page, <span class=\"hljs-string\">'html.parser'</span>)\n        <span class=\"hljs-keyword\">while</span> self.flipped &lt; nQuestions:\n            tick+=<span class=\"hljs-number\">1</span>\n            <span class=\"hljs-keyword\">for</span> title <span class=\"hljs-keyword\">in</span> self.soup.findAll(<span class=\"hljs-string\">'div'</span>, attrs={<span class=\"hljs-string\">'class'</span>: <span class=\"hljs-string\">'qa-q-item-title'</span> }):                \n                self.flipped+=<span class=\"hljs-number\">1</span>\n                link=title.find(<span class=\"hljs-string\">'a'</span>).get(<span class=\"hljs-string\">'href'</span>)\n                <span class=\"hljs-keyword\">if</span> self.checkQuestion(link):\n                    self.scrapeQuestion(link)\n            startTick=tick*<span class=\"hljs-number\">50</span>\n            <span class=\"hljs-built_in\">print</span> tick\n            <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">'data.txt'</span>, <span class=\"hljs-string\">'w'</span>) <span class=\"hljs-keyword\">as</span> outfile:\n                json.dump(self.Users, outfile)\n            nextUrl=self.home+<span class=\"hljs-string\">'?start='</span>+<span class=\"hljs-built_in\">str</span>(startTick)\n            self.page = urllib2.urlopen(nextUrl)\n            self.soup=BeautifulSoup(self.page, <span class=\"hljs-string\">'html.parser'</span>)\n                \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">checkQuestion</span>(<span class=\"hljs-params\">self, link</span>):</span>\n        <span class=\"hljs-string\">'''\n        make sure I haven't queried this question before\n        if not, \n        add the question to the dictionary of questions\n        '''</span>\n        questNumber=link.split(<span class=\"hljs-string\">'/'</span>)[<span class=\"hljs-number\">1</span>]\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> questNumber <span class=\"hljs-keyword\">in</span> self.Questions:\n            self.Questions[questNumber]=self.path+link[<span class=\"hljs-number\">1</span>:]\n            questPage=urllib2.urlopen(self.path+link[<span class=\"hljs-number\">1</span>:])\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">True</span>\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">False</span>\n    \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">scrapeQuestion</span>(<span class=\"hljs-params\">self, link</span>):</span>\n        <span class=\"hljs-string\">'''\n        Go through the page\n        1.0: the question (only one)\n        1.1: the comments on the question (0 to many) \n        2.0: the answers (0 to many)\n        2.1: the comments on each answer in turn (0 to many)\n        '''</span>\n        link=self.path+link[<span class=\"hljs-number\">1</span>:]\n        <span class=\"hljs-comment\">#print link</span>\n        page = urllib2.urlopen(link)\n        qPage=BeautifulSoup(page, <span class=\"hljs-string\">'html.parser'</span>)\n        qTitle=qPage.find(<span class=\"hljs-string\">'title'</span>).text\n        qTitle=qTitle[:(qTitle.find(<span class=\"hljs-string\">' - WikiTree G2G'</span>))]\n        question= qPage.find(<span class=\"hljs-string\">'div'</span>, attrs={<span class=\"hljs-string\">'class'</span>: <span class=\"hljs-string\">'qa-part-q-view'</span> })\n        <span class=\"hljs-keyword\">try</span>:\n            questionText=question.find(<span class=\"hljs-string\">'div'</span>, attrs={<span class=\"hljs-string\">'class'</span>: <span class=\"hljs-string\">'entry-content'</span> }).text\n        <span class=\"hljs-keyword\">except</span>:\n            questionText=<span class=\"hljs-string\">''</span>\n        questionText=qTitle+questionText\n        <span class=\"hljs-keyword\">try</span>:\n            whoQ=question.find(<span class=\"hljs-string\">'span'</span>, attrs={<span class=\"hljs-string\">'class'</span>: <span class=\"hljs-string\">'qa-q-view-who-data'</span> })\n            whoQID=whoQ.find(<span class=\"hljs-string\">'a'</span>, attrs={<span class=\"hljs-string\">'class'</span>: <span class=\"hljs-string\">'qa-user-link'</span> }).get(<span class=\"hljs-string\">'href'</span>)\n        \n            self.addUserAndText(whoQID,questionText)\n        <span class=\"hljs-keyword\">except</span>:\n            <span class=\"hljs-keyword\">pass</span>\n        \n        <span class=\"hljs-comment\"># comments on the original question</span>\n        <span class=\"hljs-keyword\">for</span> qc <span class=\"hljs-keyword\">in</span> qPage.findAll(<span class=\"hljs-string\">'div'</span>, attrs={<span class=\"hljs-string\">'class'</span>: <span class=\"hljs-string\">'qa-c-list-item hentry comment'</span> }):\n            <span class=\"hljs-keyword\">try</span>:\n                qcID=qc.find(<span class=\"hljs-string\">'a'</span>, attrs={<span class=\"hljs-string\">'class'</span> : <span class=\"hljs-string\">'qa-user-link'</span>}).get(<span class=\"hljs-string\">'href'</span>)\n                qcText=qc.find(<span class=\"hljs-string\">'div'</span>, attrs={<span class=\"hljs-string\">'class'</span>: <span class=\"hljs-string\">'entry-content'</span> }).text\n                self.addUserAndText(qcID,qcText)\n            <span class=\"hljs-keyword\">except</span>:\n                <span class=\"hljs-keyword\">pass</span>\n\n        <span class=\"hljs-keyword\">for</span> answer <span class=\"hljs-keyword\">in</span> qPage.findAll(<span class=\"hljs-string\">'div'</span>, attrs={<span class=\"hljs-string\">'class'</span>:<span class=\"hljs-string\">'qa-a-list-item  hentry answer'</span>}):\n            <span class=\"hljs-keyword\">try</span>:\n                answerText=answer.find(<span class=\"hljs-string\">'div'</span>, attrs={<span class=\"hljs-string\">'class'</span>:<span class=\"hljs-string\">'qa-a-item-content'</span>}).text\n                answerID=answer.find(<span class=\"hljs-string\">'a'</span>, attrs={<span class=\"hljs-string\">'class'</span>:<span class=\"hljs-string\">'qa-user-link'</span>}).get(<span class=\"hljs-string\">'href'</span>)\n                self.addUserAndText(answerID,answerText)\n            <span class=\"hljs-keyword\">except</span>:\n                <span class=\"hljs-keyword\">pass</span>\n            <span class=\"hljs-keyword\">for</span> comment <span class=\"hljs-keyword\">in</span> answer.findAll(<span class=\"hljs-string\">'div'</span>, attrs={<span class=\"hljs-string\">'class'</span>:<span class=\"hljs-string\">'qa-a-item-c-list'</span>}):\n                ac=comment.find(<span class=\"hljs-string\">'div'</span>, attrs={<span class=\"hljs-string\">'class'</span>:<span class=\"hljs-string\">'qa-c-item-content'</span>})\n                <span class=\"hljs-keyword\">try</span>:\n                    acText=ac.text\n                    acID=ac.find(<span class=\"hljs-string\">'a'</span>,  attrs={<span class=\"hljs-string\">'class'</span>:<span class=\"hljs-string\">'qa-user-link'</span>} ).get(<span class=\"hljs-string\">'href'</span>)\n                    self.addUserAndText(acID,acText)\n                <span class=\"hljs-keyword\">except</span>:\n                    <span class=\"hljs-keyword\">pass</span>     \n    \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">clean_text</span>(<span class=\"hljs-params\">self, text</span>):</span>\n        <span class=\"hljs-string\">'''\n        Utility function to clean tweet text by removing links, special characters\n        using simple regex statements.\n        '''</span>\n        cleanText=<span class=\"hljs-string\">' '</span>.join(re.sub(<span class=\"hljs-string\">\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\"</span>, <span class=\"hljs-string\">\" \"</span>, text).split())\n        <span class=\"hljs-keyword\">return</span> cleanText\n        \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_text_sentiment</span>(<span class=\"hljs-params\">self, text</span>):</span>\n        <span class=\"hljs-string\">'''\n        Utility function to classify sentiment of passed tweet\n        using textblob's sentiment method\n        '''</span>\n        text=self.clean_text(text)\n        <span class=\"hljs-comment\"># create TextBlob object of passed tweet text</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_0\" class=\"highlights fea_sentiment_analysis\">analysis = TextBlob(text)</div>\n        <span class=\"hljs-comment\"># set sentiment</span>\n        <span class=\"hljs-comment\">#print analysis.sentiment.polarity</span>\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_1\" class=\"highlights fea_sentiment_analysis\"><span class=\"hljs-keyword\">return</span> analysis.sentiment.polarity</div>\n        <span class=\"hljs-comment\">#if analysis.sentiment.polarity &gt; 0:</span>\n        <span class=\"hljs-comment\">#    return 1</span>\n        <span class=\"hljs-comment\">#elif analysis.sentiment.polarity == 0:</span>\n        <span class=\"hljs-comment\">#    return 0</span>\n        <span class=\"hljs-comment\">#else:</span>\n        <span class=\"hljs-comment\">#    return -1</span>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">addUserAndText</span>(<span class=\"hljs-params\">self, uID,uText</span>):</span>\n        userID=uID.split(<span class=\"hljs-string\">'/user/'</span>)[<span class=\"hljs-number\">1</span>]\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> userID <span class=\"hljs-keyword\">in</span> self.Users:\n            self.addUser(userID)\n        textVal=self.get_text_sentiment(uText)\n        self.Users[userID][<span class=\"hljs-string\">'textN'</span>]+=<span class=\"hljs-number\">1</span> <span class=\"hljs-comment\">#another measure of their text</span>\n        self.Users[userID][<span class=\"hljs-string\">'textSum'</span>]+=textVal\n        \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">addUser</span>(<span class=\"hljs-params\">self, userID</span>):</span>\n        userPath=self.path+<span class=\"hljs-string\">'user/'</span>+userID\n        userpage = urllib2.urlopen(userPath)\n        userSoup=BeautifulSoup(userpage, <span class=\"hljs-string\">'html.parser'</span>)\n        userDets={}        \n\n        userDets[<span class=\"hljs-string\">'nPosts'</span>] = userSoup.find(<span class=\"hljs-string\">'span'</span>, attrs={<span class=\"hljs-string\">'class'</span> : <span class=\"hljs-string\">\"qa-uf-user-q-posts\"</span>}).text\n        userDets[<span class=\"hljs-string\">'nAnswers'</span>]=userSoup.find(<span class=\"hljs-string\">'span'</span>, attrs={<span class=\"hljs-string\">'class'</span> : <span class=\"hljs-string\">\"qa-uf-user-a-posts\"</span>}).text\n        userDets[<span class=\"hljs-string\">'nComments'</span>] = userSoup.find(<span class=\"hljs-string\">'span'</span>, attrs={<span class=\"hljs-string\">'class'</span> : <span class=\"hljs-string\">\"qa-uf-user-c-posts\"</span>}).text\n        userDets[<span class=\"hljs-string\">'nVotes'</span>] = userSoup.find(<span class=\"hljs-string\">'span'</span>, attrs={<span class=\"hljs-string\">'class'</span> : <span class=\"hljs-string\">\"qa-uf-user-q-votes\"</span>}).text\n        userDets[<span class=\"hljs-string\">'giveUp'</span>] = userSoup.find(<span class=\"hljs-string\">'span'</span>, attrs={<span class=\"hljs-string\">'class'</span> : <span class=\"hljs-string\">\"qa-uf-user-upvotes\"</span>}).text <span class=\"hljs-comment\">#gave</span>\n        userDets[<span class=\"hljs-string\">'giveDown'</span>] = userSoup.find(<span class=\"hljs-string\">'span'</span>, attrs={<span class=\"hljs-string\">'class'</span> : <span class=\"hljs-string\">\"qa-uf-user-downvotes\"</span>}).text\n        userDets[<span class=\"hljs-string\">'getUp'</span>] = userSoup.find(<span class=\"hljs-string\">'span'</span>, attrs={<span class=\"hljs-string\">'class'</span> : <span class=\"hljs-string\">\"qa-uf-user-upvoteds\"</span>}).text <span class=\"hljs-comment\">#received</span>\n        userDets[<span class=\"hljs-string\">'getDown'</span>] = userSoup.find(<span class=\"hljs-string\">'span'</span>, attrs={<span class=\"hljs-string\">'class'</span> : <span class=\"hljs-string\">\"qa-uf-user-downvoteds\"</span>}).text\n\n        userURL=<span class=\"hljs-string\">'https://www.wikitree.com/wiki/'</span>+userID\n        userpage= urllib2.urlopen(userURL)\n        userSoup=BeautifulSoup(userpage, <span class=\"hljs-string\">'html.parser'</span>)\n\n        tick=<span class=\"hljs-number\">0</span>\n        <span class=\"hljs-keyword\">for</span> small <span class=\"hljs-keyword\">in</span> userSoup.findAll(<span class=\"hljs-string\">'div'</span>, attrs={<span class=\"hljs-string\">'class'</span> : <span class=\"hljs-string\">\"SMALL\"</span>}):\n            <span class=\"hljs-keyword\">if</span> tick==<span class=\"hljs-number\">1</span>:\n                <span class=\"hljs-comment\">#print userID</span>\n                details = small.text.split(<span class=\"hljs-string\">'|'</span>)\n                <span class=\"hljs-comment\">#print details</span>\n                years=details[<span class=\"hljs-number\">0</span>].strip()[-<span class=\"hljs-number\">5</span>:]\n                userDets[<span class=\"hljs-string\">'years'</span>]=<span class=\"hljs-number\">2018</span>-<span class=\"hljs-built_in\">int</span>(years)\n                contribs=details[<span class=\"hljs-number\">1</span>].split(<span class=\"hljs-string\">' '</span>)[<span class=\"hljs-number\">2</span>]\n                userDets[<span class=\"hljs-string\">'contribs'</span>]=<span class=\"hljs-built_in\">int</span>(contribs)\n            tick+=<span class=\"hljs-number\">1</span>\n        userDets[<span class=\"hljs-string\">'textN'</span>]=<span class=\"hljs-number\">0</span>\n        userDets[<span class=\"hljs-string\">'textSum'</span>]=<span class=\"hljs-number\">0</span>\n        self.Users[userID]=userDets\n\n    \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">queryPageParser</span>(<span class=\"hljs-params\">self, page</span>):</span>\n        <span class=\"hljs-keyword\">pass</span>\n    \naaa=G2GAnalysis(<span class=\"hljs-string\">\"https://www.wikitree.com/g2g/questions\"</span>)\naaa.runScrape(<span class=\"hljs-number\">6000</span>)</code></pre></div>",
    "sec_43": "<div class=\"codeBlock hljs python\" id=\"sec_43\"><pre id=\"sec_43_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\">#Import the necessary methods from tweepy library</span>\n<span class=\"hljs-keyword\">from</span> tweepy.streaming <span class=\"hljs-keyword\">import</span> StreamListener\n<span class=\"hljs-keyword\">from</span> tweepy <span class=\"hljs-keyword\">import</span> OAuthHandler\n<span class=\"hljs-keyword\">from</span> tweepy <span class=\"hljs-keyword\">import</span> Stream\n<span class=\"hljs-keyword\">from</span> textblob <span class=\"hljs-keyword\">import</span> TextBlob\n<span class=\"hljs-keyword\">import</span> re \n<span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">import</span> datetime\n<span class=\"hljs-keyword\">import</span> traceback\n<span class=\"hljs-keyword\">import</span> sys \n\n<span class=\"hljs-keyword\">import</span> argparse\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(<span class=\"hljs-string\">\"outputfile\"</span>, nargs=<span class=\"hljs-string\">'?'</span>, default=<span class=\"hljs-string\">\"tweets_bitcoin.json\"</span>)\nparser.add_argument(<span class=\"hljs-string\">\"errorfile\"</span>, nargs=<span class=\"hljs-string\">'?'</span>, default=<span class=\"hljs-string\">\"tweets_bitcoin_error.txt\"</span>)\n\n\nargs = parser.parse_args()\n\n<span class=\"hljs-comment\">#Variables that contains the user credentials to access Twitter API </span>\naccess_token = <span class=\"hljs-string\">\"2569501620-dOSDk5kftHZiOTlWDzu66k9S7xKEh2GrpjNn1SA\"</span>\naccess_token_secret = <span class=\"hljs-string\">\"makKH9EdIErBW8JhQdHdc594qWyH09VoQkZm4FFCAHCQk\"</span>\nconsumer_key = <span class=\"hljs-string\">\"VZ4evnFfxQ0f1vyRpyoJrf5VT\"</span>\nconsumer_secret = <span class=\"hljs-string\">\"dcSDKuSz0ODD8kqPgBdBX6u6l0JoGq8NDbGQ2Vvg5bukDo3weh\"</span>\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Tweet</span>:</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, text, sentiment_text, polarity, created_at</span>):</span>\n        self.text = text\n        self.sentiment_text = sentiment_text\n        self.polarity = polarity\n        self.created_at = created_at\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__str__</span>(<span class=\"hljs-params\">self</span>):</span>\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">'Sentiment: {} {} \\nText: {}\\nCreated_At: {}\\n'</span>.<span class=\"hljs-built_in\">format</span>(self.sentiment_text, self.polarity, self.text, self.created_at)\n\n<span class=\"hljs-comment\">#This is a basic listener that just prints received tweets to stdout.</span>\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">StdOutListener</span>(<span class=\"hljs-params\">StreamListener</span>):</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span>\n        self.mylist = []\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">on_data</span>(<span class=\"hljs-params\">self, data</span>):</span>\n                \n        tweet = json.loads(data)\n        \n        <span class=\"hljs-keyword\">try</span>:\n            <span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">'retweeted_status'</span> <span class=\"hljs-keyword\">in</span> tweet:\n            <span class=\"hljs-comment\"># Retweets </span>\n                <span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">'extended_tweet'</span> <span class=\"hljs-keyword\">in</span> tweet[<span class=\"hljs-string\">'retweeted_status'</span>]:\n                <span class=\"hljs-comment\">#When extended beyond 140 Characters limit</span>\n                    tweet_text = tweet[<span class=\"hljs-string\">'retweeted_status'</span>][<span class=\"hljs-string\">'extended_tweet'</span>][<span class=\"hljs-string\">'full_text'</span>]\n                <span class=\"hljs-keyword\">else</span>:\n                    tweet_text = tweet[<span class=\"hljs-string\">'retweeted_status'</span>][<span class=\"hljs-string\">'text'</span>]\n            <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-comment\">#Normal Tweets</span>\n                <span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">'extended_tweet'</span> <span class=\"hljs-keyword\">in</span> tweet:\n                <span class=\"hljs-comment\">#When extended beyond 140 Characters limit            </span>\n                    tweet_text = tweet[<span class=\"hljs-string\">'extended_tweet'</span>][<span class=\"hljs-string\">'full_text'</span>]\n                <span class=\"hljs-keyword\">else</span>:\n                    tweet_text = tweet[<span class=\"hljs-string\">'text'</span>]\n            \n            tweet_sentiment, polarity = self.get_tweet_sentiment(tweet_text)        \n            \n            newTweetObj = Tweet(text=tweet_text, \n                                sentiment_text=tweet_sentiment, \n                                polarity=polarity, \n                                created_at= datetime.datetime.strptime(tweet[<span class=\"hljs-string\">'created_at'</span>],<span class=\"hljs-string\">'%a %b %d %H:%M:%S %z %Y'</span>).strftime(<span class=\"hljs-string\">'%Y-%m-%d-%H-%M-%S'</span>))\n            \n            <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(args.outputfile, mode=<span class=\"hljs-string\">'a'</span>) <span class=\"hljs-keyword\">as</span> file:\n                file.write(<span class=\"hljs-string\">'{},\\n'</span>.<span class=\"hljs-built_in\">format</span>(json.dumps(newTweetObj.__dict__)))     \n\n        <span class=\"hljs-keyword\">except</span>:\n            exc_type, exc_value, exc_traceback = sys.exc_info()            \n            <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(args.errorfile, mode=<span class=\"hljs-string\">'a'</span>) <span class=\"hljs-keyword\">as</span> file:\n                file.write(<span class=\"hljs-string\">'Error: {}\\n\\nTweet Info: {}\\n--------------------------\\n'</span>.<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-built_in\">repr</span>(traceback.format_tb(exc_traceback)), tweet))            \n\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">True</span>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">on_error</span>(<span class=\"hljs-params\">self, status</span>):</span>\n        print(status)\n        print(<span class=\"hljs-string\">'-----------------'</span>)\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">clean_tweet</span>(<span class=\"hljs-params\">self, tweet</span>):</span>\n        <span class=\"hljs-string\">'''\n        Utility function to clean tweet text by removing links, special characters\n        using simple regex statements.\n        '''</span>\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">' '</span>.join(re.sub(<span class=\"hljs-string\">\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\"</span>, <span class=\"hljs-string\">\" \"</span>, tweet).split())\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_tweet_sentiment</span>(<span class=\"hljs-params\">self, tweet</span>):</span>\n        <span class=\"hljs-string\">'''\n        Utility function to classify sentiment of passed tweet\n        using textblob's sentiment method\n        '''</span>\n        <span class=\"hljs-comment\"># create TextBlob object of passed tweet text</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_0\" class=\"highlights fea_sentiment_analysis\">analysis = TextBlob(self.clean_tweet(tweet))\n\n        <span class=\"hljs-comment\"># set sentiment</span>\n        sentiment = <span class=\"hljs-literal\">None</span> \n        <span class=\"hljs-keyword\">if</span> analysis.sentiment.polarity &gt; <span class=\"hljs-number\">0</span>:\n            sentiment = <span class=\"hljs-string\">'positive'</span>\n        <span class=\"hljs-keyword\">elif</span> analysis.sentiment.polarity == <span class=\"hljs-number\">0</span>:\n            sentiment = <span class=\"hljs-string\">'neutral'</span>\n        <span class=\"hljs-keyword\">else</span>:\n            sentiment = <span class=\"hljs-string\">'negative'</span>\n\n        <span class=\"hljs-keyword\">return</span> sentiment, analysis.sentiment.polarity</div>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">convert_sentiment_to_emoticon</span>(<span class=\"hljs-params\">self, sentiment_text</span>):</span>\n        <span class=\"hljs-keyword\">if</span> sentiment_text == <span class=\"hljs-string\">'positive'</span>:\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">'✅'</span>\n        <span class=\"hljs-keyword\">elif</span> sentiment_text == <span class=\"hljs-string\">'negative'</span>:\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">'❌'</span>\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">'🤷'</span>\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n\n    <span class=\"hljs-comment\">#This handles Twitter authetification and the connection to Twitter Streaming API</span>\n    l = StdOutListener()\n    auth = OAuthHandler(consumer_key, consumer_secret)\n    auth.set_access_token(access_token, access_token_secret)\n    stream = Stream(auth, l, tweet_mode=<span class=\"hljs-string\">'extended'</span>)\n\n    <span class=\"hljs-comment\">#This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'</span>\n    stream.<span class=\"hljs-built_in\">filter</span>(track=[<span class=\"hljs-string\">'bitcoin'</span>, <span class=\"hljs-string\">'BTC'</span>])</code></pre></div>",
    "sec_44": "<div class=\"codeBlock hljs python\" id=\"sec_44\"><pre id=\"sec_44_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">from</span> textblob.classifiers <span class=\"hljs-keyword\">import</span> NaiveBayesClassifier\n<span class=\"hljs-keyword\">from</span> textblob <span class=\"hljs-keyword\">import</span> TextBlob\n<span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">import</span> os\ntrain = [\n    (<span class=\"hljs-string\">'I love this sandwich.'</span>, <span class=\"hljs-string\">'5'</span>),\n    (<span class=\"hljs-string\">'This is an amazing place!'</span>, <span class=\"hljs-string\">'5'</span>),\n    (<span class=\"hljs-string\">'I feel very good about these beers.'</span>, <span class=\"hljs-string\">'5'</span>),\n    (<span class=\"hljs-string\">'This is my best work.'</span>, <span class=\"hljs-string\">'3'</span>),\n    (<span class=\"hljs-string\">\"What an awesome view\"</span>, <span class=\"hljs-string\">'2'</span>),\n    (<span class=\"hljs-string\">'I do not like this restaurant'</span>, <span class=\"hljs-string\">'1'</span>),\n    (<span class=\"hljs-string\">'I am tired of this stuff.'</span>, <span class=\"hljs-string\">'3'</span>),\n    (<span class=\"hljs-string\">\"I can't deal with this\"</span>, <span class=\"hljs-string\">'2'</span>),\n    (<span class=\"hljs-string\">'He is my sworn enemy!'</span>, <span class=\"hljs-string\">'1'</span>),\n    (<span class=\"hljs-string\">'My boss is horrible.'</span>, <span class=\"hljs-string\">'4'</span>)\n]\ntest = [\n    (<span class=\"hljs-string\">'The beer was good.'</span>, <span class=\"hljs-string\">'4'</span>),\n    (<span class=\"hljs-string\">'I do not enjoy my job'</span>, <span class=\"hljs-string\">'2'</span>),\n    (<span class=\"hljs-string\">\"I ain't feeling dandy today.\"</span>, <span class=\"hljs-string\">'2'</span>),\n    (<span class=\"hljs-string\">\"I feel amazing!\"</span>, <span class=\"hljs-string\">'2'</span>),\n    (<span class=\"hljs-string\">'Gary is a friend of mine.'</span>, <span class=\"hljs-string\">'5'</span>),\n    (<span class=\"hljs-string\">\"I can't believe I'm doing this.\"</span>, <span class=\"hljs-string\">'3'</span>)\n]\n\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"classification_0\" class=\"highlights fea_classification\">cl = NaiveBayesClassifier(train)</div>\n\n<span class=\"hljs-comment\"># Classify some text</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"classification_1\" class=\"highlights fea_classification\">print(cl.classify(<span class=\"hljs-string\">\"Their burgers are amazing.\"</span>))  <span class=\"hljs-comment\"># \"pos\"</span>\nprint(cl.classify(<span class=\"hljs-string\">\"I don't like their pizza.\"</span>))   <span class=\"hljs-comment\"># \"neg\"</span></div><span class=\"hljs-comment\"></span>\n\n<span class=\"hljs-comment\"># Classify a TextBlob</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"classification_2\" class=\"highlights fea_classification\">blob = TextBlob(<span class=\"hljs-string\">\"The beer was amazing. But the hangover was horrible. \"</span>\n                <span class=\"hljs-string\">\"My boss was not pleased.\"</span>, classifier=cl)\nprint(blob)\nprint(blob.classify())\n\n<span class=\"hljs-keyword\">for</span> sentence <span class=\"hljs-keyword\">in</span> blob.sentences:\n    print(sentence)\n    print(sentence.classify())</div>\n\n<span class=\"hljs-comment\"># Compute accuracy</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"classification_3\" class=\"highlights fea_classification\">print(<span class=\"hljs-string\">\"Accuracy: {0}\"</span>.<span class=\"hljs-built_in\">format</span>(cl.accuracy(test)))</div>\n\n<span class=\"hljs-comment\"># Show 5 most informative features</span>\ncl.show_informative_features(<span class=\"hljs-number\">5</span>)</code></pre></div>",
    "sec_45": "<div class=\"codeBlock hljs python\" id=\"sec_45\"><pre id=\"sec_45_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">from</span> __future__ <span class=\"hljs-keyword\">import</span> unicode_literals\n<span class=\"hljs-keyword\">import</span> unittest\n<span class=\"hljs-keyword\">from</span> nose.tools <span class=\"hljs-keyword\">import</span> *  <span class=\"hljs-comment\"># PEP8 asserts</span>\n<span class=\"hljs-keyword\">from</span> nose.plugins.attrib <span class=\"hljs-keyword\">import</span> attr\n\n<span class=\"hljs-keyword\">from</span> textblob.packages <span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">from</span> textblob.base <span class=\"hljs-keyword\">import</span> BaseNPExtractor\n<span class=\"hljs-keyword\">from</span> textblob.np_extractors <span class=\"hljs-keyword\">import</span> ConllExtractor\n<span class=\"hljs-keyword\">from</span> textblob.utils <span class=\"hljs-keyword\">import</span> filter_insignificant\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">TestConllExtractor</span>(<span class=\"hljs-params\">unittest.TestCase</span>):</span>\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">setUp</span>(<span class=\"hljs-params\">self</span>):</span>\n        self.extractor = ConllExtractor()\n        self.text = <span class=\"hljs-string\">'''\nPython is a widely used general-purpose,\nhigh-level programming language. Its design philosophy emphasizes code\nreadability, and its syntax allows programmers to express concepts in fewer lines\nof code than would be possible in other languages. The language provides\nconstructs intended to enable clear programs on both a small and large scale.\n'''</span>\n        self.sentence = <span class=\"hljs-string\">\"Python is a widely used general-purpose, high-level programming language\"</span>\n\n<span class=\"hljs-meta\">    @attr(<span class=\"hljs-params\"><span class=\"hljs-string\">'slow'</span></span>)</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_extract</span>(<span class=\"hljs-params\">self</span>):</span>\n        noun_phrases = self.extractor.extract(self.text)\n        assert_true(<span class=\"hljs-string\">\"Python\"</span> <span class=\"hljs-keyword\">in</span> noun_phrases)\n        assert_true(<span class=\"hljs-string\">\"design philosophy\"</span> <span class=\"hljs-keyword\">in</span> noun_phrases)\n        assert_true(<span class=\"hljs-string\">\"code readability\"</span> <span class=\"hljs-keyword\">in</span> noun_phrases)\n\n<span class=\"hljs-meta\">    @attr(<span class=\"hljs-params\"><span class=\"hljs-string\">'slow'</span></span>)</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_parse_sentence</span>(<span class=\"hljs-params\">self</span>):</span>\n        parsed = self.extractor._parse_sentence(self.sentence)\n        assert_true(<span class=\"hljs-built_in\">isinstance</span>(parsed, nltk.tree.Tree))\n\n<span class=\"hljs-meta\">    @attr(<span class=\"hljs-params\"><span class=\"hljs-string\">'slow'</span></span>)</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_filter_insignificant</span>(<span class=\"hljs-params\">self</span>):</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"chunking_0\" class=\"highlights fea_chunking\">chunk = self.extractor._parse_sentence(self.sentence)\n        tags = [tag <span class=\"hljs-keyword\">for</span> word, tag <span class=\"hljs-keyword\">in</span> chunk.leaves()]\n        assert_true(<span class=\"hljs-string\">'DT'</span> <span class=\"hljs-keyword\">in</span> tags)\n        filtered = filter_insignificant(chunk.leaves())</div>\n        tags = [tag <span class=\"hljs-keyword\">for</span> word, tag <span class=\"hljs-keyword\">in</span> filtered]\n        assert_true(<span class=\"hljs-string\">\"DT\"</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> tags)\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">BadExtractor</span>(<span class=\"hljs-params\">BaseNPExtractor</span>):</span>\n    <span class=\"hljs-string\">'''An extractor without an extract method. How useless.'''</span>\n    <span class=\"hljs-keyword\">pass</span>\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">test_cannot_instantiate_incomplete_extractor</span>():</span>\n    assert_raises(TypeError,\n        <span class=\"hljs-keyword\">lambda</span>: BadExtractor())\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n    unittest.main()</code></pre></div>",
    "sec_46": "<div class=\"codeBlock hljs python\" id=\"sec_46\"><pre id=\"sec_46_code\" style=\"display: block;\"><code class=\"python\"># -*- coding: utf-<span class=\"hljs-number\">8</span> -*-\nfrom __future__ import unicode_literals\nimport unittest\n\nfrom nose.plugins.attrib import attr\nfrom nose.tools import *  # PEP8 asserts\nimport mock\n\nfrom textblob.translate import Translator\nfrom textblob.compat import unicode\n\nclass TestTranslator(unittest.TestCase):\n\n    def setUp(<span class=\"hljs-keyword\">self</span>):\n        <span class=\"hljs-keyword\">self</span>.translator = Translator()\n        <span class=\"hljs-keyword\">self</span>.sentence = <span class=\"hljs-string\">\"This is a sentence.\"</span>\n\n    @mock.patch(<span class=\"hljs-symbol\">'textblob</span>.translate.Translator._get_json5')\n    def test_translate(<span class=\"hljs-keyword\">self</span>, mock_get_json5):\n        mock_get_json5.return_value = unicode('[[[<span class=\"hljs-string\">\"Esta es una frase\"</span>,<span class=\"hljs-string\">\"This is a '\n            'sentence\"</span>,<span class=\"hljs-string\">\"\"</span>,<span class=\"hljs-string\">\"\"</span>]],,<span class=\"hljs-string\">\"en\"</span>,,[[<span class=\"hljs-string\">\"Esta es una\"</span>,[<span class=\"hljs-number\">1</span>],<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>,<span class=\"hljs-number\">374</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">3</span>,<span class=\"hljs-number\">0</span>]'\n            ',[<span class=\"hljs-string\">\"frase\"</span>,[<span class=\"hljs-number\">2</span>],<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>,<span class=\"hljs-number\">470</span>,<span class=\"hljs-number\">3</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">0</span>]],[[<span class=\"hljs-string\">\"This is a\"</span>,<span class=\"hljs-number\">1</span>,[[<span class=\"hljs-string\">\"Esta es'\n            ' una\"</span>,<span class=\"hljs-number\">374</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>],[<span class=\"hljs-string\">\"Se trata de una\"</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>],'\n            '[<span class=\"hljs-string\">\"Este es un\"</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>],[<span class=\"hljs-string\">\"Se trata de un\"</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>],'\n            '[<span class=\"hljs-string\">\"Esto es un\"</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>]],[[<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">9</span>]],<span class=\"hljs-string\">\"This is a sentence\"</span>],'\n            '[<span class=\"hljs-string\">\"sentence\"</span>,<span class=\"hljs-number\">2</span>,[[<span class=\"hljs-string\">\"frase\"</span>,<span class=\"hljs-number\">470</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>],[<span class=\"hljs-string\">\"sentencia\"</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>],'\n            '[<span class=\"hljs-string\">\"oraci\\xf3n\"</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>],[<span class=\"hljs-string\">\"pena\"</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>],[<span class=\"hljs-string\">\"condena\"</span>'\n            ',<span class=\"hljs-number\">0</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>]],[[<span class=\"hljs-number\">10</span>,<span class=\"hljs-number\">18</span>]],<span class=\"hljs-string\">\"\"</span>]],,,[[<span class=\"hljs-string\">\"en\"</span>]],<span class=\"hljs-number\">29</span>]')\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"translation_0\" class=\"highlights fea_translation\">t = <span class=\"hljs-keyword\">self</span>.translator.translate(<span class=\"hljs-keyword\">self</span>.sentence, to_lang=<span class=\"hljs-string\">\"es\"</span>)</div>\n        assert_equal(t, <span class=\"hljs-string\">\"Esta es una frase\"</span>)\n        assert_true(mock_get_json5.called_once)\n\n    @mock.patch(<span class=\"hljs-symbol\">'textblob</span>.translate.Translator._get_json5')\n    def test_detect(<span class=\"hljs-keyword\">self</span>, mock_get_json5):\n        mock_get_json5.return_value = unicode('[[[<span class=\"hljs-string\">\"This is a sentence\"</span>,'\n            '<span class=\"hljs-string\">\"This is a sentence\"</span>,<span class=\"hljs-string\">\"\"</span>,<span class=\"hljs-string\">\"\"</span>]],,<span class=\"hljs-string\">\"en\"</span>,,,,,,[[<span class=\"hljs-string\">\"en\"</span>]],<span class=\"hljs-number\">4</span>]')\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"language_detection_0\" class=\"highlights fea_language_detection\">lang = <span class=\"hljs-keyword\">self</span>.translator.detect(<span class=\"hljs-keyword\">self</span>.sentence)</div>\n        assert_equal(lang, <span class=\"hljs-string\">\"en\"</span>)\n        mock_get_json5.return_value = unicode('[[[<span class=\"hljs-string\">\"Hello\"</span>,<span class=\"hljs-string\">\"Hola\"</span>,<span class=\"hljs-string\">\"\"</span>,<span class=\"hljs-string\">\"\"</span>]],[[<span class=\"hljs-string\">\"interjection\"</span>,'\n                                        '[<span class=\"hljs-string\">\"Hello!\"</span>,<span class=\"hljs-string\">\"Hi!\"</span>,<span class=\"hljs-string\">\"Hey!\"</span>,<span class=\"hljs-string\">\"Hullo!\"</span>,<span class=\"hljs-string\">\"Hallo!\"</span>,'\n                                        '<span class=\"hljs-string\">\"Hoy!\"</span>,<span class=\"hljs-string\">\"Hail!\"</span>],[[<span class=\"hljs-string\">\"Hello!\"</span>,[<span class=\"hljs-string\">\"\\xa1Hola!\"</span>,<span class=\"hljs-string\">\"'\n                                        '\\xa1Caramba!\"</span>,<span class=\"hljs-string\">\"\\xa1Oiga!\"</span>,<span class=\"hljs-string\">\"\\xa1Diga!\"</span>,<span class=\"hljs-string\">\"'\n                                        '\\xa1Bueno!\"</span>,<span class=\"hljs-string\">\"\\xa1Vale!\"</span>],,<span class=\"hljs-number\">0.39160562</span>],'\n                                        '[<span class=\"hljs-string\">\"Hi!\"</span>,[<span class=\"hljs-string\">\"\\xa1Hola!\"</span>],,<span class=\"hljs-number\">0.24506053</span>],'\n                                        '[<span class=\"hljs-string\">\"Hey!\"</span>,[<span class=\"hljs-string\">\"\\xa1Hola!\"</span>,<span class=\"hljs-string\">\"\\xa1Eh!\"</span>],,<span class=\"hljs-number\">0.038173068</span>]'\n                                        ',[<span class=\"hljs-string\">\"Hullo!\"</span>,[<span class=\"hljs-string\">\"\\xa1Hola!\"</span>,<span class=\"hljs-string\">\"\\xa1Caramba!\"</span>,'\n                                        '<span class=\"hljs-string\">\"\\xa1Oiga!\"</span>,<span class=\"hljs-string\">\"\\xa1Diga!\"</span>,<span class=\"hljs-string\">\"\\xa1Bueno!\"</span>,'\n                                        '<span class=\"hljs-string\">\"\\xa1Al\\xf3!\"</span>]],[<span class=\"hljs-string\">\"Hallo!\"</span>,[<span class=\"hljs-string\">\"\\xa1Hola!\"</span>,'\n                                        '<span class=\"hljs-string\">\"\\xa1Caramba!\"</span>,<span class=\"hljs-string\">\"\\xa1Oiga!\"</span>,<span class=\"hljs-string\">\"\\xa1Bueno!\"</span>]],'\n                                        '[<span class=\"hljs-string\">\"Hoy!\"</span>,[<span class=\"hljs-string\">\"\\xa1Eh!\"</span>,<span class=\"hljs-string\">\"\\xa1Hola!\"</span>]],[<span class=\"hljs-string\">\"Hail!\"</span>,'\n                                        '[<span class=\"hljs-string\">\"\\xa1Salve!\"</span>,<span class=\"hljs-string\">\"\\xa1Hola!\"</span>]]],<span class=\"hljs-string\">\"\\xa1Hola!\"</span>,<span class=\"hljs-number\">9</span>]],'\n                                        '<span class=\"hljs-string\">\"es\"</span>,,[[<span class=\"hljs-string\">\"Hello\"</span>,[<span class=\"hljs-number\">1</span>],<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>,<span class=\"hljs-number\">783</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">0</span>]],'\n                                        '[[<span class=\"hljs-string\">\"Hola\"</span>,<span class=\"hljs-number\">1</span>,[[<span class=\"hljs-string\">\"Hello\"</span>,<span class=\"hljs-number\">783</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>],'\n                                        '[<span class=\"hljs-string\">\"Hi\"</span>,<span class=\"hljs-number\">214</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>],[<span class=\"hljs-string\">\"Hola\"</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>],'\n                                        '[<span class=\"hljs-string\">\"Hey\"</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>],[<span class=\"hljs-string\">\"Welcome\"</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-literal\">true</span>,<span class=\"hljs-literal\">false</span>]],'\n                                        '[[<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">4</span>]],<span class=\"hljs-string\">\"Hola\"</span>]],,,[],<span class=\"hljs-number\">4</span>]')\n        lang2 = <span class=\"hljs-keyword\">self</span>.translator.detect(<span class=\"hljs-string\">\"Hola\"</span>)\n        assert_equal(lang2, <span class=\"hljs-string\">\"es\"</span>)\n\n    @attr(<span class=\"hljs-symbol\">'requires_internet</span>')\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"language_detection_1\" class=\"highlights fea_language_detection\">def test_detect_non_ascii(<span class=\"hljs-keyword\">self</span>):\n        lang = <span class=\"hljs-keyword\">self</span>.translator.detect(unicode(<span class=\"hljs-string\">\"关于中文维基百科\"</span>))\n        assert_equal(lang, <span class=\"hljs-symbol\">'zh</span>-CN')\n        lang2 = <span class=\"hljs-keyword\">self</span>.translator.detect(unicode(<span class=\"hljs-string\">\"известен още с псевдонимите\"</span>))\n        assert_equal(lang2, <span class=\"hljs-string\">\"bg\"</span>)\n        lang3 = <span class=\"hljs-keyword\">self</span>.translator.detect(unicode(<span class=\"hljs-string\">\"Избранная статья\"</span>))\n        assert_equal(lang3, <span class=\"hljs-string\">\"ru\"</span>)</div>\n\n    def test_get_language_from_json5(<span class=\"hljs-keyword\">self</span>):\n        json5 = '[[[<span class=\"hljs-string\">\"This is a sentence.\"</span>,<span class=\"hljs-string\">\"This is a sentence.\"</span>,<span class=\"hljs-string\">\"\"</span>,<span class=\"hljs-string\">\"\"</span>]],,<span class=\"hljs-string\">\"en\"</span>,,,,,,[[<span class=\"hljs-string\">\"en\"</span>]],<span class=\"hljs-number\">0</span>]'\n        lang = <span class=\"hljs-keyword\">self</span>.translator._get_language_from_json5(json5)\n        assert_equal(lang, <span class=\"hljs-string\">\"en\"</span>)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-symbol\">'__main__</span>':\n    unittest.main()</code></pre></div>",
    "sec_47": "<div class=\"codeBlock hljs python\" id=\"sec_47\"><pre id=\"sec_47_code\" style=\"display: block;\"><code class=\"python\">__author__ = <span class=\"hljs-string\">'813365079'</span>\n<span class=\"hljs-keyword\">import</span> twitter\n<span class=\"hljs-keyword\">from</span> twitterkeys <span class=\"hljs-keyword\">import</span> *\n<span class=\"hljs-keyword\">from</span> collections <span class=\"hljs-keyword\">import</span> Counter\n<span class=\"hljs-keyword\">from</span> prettytable <span class=\"hljs-keyword\">import</span> PrettyTable\n<span class=\"hljs-keyword\">from</span> textblob <span class=\"hljs-keyword\">import</span> TextBlob\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Analyzer</span>():</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, feeder</span>):</span>\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-built_in\">isinstance</span>(feeder, Feeder):\n            <span class=\"hljs-keyword\">raise</span> Exception(<span class=\"hljs-string\">'Feeder instance not provided'</span>)\n        self.feeder = feeder\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">top_screen_names</span>(<span class=\"hljs-params\">self, topn=<span class=\"hljs-number\">10</span></span>):</span>\n        <span class=\"hljs-keyword\">return</span> Counter(self.feeder.screen_names).most_common()[:topn]\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">top_words</span>(<span class=\"hljs-params\">self, topn=<span class=\"hljs-number\">10</span></span>):</span>\n        <span class=\"hljs-keyword\">return</span> Counter(self.feeder.words).most_common()[:topn]\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">top_hashtags</span>(<span class=\"hljs-params\">self, topn=<span class=\"hljs-number\">10</span></span>):</span>\n        <span class=\"hljs-keyword\">return</span> Counter(self.feeder.hashtags).most_common()[:topn]\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">important_words</span>(<span class=\"hljs-params\">self</span>):</span>\n        <span class=\"hljs-string\">\"\"\"\n        Only returns words that are adjectives, adverbs, verbs (all tenses),\n        nouns, proper nouns, interjections\n        \"\"\"</span>\n        bad_tags = [<span class=\"hljs-string\">'IN'</span>, <span class=\"hljs-string\">'DT'</span>, <span class=\"hljs-string\">'CC'</span>, <span class=\"hljs-string\">'TO'</span>, ]\n        <span class=\"hljs-comment\"># bad_tags = []</span>\n        words = [word <span class=\"hljs-keyword\">for</span> status <span class=\"hljs-keyword\">in</span> self.feeder.status_text\n                 <span class=\"hljs-keyword\">for</span> word, tag <span class=\"hljs-keyword\">in</span> TextBlob(status).tags\n                 <span class=\"hljs-keyword\">if</span> tag <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> bad_tags <span class=\"hljs-keyword\">and</span> <span class=\"hljs-built_in\">len</span>(word) &gt;= <span class=\"hljs-number\">3</span>]\n        <span class=\"hljs-keyword\">return</span> Counter(words).most_common()[:<span class=\"hljs-number\">50</span>]\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">sentiment</span>(<span class=\"hljs-params\">self</span>):</span>\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_0\" class=\"highlights fea_sentiment_analysis\"><span class=\"hljs-keyword\">return</span> [TextBlob(status).sentiment[<span class=\"hljs-number\">0</span>] <span class=\"hljs-keyword\">for</span> status <span class=\"hljs-keyword\">in</span> self.feeder.status_text ]</div>\n\n\n\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">make_table</span>(<span class=\"hljs-params\">self, topn=<span class=\"hljs-number\">10</span></span>):</span>\n        <span class=\"hljs-keyword\">for</span> label, data <span class=\"hljs-keyword\">in</span> ((<span class=\"hljs-string\">'Word'</span>, self.feeder.words),\n                            (<span class=\"hljs-string\">'Screen Name'</span>, self.feeder.screen_names),\n                            (<span class=\"hljs-string\">'Hashtag'</span>, self.feeder.hashtags)):\n            pt = PrettyTable(field_names=[label, <span class=\"hljs-string\">'Count'</span>])\n            c = Counter(data)\n            [pt.add_row(kv) <span class=\"hljs-keyword\">for</span> kv <span class=\"hljs-keyword\">in</span> c.most_common()[:topn]]\n            pt.align[label], pt.align[<span class=\"hljs-string\">'Count'</span>] = <span class=\"hljs-string\">'l'</span>, <span class=\"hljs-string\">'r'</span>\n            <span class=\"hljs-built_in\">print</span> (pt)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n    feeder = Feeder(<span class=\"hljs-string\">\"#whales\"</span>)\n    analyzer = Analyzer(feeder)\n    print(analyzer.important_words())\n\n\n\n\n\n</code></pre></div>",
    "sec_48": "<div class=\"codeBlock hljs python\" id=\"sec_48\"><pre id=\"sec_48_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\"># -*- coding: utf-8 -*-</span>\n<span class=\"hljs-string\">\"\"\"Sentiment analysis implementations.\n\n.. versionadded:: 0.5.0\n\"\"\"</span>\n<span class=\"hljs-keyword\">from</span> __future__ <span class=\"hljs-keyword\">import</span> absolute_import\n<span class=\"hljs-keyword\">from</span> collections <span class=\"hljs-keyword\">import</span> namedtuple\n\n<span class=\"hljs-keyword\">from</span> textblob.packages <span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">from</span> textblob.en <span class=\"hljs-keyword\">import</span> sentiment <span class=\"hljs-keyword\">as</span> pattern_sentiment\n<span class=\"hljs-keyword\">from</span> textblob.tokenizers <span class=\"hljs-keyword\">import</span> word_tokenize\n<span class=\"hljs-keyword\">from</span> textblob.decorators <span class=\"hljs-keyword\">import</span> requires_nltk_corpus\n<span class=\"hljs-keyword\">from</span> textblob.base <span class=\"hljs-keyword\">import</span> BaseSentimentAnalyzer, DISCRETE, CONTINUOUS\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">PatternAnalyzer</span>(<span class=\"hljs-params\">BaseSentimentAnalyzer</span>):</span>\n    <span class=\"hljs-string\">\"\"\"Sentiment analyzer that uses the same implementation as the\n    pattern library. Returns results as a named tuple of the form:\n\n    ``Sentiment(polarity, subjectivity)``\n    \"\"\"</span>\n\n    kind = CONTINUOUS\n    <span class=\"hljs-comment\">#: Return type declaration</span>\n    RETURN_TYPE = namedtuple(<span class=\"hljs-string\">'Sentiment'</span>, [<span class=\"hljs-string\">'polarity'</span>, <span class=\"hljs-string\">'subjectivity'</span>])\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">analyze</span>(<span class=\"hljs-params\">self, text</span>):</span>\n        <span class=\"hljs-string\">\"\"\"Return the sentiment as a named tuple of the form:\n        ``Sentiment(polarity, subjectivity)``.\n        \"\"\"</span>\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_0\" class=\"highlights fea_sentiment_analysis\"><span class=\"hljs-keyword\">return</span> self.RETURN_TYPE(*pattern_sentiment(text))</div>\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">_default_feature_extractor</span>(<span class=\"hljs-params\">words</span>):</span>\n    <span class=\"hljs-string\">\"\"\"Default feature extractor for the NaiveBayesAnalyzer.\"\"\"</span>\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">dict</span>(((word, <span class=\"hljs-literal\">True</span>) <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> words))\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">NaiveBayesAnalyzer</span>(<span class=\"hljs-params\">BaseSentimentAnalyzer</span>):</span>\n    <span class=\"hljs-string\">\"\"\"Naive Bayes analyzer that is trained on a dataset of movie reviews.\n    Returns results as a named tuple of the form:\n    ``Sentiment(classification, p_pos, p_neg)``\n\n    :param callable feature_extractor: Function that returns a dictionary of\n        features, given a list of words.\n    \"\"\"</span>\n\n    kind = DISCRETE\n    <span class=\"hljs-comment\">#: Return type declaration</span>\n    RETURN_TYPE = namedtuple(<span class=\"hljs-string\">'Sentiment'</span>, [<span class=\"hljs-string\">'classification'</span>, <span class=\"hljs-string\">'p_pos'</span>, <span class=\"hljs-string\">'p_neg'</span>])\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, feature_extractor=_default_feature_extractor</span>):</span>\n        <span class=\"hljs-built_in\">super</span>(NaiveBayesAnalyzer, self).__init__()\n        self._classifier = <span class=\"hljs-literal\">None</span>\n        self.feature_extractor = feature_extractor\n\n<span class=\"hljs-meta\">    @requires_nltk_corpus</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">train</span>(<span class=\"hljs-params\">self</span>):</span>\n        <span class=\"hljs-string\">\"\"\"Train the Naive Bayes classifier on the movie review corpus.\"\"\"</span>\n        <span class=\"hljs-built_in\">super</span>(NaiveBayesAnalyzer, self).train()\n        neg_ids = nltk.corpus.movie_reviews.fileids(<span class=\"hljs-string\">'neg'</span>)\n        pos_ids = nltk.corpus.movie_reviews.fileids(<span class=\"hljs-string\">'pos'</span>)\n        neg_feats = [(self.feature_extractor(\n            nltk.corpus.movie_reviews.words(fileids=[f])), <span class=\"hljs-string\">'neg'</span>) <span class=\"hljs-keyword\">for</span> f <span class=\"hljs-keyword\">in</span> neg_ids]\n        pos_feats = [(self.feature_extractor(\n            nltk.corpus.movie_reviews.words(fileids=[f])), <span class=\"hljs-string\">'pos'</span>) <span class=\"hljs-keyword\">for</span> f <span class=\"hljs-keyword\">in</span> pos_ids]\n        train_data = neg_feats + pos_feats\n        self._classifier = nltk.classify.NaiveBayesClassifier.train(train_data)\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">analyze</span>(<span class=\"hljs-params\">self, text</span>):</span>\n        <span class=\"hljs-string\">\"\"\"Return the sentiment as a named tuple of the form:\n        ``Sentiment(classification, p_pos, p_neg)``\n        \"\"\"</span>\n        <span class=\"hljs-comment\"># Lazily train the classifier</span>\n        <span class=\"hljs-built_in\">super</span>(NaiveBayesAnalyzer, self).analyze(text)\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">tokens = word_tokenize(text, include_punc=<span class=\"hljs-literal\">False</span>)\n        filtered = (t.lower() <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> tokens <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(t) &gt;= <span class=\"hljs-number\">3</span>)</div>\n        feats = self.feature_extractor(filtered)\n        prob_dist = self._classifier.prob_classify(feats)\n        <span class=\"hljs-keyword\">return</span> self.RETURN_TYPE(\n            classification=prob_dist.<span class=\"hljs-built_in\">max</span>(),\n            p_pos=prob_dist.prob(<span class=\"hljs-string\">'pos'</span>),\n            p_neg=prob_dist.prob(<span class=\"hljs-string\">\"neg\"</span>)\n        )</code></pre></div>",
    "sec_49": "<div class=\"codeBlock hljs python\" id=\"sec_49\"><pre id=\"sec_49_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">import</span> csv\n<span class=\"hljs-keyword\">from</span> textblob <span class=\"hljs-keyword\">import</span> TextBlob\n<span class=\"hljs-keyword\">from</span> textblob <span class=\"hljs-keyword\">import</span> Blobber\n<span class=\"hljs-keyword\">from</span> textblob.sentiments <span class=\"hljs-keyword\">import</span> NaiveBayesAnalyzer\n<span class=\"hljs-keyword\">from</span> emoji <span class=\"hljs-keyword\">import</span> UNICODE_EMOJI\n<span class=\"hljs-keyword\">import</span> time\n\n<span class=\"hljs-string\">\"\"\"\ntwitter = \"The Buddha, the U.S.A., resides quite as comfortably in the circuits of a digital\ncomputer or the gears of a cycle transmission as he does at the top of a mountain\nor in the petals of a flower. To think otherwise is to demean the Buddha...which is\nto demean oneself .\"\n\ntwitter1 = \"\"I would be very careful. because smiley face emotions as associated with negative #sentiment as positive\"\"\n\n\ntwitter2 = \"\"This is awlful, such a terrible people\"\"\n\ntwitter3 = \"\"I don't like it, since it is not a good idea @happyday\"\"\n\ntwitter4 = \"\"🤔 🙈 me así, bla es se 😌 ds 💕👭👙\"\"\n\n\"\"\"</span>\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">classify_polarity</span>(<span class=\"hljs-params\">polarity</span>):</span>\n    <span class=\"hljs-keyword\">if</span> polarity &gt; <span class=\"hljs-number\">0.2</span>:\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">'positive'</span>\n    <span class=\"hljs-keyword\">elif</span> polarity &lt; -<span class=\"hljs-number\">0.2</span>:\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">'negative'</span>\n    <span class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">'neutral'</span>\n\n\n<span class=\"hljs-comment\"># since the sentiment of emoji is independent, we can analyse it separately without including it into machine learning</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">emoji_analysis</span>(<span class=\"hljs-params\">twitter, emoji_rank</span>):</span>\n    emo_polarity = <span class=\"hljs-number\">0</span>\n    total_count = <span class=\"hljs-number\">0</span>\n    <span class=\"hljs-keyword\">for</span> emoji <span class=\"hljs-keyword\">in</span> twitter:\n        <span class=\"hljs-keyword\">if</span> emoji <span class=\"hljs-keyword\">in</span> emoji_rank:\n            total_count += <span class=\"hljs-number\">1</span>\n            emo_polarity += emoji_rank[emoji]\n    <span class=\"hljs-keyword\">if</span> total_count == <span class=\"hljs-number\">0</span>:\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">None</span>\n    <span class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-comment\"># average of emoji's sentiment</span>\n        emo_polarity = emo_polarity / total_count\n        <span class=\"hljs-keyword\">return</span> emo_polarity\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_emoji_rank</span>():</span>\n    <span class=\"hljs-comment\"># emoji sentiment rank from http://kt.ijs.si/data/Emoji_sentiment_ranking/</span>\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">'sentiment_analysis/Emoji_Sentiment_Data_v1.0.csv'</span>, newline=<span class=\"hljs-string\">''</span>, encoding=<span class=\"hljs-string\">'utf-8'</span>) <span class=\"hljs-keyword\">as</span> csvfile:\n        emoji_list = csv.reader(csvfile, delimiter=<span class=\"hljs-string\">' '</span>, quotechar=<span class=\"hljs-string\">'|'</span>)\n        <span class=\"hljs-built_in\">next</span>(emoji_list, <span class=\"hljs-literal\">None</span>)\n        emoji_rank = {}\n        <span class=\"hljs-keyword\">for</span> emoji <span class=\"hljs-keyword\">in</span> emoji_list:\n            emoji = emoji[<span class=\"hljs-number\">0</span>].split(<span class=\"hljs-string\">','</span>)\n            <span class=\"hljs-comment\"># only use emoji which occurs more than 50 times, less occurrences leads to unreliable result</span>\n            <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">int</span>(emoji[<span class=\"hljs-number\">2</span>]) &gt;= <span class=\"hljs-number\">50</span>:\n                <span class=\"hljs-comment\"># [0] is Emoji, [2] is Total Occurrence, [4] is Negative, [5] is Neutral, [6] is Positive</span>\n                <span class=\"hljs-comment\"># here multiplied by (1 - Neutral / Occurrence) to take neutral tweets into consideration</span>\n                emoji_rank[emoji[<span class=\"hljs-number\">0</span>]] = ((<span class=\"hljs-built_in\">int</span>(emoji[<span class=\"hljs-number\">6</span>]) - <span class=\"hljs-built_in\">int</span>(emoji[<span class=\"hljs-number\">4</span>])) / <span class=\"hljs-built_in\">int</span>(emoji[<span class=\"hljs-number\">2</span>])) * (<span class=\"hljs-number\">1</span> - <span class=\"hljs-built_in\">int</span>(emoji[<span class=\"hljs-number\">5</span>]) / <span class=\"hljs-built_in\">int</span>(emoji[<span class=\"hljs-number\">2</span>]))\n    <span class=\"hljs-keyword\">return</span> emoji_rank\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_sentiment_analyzer</span>():</span>\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_0\" class=\"highlights fea_sentiment_analysis\"><span class=\"hljs-keyword\">return</span> Blobber(analyzer=NaiveBayesAnalyzer())</div>\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">sentiment_analysis</span>(<span class=\"hljs-params\">tweet, sentiment_analyzer, emoji_rank</span>):</span>\n    emoji_factor = <span class=\"hljs-number\">0.5</span>\n    emoji_polarity = emoji_analysis(tweet, emoji_rank)\n    tweet = <span class=\"hljs-string\">' '</span>.join(re.sub(<span class=\"hljs-string\">\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\"</span>, <span class=\"hljs-string\">\" \"</span>, tweet).split())\n    sentiment = sentiment_analyzer(tweet).sentiment\n    text_polarity = sentiment.p_pos - sentiment.p_neg\n\n    <span class=\"hljs-keyword\">if</span> emoji_polarity <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n        polarity = emoji_polarity * emoji_factor + text_polarity * (<span class=\"hljs-number\">1</span> - emoji_factor)\n        <span class=\"hljs-keyword\">return</span> classify_polarity(polarity)\n    <span class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-keyword\">return</span> classify_polarity(text_polarity)\n\n<span class=\"hljs-comment\"># print(\"start\")</span>\n<span class=\"hljs-comment\"># start_time = time.time()</span>\n<span class=\"hljs-comment\"># emoji_ranker = get_emoji_rank()</span>\n<span class=\"hljs-comment\"># sentiment_analyzer = Blobber(analyzer=NaiveBayesAnalyzer())</span>\n<span class=\"hljs-comment\">#</span>\n<span class=\"hljs-comment\"># tweet = \"Fuck you\"</span>\n<span class=\"hljs-comment\"># tweet2 = \"I don't like it, since it is not a good idea @happyday\"</span>\n<span class=\"hljs-comment\"># tweet3 = \"🤔 🙈 me así, bla es se 😌 ds 💕👭👙 🤔 🙈 me así, bla es se 😌 ds 💕👭👙 🤔 🙈 me así, bla es se 😌 ds 💕👭👙\"</span>\n<span class=\"hljs-comment\">#</span>\n<span class=\"hljs-comment\"># for i in range(1, 500):</span>\n<span class=\"hljs-comment\">#     print(sentiment_analysis(tweet, sentiment_analyzer, emoji_ranker))</span>\n<span class=\"hljs-comment\"># # print(sentiment_analysis(tweet, sentiment_analyzer, emoji_ranker))</span>\n<span class=\"hljs-comment\"># # print(sentiment_analysis(tweet2, sentiment_analyzer, emoji_ranker))</span>\n<span class=\"hljs-comment\"># # print(sentiment_analysis(tweet3, sentiment_analyzer, emoji_ranker))</span>\n<span class=\"hljs-comment\">#</span>\n<span class=\"hljs-comment\"># print(\"Time taken: {}\".format(time.time() - start_time))</span>\n\n<span class=\"hljs-comment\">#sentiment_analysis(twitter1, get_emoji_rank())</span></code></pre></div>",
    "sec_50": "<div class=\"codeBlock hljs python\" id=\"sec_50\"><pre id=\"sec_50_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\"># -*- coding: utf-8 -*-</span>\n<span class=\"hljs-string\">\"\"\"Sentiment analysis implementations.\n\n.. versionadded:: 0.5.0\n\"\"\"</span>\n<span class=\"hljs-keyword\">from</span> __future__ <span class=\"hljs-keyword\">import</span> absolute_import\n<span class=\"hljs-keyword\">from</span> collections <span class=\"hljs-keyword\">import</span> namedtuple\n\n<span class=\"hljs-keyword\">import</span> nltk\n\n<span class=\"hljs-keyword\">from</span> textblob.en <span class=\"hljs-keyword\">import</span> sentiment <span class=\"hljs-keyword\">as</span> pattern_sentiment\n<span class=\"hljs-keyword\">from</span> textblob.tokenizers <span class=\"hljs-keyword\">import</span> word_tokenize\n<span class=\"hljs-keyword\">from</span> textblob.decorators <span class=\"hljs-keyword\">import</span> requires_nltk_corpus\n<span class=\"hljs-keyword\">from</span> textblob.base <span class=\"hljs-keyword\">import</span> BaseSentimentAnalyzer, DISCRETE, CONTINUOUS\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">PatternAnalyzer</span>(<span class=\"hljs-params\">BaseSentimentAnalyzer</span>):</span>\n    <span class=\"hljs-string\">\"\"\"Sentiment analyzer that uses the same implementation as the\n    pattern library. Returns results as a named tuple of the form:\n\n    ``Sentiment(polarity, subjectivity, [assessments])``\n\n    where [assessments] is a list of the assessed tokens and their\n    polarity and subjectivity scores\n    \"\"\"</span>\n    kind = CONTINUOUS\n    <span class=\"hljs-comment\"># This is only here for backwards-compatibility.</span>\n    <span class=\"hljs-comment\"># The return type is actually determined upon calling analyze()</span>\n    RETURN_TYPE = namedtuple(<span class=\"hljs-string\">'Sentiment'</span>, [<span class=\"hljs-string\">'polarity'</span>, <span class=\"hljs-string\">'subjectivity'</span>])\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">analyze</span>(<span class=\"hljs-params\">self, text, keep_assessments=<span class=\"hljs-literal\">False</span></span>):</span>\n        <span class=\"hljs-string\">\"\"\"Return the sentiment as a named tuple of the form:\n        ``Sentiment(polarity, subjectivity, [assessments])``.\n        \"\"\"</span>\n        <span class=\"hljs-comment\">#: Return type declaration</span>\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"sentiment_analysis_0\" class=\"highlights fea_sentiment_analysis\"><span class=\"hljs-keyword\">if</span> keep_assessments:\n            Sentiment = namedtuple(<span class=\"hljs-string\">'Sentiment'</span>, [<span class=\"hljs-string\">'polarity'</span>, <span class=\"hljs-string\">'subjectivity'</span>, <span class=\"hljs-string\">'assessments'</span>])\n            assessments = pattern_sentiment(text).assessments\n            polarity, subjectivity = pattern_sentiment(text)\n            <span class=\"hljs-keyword\">return</span> Sentiment(polarity, subjectivity, assessments)\n\n        <span class=\"hljs-keyword\">else</span>:\n            Sentiment = namedtuple(<span class=\"hljs-string\">'Sentiment'</span>, [<span class=\"hljs-string\">'polarity'</span>, <span class=\"hljs-string\">'subjectivity'</span>])\n            <span class=\"hljs-keyword\">return</span> Sentiment(*pattern_sentiment(text))</div>\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">_default_feature_extractor</span>(<span class=\"hljs-params\">words</span>):</span>\n    <span class=\"hljs-string\">\"\"\"Default feature extractor for the NaiveBayesAnalyzer.\"\"\"</span>\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">dict</span>(((word, <span class=\"hljs-literal\">True</span>) <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> words))\n\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">NaiveBayesAnalyzer</span>(<span class=\"hljs-params\">BaseSentimentAnalyzer</span>):</span>\n    <span class=\"hljs-string\">\"\"\"Naive Bayes analyzer that is trained on a dataset of movie reviews.\n    Returns results as a named tuple of the form:\n    ``Sentiment(classification, p_pos, p_neg)``\n\n    :param callable feature_extractor: Function that returns a dictionary of\n        features, given a list of words.\n    \"\"\"</span>\n\n    kind = DISCRETE\n    <span class=\"hljs-comment\">#: Return type declaration</span>\n    RETURN_TYPE = namedtuple(<span class=\"hljs-string\">'Sentiment'</span>, [<span class=\"hljs-string\">'classification'</span>, <span class=\"hljs-string\">'p_pos'</span>, <span class=\"hljs-string\">'p_neg'</span>])\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, feature_extractor=_default_feature_extractor</span>):</span>\n        <span class=\"hljs-built_in\">super</span>(NaiveBayesAnalyzer, self).__init__()\n        self._classifier = <span class=\"hljs-literal\">None</span>\n        self.feature_extractor = feature_extractor\n\n<span class=\"hljs-meta\">    @requires_nltk_corpus</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">train</span>(<span class=\"hljs-params\">self</span>):</span>\n        <span class=\"hljs-string\">\"\"\"Train the Naive Bayes classifier on the movie review corpus.\"\"\"</span>\n        <span class=\"hljs-built_in\">super</span>(NaiveBayesAnalyzer, self).train()\n        neg_ids = nltk.corpus.movie_reviews.fileids(<span class=\"hljs-string\">'neg'</span>)\n        pos_ids = nltk.corpus.movie_reviews.fileids(<span class=\"hljs-string\">'pos'</span>)\n        neg_feats = [(self.feature_extractor(\n            nltk.corpus.movie_reviews.words(fileids=[f])), <span class=\"hljs-string\">'neg'</span>) <span class=\"hljs-keyword\">for</span> f <span class=\"hljs-keyword\">in</span> neg_ids]\n        pos_feats = [(self.feature_extractor(\n            nltk.corpus.movie_reviews.words(fileids=[f])), <span class=\"hljs-string\">'pos'</span>) <span class=\"hljs-keyword\">for</span> f <span class=\"hljs-keyword\">in</span> pos_ids]\n        train_data = neg_feats + pos_feats\n        self._classifier = nltk.classify.NaiveBayesClassifier.train(train_data)\n\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">analyze</span>(<span class=\"hljs-params\">self, text</span>):</span>\n        <span class=\"hljs-string\">\"\"\"Return the sentiment as a named tuple of the form:\n        ``Sentiment(classification, p_pos, p_neg)``\n        \"\"\"</span>\n        <span class=\"hljs-comment\"># Lazily train the classifier</span>\n        <span class=\"hljs-built_in\">super</span>(NaiveBayesAnalyzer, self).analyze(text)\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">tokens = word_tokenize(text, include_punc=<span class=\"hljs-literal\">False</span>)\n        filtered = (t.lower() <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> tokens <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(t) &gt;= <span class=\"hljs-number\">3</span>)</div>\n        feats = self.feature_extractor(filtered)\n        prob_dist = self._classifier.prob_classify(feats)\n        <span class=\"hljs-keyword\">return</span> self.RETURN_TYPE(\n            classification=prob_dist.<span class=\"hljs-built_in\">max</span>(),\n            p_pos=prob_dist.prob(<span class=\"hljs-string\">'pos'</span>),\n            p_neg=prob_dist.prob(<span class=\"hljs-string\">\"neg\"</span>)\n        )</code></pre></div>",
    "thr_31": "<div class=\"codeBlock hljs python\" id=\"thr_31\"><pre id=\"thr_31_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> spacy\n<span class=\"hljs-keyword\">import</span> logging\n<span class=\"hljs-keyword\">from</span> normality <span class=\"hljs-keyword\">import</span> collapse_spaces\n<span class=\"hljs-keyword\">from</span> fingerprints <span class=\"hljs-keyword\">import</span> clean_entity_name\n<span class=\"hljs-keyword\">from</span> followthemoney.types <span class=\"hljs-keyword\">import</span> registry\n\n<span class=\"hljs-keyword\">from</span> ingestors <span class=\"hljs-keyword\">import</span> settings\n<span class=\"hljs-keyword\">from</span> ingestors.analysis.country <span class=\"hljs-keyword\">import</span> location_country\n<span class=\"hljs-keyword\">from</span> ingestors.analysis.util <span class=\"hljs-keyword\">import</span> TAG_PERSON, TAG_COMPANY\n<span class=\"hljs-keyword\">from</span> ingestors.analysis.util <span class=\"hljs-keyword\">import</span> TAG_LOCATION, TAG_COUNTRY\n\nlog = logging.getLogger(__name__)\nNAME_MAX_LENGTH = <span class=\"hljs-number\">100</span>\nNAME_MIN_LENGTH = <span class=\"hljs-number\">4</span>\n<span class=\"hljs-comment\"># https://spacy.io/api/annotation#named-entities</span>\nSPACY_TYPES = {\n    <span class=\"hljs-string\">'PER'</span>: TAG_PERSON,\n    <span class=\"hljs-string\">'PERSON'</span>: TAG_PERSON,\n    <span class=\"hljs-string\">'ORG'</span>: TAG_COMPANY,\n    <span class=\"hljs-string\">'LOC'</span>: TAG_LOCATION,\n    <span class=\"hljs-string\">'GPE'</span>: TAG_LOCATION\n}\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">clean_name</span>(<span class=\"hljs-params\">text</span>):</span>\n    <span class=\"hljs-keyword\">if</span> text <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span> <span class=\"hljs-keyword\">or</span> <span class=\"hljs-built_in\">len</span>(text) &gt; NAME_MAX_LENGTH:\n        <span class=\"hljs-keyword\">return</span>\n    text = clean_entity_name(text)\n    text = collapse_spaces(text)\n    <span class=\"hljs-keyword\">if</span> text <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span> <span class=\"hljs-keyword\">or</span> <span class=\"hljs-built_in\">len</span>(text) &lt;= NAME_MIN_LENGTH <span class=\"hljs-keyword\">or</span> <span class=\"hljs-string\">' '</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> text:\n        <span class=\"hljs-keyword\">return</span>\n    <span class=\"hljs-keyword\">return</span> text\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">_load_model</span>(<span class=\"hljs-params\">lang</span>):</span>\n    <span class=\"hljs-string\">\"\"\"Load the spaCy model for the specified language\"\"\"</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_1\" class=\"highlights fea_named_entity_recognition\">attr_name = <span class=\"hljs-string\">'_nlp_%s'</span> % lang\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-built_in\">hasattr</span>(settings, attr_name):\n        log.info(<span class=\"hljs-string\">\"Loading spaCy model: %s...\"</span> % lang)\n        <span class=\"hljs-keyword\">try</span>:\n            model = spacy.load(lang, disable=[<span class=\"hljs-string\">\"tagger\"</span>, <span class=\"hljs-string\">\"parser\"</span>])\n            <span class=\"hljs-built_in\">setattr</span>(settings, attr_name, model)\n        <span class=\"hljs-keyword\">except</span> OSError:\n            log.error(<span class=\"hljs-string\">\"Cannot load spaCy model: %s\"</span>, lang)</div>\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">getattr</span>(settings, attr_name)\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_models</span>(<span class=\"hljs-params\">entity</span>):</span>\n    <span class=\"hljs-string\">\"\"\"Iterate over the NER models applicable to the given entity.\"\"\"</span>\n    languages = entity.get_type_values(registry.language)\n    models = []\n    <span class=\"hljs-keyword\">for</span> lang <span class=\"hljs-keyword\">in</span> languages:\n        <span class=\"hljs-keyword\">if</span> lang <span class=\"hljs-keyword\">in</span> settings.NER_MODELS:\n            models.append(_load_model(lang))\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-built_in\">len</span>(models):\n        models.append(_load_model(settings.NER_DEFAULT_MODEL))\n    <span class=\"hljs-keyword\">return</span> models\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">extract_entities</span>(<span class=\"hljs-params\">entity, text</span>):</span>\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_0\" class=\"highlights fea_named_entity_recognition\"><span class=\"hljs-keyword\">for</span> model <span class=\"hljs-keyword\">in</span> get_models(entity):\n        log.debug(<span class=\"hljs-string\">\"NER tagging %d chars (%s)\"</span>, <span class=\"hljs-built_in\">len</span>(text), model.lang)\n        doc = model(text)\n        <span class=\"hljs-keyword\">for</span> ent <span class=\"hljs-keyword\">in</span> doc.ents:\n            prop_name = SPACY_TYPES.get(ent.label_)\n            <span class=\"hljs-keyword\">if</span> prop_name <span class=\"hljs-keyword\">in</span> (TAG_COMPANY, TAG_PERSON):\n                name = clean_name(ent.text)\n                <span class=\"hljs-keyword\">yield</span> (prop_name, name)\n            <span class=\"hljs-keyword\">if</span> prop_name == TAG_LOCATION:\n                <span class=\"hljs-keyword\">for</span> country <span class=\"hljs-keyword\">in</span> location_country(ent.text):\n                    <span class=\"hljs-keyword\">yield</span> (TAG_COUNTRY, country)</div></code></pre></div>",
    "thr_32": "<div class=\"codeBlock hljs python\" id=\"thr_32\"><pre id=\"thr_32_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\">#!/usr/bin/env python</span>\n<span class=\"hljs-comment\"># coding: utf8</span>\n<span class=\"hljs-string\">\"\"\"Example of training an additional entity type\nThis script shows how to add a new entity type to an existing pre-trained NER\nmodel. To keep the example short and simple, only four sentences are provided\nas examples. In practice, you'll need many more — a few hundred would be a\ngood start. You will also likely need to mix in examples of other entity\ntypes, which might be obtained by running the entity recognizer over unlabelled\nsentences, and adding their annotations to the training set.\nThe actual training is performed by looping over the examples, and calling\n`nlp.entity.update()`. The `update()` method steps through the words of the\ninput. At each word, it makes a prediction. It then consults the annotations\nprovided on the GoldParse instance, to see whether it was right. If it was\nwrong, it adjusts its weights so that the correct action will score higher\nnext time.\nAfter training your model, you can save it to a directory. We recommend\nwrapping models as Python packages, for ease of deployment.\nFor more details, see the documentation:\n* Training: https://spacy.io/usage/training\n* NER: https://spacy.io/usage/linguistic-features#named-entities\nCompatible with: spaCy v2.0.0+\n\"\"\"</span>\n<span class=\"hljs-keyword\">from</span> __future__ <span class=\"hljs-keyword\">import</span> unicode_literals, print_function\n\n<span class=\"hljs-keyword\">import</span> plac\n<span class=\"hljs-keyword\">import</span> random\n<span class=\"hljs-keyword\">from</span> pathlib <span class=\"hljs-keyword\">import</span> Path\n<span class=\"hljs-keyword\">import</span> spacy\n<span class=\"hljs-keyword\">import</span> joblib\n\n\n<span class=\"hljs-comment\"># training data</span>\n<span class=\"hljs-comment\"># Note: If you're using an existing model, make sure to mix in examples of</span>\n<span class=\"hljs-comment\"># other entity types that spaCy correctly recognized before. Otherwise, your</span>\n<span class=\"hljs-comment\"># model might learn the new type, but \"forget\" what it previously knew.</span>\n<span class=\"hljs-comment\"># https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting</span>\n<span class=\"hljs-keyword\">from</span> cytoolz.itertoolz <span class=\"hljs-keyword\">import</span> partition_all\n<span class=\"hljs-keyword\">from</span> spacy.gold <span class=\"hljs-keyword\">import</span> offsets_from_biluo_tags\n\n\n<span class=\"hljs-meta\">@plac.annotations(<span class=\"hljs-params\">\n    model=(<span class=\"hljs-params\"><span class=\"hljs-string\">\"Model name. Defaults to blank 'en' model.\"</span>, <span class=\"hljs-string\">\"option\"</span>, <span class=\"hljs-string\">\"m\"</span>, <span class=\"hljs-built_in\">str</span></span>),\n    new_model_name=(<span class=\"hljs-params\"><span class=\"hljs-string\">\"New model name for model meta.\"</span>, <span class=\"hljs-string\">\"option\"</span>, <span class=\"hljs-string\">\"nm\"</span>, <span class=\"hljs-built_in\">str</span></span>),\n    output_dir=(<span class=\"hljs-params\"><span class=\"hljs-string\">\"Optional output directory\"</span>, <span class=\"hljs-string\">\"option\"</span>, <span class=\"hljs-string\">\"o\"</span>, Path</span>),\n    n_iter=(<span class=\"hljs-params\"><span class=\"hljs-string\">\"Number of training iterations\"</span>, <span class=\"hljs-string\">\"option\"</span>, <span class=\"hljs-string\">\"n\"</span>, <span class=\"hljs-built_in\">int</span></span>)</span>)</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">train_entity</span>(<span class=\"hljs-params\">data,labels,model=<span class=\"hljs-literal\">None</span>, new_model_name=<span class=\"hljs-string\">'animal'</span>,output_dir=<span class=\"hljs-literal\">None</span>, n_iter=<span class=\"hljs-number\">5</span></span>):</span>\n    <span class=\"hljs-string\">\"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"</span>\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_0\" class=\"highlights fea_named_entity_recognition\"><span class=\"hljs-keyword\">if</span> model <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n        nlp = spacy.load(model)  <span class=\"hljs-comment\"># load existing spaCy model</span>\n        print(<span class=\"hljs-string\">\"Loaded model '%s'\"</span> % model)\n    <span class=\"hljs-keyword\">else</span>:\n        nlp = spacy.blank(<span class=\"hljs-string\">'en'</span>)  <span class=\"hljs-comment\"># create blank Language class</span>\n        print(<span class=\"hljs-string\">\"Created blank 'en' model\"</span>)</div>\n\n    <span class=\"hljs-comment\"># Add entity recognizer to model if it's not in the pipeline</span>\n    <span class=\"hljs-comment\"># nlp.create_pipe works for built-ins that are registered with spaCy</span>\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_1\" class=\"highlights fea_named_entity_recognition\"><span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">'ner'</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> nlp.pipe_names:\n        ner = nlp.create_pipe(<span class=\"hljs-string\">'ner'</span>)\n        nlp.add_pipe(ner)</div>\n    <span class=\"hljs-comment\"># otherwise, get it, so we can add labels to it</span>\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_2\" class=\"highlights fea_named_entity_recognition\"><span class=\"hljs-keyword\">else</span>:\n        ner = nlp.get_pipe(<span class=\"hljs-string\">'ner'</span>)</div>\n\n    <span class=\"hljs-keyword\">for</span> LABEL <span class=\"hljs-keyword\">in</span> labels:\n     ner.add_label(LABEL)   <span class=\"hljs-comment\"># add new entity label to entity recognizer</span>\n\n    split = <span class=\"hljs-number\">0.8</span>\n    split = <span class=\"hljs-built_in\">int</span>(<span class=\"hljs-built_in\">len</span>(data) * split)\n    random.shuffle(data)\n    TRAIN_DATA = data[:split]\n    EVAL_DATA = data[split:]\n\n    <span class=\"hljs-comment\"># get names of other pipes to disable them during training</span>\n   <div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_3\" class=\"highlights fea_named_entity_recognition\"> other_pipes = [pipe <span class=\"hljs-keyword\">for</span> pipe <span class=\"hljs-keyword\">in</span> nlp.pipe_names <span class=\"hljs-keyword\">if</span> pipe != <span class=\"hljs-string\">'ner'</span>]</div>\n    <span class=\"hljs-comment\">#other_pipes = []</span>\n    print(<span class=\"hljs-string\">'{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'</span>.<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-string\">'LOSS'</span>, <span class=\"hljs-string\">'P'</span>, <span class=\"hljs-string\">'R'</span>, <span class=\"hljs-string\">'F'</span>))\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_4\" class=\"highlights fea_named_entity_recognition\">batch_size = <span class=\"hljs-number\">32</span>\n    <span class=\"hljs-keyword\">with</span> nlp.disable_pipes(*other_pipes):  <span class=\"hljs-comment\"># only train NER</span>\n        optimizer = nlp.begin_training()\n        <span class=\"hljs-keyword\">for</span> itn <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(n_iter):\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            <span class=\"hljs-keyword\">for</span> batch <span class=\"hljs-keyword\">in</span> partition_all(batch_size, TRAIN_DATA):\n                docs, golds = <span class=\"hljs-built_in\">zip</span>(*batch)\n                nlp.update(docs, golds, drop=<span class=\"hljs-number\">0.35</span>,sgd=optimizer,\n                           losses=losses)</div>\n            <span class=\"hljs-comment\">#print(losses)</span>\n            results = evaluate_ner_model_all_labels(nlp.tokenizer,EVAL_DATA,ner_pipe=ner)\n            print(<span class=\"hljs-string\">'{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'</span>  <span class=\"hljs-comment\"># print a simple table</span>\n              .<span class=\"hljs-built_in\">format</span>(losses[<span class=\"hljs-string\">'ner'</span>], results[<span class=\"hljs-string\">'textner_p'</span>],\n                      results[<span class=\"hljs-string\">'textner_r'</span>], results[<span class=\"hljs-string\">'textner_f'</span>]))\n\n    <span class=\"hljs-comment\"># test the trained model</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_5\" class=\"highlights fea_named_entity_recognition\">test_text = <span class=\"hljs-string\">'Following are U.S bank funds from 2010 to 2017?'</span>\n    doc = nlp(test_text)\n    print(<span class=\"hljs-string\">\"Entities in '%s'\"</span> % test_text)\n    <span class=\"hljs-keyword\">for</span> ent <span class=\"hljs-keyword\">in</span> doc.ents:\n        print(ent.label_, ent.text)</div>\n\n    <span class=\"hljs-comment\"># save model to output directory</span>\n    <span class=\"hljs-keyword\">if</span> output_dir <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n        output_dir = Path(output_dir)\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> output_dir.exists():\n            output_dir.mkdir()\n        nlp.meta[<span class=\"hljs-string\">'name'</span>] = new_model_name  <span class=\"hljs-comment\"># rename model</span>\n        nlp.to_disk(output_dir)\n        print(<span class=\"hljs-string\">\"Saved model to\"</span>, output_dir)\n\n        <span class=\"hljs-comment\"># test the saved model</span>\n        print(<span class=\"hljs-string\">\"Loading from\"</span>, output_dir)\n        nlp2 = spacy.load(output_dir)\n        doc2 = nlp2(test_text)\n        <span class=\"hljs-keyword\">for</span> ent <span class=\"hljs-keyword\">in</span> doc2.ents:\n            print(ent.label_, ent.text)\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">evaluate_ner_model</span>(<span class=\"hljs-params\">tokenizer,data,ner_pipe,label</span>):</span>\n    <span class=\"hljs-comment\">#for text in data:</span>\n    <span class=\"hljs-comment\">#    print(text[0].text)</span>\n    docs = (tokenizer(t[<span class=\"hljs-number\">0</span>].text) <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> data)\n    tp = <span class=\"hljs-number\">1e-8</span>  <span class=\"hljs-comment\"># True positives</span>\n    fp = <span class=\"hljs-number\">1e-8</span>  <span class=\"hljs-comment\"># False positives</span>\n    fn = <span class=\"hljs-number\">1e-8</span>  <span class=\"hljs-comment\"># False negatives</span>\n    tn = <span class=\"hljs-number\">1e-8</span>  <span class=\"hljs-comment\"># True negatives</span>\n    <span class=\"hljs-keyword\">for</span> i, doc <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(ner_pipe.pipe(docs)):\n        gold = offsets_from_biluo_tags(data[i][<span class=\"hljs-number\">0</span>], data[i][<span class=\"hljs-number\">1</span>].ner)\n        <span class=\"hljs-comment\">#gold = data[i][1]['entities']</span>\n        anchors = [(ele[<span class=\"hljs-number\">0</span>],ele[<span class=\"hljs-number\">1</span>]) <span class=\"hljs-keyword\">for</span> ele <span class=\"hljs-keyword\">in</span> gold]\n        <span class=\"hljs-keyword\">for</span> ent <span class=\"hljs-keyword\">in</span> doc.ents:\n            ent_test = (ent.start_char,ent.end_char)\n            <span class=\"hljs-keyword\">if</span> ent.label_ == label:\n                <span class=\"hljs-keyword\">if</span> ent_test <span class=\"hljs-keyword\">in</span> anchors:\n                    tp += <span class=\"hljs-number\">1</span>\n                <span class=\"hljs-keyword\">else</span>:\n                    fp += <span class=\"hljs-number\">1</span>\n            <span class=\"hljs-keyword\">else</span>:\n                <span class=\"hljs-keyword\">if</span> ent_test <span class=\"hljs-keyword\">in</span> anchors:\n                    fn += <span class=\"hljs-number\">1</span>\n\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f_score = <span class=\"hljs-number\">2</span> * (precision * recall) / (precision + recall)\n\n    <span class=\"hljs-keyword\">return</span> {<span class=\"hljs-string\">'textner_p'</span>: precision, <span class=\"hljs-string\">'textner_r'</span>: recall, <span class=\"hljs-string\">'textner_f'</span>: f_score}\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">evaluate_ner_model_all_labels</span>(<span class=\"hljs-params\">tokenizer,data,ner_pipe</span>):</span>\n    <span class=\"hljs-comment\">#for text in data:</span>\n    <span class=\"hljs-comment\">#    print(text[0].text)</span>\n    docs = (tokenizer(t[<span class=\"hljs-number\">0</span>].text) <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> data)\n    gold = []\n    <span class=\"hljs-keyword\">for</span> d <span class=\"hljs-keyword\">in</span> data:\n       gold.extend(offsets_from_biluo_tags(d[<span class=\"hljs-number\">0</span>], d[<span class=\"hljs-number\">1</span>].ner))\n    anchors = [(ele[<span class=\"hljs-number\">0</span>], ele[<span class=\"hljs-number\">1</span>]) <span class=\"hljs-keyword\">for</span> ele <span class=\"hljs-keyword\">in</span> gold]\n    labels = [ele[<span class=\"hljs-number\">2</span>] <span class=\"hljs-keyword\">for</span> ele <span class=\"hljs-keyword\">in</span> gold]\n    tp = <span class=\"hljs-number\">1e-8</span>  <span class=\"hljs-comment\"># True positives</span>\n    fp = <span class=\"hljs-number\">1e-8</span>  <span class=\"hljs-comment\"># False positives</span>\n    fn = <span class=\"hljs-number\">1e-8</span>  <span class=\"hljs-comment\"># False negatives</span>\n    tn = <span class=\"hljs-number\">1e-8</span>  <span class=\"hljs-comment\"># True negatives</span>\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_6\" class=\"highlights fea_named_entity_recognition\"><span class=\"hljs-keyword\">for</span> i, doc <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(ner_pipe.pipe(docs)):\n        <span class=\"hljs-comment\">#gold = data[i][1]['entities']</span>\n        <span class=\"hljs-keyword\">for</span> ent <span class=\"hljs-keyword\">in</span> doc.ents:\n            ent_test = (ent.start_char,ent.end_char)\n            <span class=\"hljs-keyword\">if</span> ent_test <span class=\"hljs-keyword\">in</span> anchors:\n                ind = anchors.index(ent_test)\n                <span class=\"hljs-keyword\">if</span> ent.label_ == labels[ind]:\n                  tp += <span class=\"hljs-number\">1</span>\n                <span class=\"hljs-keyword\">else</span>:\n                   fn += <span class=\"hljs-number\">1</span>\n            <span class=\"hljs-keyword\">else</span>:\n                <span class=\"hljs-keyword\">if</span> ent.label_ <span class=\"hljs-keyword\">in</span> labels:\n                   fp += <span class=\"hljs-number\">1</span></div><span class=\"hljs-number\"></span>\n\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f_score = <span class=\"hljs-number\">2</span> * (precision * recall) / (precision + recall)\n\n    <span class=\"hljs-keyword\">return</span> {<span class=\"hljs-string\">'textner_p'</span>: precision, <span class=\"hljs-string\">'textner_r'</span>: recall, <span class=\"hljs-string\">'textner_f'</span>: f_score}\n\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n    plac.call(train_entity)</code></pre></div>",
    "thr_33": "<div class=\"codeBlock hljs python\" id=\"thr_33\"><pre id=\"thr_33_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> sys\n<span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n<span class=\"hljs-keyword\">import</span> spacy\n\n<span class=\"hljs-keyword\">from</span> sklearn.feature_extraction.text <span class=\"hljs-keyword\">import</span> CountVectorizer\n<span class=\"hljs-keyword\">from</span> sklearn.feature_extraction.text <span class=\"hljs-keyword\">import</span> TfidfTransformer\n<span class=\"hljs-keyword\">from</span> string <span class=\"hljs-keyword\">import</span> punctuation, printable\n<span class=\"hljs-keyword\">from</span> sklearn.feature_extraction.stop_words <span class=\"hljs-keyword\">import</span> ENGLISH_STOP_WORDS\n\n<span class=\"hljs-keyword\">from</span> nltk.stem <span class=\"hljs-keyword\">import</span> WordNetLemmatizer \nlemmatizer = WordNetLemmatizer()\n\n\n<span class=\"hljs-comment\">## count vectorizer expects a list of strings</span>\ntext1 = <span class=\"hljs-string\">\"oh the thinks you can thing if only you try\"</span>\ntext2 = <span class=\"hljs-string\">\"you can think up a guff going by if you try\"</span>\ntext3 = <span class=\"hljs-string\">\"i am a guff you are a guff we are all guffs\"</span>\n\nSTOPLIST = ENGLISH_STOP_WORDS\nSTOPLIST = <span class=\"hljs-built_in\">set</span>(<span class=\"hljs-built_in\">list</span>(STOPLIST) + [<span class=\"hljs-string\">\"foo\"</span>])\n\n<span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-string\">'nlp'</span> <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">locals</span>():\n    print(<span class=\"hljs-string\">\"Loading English Module...\"</span>)\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"lemmatization_0\" class=\"highlights fea_lemmatization\">nlp = spacy.load(<span class=\"hljs-string\">'en'</span>)</div>\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">lemmatize_string</span>(<span class=\"hljs-params\">doc, stop_words</span>):</span>\n    <span class=\"hljs-string\">\"\"\"\n    takes a list of strings where each string is a document\n    returns a processed list of strings\n    \"\"\"</span>\n\n    <span class=\"hljs-comment\"># First remove punctuation form string</span>\n    <span class=\"hljs-keyword\">if</span> sys.version_info.major == <span class=\"hljs-number\">3</span>:\n        PUNCT_DICT = {<span class=\"hljs-built_in\">ord</span>(punc): <span class=\"hljs-literal\">None</span> <span class=\"hljs-keyword\">for</span> punc <span class=\"hljs-keyword\">in</span> punctuation}\n        doc = doc.translate(PUNCT_DICT)\n\n    <span class=\"hljs-comment\"># remove unicode</span>\n    clean_doc = <span class=\"hljs-string\">\"\"</span>.join([char <span class=\"hljs-keyword\">for</span> char <span class=\"hljs-keyword\">in</span> doc <span class=\"hljs-keyword\">if</span> char <span class=\"hljs-keyword\">in</span> printable])\n            \n    <span class=\"hljs-comment\"># Run the doc through spaCy</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"lemmatization_1\" class=\"highlights fea_lemmatization\">doc = nlp(clean_doc)</div>\n\n    <span class=\"hljs-comment\"># Lemmatize and lower text</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"lemmatization_2\" class=\"highlights fea_lemmatization\">tokens = [re.sub(<span class=\"hljs-string\">\"\\W+\"</span>,<span class=\"hljs-string\">\"\"</span>,token.lemma_.lower()) <span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> doc ]</div>\n    tokens = [t <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> tokens <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(t) &gt; <span class=\"hljs-number\">1</span>]\n    \n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">' '</span>.join(w <span class=\"hljs-keyword\">for</span> w <span class=\"hljs-keyword\">in</span> tokens <span class=\"hljs-keyword\">if</span> w <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> stop_words)\n\ncorpus = [text1,text2,text3]\nprocessed = [lemmatize_string(doc, STOPLIST) <span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> corpus]\nprint(<span class=\"hljs-string\">\"good\"</span>)\nsys.exit()\n\n<span class=\"hljs-comment\">## count vectorizer for 1-grame</span>\nvectorizer = CountVectorizer()\ncounts = vectorizer.fit_transform(corpus)\nfeatures = vectorizer.get_feature_names()\nmatrix = counts.toarray()\nprint(features)\nprint(matrix)\n\n<span class=\"hljs-comment\">## count vectorizer for bi-grams</span>\nbigram_vectorizer = CountVectorizer(ngram_range=(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>),token_pattern=<span class=\"hljs-string\">r'\\b\\w+\\b'</span>, min_df=<span class=\"hljs-number\">1</span>)\ncounts = bigram_vectorizer.fit_transform(corpus)\nfeatures = bigram_vectorizer.get_feature_names()\nmatrix = counts.toarray() \nprint(features)\nprint(matrix)\n\n<span class=\"hljs-comment\">## use tfidf to transform the occurances into relative frequencies</span>\ntransformer = TfidfTransformer(smooth_idf=<span class=\"hljs-literal\">False</span>)\ntfidf = transformer.fit_transform(counts)\ntfidf_matrix = tfidf.toarray()\nprint(np.<span class=\"hljs-built_in\">round</span>(tfidf_matrix,<span class=\"hljs-number\">2</span>))\n\n<span class=\"hljs-comment\">## the same result can be obtained with the tfidf class</span>\n<span class=\"hljs-comment\">## Occurrence count is a good start but there is an issue: l</span>\n<span class=\"hljs-comment\">## longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.</span>\n</code></pre></div>",
    "thr_34": "<div class=\"codeBlock hljs python\" id=\"thr_34\"><pre id=\"thr_34_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> spacy\n<span class=\"hljs-keyword\">import</span> newspaper\n<span class=\"hljs-keyword\">from</span> newspaper <span class=\"hljs-keyword\">import</span> Article\n<span class=\"hljs-comment\">#import nltk</span>\n\nnlp = spacy.load(<span class=\"hljs-string\">'en'</span>)\n<span class=\"hljs-comment\">#nltk.download('punkt')</span>\n\n\n<span class=\"hljs-comment\">#nlp = spacy.load('en_core_web_md')</span>\n<span class=\"hljs-comment\">#nlp = spacy.load('en_core_web_sm')</span>\n\n\narticle = Article(<span class=\"hljs-string\">\"https://www.zacks.com/stock/news/317469/synaptics-syna-q4-earnings-beat-estimates-revenues-miss?cid=CS-CNN-HL-317469\"</span>)\n\narticle.download()\narticle.parse()\ntext = article.text\n<span class=\"hljs-comment\">#print(text)</span>\n\narticle.nlp()\nsummary = article.summary\n<span class=\"hljs-comment\">#print(article.summary)</span>\n<span class=\"hljs-built_in\">print</span>(article.keywords)\n\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">doc = nlp(summary)\n<span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> doc:</div>\n    <span class=\"hljs-built_in\">print</span>(token.text, <div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_0\" class=\"highlights fea_Part_of_Speech\">token.pos</div>, token.dep_)</code></pre></div>",
    "thr_35": "<div class=\"codeBlock hljs python\" id=\"thr_35\"><pre id=\"thr_35_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">from</span> __future__ <span class=\"hljs-keyword\">import</span> unicode_literals, print_function\n\n<span class=\"hljs-keyword\">import</span> plac\n<span class=\"hljs-keyword\">import</span> random\n<span class=\"hljs-keyword\">from</span> pathlib <span class=\"hljs-keyword\">import</span> Path\n<span class=\"hljs-keyword\">import</span> spacy\n\n\n<span class=\"hljs-comment\"># new entity label</span>\nLABEL = <span class=\"hljs-string\">'Prescience_Entity'</span>\n\n\nTRAIN_DATA = [(<span class=\"hljs-string\">'How is Bitcoin doing today?'</span>, {<span class=\"hljs-string\">'entities'</span>: [(<span class=\"hljs-number\">15</span>, <span class=\"hljs-number\">20</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>), (<span class=\"hljs-number\">21</span>, <span class=\"hljs-number\">26</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>), (<span class=\"hljs-number\">7</span>, <span class=\"hljs-number\">14</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>)]}), (<span class=\"hljs-string\">'What People are talking about Bitcoin?'</span>, {<span class=\"hljs-string\">'entities'</span>: [(<span class=\"hljs-number\">5</span>, <span class=\"hljs-number\">11</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>), (<span class=\"hljs-number\">16</span>, <span class=\"hljs-number\">23</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>), (<span class=\"hljs-number\">30</span>, <span class=\"hljs-number\">37</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>)]}), (<span class=\"hljs-string\">'How Bitcoin will be doing tomorrow?'</span>, {<span class=\"hljs-string\">'entities'</span>: [(<span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">11</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>), (<span class=\"hljs-number\">26</span>, <span class=\"hljs-number\">34</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>), (<span class=\"hljs-number\">20</span>, <span class=\"hljs-number\">25</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>)]}), (<span class=\"hljs-string\">'How Bitcoin will be doing one week?'</span>, {<span class=\"hljs-string\">'entities'</span>: [(<span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">11</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>), (<span class=\"hljs-number\">26</span>, <span class=\"hljs-number\">34</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>), (<span class=\"hljs-number\">30</span>, <span class=\"hljs-number\">34</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>)]}), (<span class=\"hljs-string\">'How Bitcoin will be doing one month ahead?'</span>, {<span class=\"hljs-string\">'entities'</span>: [(<span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">11</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>), (<span class=\"hljs-number\">20</span>, <span class=\"hljs-number\">25</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>), (<span class=\"hljs-number\">30</span>, <span class=\"hljs-number\">35</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>), (<span class=\"hljs-number\">26</span>, <span class=\"hljs-number\">35</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>), (<span class=\"hljs-number\">36</span>, <span class=\"hljs-number\">41</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>)]}), (<span class=\"hljs-string\">'What are the factors influencing Bitcoin prices?'</span>, {<span class=\"hljs-string\">'entities'</span>: [(<span class=\"hljs-number\">13</span>, <span class=\"hljs-number\">20</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>), (<span class=\"hljs-number\">33</span>, <span class=\"hljs-number\">40</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>), (<span class=\"hljs-number\">21</span>, <span class=\"hljs-number\">32</span>, <span class=\"hljs-string\">'Prescience_Entity'</span>)]})]\n\n\n\n<span class=\"hljs-meta\">@plac.annotations(<span class=\"hljs-params\">\n    model=(<span class=\"hljs-params\"><span class=\"hljs-string\">\"Model name. Defaults to blank 'en' model.\"</span>, <span class=\"hljs-string\">\"option\"</span>, <span class=\"hljs-string\">\"m\"</span>, <span class=\"hljs-built_in\">str</span></span>),\n    new_model_name=(<span class=\"hljs-params\"><span class=\"hljs-string\">\"New model name for model meta.\"</span>, <span class=\"hljs-string\">\"option\"</span>, <span class=\"hljs-string\">\"nm\"</span>, <span class=\"hljs-built_in\">str</span></span>),\n    output_dir=(<span class=\"hljs-params\"><span class=\"hljs-string\">\"Optional output directory\"</span>, <span class=\"hljs-string\">\"option\"</span>, <span class=\"hljs-string\">\"o\"</span>, Path</span>),\n    n_iter=(<span class=\"hljs-params\"><span class=\"hljs-string\">\"Number of training iterations\"</span>, <span class=\"hljs-string\">\"option\"</span>, <span class=\"hljs-string\">\"n\"</span>, <span class=\"hljs-built_in\">int</span></span>)</span>)</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">main</span>(<span class=\"hljs-params\">model=<span class=\"hljs-string\">'en'</span>, new_model_name=<span class=\"hljs-string\">'animal'</span>, output_dir=<span class=\"hljs-string\">'/Users/exepaul/'</span>, n_iter=<span class=\"hljs-number\">20</span></span>):</span>\n    \n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_0\" class=\"highlights fea_named_entity_recognition\"><span class=\"hljs-keyword\">if</span> model <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n        nlp = spacy.load(model)  \n        print(<span class=\"hljs-string\">\"Loaded model '%s'\"</span> % model)\n\n    nlp = spacy.load(<span class=\"hljs-string\">'en'</span>)</div>\n\n\n \n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_1\" class=\"highlights fea_named_entity_recognition\"><span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">'ner'</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> nlp.pipe_names:\n        ner = nlp.create_pipe(<span class=\"hljs-string\">'ner'</span>)\n        nlp.add_pipe(ner)\n    <span class=\"hljs-comment\"># otherwise, get it, so we can add labels to it</span>\n    <span class=\"hljs-keyword\">else</span>:\n        ner = nlp.get_pipe(<span class=\"hljs-string\">'ner'</span>)\n</div>\n    ner.add_label(LABEL)   <span class=\"hljs-comment\"># add new entity label to entity recognizer</span>\n\n    <span class=\"hljs-comment\"># get names of other pipes to disable them during training</span>\n    other_pipes = [pipe <span class=\"hljs-keyword\">for</span> pipe <span class=\"hljs-keyword\">in</span> nlp.pipe_names <span class=\"hljs-keyword\">if</span> pipe != <span class=\"hljs-string\">'ner'</span>]\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_2\" class=\"highlights fea_named_entity_recognition\"><span class=\"hljs-keyword\">with</span> nlp.disable_pipes(*other_pipes):  <span class=\"hljs-comment\"># only train NER</span>\n        optimizer = nlp.begin_training()\n        <span class=\"hljs-keyword\">for</span> itn <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(n_iter):\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            <span class=\"hljs-keyword\">for</span> text, annotations <span class=\"hljs-keyword\">in</span> TRAIN_DATA:\n                nlp.update([text], [annotations], sgd=optimizer, drop=<span class=\"hljs-number\">0.35</span>,\n                           losses=losses)\n            print(losses)</div>\n\n    <span class=\"hljs-comment\"># test the trained model</span>\n    test_text = <span class=\"hljs-string\">'What People are talking about Bitcoin?'</span>\n    doc = nlp(test_text)\n    print(<span class=\"hljs-string\">\"Entities in '%s'\"</span> % test_text)\n    <span class=\"hljs-keyword\">for</span> ent <span class=\"hljs-keyword\">in</span> doc.ents:\n        print(ent.label_, ent.text)\n\n    <span class=\"hljs-comment\"># save model to output directory</span>\n    <span class=\"hljs-keyword\">if</span> output_dir <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n        output_dir = Path(output_dir)\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> output_dir.exists():\n            output_dir.mkdir()\n        nlp.meta[<span class=\"hljs-string\">'name'</span>] = new_model_name  <span class=\"hljs-comment\"># rename model</span>\n        nlp.to_disk(output_dir)\n        print(<span class=\"hljs-string\">\"Saved model to\"</span>, output_dir)\n\n        <span class=\"hljs-comment\"># test the saved model</span>\n        print(<span class=\"hljs-string\">\"Loading from\"</span>, output_dir)\n        nlp2 = spacy.load(output_dir)\n        doc2 = nlp2(test_text)\n        <span class=\"hljs-keyword\">for</span> ent <span class=\"hljs-keyword\">in</span> doc2.ents:\n            print(ent.label_, ent.text)\n\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n    plac.call(main)</code></pre></div>",
    "thr_36": "<div class=\"codeBlock hljs python\" id=\"thr_36\"><pre id=\"thr_36_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">from</span> flask <span class=\"hljs-keyword\">import</span> Flask, abort, request, Response\n<span class=\"hljs-keyword\">import</span> signal\n<span class=\"hljs-keyword\">import</span> threading\n<span class=\"hljs-keyword\">import</span> time\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">import</span> requests\n\n<span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">import</span> random\n\n<span class=\"hljs-keyword\">import</span> spacy\n<span class=\"hljs-keyword\">import</span> newspaper\n<span class=\"hljs-keyword\">from</span> newspaper <span class=\"hljs-keyword\">import</span> Article\n<span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">import</span> en_core_web_sm\n<span class=\"hljs-comment\">#import en_core_web_md</span>\n\napp = Flask(__name__)\nshuttingDown = <span class=\"hljs-literal\">False</span>\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">exit_call</span>():</span>\n    time.sleep(<span class=\"hljs-number\">20</span>)\n    requests.post(<span class=\"hljs-string\">\"http://localhost:5420/_shutdown\"</span>)\n    <span class=\"hljs-comment\"># os._exit(0)</span>\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">exit_gracefully</span>(<span class=\"hljs-params\">self, signum</span>):</span>\n    app.logger.error(<span class=\"hljs-string\">'Received shutdown signal. Exiting gracefully'</span>)\n    <span class=\"hljs-keyword\">global</span> shuttingDown\n    shuttingDown = <span class=\"hljs-literal\">True</span>\n    <span class=\"hljs-comment\"># <span class=\"hljs-doctag\">TODO:</span> wait for some time here to ensure we are not receiving any more</span>\n    <span class=\"hljs-comment\"># traffic</span>\n    _et = threading.Thread(target=exit_call)\n    _et.daemon = <span class=\"hljs-literal\">True</span>\n    _et.start()\n\n\nsignal.signal(signal.SIGTERM, exit_gracefully)\n\n\n<span class=\"hljs-meta\">@app.route(<span class=\"hljs-params\"><span class=\"hljs-string\">\"/\"</span></span>)</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">hello</span>():</span>\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"Hello World!\"</span>\n\n\n<span class=\"hljs-meta\">@app.route(<span class=\"hljs-params\"><span class=\"hljs-string\">\"/_status/latestdata\"</span></span>)</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">retrieveLatestData</span>():</span>\n\n        <span class=\"hljs-comment\">#TODO - Move all the processing code to a different module</span>\n        nltk.download(<span class=\"hljs-string\">'punkt'</span>)\n\n        print(<span class=\"hljs-string\">\" Loading modules \"</span>)\n\n        <span class=\"hljs-comment\">#nlp = spacy.load('en_core_web_md')</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"summarizer_0\" class=\"highlights fea_summarizer\">nlp = spacy.load(<span class=\"hljs-string\">'en_core_web_sm'</span>)</div>\n\n\n        <span class=\"hljs-comment\">#article = Article(\"https://www.zacks.com/stock/news/317469/synaptics-syna-q4-earnings-beat-estimates-revenues-miss?cid=CS-CNN-HL-317469\")</span>\n        article = Article(<span class=\"hljs-string\">\"https://www.cnbc.com/2018/08/30/microsoft-will-require-partners-suppliers-to-offer-paid-family-leave.html\"</span>)\n\n        article.download()\n        article.parse()\n        text = article.text\n        print(text)\n\n        print(<span class=\"hljs-string\">\" Printed text \"</span>)\n\n        <span class=\"hljs-comment\">#NLP using newspaper library</span>\n        article.nlp()\n        summary = article.summary\n        <span class=\"hljs-comment\">#print(article.summary)</span>\n        <span class=\"hljs-comment\">#print(article.keywords)</span>\n\n        print(<span class=\"hljs-string\">\" nlp on summary \"</span>)\n\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"summarizer_1\" class=\"highlights fea_summarizer\">doc = nlp(summary)</div>\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\"><span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> doc:</div>\n            print(token.text, <div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_0\" class=\"highlights fea_Part_of_Speech\">token.pos</div>, token.dep_)\n\n        print(<span class=\"hljs-string\">\" tokens \"</span>)\n\n        stockData = getStockData()\n\n        <span class=\"hljs-keyword\">return</span> Response(json.dumps(stockData), mimetype=<span class=\"hljs-string\">'application/json'</span>)\n    \n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">getStockData</span>():</span>\n\n        stockData = {\n                <span class=\"hljs-string\">\"stock\"</span>: stockName,\n                <span class=\"hljs-string\">\"score\"</span>: <span class=\"hljs-built_in\">str</span>(random.randint(<span class=\"hljs-number\">51</span>, <span class=\"hljs-number\">100</span>)),\n                <span class=\"hljs-string\">\"history\"</span>: [\n                    <span class=\"hljs-number\">82</span>,\n                    <span class=\"hljs-number\">84</span>,\n                    <span class=\"hljs-number\">81</span>,\n                    <span class=\"hljs-number\">53</span>,\n                    <span class=\"hljs-number\">74</span>,\n                    <span class=\"hljs-number\">94</span>,\n                    <span class=\"hljs-number\">65</span>\n                ]\n            }\n\n        <span class=\"hljs-keyword\">return</span> stockData\n\n<span class=\"hljs-meta\">@app.route(<span class=\"hljs-params\"><span class=\"hljs-string\">\"/_shutdown\"</span>, methods=[<span class=\"hljs-string\">\"POST\"</span>]</span>)</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">shutdown</span>():</span>\n    func = request.environ.get(<span class=\"hljs-string\">'werkzeug.server.shutdown'</span>)\n    <span class=\"hljs-keyword\">if</span> func <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"Not a werkzeug server\"</span>\n    func()\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"shutdown\"</span>\n\nstockName = <span class=\"hljs-string\">\"MSFT\"</span>\ncompanyName = <span class=\"hljs-string\">\"Microsoft\"</span>\n\n<span class=\"hljs-meta\">@app.route(<span class=\"hljs-params\"><span class=\"hljs-string\">\"/_status/readiness\"</span></span>)</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">readiness</span>():</span>\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> shuttingDown:\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"I am ready\"</span>\n        <span class=\"hljs-keyword\">else</span>:\n            abort(<span class=\"hljs-number\">500</span>, <span class=\"hljs-string\">'not ready anymore'</span>)\n\n\napp.run(port=<span class=\"hljs-number\">5420</span>)</code></pre></div>",
    "thr_37": "<div class=\"codeBlock hljs python\" id=\"thr_37\"><pre id=\"thr_37_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n<span class=\"hljs-keyword\">import</span> spacy\n<span class=\"hljs-keyword\">from</span> sklearn.feature_extraction.stop_words <span class=\"hljs-keyword\">import</span> ENGLISH_STOP_WORDS\n<span class=\"hljs-keyword\">from</span> string <span class=\"hljs-keyword\">import</span> punctuation\nnlp = spacy.load(<span class=\"hljs-string\">'en'</span>)\n\n<span class=\"hljs-comment\"># Create custom stoplist</span>\nSTOPLIST = <span class=\"hljs-built_in\">set</span>(<span class=\"hljs-built_in\">list</span>(ENGLISH_STOP_WORDS) + [<span class=\"hljs-string\">\"n't\"</span>, <span class=\"hljs-string\">\"'s\"</span>, <span class=\"hljs-string\">\"'m\"</span>, <span class=\"hljs-string\">\"ca\"</span>, <span class=\"hljs-string\">\"'\"</span>, <span class=\"hljs-string\">\"'re\"</span>])\nPUNCT_DICT = {<span class=\"hljs-built_in\">ord</span>(punc): <span class=\"hljs-literal\">None</span> <span class=\"hljs-keyword\">for</span> punc <span class=\"hljs-keyword\">in</span> punctuation <span class=\"hljs-keyword\">if</span> punc <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> [<span class=\"hljs-string\">'_'</span>, <span class=\"hljs-string\">'*'</span>]}\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">clean_article</span>(<span class=\"hljs-params\">article</span>):</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_0\" class=\"highlights fea_named_entity_recognition\">doc = nlp(article)</div>\n\n    <span class=\"hljs-comment\"># Let's merge all of the proper entities</span>\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_1\" class=\"highlights fea_named_entity_recognition\"><span class=\"hljs-keyword\">for</span> ent <span class=\"hljs-keyword\">in</span> doc.ents:\n        <span class=\"hljs-keyword\">if</span> ent.root.tag_ != <span class=\"hljs-string\">'DT'</span>:\n            ent.merge(ent.root.tag_, ent.text, ent.label_)</div>\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-comment\"># Keep entities like 'the New York Times' from getting dropped</span>\n            <div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_2\" class=\"highlights fea_named_entity_recognition\">ent.merge(ent[-<span class=\"hljs-number\">1</span>].tag_, ent.text, ent.label_)</div>\n\n    <span class=\"hljs-comment\"># Part's of speech to keep in the result</span>\n    pos_lst = [<span class=\"hljs-string\">'ADJ'</span>, <span class=\"hljs-string\">'ADV'</span>, <span class=\"hljs-string\">'NOUN'</span>, <span class=\"hljs-string\">'PROPN'</span>, <span class=\"hljs-string\">'VERB'</span>] <span class=\"hljs-comment\"># NUM?</span>\n\n    tokens = [<div style=\"background-color: #f6b73c; display: inline;\" id=\"lemmatization_0\" class=\"highlights fea_lemmatization\">token.lemma_.lower().replace(<span class=\"hljs-string\">' '</span>, <span class=\"hljs-string\">'_'</span>)</div> <span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> doc <span class=\"hljs-keyword\">if</span> <div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_0\" class=\"highlights fea_Part_of_Speech\">token.pos_</div> <span class=\"hljs-keyword\">in</span> pos_lst]\n\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">' '</span>.join(token <span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> tokens <span class=\"hljs-keyword\">if</span> token <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> STOPLIST).replace(<span class=\"hljs-string\">\"'s\"</span>, <span class=\"hljs-string\">''</span>).translate(PUNCT_DICT)\n\n\n<span class=\"hljs-keyword\">if</span> __name__==<span class=\"hljs-string\">'__main__'</span>:\n    df = pd.read_csv(<span class=\"hljs-string\">'npr_articles.csv'</span>, parse_dates=[<span class=\"hljs-string\">'date_published'</span>])\n\n    df[<span class=\"hljs-string\">'processed_text'</span>] = df[<span class=\"hljs-string\">'article_text'</span>].apply(clean_article)\n\n    df.to_csv(<span class=\"hljs-string\">'npr_articles.csv'</span>, index=<span class=\"hljs-literal\">False</span>)</code></pre></div>",
    "thr_38": "<div class=\"codeBlock hljs python\" id=\"thr_38\"><pre id=\"thr_38_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">from</span> flask <span class=\"hljs-keyword\">import</span> Flask, abort, request, Response\n<span class=\"hljs-keyword\">import</span> signal\n<span class=\"hljs-keyword\">import</span> threading\n<span class=\"hljs-keyword\">import</span> time\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">import</span> requests\n\n<span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">import</span> random\n\n<span class=\"hljs-keyword\">import</span> spacy\n<span class=\"hljs-keyword\">import</span> newspaper\n<span class=\"hljs-keyword\">from</span> newspaper <span class=\"hljs-keyword\">import</span> Article\n\n<span class=\"hljs-keyword\">import</span> en_core_web_sm\n\napp = Flask(__name__)\nshuttingDown = <span class=\"hljs-literal\">False</span>\n\n\n<span class=\"hljs-meta\">@app.route(<span class=\"hljs-params\"><span class=\"hljs-string\">\"/\"</span></span>)</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">hello</span>():</span>\n\t\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"Hello World!\"</span>\n\n<span class=\"hljs-meta\">@app.route(<span class=\"hljs-params\"><span class=\"hljs-string\">\"/news_stockdata\"</span></span>)</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">liveness</span>():</span>\n\n\tarticleLink = <span class=\"hljs-string\">\"https://www.cnbc.com/2018/08/30/microsoft-will-require-partners-suppliers-to-offer-paid-family-leave.html\"</span>\n\t\n\tprint(<span class=\"hljs-string\">\" Loading modules \"</span>)\n\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"summarizer_0\" class=\"highlights fea_summarizer\">nlp = spacy.load(<span class=\"hljs-string\">'en_core_web_sm'</span>)</div>\n\n\tprint(<span class=\"hljs-string\">\" Retrieving the article \"</span>)\n\tarticle = Article(articleLink)\n\tarticle.download()\n\tarticle.parse()\n\tprint(<span class=\"hljs-string\">\" Parsed article \"</span>)\n\n\ttext = article.text\n\tprint(<span class=\"hljs-string\">\" Printed article text \"</span>)\n\n\t<div style=\"background-color: #f6b73c; display: inline;\" id=\"summarizer_1\" class=\"highlights fea_summarizer\">print(<span class=\"hljs-string\">\" nlp on summary \"</span>)\n\tsummary = article.summary\n\tdoc = nlp(summary)</div>\n\n\t<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\"><span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> doc:</div>\n\t\tprint(token.text, <div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_0\" class=\"highlights fea_Part_of_Speech\">token.pos</div>, token.dep_)\n\n\tprint(<span class=\"hljs-string\">\" Retrieved tokens from doc \"</span>)\n\n\tstockName = <span class=\"hljs-string\">\"AAPL\"</span>\n\tstockData = getStockData(articleLink, stockName)\n\n\t<span class=\"hljs-keyword\">return</span> Response(json.dumps(stockData), mimetype=<span class=\"hljs-string\">'application/json'</span>)</code></pre></div>",
    "thr_39": "<div class=\"codeBlock hljs python\" id=\"thr_39\"><pre id=\"thr_39_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> spacy\n<span class=\"hljs-keyword\">from</span> gensim.corpora <span class=\"hljs-keyword\">import</span> Dictionary, MmCorpus\n<span class=\"hljs-keyword\">from</span> gensim <span class=\"hljs-keyword\">import</span> models\n\nprint(<span class=\"hljs-string\">'Loading models...'</span>)\n\nnlp = spacy.load(<span class=\"hljs-string\">'en_core_web_lg'</span>)\n\n<span class=\"hljs-keyword\">try</span>:\n    dictionary = Dictionary.load(<span class=\"hljs-string\">'dictionary'</span>)\n    corpus = MmCorpus(<span class=\"hljs-string\">'corpus'</span>)\n<span class=\"hljs-keyword\">except</span> Exception <span class=\"hljs-keyword\">as</span> e:\n    print(e)\n\n    print(<span class=\"hljs-string\">'Loading source data...'</span>)\n\n    texts = []\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">'ap.txt'</span>) <span class=\"hljs-keyword\">as</span> input_file:\n        texts = input_file.readlines()\n\n    print(<span class=\"hljs-string\">'Generating documents...'</span>)\n\n    documents = [[<div style=\"background-color: #f6b73c; display: inline;\" id=\"lemmatization_0\" class=\"highlights fea_lemmatization\">t.lemma_</div> <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> nlp(text) <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> t.is_stop <span class=\"hljs-keyword\">and</span> <div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_0\" class=\"highlights fea_Part_of_Speech\">t.pos_ == <span class=\"hljs-string\">'NOUN'</span></div><span class=\"hljs-string\"></span>] <span class=\"hljs-keyword\">for</span> text <span class=\"hljs-keyword\">in</span> texts]\n\n    print(<span class=\"hljs-string\">'Building dictionary...'</span>)\n\n    dictionary = Dictionary()\n    dictionary.add_documents(documents)\n\n    print(<span class=\"hljs-string\">'Postprocessing dictionary...'</span>)\n\n    dictionary.filter_extremes()\n    dictionary.compactify()\n    dictionary.save(<span class=\"hljs-string\">'dictionary'</span>)\n\n    print(<span class=\"hljs-string\">'Building corpus...'</span>)\n\n    corpus = [dictionary.doc2bow(doc) <span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> documents]\n    MmCorpus.serialize(<span class=\"hljs-string\">'corpus'</span>, corpus, id2word=dictionary)\n\nprint(<span class=\"hljs-string\">'Building models...'</span>)\n\nmdl = models.LsiModel(corpus, id2word=dictionary, num_topics=<span class=\"hljs-number\">50</span>)\nprint(<span class=\"hljs-string\">'LSI Topics'</span>)\n<span class=\"hljs-keyword\">for</span> topic <span class=\"hljs-keyword\">in</span> mdl.print_topics():\n    print(topic)\n\nmdl = models.LdaModel(corpus, id2word=dictionary, num_topics=<span class=\"hljs-number\">50</span>)\nprint(<span class=\"hljs-string\">'LDA Topics'</span>)\n<span class=\"hljs-keyword\">for</span> topic <span class=\"hljs-keyword\">in</span> mdl.print_topics():\n    print(topic)</code></pre></div>",
    "thr_41": "<div class=\"codeBlock hljs python\" id=\"thr_41\"><pre id=\"thr_41_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">import</span> LIWC\n<span class=\"hljs-keyword\">import</span> settings\n<span class=\"hljs-keyword\">import</span> spacy\n<span class=\"hljs-keyword\">import</span> subprocess\n<span class=\"hljs-keyword\">import</span> pickle\n<span class=\"hljs-keyword\">from</span> spacy.lang.en <span class=\"hljs-keyword\">import</span> English\n<span class=\"hljs-keyword\">from</span> stat_parser <span class=\"hljs-keyword\">import</span> Parser\n<span class=\"hljs-keyword\">from</span> collections <span class=\"hljs-keyword\">import</span> Counter\n<span class=\"hljs-keyword\">from</span> nltk.util <span class=\"hljs-keyword\">import</span> ngrams\n\n<span class=\"hljs-keyword\">from</span> nltk.tree <span class=\"hljs-keyword\">import</span> Tree, ParentedTree\n\nnlp = English()\nmodel = spacy.load(<span class=\"hljs-string\">'en_core_web_sm'</span>) <span class=\"hljs-comment\">#Note: Can experiment with larger corpus</span>\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">LIWC_feats</span>(<span class=\"hljs-params\">text</span>):</span>\n    <span class=\"hljs-built_in\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"lemmatization_0\" class=\"highlights fea_lemmatization\"><span class=\"hljs-built_in\">input</span> = [token.lemma_ <span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> nlp(text.lower())]</div> <span class=\"hljs-comment\"># Seems to do better as a split</span>\n\n    liwc = settings.loader.loadLIWC() <span class=\"hljs-comment\"># Caches LIWC after first load</span>\n    liwc_feats = liwc.count_matches(<span class=\"hljs-built_in\">input</span>)\n    <span class=\"hljs-keyword\">return</span> liwc_feats\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">POS_feats</span>(<span class=\"hljs-params\">text, detailed = <span class=\"hljs-literal\">True</span></span>):</span>\n    <span class=\"hljs-keyword\">if</span> detailed:\n        <span class=\"hljs-keyword\">return</span> Counter(<div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_0\" class=\"highlights fea_tagger\">token.tag_</div> <span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> model(text.lower()))\n    <span class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-keyword\">return</span> Counter(token.pos_ <span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> model(text.lower()))\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">BIPOS_feats</span>(<span class=\"hljs-params\">text, detailed = <span class=\"hljs-literal\">True</span>, combined=<span class=\"hljs-literal\">False</span></span>):</span>\n    <span class=\"hljs-keyword\">if</span> detailed:\n        bipos_list = ngrams((t.tag_ <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> model(text.lower())), <span class=\"hljs-number\">2</span>, <span class=\"hljs-literal\">True</span>, <span class=\"hljs-literal\">True</span>, <span class=\"hljs-string\">\"&lt;s&gt;\"</span>, <span class=\"hljs-string\">\"&lt;/s&gt;\"</span>)\n    <span class=\"hljs-keyword\">else</span>:\n        bipos_list = ngrams((t.pos_ <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> model(text.lower())), <span class=\"hljs-number\">2</span>, <span class=\"hljs-literal\">True</span>, <span class=\"hljs-literal\">True</span>, <span class=\"hljs-string\">\"&lt;s&gt;\"</span>, <span class=\"hljs-string\">\"&lt;/s&gt;\"</span>)\n\n    bipos_feats = Counter(<span class=\"hljs-built_in\">str</span>(gram) <span class=\"hljs-keyword\">for</span> gram <span class=\"hljs-keyword\">in</span> bipos_list)\n\n    <span class=\"hljs-keyword\">if</span> combined:\n        bipos_feats.update(POS_feats(text, detailed))\n\n    <span class=\"hljs-keyword\">return</span> bipos_feats\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">DEP_feats</span>(<span class=\"hljs-params\">text</span>):</span>\n    <span class=\"hljs-comment\"># Note: other experiments included head tokens)</span>\n    <span class=\"hljs-comment\"># return Counter((token.dep_, token.lemma_, token.head.lemma_) for token in model(text.lower()))</span>\n    <span class=\"hljs-keyword\">return</span> Counter(token.dep_ <span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> model(text.lower()))\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">trigram_featurizer</span>(<span class=\"hljs-params\">text, lemmatize=<span class=\"hljs-literal\">True</span></span>):</span>\n    feats = bigram_featurizer(text, lemmatize)\n\n    <span class=\"hljs-keyword\">if</span> lemmatize:\n        trigrams_list = ngrams((t.lemma_ <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> nlp(text.lower())), <span class=\"hljs-number\">3</span>, <span class=\"hljs-literal\">True</span>, <span class=\"hljs-literal\">True</span>, <span class=\"hljs-string\">\"&lt;s&gt;\"</span>, <span class=\"hljs-string\">\"&lt;/s&gt;\"</span>)\n    <span class=\"hljs-keyword\">else</span>:\n        trigrams_list = ngrams((t.text <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> nlp(text.lower())), <span class=\"hljs-number\">3</span>, <span class=\"hljs-literal\">True</span>, <span class=\"hljs-literal\">True</span>, <span class=\"hljs-string\">\"&lt;s&gt;\"</span>, <span class=\"hljs-string\">\"&lt;/s&gt;\"</span>)\n\n    feats.update(<span class=\"hljs-built_in\">str</span>(gram) <span class=\"hljs-keyword\">for</span> gram <span class=\"hljs-keyword\">in</span> trigrams_list)\n\n    <span class=\"hljs-keyword\">return</span> feats\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">bigram_featurizer</span>(<span class=\"hljs-params\">text, lemmatize=<span class=\"hljs-literal\">True</span></span>):</span>\n    feats = unigram_featurizer(text, lemmatize)\n\n    <span class=\"hljs-keyword\">if</span> lemmatize:\n        bigrams_list = ngrams((t.lemma_ <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> nlp(text.lower())), <span class=\"hljs-number\">2</span>, <span class=\"hljs-literal\">True</span>, <span class=\"hljs-literal\">True</span>, <span class=\"hljs-string\">\"&lt;s&gt;\"</span>, <span class=\"hljs-string\">\"&lt;/s&gt;\"</span>)\n    <span class=\"hljs-keyword\">else</span>:\n        bigrams_list = ngrams((t.text <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> nlp(text.lower())), <span class=\"hljs-number\">2</span>, <span class=\"hljs-literal\">True</span>, <span class=\"hljs-literal\">True</span>, <span class=\"hljs-string\">\"&lt;s&gt;\"</span>, <span class=\"hljs-string\">\"&lt;/s&gt;\"</span>)\n\n    <span class=\"hljs-comment\"># Porter Stemmer</span>\n    <span class=\"hljs-comment\"># bigrams_list = ngrams((stemmer.stem(t.text) for t in nlp(text.lower())), 2, True, True, \"&lt;s&gt;\", \"&lt;/s&gt;\")</span>\n\n\n    feats.update(<span class=\"hljs-built_in\">str</span>(gram) <span class=\"hljs-keyword\">for</span> gram <span class=\"hljs-keyword\">in</span> bigrams_list)\n\n    <span class=\"hljs-keyword\">return</span> feats\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">unigram_featurizer</span>(<span class=\"hljs-params\">text, lemmatize=<span class=\"hljs-literal\">True</span></span>):</span>\n    <span class=\"hljs-keyword\">if</span>(lemmatize): <span class=\"hljs-comment\"># Lemmatization</span>\n        <span class=\"hljs-keyword\">return</span> Counter(token.lemma_ <span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> nlp(text.lower()))\n    <span class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-keyword\">return</span> Counter(token.text <span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> nlp(text.lower())) \n\n    <span class=\"hljs-comment\"># return Counter(stemmer.stem(token.text) for token in nlp(text.lower())) # Porter Stemmer</span>\n    \n    <span class=\"hljs-comment\"># Stemming improves precision but hurts recall, resulting in slightly lower average f1-score</span>\n    <span class=\"hljs-comment\"># Removing stop words hurts performance</span>\n    <span class=\"hljs-comment\"># Lemmatization slightly improves performance</span></code></pre></div>",
    "thr_42": "<div class=\"codeBlock hljs python\" id=\"thr_42\"><pre id=\"thr_42_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> spacy\n<span class=\"hljs-keyword\">import</span> re\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">getstopwordlist</span>():</span>\n    <span class=\"hljs-comment\"># Spacy's Stop word list</span>\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"_0\" class=\"highlights fea_\"><span class=\"hljs-keyword\">from</span> spacy.lang.en.stop_words <span class=\"hljs-keyword\">import</span> STOP_WORDS\n    count_Spacy = <span class=\"hljs-built_in\">len</span>(STOP_WORDS)</div>\n\n    <span class=\"hljs-comment\"># Stop words from Link Assistant</span>\n    <span class=\"hljs-comment\"># https://www.link-assistant.com/seo-stop-words.html Oct 29 2017</span>\n    <span class=\"hljs-keyword\">try</span>:\n        listo = <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">\"stopwords.txt\"</span>).read()\n        listo = re.split(<span class=\"hljs-string\">\"\\s+\"</span>, listo)  <span class=\"hljs-comment\"># '/s' Matches Unicode whitespace characters; '+' for global search</span>\n    <span class=\"hljs-keyword\">except</span> IOError:\n        print(<span class=\"hljs-string\">\"Text File couldn't be opened\"</span>)\n\n    link_assistant_set = <span class=\"hljs-built_in\">set</span>()\n    link_assistant_set.update(listo)\n\n    <span class=\"hljs-comment\"># Desired List</span>\n    union_set = link_assistant_set.union(STOP_WORDS)  <span class=\"hljs-comment\"># 'set' of 683 words # include empty '' also</span>\n    <span class=\"hljs-comment\">#diff_set = link_assistant_set.difference(STOP_WORDS)</span>\n    <span class=\"hljs-keyword\">try</span>:\n        union_set.remove(<span class=\"hljs-string\">''</span>)\n    <span class=\"hljs-keyword\">except</span> KeyError:\n        print(<span class=\"hljs-string\">\"'' wasn't there in the union set.\"</span>)\n\n    <span class=\"hljs-keyword\">return</span> union_set</code></pre></div>",
    "thr_44": "<div class=\"codeBlock hljs python\" id=\"thr_44\"><pre id=\"thr_44_code\" style=\"display: block;\"><code class=\"python\"><div style=\"background-color: #f6b73c; display: inline;\" id=\"word_vectors_0\" class=\"highlights fea_word_vectors\">nlp = spacy.load(<span class=\"hljs-string\">'en_core_web_lg'</span>)\n<span class=\"hljs-built_in\">print</span>(nlp.vocab[<span class=\"hljs-string\">'banana'</span>].vector)</div>\n\nfrom scipy <span class=\"hljs-keyword\">import</span> spatial\n \ncosine_similarity = lambda x, y: <span class=\"hljs-number\">1</span> - spatial.distance.cosine(x, y)\n \n<div style=\"background-color: #f6b73c; display: inline;\" id=\"word_vectors_1\" class=\"highlights fea_word_vectors\">man = nlp.vocab[<span class=\"hljs-string\">'man'</span>].vector\nwoman = nlp.vocab[<span class=\"hljs-string\">'woman'</span>].vector\nqueen = nlp.vocab[<span class=\"hljs-string\">'queen'</span>].vector\nking = nlp.vocab[<span class=\"hljs-string\">'king'</span>].vector</div>\n \n# We now need to find the closest vector in the vocabulary to the result of <span class=\"hljs-string\">\"man\"</span> - <span class=\"hljs-string\">\"woman\"</span> + <span class=\"hljs-string\">\"queen\"</span>\nmaybe_king = man - woman + queen\ncomputed_similarities = []\n \n<span class=\"hljs-keyword\">for</span> word in nlp.vocab:\n    # Ignore words without vectors\n    <span class=\"hljs-keyword\">if</span> not word.has_vector:\n        <span class=\"hljs-keyword\">continue</span>\n \n    similarity = cosine_similarity(maybe_king, word.vector)\n    computed_similarities.<span class=\"hljs-built_in\">append</span>((word, similarity))\n \ncomputed_similarities = sorted(computed_similarities, key=lambda item: -item[<span class=\"hljs-number\">1</span>])\n<span class=\"hljs-built_in\">print</span>([w[<span class=\"hljs-number\">0</span>].text <span class=\"hljs-keyword\">for</span> w in computed_similarities[:<span class=\"hljs-number\">10</span>]])\n \n# [<span class=\"hljs-string\">'Queen'</span>, <span class=\"hljs-string\">'QUEEN'</span>, <span class=\"hljs-string\">'queen'</span>, <span class=\"hljs-string\">'King'</span>, <span class=\"hljs-string\">'KING'</span>, <span class=\"hljs-string\">'king'</span>, <span class=\"hljs-string\">'KIng'</span>, <span class=\"hljs-string\">'KINGS'</span>, <span class=\"hljs-string\">'kings'</span>, <span class=\"hljs-string\">'Kings'</span>]</code></pre></div>",
    "thr_45": "<div class=\"codeBlock hljs python\" id=\"thr_45\"><pre id=\"thr_45_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">from</span> nltk.corpus <span class=\"hljs-keyword\">import</span> wordnet <span class=\"hljs-keyword\">as</span> wn\n<span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_1\" class=\"highlights fea_tokenization\"><span class=\"hljs-keyword\">from</span> spacy.tokens <span class=\"hljs-keyword\">import</span> Token</div>\n \n \n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">penn_to_wn</span>(<span class=\"hljs-params\">tag</span>):</span>\n    <span class=\"hljs-keyword\">if</span> tag.startswith(<span class=\"hljs-string\">'N'</span>):\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">'n'</span>\n \n    <span class=\"hljs-keyword\">if</span> tag.startswith(<span class=\"hljs-string\">'V'</span>):\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">'v'</span>\n \n    <span class=\"hljs-keyword\">if</span> tag.startswith(<span class=\"hljs-string\">'J'</span>):\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">'a'</span>\n \n    <span class=\"hljs-keyword\">if</span> tag.startswith(<span class=\"hljs-string\">'R'</span>):\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">'r'</span>\n \n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-literal\">None</span>\n \n \n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">WordnetPipeline</span>(<span class=\"hljs-params\"><span class=\"hljs-built_in\">object</span></span>):</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self, nlp</span>):</span>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">Token.set_extension(<span class=\"hljs-string\">'synset'</span>, default=<span class=\"hljs-literal\">None</span>)</div>\n \n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__call__</span>(<span class=\"hljs-params\">self, doc</span>):</span>\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_2\" class=\"highlights fea_tokenization\"><span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> doc:</div>\n            wn_tag = penn_to_wn(<div style=\"background-color: #f6b73c; display: inline;\" id=\"tagger_0\" class=\"highlights fea_tagger\">token.tag_</div>)\n            <span class=\"hljs-keyword\">if</span> wn_tag <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n                <span class=\"hljs-keyword\">continue</span>\n \n            ss = wn.synsets(token.text, wn_tag)[<span class=\"hljs-number\">0</span>]\n            token._.<span class=\"hljs-built_in\">set</span>(<span class=\"hljs-string\">'synset'</span>, ss)\n \n        <span class=\"hljs-keyword\">return</span> doc\n \n \n<div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_3\" class=\"highlights fea_tokenization\">nlp = spacy.load(<span class=\"hljs-string\">'en'</span>)\nwn_pipeline = WordnetPipeline(nlp)\nnlp.add_pipe(wn_pipeline, name=<span class=\"hljs-string\">'wn_synsets'</span>)\ndoc = nlp(<span class=\"hljs-string\">\"Paris is the awesome capital of France.\"</span>)\n \n<span class=\"hljs-keyword\">for</span> token <span class=\"hljs-keyword\">in</span> doc:\n    print(token.text, <span class=\"hljs-string\">\"-\"</span>, token._.synset)</div>\n \n<span class=\"hljs-comment\"># Paris - Synset('paris.n.01')</span>\n<span class=\"hljs-comment\"># is - Synset('be.v.01')</span>\n<span class=\"hljs-comment\"># the - None</span>\n<span class=\"hljs-comment\"># awesome - Synset('amazing.s.02')</span>\n<span class=\"hljs-comment\"># capital - Synset('capital.n.01')</span>\n<span class=\"hljs-comment\"># of - None</span>\n<span class=\"hljs-comment\"># France - Synset('france.n.01')</span>\n<span class=\"hljs-comment\"># . - None</span></code></pre></div>",
    "thr_46": "<div class=\"codeBlock hljs python\" id=\"thr_46\"><pre id=\"thr_46_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">import</span> spacy\n<span class=\"hljs-keyword\">from</span> spacy.tokenizer <span class=\"hljs-keyword\">import</span> Tokenizer\n\nspecial_cases = {<span class=\"hljs-string\">\":)\"</span>: [{<span class=\"hljs-string\">\"ORTH\"</span>: <span class=\"hljs-string\">\":)\"</span>}]}\nprefix_re = re.<span class=\"hljs-built_in\">compile</span>(<span class=\"hljs-string\">r'''^[\\[\\(\"']'''</span>)\nsuffix_re = re.<span class=\"hljs-built_in\">compile</span>(<span class=\"hljs-string\">r'''[\\]\\)\"']$'''</span>)\ninfix_re = re.<span class=\"hljs-built_in\">compile</span>(<span class=\"hljs-string\">r'''[-~]'''</span>)\nsimple_url_re = re.<span class=\"hljs-built_in\">compile</span>(<span class=\"hljs-string\">r'''^https?://'''</span>)\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">custom_tokenizer</span>(<span class=\"hljs-params\">nlp</span>):</span>\n    <span class=\"hljs-keyword\">return</span> <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">Tokenizer(nlp.vocab, rules=special_cases,\n                                prefix_search=prefix_re.search,\n                                suffix_search=suffix_re.search,\n                                infix_finditer=infix_re.finditer,\n                                url_match=simple_url_re.match)\n\nnlp = spacy.load(<span class=\"hljs-string\">\"en_core_web_sm\"</span>)\nnlp.tokenizer = custom_tokenizer(nlp)\ndoc = nlp(<span class=\"hljs-string\">\"hello-world. :)\"</span>)</div>\nprint([t.text <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> doc]) </code></pre></div>",
    "thr_47": "<div class=\"codeBlock hljs python\" id=\"thr_47\"><pre id=\"thr_47_code\" style=\"display: block;\"><code class=\"python\">    <span class=\"hljs-keyword\">from</span> spacy.matcher import DependencyMatcher\n    <span class=\"hljs-keyword\">from</span> spacy.pipeline import merge_entities\n\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_0\" class=\"highlights fea_named_entity_recognition\">nlp <span class=\"hljs-operator\">=</span> spacy.load(\"en_core_web_sm\")\n    nlp.add_pipe(merge_entities)</div>\n\n    example <span class=\"hljs-operator\">=</span> [\n        [\"founded\", \"nsubj\", \"START_ENTITY\"],\n        [\"founded\", \"dobj\", \"END_ENTITY\"]\n        ]\n\n    <span class=\"hljs-keyword\">pattern</span> <span class=\"hljs-operator\">=</span> construct_pattern(example)\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"dependency_parsing_0\" class=\"highlights fea_dependency_parsing\">matcher <span class=\"hljs-operator\">=</span> DependencyMatcher(nlp.vocab)</div>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"dependency_parsing_1\" class=\"highlights fea_dependency_parsing\">matcher.add(\"pattern1\", <span class=\"hljs-keyword\">None</span>, <span class=\"hljs-keyword\">pattern</span>)\n\n    doc1 <span class=\"hljs-operator\">=</span> nlp(\"Bill Gates founded Microsoft.\")\n    doc2 <span class=\"hljs-operator\">=</span> nlp(\"Bill Gates, the Seattle Seahawks owner, founded Microsoft.\")\n\n    <span class=\"hljs-keyword\">match</span> <span class=\"hljs-operator\">=</span> matcher(doc1)[<span class=\"hljs-number\">0</span>]\n    subtree <span class=\"hljs-operator\">=</span> <span class=\"hljs-keyword\">match</span>[<span class=\"hljs-number\">1</span>][<span class=\"hljs-number\">0</span>]\n    visualise_subtrees(doc1, subtree)\n\n    <span class=\"hljs-keyword\">match</span> <span class=\"hljs-operator\">=</span> matcher(doc2)[<span class=\"hljs-number\">0</span>]\n    subtree <span class=\"hljs-operator\">=</span> <span class=\"hljs-keyword\">match</span>[<span class=\"hljs-number\">1</span>][<span class=\"hljs-number\">0</span>]\n    visualise_subtrees(doc2, subtree)</div></code></pre></div>",
    "thr_48": "<div class=\"codeBlock hljs python\" id=\"thr_48\"><pre id=\"thr_48_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-comment\">#Import the requisite library</span>\nimport spacy\n\n<span class=\"hljs-comment\">#Sample text</span>\ntext = <span class=\"hljs-string\">\"This is a sample number 5555555.\"</span>\n<span class=\"hljs-comment\">#Build upon the spaCy Small Model</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_0\" class=\"highlights fea_named_entity_recognition\">nlp = spacy.blank(<span class=\"hljs-string\">\"en\"</span>)</div>\n\n<span class=\"hljs-comment\">#Create the Ruler and Add it</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"Part_of_Speech_0\" class=\"highlights fea_Part_of_Speech\">ruler = nlp.add_pipe(<span class=\"hljs-string\">\"entity_ruler\"</span>)</div>\n\n<span class=\"hljs-comment\">#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)</span>\npatterns = [\n                {\n                    <span class=\"hljs-string\">\"label\"</span>: <span class=\"hljs-string\">\"PHONE_NUMBER\"</span>, <span class=\"hljs-string\">\"pattern\"</span>: [{<span class=\"hljs-string\">\"TEXT\"</span>: {<span class=\"hljs-string\">\"REGEX\"</span>: <span class=\"hljs-string\">\"((\\d){5})\"</span>}}\n                                                        ]\n                }\n            ]\n<span class=\"hljs-comment\">#add patterns to ruler</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_1\" class=\"highlights fea_named_entity_recognition\">ruler.add_patterns(patterns)</div>\n\n\n<span class=\"hljs-comment\">#create the doc</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_2\" class=\"highlights fea_named_entity_recognition\">doc = nlp(text)</div>\n\n<span class=\"hljs-comment\">#extract entities</span>\n<div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_3\" class=\"highlights fea_named_entity_recognition\">for ent in doc.ents:\n    print (ent.text, ent.label_)</div></code></pre></div>",
    "thr_49": "<div class=\"codeBlock hljs python\" id=\"thr_49\"><pre id=\"thr_49_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">main</span>(<span class=\"hljs-params\">model=<span class=\"hljs-literal\">None</span>, output_dir=<span class=\"hljs-string\">r'model'</span>, n_iter=<span class=\"hljs-number\">100</span></span>):</span>\n    <span class=\"hljs-string\">\"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"</span>\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_1\" class=\"highlights fea_named_entity_recognition\"><span class=\"hljs-keyword\">if</span> model <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n        nlp = spacy.load(model)  <span class=\"hljs-comment\"># load existing spaCy model</span>\n        print(<span class=\"hljs-string\">\"Loaded model '%s'\"</span> % model)\n    <span class=\"hljs-keyword\">else</span>:\n        nlp = spacy.blank(<span class=\"hljs-string\">\"en\"</span>)  <span class=\"hljs-comment\"># create blank Language class</span>\n        print(<span class=\"hljs-string\">\"Created blank 'en' model\"</span>)</div>\n\n    <span class=\"hljs-comment\"># create the built-in pipeline components and add them to the pipeline</span>\n    <span class=\"hljs-comment\"># nlp.create_pipe works for built-ins that are registered with spaCy</span>\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_0\" class=\"highlights fea_named_entity_recognition\"><span class=\"hljs-keyword\">i</span><span class=\"hljs-keyword\"></span><span class=\"hljs-keyword\">f</span> <span class=\"hljs-string\">\"ner\"</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> nlp.pipe_names:\n        ner = nlp.create_pipe(<span class=\"hljs-string\">\"ner\"</span>)\n        nlp.add_pipe(ner, last=<span class=\"hljs-literal\">True</span>)\n    <span class=\"hljs-comment\"># otherwise, get it so we can add labels</span>\n    <span class=\"hljs-keyword\">else</span>:\n        ner = nlp.get_pipe(<span class=\"hljs-string\">\"ner\"</span>)</div>\n\n    <span class=\"hljs-comment\"># add labels</span>\n    <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_3\" class=\"highlights fea_named_entity_recognition\"><span class=\"hljs-keyword\">for</span> _, annotations <span class=\"hljs-keyword\">in</span> TRAIN_DATA:\n        <span class=\"hljs-keyword\">for</span> ent <span class=\"hljs-keyword\">in</span> annotations.get(<span class=\"hljs-string\">\"entities\"</span>):\n            ner.add_label(ent[<span class=\"hljs-number\">2</span>])</div>\n\n    <span class=\"hljs-comment\"># get names of other pipes to disable them during training</span>\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_2\" class=\"highlights fea_named_entity_recognition\">other_pipes = [pipe <span class=\"hljs-keyword\">for</span> pipe <span class=\"hljs-keyword\">in</span> nlp.pipe_names <span class=\"hljs-keyword\">if</span> pipe != <span class=\"hljs-string\">\"ner\"</span>]\n    <span class=\"hljs-keyword\">with</span> nlp.disable_pipes(*other_pipes):</div>  <span class=\"hljs-comment\"># only train NER</span>\n        <span class=\"hljs-comment\"># reset and initialize the weights randomly – but only if we're</span>\n        <span class=\"hljs-comment\"># training a new model</span>\n        <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_4\" class=\"highlights fea_named_entity_recognition\"><span class=\"hljs-keyword\">if</span> model <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n            nlp.begin_training()\n        <span class=\"hljs-keyword\">for</span> itn <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(n_iter):\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            <span class=\"hljs-comment\"># batch up the examples using spaCy's minibatch</span>\n            batches = minibatch(TRAIN_DATA, size=compounding(<span class=\"hljs-number\">4.0</span>, <span class=\"hljs-number\">32.0</span>, <span class=\"hljs-number\">1.001</span>))\n            <span class=\"hljs-keyword\">for</span> batch <span class=\"hljs-keyword\">in</span> batches:\n                texts, annotations = <span class=\"hljs-built_in\">zip</span>(*batch)\n                nlp.update(\n                    texts,  <span class=\"hljs-comment\"># batch of texts</span>\n                    annotations,  <span class=\"hljs-comment\"># batch of annotations</span>\n                    drop=<span class=\"hljs-number\">0.5</span>,  <span class=\"hljs-comment\"># dropout - make it harder to memorise data</span>\n                    losses=losses,\n                )\n            print(<span class=\"hljs-string\">\"Losses\"</span>, losses)</div>\n\n    <span class=\"hljs-comment\"># test the trained model</span>\n    <span class=\"hljs-keyword\">for</span> text, _ <span class=\"hljs-keyword\">in</span> TRAIN_DATA:\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"named_entity_recognition_5\" class=\"highlights fea_named_entity_recognition\">doc = nlp(text)\n        print(<span class=\"hljs-string\">\"Entities\"</span>, [(ent.text, ent.label_) <span class=\"hljs-keyword\">for</span> ent <span class=\"hljs-keyword\">in</span> doc.ents])</div>\n        <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">print(<span class=\"hljs-string\">\"Tokens\"</span>, [(t.text, t.ent_type_, t.ent_iob) <span class=\"hljs-keyword\">for</span> t <span class=\"hljs-keyword\">in</span> doc])</div>\n\n    <span class=\"hljs-comment\"># save model to output directory</span>\n    <span class=\"hljs-keyword\">if</span> output_dir <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n        output_dir = Path(output_dir)\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> output_dir.exists():\n            output_dir.mkdir()\n        nlp.to_disk(output_dir)\n        print(<span class=\"hljs-string\">\"Saved model to\"</span>, output_dir)</code></pre></div>",
    "thr_50": "<div class=\"codeBlock hljs python\" id=\"thr_50\"><pre id=\"thr_50_code\" style=\"display: block;\"><code class=\"python\"><span class=\"hljs-keyword\">from</span> typing <span class=\"hljs-keyword\">import</span> Optional, List, Tuple\n<span class=\"hljs-keyword\">from</span> enum <span class=\"hljs-keyword\">import</span> Enum, unique\n<span class=\"hljs-keyword\">from</span> common.ml <span class=\"hljs-keyword\">import</span> one_hot, MLDataPreprocessor\n<span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\">from</span> spacy.tokens <span class=\"hljs-keyword\">import</span> Token, Doc\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">create_nlp_instance</span>():</span>\n    <span class=\"hljs-keyword\">import</span> spacy\n    <span class=\"hljs-keyword\">from</span> spacymoji <span class=\"hljs-keyword\">import</span> Emoji\n\n    nlp = spacy.load(<span class=\"hljs-string\">'en'</span>)\n    emoji_pipe = Emoji(nlp)\n    nlp.add_pipe(emoji_pipe, first=<span class=\"hljs-literal\">True</span>)\n\n    <span class=\"hljs-comment\"># Merge hashtag tokens which were split by spacy</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">hashtag_pipe</span>(<span class=\"hljs-params\">doc</span>):</span>\n        merged_hashtag = <span class=\"hljs-literal\">False</span>\n        <span class=\"hljs-keyword\">while</span> <span class=\"hljs-literal\">True</span>:\n            <span class=\"hljs-keyword\"></span><div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_1\" class=\"highlights fea_tokenization\"><span class=\"hljs-keyword\">for</span> token_index, token <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(doc):\n                <span class=\"hljs-keyword\">if</span> token.text == <span class=\"hljs-string\">'#'</span>:\n                    <span class=\"hljs-keyword\">if</span> token.head <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n                        start_index = token.idx\n                        end_index = start_index + <span class=\"hljs-built_in\">len</span>(token.head.text) + <span class=\"hljs-number\">1</span>\n                        <span class=\"hljs-keyword\">if</span> doc.merge(start_index, end_index) <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n                            merged_hashtag = <span class=\"hljs-literal\">True</span>\n                            <span class=\"hljs-keyword\">break</span></div><span class=\"hljs-keyword\"></span>\n            <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> merged_hashtag:\n                <span class=\"hljs-keyword\">break</span>\n            merged_hashtag = <span class=\"hljs-literal\">False</span>\n        <span class=\"hljs-keyword\">return</span> doc\n\n    <div style=\"background-color: #f6b73c; display: inline;\" id=\"tokenization_0\" class=\"highlights fea_tokenization\">nlp.add_pipe(hashtag_pipe)</div>\n    <span class=\"hljs-keyword\">return</span> nlp</code></pre></div>"
  },
  "file concepts": {
    "fir_4": [
      "fea_dependency_parsing",
      "fea_Part_of_Speech",
      "fea_lemmatization",
      "fea_tokenization",
      "cat_Preprocessing",
      "fea_parsing",
      "cat_Basic_Analysis",
      "fea_tagger",
      "fea_named_entity_recognition"
    ],
    "fir_16": [
      "fea_Part_of_Speech",
      "fea_tokenization",
      "cat_Preprocessing",
      "fea_parsing",
      "cat_Basic_Analysis"
    ],
    "fir_17": [
      "fea_Part_of_Speech",
      "fea_tokenization",
      "cat_Preprocessing",
      "fea_parsing",
      "cat_Basic_Analysis"
    ],
    "fir_23": [
      "fea_Part_of_Speech",
      "fea_tokenization",
      "cat_Preprocessing",
      "fea_parsing",
      "cat_Basic_Analysis"
    ],
    "fir_1": [
      "fea_lemmatization",
      "fea_tokenization",
      "fea_nlp_datasets",
      "cat_Preprocessing"
    ],
    "fir_2": [
      "fea_tokenization",
      "fea_nlp_datasets",
      "fea_stemming",
      "cat_Preprocessing"
    ],
    "fir_3": [
      "fea_summarizer",
      "fea_lemmatization",
      "fea_tokenization",
      "fea_nlp_datasets",
      "cat_Advanced_Analysis",
      "cat_Preprocessing"
    ],
    "fir_6": [
      "fea_n_grams",
      "fea_Part_of_Speech",
      "fea_lemmatization",
      "fea_tokenization",
      "cat_Preprocessing",
      "cat_Basic_Analysis",
      "fea_tagger"
    ],
    "fir_8": [
      "fea_summarizer",
      "fea_stemming",
      "fea_tokenization",
      "fea_nlp_datasets",
      "cat_Advanced_Analysis",
      "cat_Preprocessing",
      "cat_Basic_Analysis",
      "fea_text_scoring"
    ],
    "fir_9": [
      "fea_lemmatization",
      "fea_tokenization",
      "fea_nlp_datasets",
      "cat_Advanced_Analysis",
      "cat_Preprocessing",
      "fea_chatbot"
    ],
    "fir_10": [
      "fea_lemmatization",
      "fea_tokenization",
      "cat_Preprocessing"
    ],
    "fir_11": [
      "fea_tokenization",
      "fea_Part_of_Speech",
      "cat_Basic_Analysis",
      "cat_Preprocessing"
    ],
    "fir_12": [
      "fea_Part_of_Speech",
      "fea_lemmatization",
      "fea_tokenization",
      "fea_nlp_datasets",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "fir_13": [
      "fea_tokenization",
      "fea_nlp_datasets",
      "cat_Preprocessing"
    ],
    "fir_14": [
      "fea_lemmatization",
      "fea_tokenization",
      "fea_nlp_datasets",
      "cat_Preprocessing"
    ],
    "fir_18": [
      "fea_tokenization",
      "cat_Preprocessing"
    ],
    "fir_19": [
      "fea_text_similarity",
      "fea_tokenization",
      "cat_Basic_Analysis",
      "cat_Preprocessing"
    ],
    "fir_20": [
      "fea_stemming",
      "fea_tokenization",
      "fea_nlp_datasets",
      "cat_Preprocessing",
      "cat_Basic_Analysis",
      "fea_text_scoring"
    ],
    "fir_21": [
      "fea_summarizer",
      "fea_stemming",
      "fea_tokenization",
      "fea_nlp_datasets",
      "cat_Advanced_Analysis",
      "cat_Preprocessing",
      "cat_Basic_Analysis",
      "fea_text_scoring"
    ],
    "fir_25": [
      "fea_lemmatization",
      "fea_tokenization",
      "cat_Preprocessing"
    ],
    "fir_29": [
      "fea_lemmatization",
      "fea_tokenization",
      "fea_nlp_datasets",
      "cat_Preprocessing",
      "cat_Basic_Analysis",
      "fea_text_scoring"
    ],
    "fir_30": [
      "fea_classification",
      "fea_Part_of_Speech",
      "fea_lemmatization",
      "fea_tokenization",
      "fea_nlp_datasets",
      "cat_Advanced_Analysis",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "fir_24": [
      "fea_word_frequency",
      "fea_classification",
      "fea_Part_of_Speech",
      "fea_lemmatization",
      "fea_nlp_datasets",
      "cat_Advanced_Analysis",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "fir_5": [
      "fea_regular_expression",
      "fea_nlp_datasets",
      "cat_Preprocessing"
    ],
    "fir_7": [
      "fea_nlp_datasets",
      "cat_Preprocessing"
    ],
    "fir_28": [
      "fea_classification",
      "fea_sentiment_analysis",
      "fea_nlp_datasets",
      "cat_Advanced_Analysis",
      "cat_Preprocessing"
    ],
    "fir_22": [
      "fea_classification",
      "fea_word_frequency",
      "cat_Advanced_Analysis",
      "cat_Preprocessing"
    ],
    "fir_27": [
      "fea_classification",
      "fea_word_frequency",
      "cat_Advanced_Analysis",
      "cat_Preprocessing",
      "fea_tagger"
    ],
    "fir_26": [
      "fea_n_grams",
      "cat_Preprocessing"
    ],
    "fir_15": [
      "fea_classification",
      "cat_Advanced_Analysis"
    ],
    "sec_7": [
      "fea_parsing",
      "cat_Preprocessing"
    ],
    "sec_10": [
      "fea_parsing",
      "fea_tagger",
      "cat_Preprocessing"
    ],
    "sec_2": [
      "fea_Part_of_Speech",
      "fea_text_simplify",
      "fea_lemmatization",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "sec_12": [
      "fea_language_detection",
      "fea_translation",
      "fea_spellcheck",
      "fea_lemmatization",
      "cat_Advanced_Analysis",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "sec_6": [
      "cat_Basic_Analysis",
      "fea_Part_of_Speech",
      "fea_tagger",
      "cat_Preprocessing"
    ],
    "sec_9": [
      "fea_text_simplify",
      "fea_classification",
      "cat_Advanced_Analysis",
      "cat_Preprocessing"
    ],
    "sec_27": [
      "fea_text_simplify",
      "fea_n_grams",
      "cat_Preprocessing"
    ],
    "sec_1": [
      "fea_n_grams",
      "cat_Preprocessing"
    ],
    "sec_8": [
      "fea_n_grams",
      "cat_Preprocessing"
    ],
    "sec_17": [
      "fea_spellcheck",
      "cat_Basic_Analysis"
    ],
    "sec_26": [
      "fea_sentiment_analysis",
      "cat_Basic_Analysis",
      "fea_text_scoring",
      "cat_Advanced_Analysis"
    ],
    "sec_29": [
      "fea_sentiment_analysis",
      "cat_Basic_Analysis",
      "fea_text_scoring",
      "cat_Advanced_Analysis"
    ],
    "sec_15": [
      "fea_sentiment_analysis",
      "cat_Advanced_Analysis"
    ],
    "sec_20": [
      "fea_sentiment_analysis",
      "fea_classification",
      "cat_Advanced_Analysis"
    ],
    "sec_21": [
      "fea_sentiment_analysis",
      "cat_Advanced_Analysis"
    ],
    "sec_24": [
      "fea_sentiment_analysis",
      "cat_Advanced_Analysis"
    ],
    "sec_25": [
      "fea_sentiment_analysis",
      "cat_Advanced_Analysis"
    ],
    "sec_28": [
      "fea_sentiment_analysis",
      "cat_Advanced_Analysis"
    ],
    "sec_30": [
      "fea_sentiment_analysis",
      "cat_Advanced_Analysis"
    ],
    "sec_14": [
      "fea_translation",
      "cat_Advanced_Analysis"
    ],
    "sec_16": [
      "fea_translation",
      "cat_Advanced_Analysis"
    ],
    "sec_19": [
      "fea_language_detection",
      "fea_translation",
      "cat_Advanced_Analysis"
    ],
    "sec_22": [
      "fea_translation",
      "cat_Advanced_Analysis"
    ],
    "sec_23": [
      "fea_language_detection",
      "fea_translation",
      "cat_Advanced_Analysis"
    ],
    "sec_11": [
      "fea_language_detection",
      "cat_Advanced_Analysis"
    ],
    "sec_18": [
      "fea_language_detection",
      "cat_Advanced_Analysis"
    ],
    "sec_4": [
      "fea_classification",
      "cat_Advanced_Analysis"
    ],
    "sec_5": [
      "fea_classification",
      "cat_Advanced_Analysis"
    ],
    "sec_13": [
      "fea_classification",
      "cat_Advanced_Analysis"
    ],
    "thr_17": [
      "fea_word_vectors",
      "fea_classification",
      "fea_lemmatization",
      "fea_nlp_datasets",
      "cat_Advanced_Analysis",
      "cat_Preprocessing",
      "fea_parsing"
    ],
    "thr_10": [
      "fea_text_similarity",
      "fea_tokenization",
      "cat_Basic_Analysis",
      "cat_Preprocessing"
    ],
    "thr_12": [
      "fea_Part_of_Speech",
      "fea_lemmatization",
      "fea_tokenization",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "thr_18": [
      "fea_tokenization",
      "cat_Basic_Analysis",
      "fea_named_entity_recognition",
      "cat_Preprocessing"
    ],
    "thr_20": [
      "fea_tokenization",
      "cat_Preprocessing"
    ],
    "thr_27": [
      "fea_regular_expression",
      "fea_text_simplify",
      "fea_tokenization",
      "cat_Preprocessing"
    ],
    "thr_22": [
      "fea_lemmatization",
      "cat_Preprocessing"
    ],
    "thr_26": [
      "fea_lemmatization",
      "cat_Preprocessing"
    ],
    "thr_13": [
      "fea_word_vectors",
      "cat_Preprocessing"
    ],
    "thr_5": [
      "fea_nlp_datasets",
      "cat_Preprocessing"
    ],
    "thr_6": [
      "fea_summarizer",
      "fea_nlp_datasets",
      "cat_Advanced_Analysis",
      "cat_Preprocessing"
    ],
    "thr_25": [
      "fea_word_frequency",
      "cat_Preprocessing"
    ],
    "thr_24": [
      "fea_n_grams",
      "fea_Part_of_Speech",
      "cat_Preprocessing",
      "cat_Basic_Analysis",
      "fea_tagger"
    ],
    "thr_11": [
      "fea_n_grams",
      "cat_Preprocessing"
    ],
    "thr_19": [
      "fea_n_grams",
      "cat_Preprocessing"
    ],
    "thr_8": [
      "fea_spellcheck",
      "cat_Basic_Analysis"
    ],
    "thr_7": [
      "fea_sentiment_analysis",
      "cat_Basic_Analysis",
      "fea_text_scoring",
      "cat_Advanced_Analysis"
    ],
    "thr_9": [
      "fea_language_detection",
      "cat_Advanced_Analysis"
    ],
    "thr_1": [
      "fea_classification",
      "fea_Neural_Network_Models",
      "fea_tokenization",
      "cat_Advanced_Analysis",
      "cat_Preprocessing"
    ],
    "thr_2": [
      "fea_Neural_Network_Models",
      "fea_tokenization",
      "cat_Advanced_Analysis",
      "cat_Preprocessing"
    ],
    "thr_3": [
      "fea_Neural_Network_Models",
      "fea_lemmatization",
      "fea_tokenization",
      "cat_Advanced_Analysis",
      "cat_Preprocessing"
    ],
    "thr_4": [
      "fea_Neural_Network_Models",
      "fea_dependency_parsing",
      "cat_Basic_Analysis",
      "cat_Advanced_Analysis"
    ],
    "thr_14": [
      "fea_Neural_Network_Models",
      "fea_tokenization",
      "cat_Advanced_Analysis",
      "cat_Preprocessing",
      "cat_Basic_Analysis",
      "fea_named_entity_recognition"
    ],
    "thr_15": [
      "fea_Neural_Network_Models",
      "fea_classification",
      "cat_Advanced_Analysis"
    ],
    "fir_31": [
      "fea_tokenization",
      "fea_nlp_datasets",
      "cat_Preprocessing"
    ],
    "fir_32": [
      "fea_classification",
      "fea_Part_of_Speech",
      "fea_tagger",
      "fea_chunking",
      "cat_Basic_Analysis",
      "cat_Preprocessing",
      "cat_Advanced_Analysis"
    ],
    "fir_33": [
      "fea_tokenization",
      "fea_nlp_datasets",
      "fea_text_frequency",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "fir_34": [
      "fea_tokenization",
      "fea_nlp_datasets",
      "fea_tagger",
      "cat_Preprocessing"
    ],
    "fir_35": [
      "fea_nlp_datasets",
      "fea_tagger",
      "fea_text_frequency",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "fir_36": [
      "fea_clustering",
      "fea_nlp_datasets",
      "fea_text_frequency",
      "cat_Preprocessing",
      "cat_Basic_Analysis",
      "cat_Advanced_Analysis"
    ],
    "fir_37": [
      "fea_Part_of_Speech",
      "fea_lemmatization",
      "fea_tokenization",
      "fea_tree",
      "fea_parsing",
      "cat_Preprocessing",
      "cat_Advanced_Analysis",
      "cat_Basic_Analysis"
    ],
    "fir_38": [
      "fea_nlp_datasets",
      "fea_stemming",
      "cat_Preprocessing"
    ],
    "fir_39": [
      "fea_nlp_datasets",
      "cat_Preprocessing"
    ],
    "fir_40": [
      "fea_tree",
      "fea_tokenization",
      "fea_parsing",
      "cat_Advanced_Analysis",
      "cat_Preprocessing"
    ],
    "fir_41": [
      "fea_tokenization",
      "fea_nlp_datasets",
      "fea_text_frequency",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "fir_42": [
      "fea_text_similarity",
      "fea_lemmatization",
      "fea_tokenization",
      "fea_text_frequency",
      "cat_Basic_Analysis",
      "cat_Preprocessing"
    ],
    "fir_43": [
      "fea_tokenization",
      "fea_nlp_datasets",
      "fea_stemming",
      "cat_Preprocessing"
    ],
    "fir_44": [
      "fea_n_grams",
      "fea_nlp_datasets",
      "fea_Part_of_Speech",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "fir_45": [
      "fea_classification",
      "fea_tokenization",
      "fea_Part_of_Speech",
      "cat_Preprocessing",
      "cat_Basic_Analysis",
      "cat_Advanced_Analysis"
    ],
    "fir_46": [
      "fea_classification",
      "cat_Advanced_Analysis"
    ],
    "fir_47": [
      "fea_tokenization",
      "fea_n_grams",
      "fea_text_frequency",
      "cat_Basic_Analysis",
      "cat_Preprocessing"
    ],
    "fir_48": [
      "fea_tokenization",
      "fea_nlp_datasets",
      "fea_Part_of_Speech",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "fir_49": [
      "fea_nlp_datasets",
      "fea_tagger",
      "fea_Part_of_Speech",
      "fea_chunking",
      "cat_Basic_Analysis",
      "cat_Preprocessing"
    ],
    "fir_50": [
      "fea_nlp_datasets",
      "fea_text_frequency",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "sec_31": [
      "fea_spellcheck",
      "fea_text_segmentation",
      "fea_language_detection",
      "fea_lemmatization",
      "cat_Basic_Analysis",
      "cat_Preprocessing",
      "cat_Advanced_Analysis"
    ],
    "sec_32": [
      "fea_language_detection",
      "fea_sentiment_analysis",
      "fea_text_simplify",
      "cat_Advanced_Analysis",
      "cat_Preprocessing"
    ],
    "sec_33": [
      "fea_sentiment_analysis",
      "fea_translation",
      "fea_lemmatization",
      "cat_Advanced_Analysis",
      "cat_Preprocessing"
    ],
    "sec_34": [
      "fea_tokenization",
      "fea_tagger",
      "cat_Preprocessing"
    ],
    "sec_35": [
      "fea_tagger",
      "cat_Preprocessing"
    ],
    "sec_36": [
      "fea_classification",
      "cat_Advanced_Analysis"
    ],
    "sec_37": [
      "fea_sentiment_analysis",
      "fea_tokenization",
      "cat_Advanced_Analysis",
      "cat_Preprocessing"
    ],
    "sec_38": [
      "fea_tagger",
      "cat_Preprocessing"
    ],
    "sec_39": [
      "fea_tokenization",
      "fea_tagger",
      "cat_Preprocessing"
    ],
    "sec_40": [
      "fea_tagger",
      "cat_Preprocessing"
    ],
    "sec_41": [
      "fea_tagger",
      "cat_Preprocessing"
    ],
    "sec_42": [
      "fea_sentiment_analysis",
      "cat_Advanced_Analysis"
    ],
    "sec_43": [
      "fea_sentiment_analysis",
      "cat_Advanced_Analysis"
    ],
    "sec_44": [
      "fea_classification",
      "cat_Advanced_Analysis"
    ],
    "sec_45": [
      "fea_chunking",
      "cat_Basic_Analysis"
    ],
    "sec_46": [
      "fea_language_detection",
      "fea_translation",
      "cat_Advanced_Analysis"
    ],
    "sec_47": [
      "fea_sentiment_analysis",
      "cat_Advanced_Analysis"
    ],
    "sec_48": [
      "fea_sentiment_analysis",
      "fea_tokenization",
      "cat_Advanced_Analysis",
      "cat_Preprocessing"
    ],
    "sec_49": [
      "fea_sentiment_analysis",
      "cat_Advanced_Analysis"
    ],
    "sec_50": [
      "fea_sentiment_analysis",
      "fea_tokenization",
      "cat_Advanced_Analysis",
      "cat_Preprocessing"
    ],
    "thr_31": [
      "fea_named_entity_recognition",
      "cat_Basic_Analysis"
    ],
    "thr_32": [
      "fea_named_entity_recognition",
      "cat_Basic_Analysis"
    ],
    "thr_33": [
      "fea_lemmatization",
      "cat_Preprocessing"
    ],
    "thr_34": [
      "fea_tokenization",
      "fea_Part_of_Speech",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "thr_35": [
      "fea_named_entity_recognition",
      "cat_Basic_Analysis"
    ],
    "thr_36": [
      "fea_tokenization",
      "fea_Part_of_Speech",
      "fea_summarizer",
      "cat_Advanced_Analysis",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "thr_37": [
      "fea_lemmatization",
      "fea_Part_of_Speech",
      "fea_named_entity_recognition",
      "cat_Basic_Analysis",
      "cat_Preprocessing"
    ],
    "thr_38": [
      "fea_tokenization",
      "fea_Part_of_Speech",
      "fea_summarizer",
      "cat_Advanced_Analysis",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "thr_39": [
      "fea_lemmatization",
      "fea_Part_of_Speech",
      "cat_Preprocessing",
      "cat_Basic_Analysis"
    ],
    "thr_41": [
      "fea_lemmatization",
      "fea_tagger",
      "cat_Preprocessing"
    ],
    "thr_42": [
      "fea_",
      null
    ],
    "thr_44": [
      "fea_word_vectors",
      "cat_Preprocessing"
    ],
    "thr_45": [
      "fea_tokenization",
      "fea_tagger",
      "cat_Preprocessing"
    ],
    "thr_46": [
      "fea_tokenization",
      "cat_Preprocessing"
    ],
    "thr_47": [
      "fea_dependency_parsing",
      "fea_named_entity_recognition",
      "cat_Basic_Analysis"
    ],
    "thr_48": [
      "fea_Part_of_Speech",
      "fea_named_entity_recognition",
      "cat_Basic_Analysis"
    ],
    "thr_49": [
      "fea_tokenization",
      "fea_named_entity_recognition",
      "cat_Basic_Analysis",
      "cat_Preprocessing"
    ],
    "thr_50": [
      "fea_tokenization",
      "cat_Preprocessing"
    ]
  },
  "file info": {
    "fir_4": {
      "source": "https://github.com/rameshjes/Semantic-Textual-Similarity/blob/master/monolingualWordAligner/nltkUtil",
      "nlines": 427,
      "nchara": 30785
    },
    "fir_16": {
      "source": "https://github.com/nityansuman/marvin/blob/main/src/subjective",
      "nlines": 186,
      "nchara": 11394
    },
    "fir_17": {
      "source": "https://github.com/nityansuman/marvin/blob/main/src/objective",
      "nlines": 215,
      "nchara": 14346
    },
    "fir_23": {
      "source": "https://github.com/alexgreene/WikiQuiz/blob/master/python/Article",
      "nlines": 119,
      "nchara": 9583
    },
    "fir_1": {
      "source": "https://github.com/dhyeythumar/Search-Engine/blob/master/Python_scripts/text_processing",
      "nlines": 62,
      "nchara": 3897
    },
    "fir_2": {
      "source": "https://github.com/maneeshavinayak/Clustering-News-Headlines/blob/master/clustering/hierarchical",
      "nlines": 61,
      "nchara": 5381
    },
    "fir_3": {
      "source": "https://github.com/codequipo/TheDailyNews/blob/master/flask_server/app",
      "nlines": 230,
      "nchara": 18194
    },
    "fir_6": {
      "source": "https://github.com/nickduran/align-linguistic-alignment/blob/master/align/prepare_transcripts",
      "nlines": 542,
      "nchara": 44964
    },
    "fir_8": {
      "source": "https://github.com/akashp1712/summarize-webpage/blob/master/implementation/word_frequency_summarize_parser",
      "nlines": 169,
      "nchara": 40050
    },
    "fir_9": {
      "source": "https://github.com/LuciaLlavero/ryuzaki_bot/blob/master/ryuzaki_bot_desktop",
      "nlines": 64,
      "nchara": 6301
    },
    "fir_10": {
      "source": "https://github.com/LuciaLlavero/ryuzaki_bot/blob/master/ryuzaki_bot",
      "nlines": 64,
      "nchara": 6077
    },
    "fir_11": {
      "source": "https://github.com/somnathrakshit/geograpy3/blob/master/geograpy/extraction",
      "nlines": 80,
      "nchara": 4754
    },
    "fir_12": {
      "source": "https://github.com/rocketk/wordcounter/blob/master/wordcounter/word_counter",
      "nlines": 108,
      "nchara": 8623
    },
    "fir_13": {
      "source": "https://github.com/kethort/TwitterLDATopicModeling/blob/master/src/tweets_on_LDA",
      "nlines": 158,
      "nchara": 17324
    },
    "fir_14": {
      "source": "https://github.com/kethort/TwitterLDATopicModeling/blob/master/src/create_LDA_model",
      "nlines": 160,
      "nchara": 17858
    },
    "fir_18": {
      "source": "https://github.com/daltonfury42/truecase/blob/master/truecase/TrueCaser",
      "nlines": 178,
      "nchara": 13017
    },
    "fir_19": {
      "source": "https://github.com/akashp1712/nlp-akash/blob/master/text-summarization/text_rank_sentences",
      "nlines": 180,
      "nchara": 12899
    },
    "fir_20": {
      "source": "https://github.com/akashp1712/nlp-akash/blob/master/text-summarization/TF_IDF_Summarization",
      "nlines": 242,
      "nchara": 18329
    },
    "fir_21": {
      "source": "https://github.com/akashp1712/nlp-akash/blob/master/text-summarization/Word_Frequency_Summarization",
      "nlines": 144,
      "nchara": 13685
    },
    "fir_25": {
      "source": "",
      "nlines": 74,
      "nchara": 6426
    },
    "fir_29": {
      "source": "https://github.com/weblineindia/AIML-NLP-Text-Scoring/blob/master/scoring",
      "nlines": 119,
      "nchara": 9361
    },
    "fir_30": {
      "source": "https://github.com/gregyjames/twitter-stock-sentiment/blob/main/main",
      "nlines": 439,
      "nchara": 34069
    },
    "fir_24": {
      "source": "https://github.com/g-paras/sentiment-analysis-api/blob/master/model_nltk",
      "nlines": 123,
      "nchara": 8968
    },
    "fir_5": {
      "source": "https://github.com/vasisouv/tweets-preprocessor/blob/master/twitter_preprocessor",
      "nlines": 143,
      "nchara": 12738
    },
    "fir_7": {
      "source": "https://github.com/emdaniels/character-extraction/blob/master/characterExtraction",
      "nlines": 215,
      "nchara": 13964
    },
    "fir_28": {
      "source": "",
      "nlines": 50,
      "nchara": 4219
    },
    "fir_22": {
      "source": "https://github.com/mertkahyaoglu/twitter-sentiment-analysis/blob/master/classify",
      "nlines": 37,
      "nchara": 2671
    },
    "fir_27": {
      "source": "",
      "nlines": 33,
      "nchara": 2733
    },
    "fir_26": {
      "source": "",
      "nlines": 40,
      "nchara": 4039
    },
    "fir_15": {
      "source": "https://github.com/arne-cl/nltk-maxent-pos-tagger/blob/master/mxpost",
      "nlines": 435,
      "nchara": 25111
    },
    "sec_7": {
      "source": "https://textblob.readthedocs.io/en/dev/advanced_usage.html",
      "nlines": 5,
      "nchara": 683
    },
    "sec_10": {
      "source": "https://github.com/huudangdev/generator-question-textblob-nlp/blob/master/app",
      "nlines": 162,
      "nchara": 16542
    },
    "sec_2": {
      "source": "https://stackabuse.com/python-for-nlp-introduction-to-the-textblob-library/",
      "nlines": 21,
      "nchara": 1792
    },
    "sec_12": {
      "source": "https://github.com/dpasch01/textblob-service/blob/master/textblob-service",
      "nlines": 75,
      "nchara": 5423
    },
    "sec_6": {
      "source": "https://textblob.readthedocs.io/en/dev/advanced_usage.html",
      "nlines": 6,
      "nchara": 720
    },
    "sec_9": {
      "source": "https://github.com/arif-zaman/airplane-crash/blob/master/Airplane.ipynb",
      "nlines": 50,
      "nchara": 5532
    },
    "sec_27": {
      "source": "https://github.com/platisd/bad-commit-message-blocker/blob/master/bad_commit_message_blocker",
      "nlines": 98,
      "nchara": 7517
    },
    "sec_1": {
      "source": "https://stackabuse.com/python-for-nlp-introduction-to-the-textblob-library/",
      "nlines": 21,
      "nchara": 2388
    },
    "sec_8": {
      "source": "https://textblob.readthedocs.io/en/dev/advanced_usage.html",
      "nlines": 6,
      "nchara": 671
    },
    "sec_17": {
      "source": "https://github.com/OjasBarawal/Spell-Checker/blob/main/app",
      "nlines": 20,
      "nchara": 1373
    },
    "sec_26": {
      "source": "https://github.com/stepthom/textblob-sentiment-analysis/blob/master/doAnalysis",
      "nlines": 147,
      "nchara": 13678
    },
    "sec_29": {
      "source": "https://github.com/shirosaidev/stocksight/blob/master/sentiment",
      "nlines": 111,
      "nchara": 8876
    },
    "sec_15": {
      "source": "https://github.com/quantumsnowball/AppleDaily20200907/blob/master/sentiment",
      "nlines": 48,
      "nchara": 5194
    },
    "sec_20": {
      "source": "https://github.com/cyschneck/Billy-Bot/blob/master/shakespeare_sentiment",
      "nlines": 80,
      "nchara": 6910
    },
    "sec_21": {
      "source": "https://github.com/avaiyang/Movie-Rating-and-Prediction-Model/blob/master/twittersearch",
      "nlines": 61,
      "nchara": 3753
    },
    "sec_24": {
      "source": "https://github.com/g-paras/sentiment-analysis-api/blob/master/app",
      "nlines": 127,
      "nchara": 9000
    },
    "sec_25": {
      "source": "https://github.com/cosimoiaia/Facebook-Sentiment-Analysis/blob/master/simple_facebook_sentiment_analysis",
      "nlines": 52,
      "nchara": 3712
    },
    "sec_28": {
      "source": "https://github.com/vinitshahdeo/jobtweets/blob/master/jobtweets",
      "nlines": 52,
      "nchara": 3472
    },
    "sec_30": {
      "source": "https://github.com/the-javapocalypse/Twitter-Sentiment-Analysis/blob/master/main",
      "nlines": 147,
      "nchara": 13371
    },
    "sec_14": {
      "source": "https://github.com/DeepakJha01/GUI-Translator/blob/master/main/gui_translator",
      "nlines": 14,
      "nchara": 1650
    },
    "sec_16": {
      "source": "https://github.com/RomanKornev/Translate/blob/master/main",
      "nlines": 42,
      "nchara": 3889
    },
    "sec_19": {
      "source": "https://github.com/PasaOpasen/TranslatorBot/blob/master/translator_tools",
      "nlines": 141,
      "nchara": 7667
    },
    "sec_22": {
      "source": "https://github.com/dhyeythumar/Search-Engine/blob/master/Python_scripts/lang_trans1",
      "nlines": 163,
      "nchara": 11450
    },
    "sec_23": {
      "source": "https://github.com/dhyeythumar/Search-Engine/blob/master/Python_scripts/lang_trans",
      "nlines": 37,
      "nchara": 2815
    },
    "sec_11": {
      "source": "https://github.com/khaledabbud/SA_of_Tweets_After_QS_Assassination_AR_FA/blob/master/textblob_lang_classification",
      "nlines": 33,
      "nchara": 2506
    },
    "sec_18": {
      "source": "https://github.com/PasaOpasen/SpeechLogger/blob/master/ThirdTry/text_logger5",
      "nlines": 28,
      "nchara": 2630
    },
    "sec_4": {
      "source": "https://gist.github.com/sloria/6342158",
      "nlines": 39,
      "nchara": 2994
    },
    "sec_5": {
      "source": "https://gist.github.com/sloria/6338202#file-tweet_classify-py",
      "nlines": 39,
      "nchara": 3301
    },
    "sec_13": {
      "source": "https://github.com/muthuvenki/Trend-Analysis/blob/master/sentimental_anlysis/views",
      "nlines": 67,
      "nchara": 4194
    },
    "thr_17": {
      "source": "",
      "nlines": 83,
      "nchara": 6607
    },
    "thr_10": {
      "source": "https://www.machinelearningplus.com/spacy-tutorial-nlp/#mergingandsplittingtokenswithretokenize",
      "nlines": 23,
      "nchara": 1770
    },
    "thr_12": {
      "source": "",
      "nlines": 129,
      "nchara": 7019
    },
    "thr_18": {
      "source": "",
      "nlines": 18,
      "nchara": 1742
    },
    "thr_20": {
      "source": "",
      "nlines": 26,
      "nchara": 2326
    },
    "thr_27": {
      "source": "",
      "nlines": 20,
      "nchara": 1723
    },
    "thr_22": {
      "source": "",
      "nlines": 17,
      "nchara": 1530
    },
    "thr_26": {
      "source": "",
      "nlines": 10,
      "nchara": 902
    },
    "thr_13": {
      "source": "https://spacy.io/api/tok2vec",
      "nlines": 11,
      "nchara": 894
    },
    "thr_5": {
      "source": "",
      "nlines": 8,
      "nchara": 571
    },
    "thr_6": {
      "source": "",
      "nlines": 40,
      "nchara": 3965
    },
    "thr_25": {
      "source": "",
      "nlines": 30,
      "nchara": 2747
    },
    "thr_24": {
      "source": "",
      "nlines": 8,
      "nchara": 874
    },
    "thr_11": {
      "source": "",
      "nlines": 127,
      "nchara": 10367
    },
    "thr_19": {
      "source": "",
      "nlines": 6,
      "nchara": 610
    },
    "thr_8": {
      "source": "https://spacy.io/universe/project/contextualSpellCheck",
      "nlines": 10,
      "nchara": 909
    },
    "thr_7": {
      "source": "https://spacy.io/universe/project/spacy-textblob",
      "nlines": 11,
      "nchara": 1738
    },
    "thr_9": {
      "source": "https://spacy.io/universe/project/spacy-langdetect",
      "nlines": 12,
      "nchara": 1074
    },
    "thr_1": {
      "source": "",
      "nlines": 121,
      "nchara": 11894
    },
    "thr_2": {
      "source": "",
      "nlines": 69,
      "nchara": 5490
    },
    "thr_3": {
      "source": "",
      "nlines": 49,
      "nchara": 5293
    },
    "thr_4": {
      "source": "",
      "nlines": 82,
      "nchara": 6636
    },
    "thr_14": {
      "source": "",
      "nlines": 117,
      "nchara": 10414
    },
    "thr_15": {
      "source": "",
      "nlines": 160,
      "nchara": 14810
    },
    "fir_31": {
      "source": "https://gitlab.com/death-of-the-authors/1941",
      "nlines": 145,
      "nchara": 10349
    },
    "fir_32": {
      "source": "https://bitbucket.org/miha_stopar/fconsole",
      "nlines": 40,
      "nchara": 4771
    },
    "fir_33": {
      "source": "https://github.com/colgur/reader_pipeline",
      "nlines": 181,
      "nchara": 14642
    },
    "fir_34": {
      "source": "https://github.com/gyenyame/infochimps-data",
      "nlines": 118,
      "nchara": 10442
    },
    "fir_35": {
      "source": "https://gitlab.com/kidaa/eptmk-cval",
      "nlines": 140,
      "nchara": 12494
    },
    "fir_36": {
      "source": "https://bitbucket.org/jaganadhg/ilugcbe_presentations",
      "nlines": 113,
      "nchara": 11391
    },
    "fir_37": {
      "source": "https://bitbucket.org/miha_stopar/fconsole",
      "nlines": 116,
      "nchara": 10040
    },
    "fir_38": {
      "source": "https://bitbucket.org/Kolvia/angrywordscomps",
      "nlines": 48,
      "nchara": 3794
    },
    "fir_39": {
      "source": "https://github.com/jperla/pi",
      "nlines": 56,
      "nchara": 7026
    },
    "fir_40": {
      "source": "https://bitbucket.org/vsapsai/pcfg-gui",
      "nlines": 76,
      "nchara": 6614
    },
    "fir_41": {
      "source": "https://github.com/colgur/reader_pipeline",
      "nlines": 96,
      "nchara": 5414
    },
    "fir_42": {
      "source": "https://bitbucket.org/miha_stopar/mosaic-demo",
      "nlines": 97,
      "nchara": 7042
    },
    "fir_43": {
      "source": "https://bitbucket.org/AlexDel/translate_sentinel",
      "nlines": 65,
      "nchara": 6905
    },
    "fir_44": {
      "source": "https://github.com/robennals/think-link",
      "nlines": 44,
      "nchara": 4965
    },
    "fir_45": {
      "source": "https://github.com/robennals/think-link",
      "nlines": 163,
      "nchara": 14469
    },
    "fir_46": {
      "source": "https://gitlab.com/dashboard-comissoes/dashboard-comissoes",
      "nlines": 78,
      "nchara": 5980
    },
    "fir_47": {
      "source": "https://gitlab.com/mojomojomojo/dataware",
      "nlines": 87,
      "nchara": 5845
    },
    "fir_48": {
      "source": "https://gitlab.com/tlevine/cs4740_3",
      "nlines": 51,
      "nchara": 4431
    },
    "fir_49": {
      "source": "https://gitlab.com/tlevine/cs4740_4",
      "nlines": 80,
      "nchara": 6710
    },
    "fir_50": {
      "source": "https://github.com/aacharya-cs/iir",
      "nlines": 76,
      "nchara": 8316
    },
    "sec_31": {
      "source": "https://gitlab.com/mahendra-r/DAT7",
      "nlines": 369,
      "nchara": 22664
    },
    "sec_32": {
      "source": "https://bitbucket.org/singhsidhukuldeep/microsoft-academia-accelerator-app",
      "nlines": 82,
      "nchara": 7600
    },
    "sec_33": {
      "source": "https://bitbucket.org/jg3d/calarts_porsche_programming",
      "nlines": 151,
      "nchara": 9751
    },
    "sec_34": {
      "source": "https://bitbucket.org/cccteam1/automation",
      "nlines": 38,
      "nchara": 3001
    },
    "sec_35": {
      "source": "https://github.com/octaviomtz/TextBlob",
      "nlines": 62,
      "nchara": 6603
    },
    "sec_36": {
      "source": "https://bitbucket.org/AlgirdasKartaviciusKT/ai",
      "nlines": 107,
      "nchara": 7755
    },
    "sec_37": {
      "source": "https://github.com/octaviomtz/TextBlob",
      "nlines": 79,
      "nchara": 6988
    },
    "sec_38": {
      "source": "https://github.com/octaviomtz/TextBlob",
      "nlines": 82,
      "nchara": 7331
    },
    "sec_39": {
      "source": "https://github.com/octaviomtz/TextBlob",
      "nlines": 36,
      "nchara": 2829
    },
    "sec_40": {
      "source": "https://github.com/jefarrell/RWETbot",
      "nlines": 82,
      "nchara": 5076
    },
    "sec_41": {
      "source": "https://github.com/oroszgy/textblob-aptagger",
      "nlines": 88,
      "nchara": 8346
    },
    "sec_42": {
      "source": "https://bitbucket.org/brfoley76/g2g_analysis",
      "nlines": 181,
      "nchara": 16024
    },
    "sec_43": {
      "source": "https://bitbucket.org/jai_anipr/anipr_bitcoin",
      "nlines": 127,
      "nchara": 10072
    },
    "sec_44": {
      "source": "https://bitbucket.org/AlgirdasKartaviciusKT/ai",
      "nlines": 46,
      "nchara": 3817
    },
    "sec_45": {
      "source": "https://github.com/octaviomtz/TextBlob",
      "nlines": 57,
      "nchara": 4726
    },
    "sec_46": {
      "source": "https://github.com/octaviomtz/TextBlob",
      "nlines": 75,
      "nchara": 11269
    },
    "sec_47": {
      "source": "https://github.com/multiphrenic/TwitterAnalysis",
      "nlines": 60,
      "nchara": 5186
    },
    "sec_48": {
      "source": "https://github.com/Camuslu/TextBlob",
      "nlines": 84,
      "nchara": 6630
    },
    "sec_49": {
      "source": "https://bitbucket.org/cccteam1/automation",
      "nlines": 102,
      "nchara": 8205
    },
    "sec_50": {
      "source": "https://bitbucket.org/cccteam1/automation",
      "nlines": 97,
      "nchara": 7693
    },
    "thr_31": {
      "source": "https://github.com/alephdata/aleph",
      "nlines": 71,
      "nchara": 5920
    },
    "thr_32": {
      "source": "https://bitbucket.org/sandeep118/spacy_train",
      "nlines": 180,
      "nchara": 15819
    },
    "thr_33": {
      "source": "https://github.com/ajrichards/notebook",
      "nlines": 81,
      "nchara": 5821
    },
    "thr_34": {
      "source": "https://bitbucket.org/raditz/aicommunicator",
      "nlines": 28,
      "nchara": 1538
    },
    "thr_35": {
      "source": "https://bitbucket.org/prescience_chatbot/chatbot",
      "nlines": 78,
      "nchara": 9285
    },
    "thr_36": {
      "source": "https://bitbucket.org/raditz/aicommunicator",
      "nlines": 124,
      "nchara": 7327
    },
    "thr_37": {
      "source": "https://github.com/ajrichards/notebook",
      "nlines": 37,
      "nchara": 4539
    },
    "thr_38": {
      "source": "https://bitbucket.org/raditz/aicommunicator",
      "nlines": 54,
      "nchara": 3184
    },
    "thr_39": {
      "source": "https://bitbucket.org/hallmark_uni/ehumanities-grundlagen",
      "nlines": 51,
      "nchara": 3107
    },
    "thr_41": {
      "source": "https://bitbucket.org/gblaketx/cs224u-deception-detection",
      "nlines": 89,
      "nchara": 8430
    },
    "thr_42": {
      "source": "https://bitbucket.org/techrisala/pogo",
      "nlines": 28,
      "nchara": 2073
    },
    "thr_44": {
      "source": "https://nlpforhackers.io/complete-guide-to-spacy/",
      "nlines": 28,
      "nchara": 2348
    },
    "thr_45": {
      "source": "https://nlpforhackers.io/complete-guide-to-spacy/",
      "nlines": 52,
      "nchara": 4200
    },
    "thr_46": {
      "source": "https://machinelearningknowledge.ai/complete-guide-to-spacy-tokenizer-with-examples/",
      "nlines": 21,
      "nchara": 1819
    },
    "thr_47": {
      "source": "http://markneumann.xyz/blog/dependency_matcher/",
      "nlines": 25,
      "nchara": 2166
    },
    "thr_48": {
      "source": "https://ner.pythonhumanities.com/02_02_intro_to_regex.html",
      "nlines": 28,
      "nchara": 2163
    },
    "thr_49": {
      "source": "https://stackoverflow.com/questions/51608482/how-to-do-the-custom-ner-tagging-using-spacy-and-nltk",
      "nlines": 58,
      "nchara": 6445
    },
    "thr_50": {
      "source": "https://www.programcreek.com/python/?code=csvance%2Farmchair-expert%2Farmchair-expert-master%2Fcommon%2Fnlp.py",
      "nlines": 34,
      "nchara": 3156
    }
  }
}