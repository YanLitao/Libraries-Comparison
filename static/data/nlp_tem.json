{"api templates": {"fir": [{"cat_Preprocessing": {"color": "rgb(246, 183, 60)", "num": 29, "codes": ["fir_4", "fir_16", "fir_17", "fir_23", "fir_1", "fir_2", "fir_3", "fir_6", "fir_8", "fir_9", "fir_10", "fir_11", "fir_12", "fir_13", "fir_14", "fir_18", "fir_19", "fir_20", "fir_21", "fir_25", "fir_29", "fir_30", "fir_24", "fir_5", "fir_7", "fir_28", "fir_22", "fir_27", "fir_26"]}}, {"fea_parsing": {"color": "rgb(246, 183, 60)", "num": 4, "codes": ["fir_4", "fir_16", "fir_17", "fir_23"]}}, {"fea_tokenization": {"color": "rgb(246, 183, 60)", "num": 22, "codes": ["fir_4", "fir_16", "fir_17", "fir_23", "fir_1", "fir_2", "fir_3", "fir_6", "fir_8", "fir_9", "fir_10", "fir_11", "fir_12", "fir_13", "fir_14", "fir_18", "fir_19", "fir_20", "fir_21", "fir_25", "fir_29", "fir_30"]}}, {"fea_lemmatization": {"color": "rgb(246, 183, 60)", "num": 12, "codes": ["fir_4", "fir_1", "fir_3", "fir_6", "fir_9", "fir_10", "fir_12", "fir_14", "fir_25", "fir_29", "fir_30", "fir_24"]}}, {"fea_word_vectors": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_nlp_datasets": {"color": "rgb(246, 183, 60)", "num": 16, "codes": ["fir_1", "fir_2", "fir_3", "fir_8", "fir_9", "fir_12", "fir_13", "fir_14", "fir_20", "fir_21", "fir_29", "fir_30", "fir_24", "fir_5", "fir_7", "fir_28"]}}, {"fea_stemming": {"color": "rgb(246, 183, 60)", "num": 4, "codes": ["fir_2", "fir_8", "fir_20", "fir_21"]}}, {"fea_regular_expression": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["fir_5"]}}, {"fea_word_frequency": {"color": "rgb(246, 183, 60)", "num": 3, "codes": ["fir_24", "fir_22", "fir_27"]}}, {"fea_tagger": {"color": "rgb(246, 183, 60)", "num": 3, "codes": ["fir_4", "fir_6", "fir_27"]}}, {"fea_text_simplify": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_n_grams": {"color": "rgb(246, 183, 60)", "num": 2, "codes": ["fir_6", "fir_26"]}}, {"cat_Basic_Analysis": {"color": "rgb(246, 183, 60)", "num": 14, "codes": ["fir_4", "fir_6", "fir_11", "fir_12", "fir_16", "fir_17", "fir_23", "fir_24", "fir_30", "fir_19", "fir_8", "fir_20", "fir_21", "fir_29"]}}, {"fea_Part_of_Speech": {"color": "rgb(246, 183, 60)", "num": 9, "codes": ["fir_4", "fir_6", "fir_11", "fir_12", "fir_16", "fir_17", "fir_23", "fir_24", "fir_30"]}}, {"fea_named_entity_recognition": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["fir_4"]}}, {"fea_dependency_parsing": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["fir_4"]}}, {"fea_spellcheck": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_text_similarity": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["fir_19"]}}, {"fea_text_scoring": {"color": "rgb(246, 183, 60)", "num": 4, "codes": ["fir_8", "fir_20", "fir_21", "fir_29"]}}, {"cat_Advanced_Analysis": {"color": "rgb(246, 183, 60)", "num": 10, "codes": ["fir_28", "fir_15", "fir_22", "fir_24", "fir_27", "fir_30", "fir_9", "fir_3", "fir_8", "fir_21"]}}, {"fea_sentiment_analysis": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["fir_28"]}}, {"fea_translation": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_language_detection": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_classification": {"color": "rgb(246, 183, 60)", "num": 6, "codes": ["fir_28", "fir_15", "fir_22", "fir_24", "fir_27", "fir_30"]}}, {"fea_chatbot": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["fir_9"]}}, {"fea_summarizer": {"color": "rgb(246, 183, 60)", "num": 3, "codes": ["fir_3", "fir_8", "fir_21"]}}, {"fea_text_segmentation": {"color": "#12232E", "num": 0, "codes": []}}], "sec": [{"cat_Preprocessing": {"color": "rgb(246, 183, 60)", "num": 9, "codes": ["sec_7", "sec_10", "sec_2", "sec_12", "sec_6", "sec_9", "sec_27", "sec_1", "sec_8"]}}, {"fea_parsing": {"color": "rgb(246, 183, 60)", "num": 2, "codes": ["sec_7", "sec_10"]}}, {"fea_tokenization": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_lemmatization": {"color": "rgb(246, 183, 60)", "num": 2, "codes": ["sec_2", "sec_12"]}}, {"fea_word_vectors": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_nlp_datasets": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_stemming": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_regular_expression": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_word_frequency": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_tagger": {"color": "rgb(246, 183, 60)", "num": 2, "codes": ["sec_10", "sec_6"]}}, {"fea_text_simplify": {"color": "rgb(246, 183, 60)", "num": 3, "codes": ["sec_2", "sec_9", "sec_27"]}}, {"fea_n_grams": {"color": "rgb(246, 183, 60)", "num": 3, "codes": ["sec_27", "sec_1", "sec_8"]}}, {"cat_Basic_Analysis": {"color": "rgb(246, 183, 60)", "num": 6, "codes": ["sec_2", "sec_6", "sec_12", "sec_17", "sec_26", "sec_29"]}}, {"fea_Part_of_Speech": {"color": "rgb(246, 183, 60)", "num": 2, "codes": ["sec_2", "sec_6"]}}, {"fea_named_entity_recognition": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_dependency_parsing": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_spellcheck": {"color": "rgb(246, 183, 60)", "num": 2, "codes": ["sec_12", "sec_17"]}}, {"fea_text_similarity": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_text_scoring": {"color": "rgb(246, 183, 60)", "num": 2, "codes": ["sec_26", "sec_29"]}}, {"cat_Advanced_Analysis": {"color": "rgb(246, 183, 60)", "num": 21, "codes": ["sec_15", "sec_20", "sec_21", "sec_24", "sec_25", "sec_26", "sec_28", "sec_29", "sec_30", "sec_12", "sec_14", "sec_16", "sec_19", "sec_22", "sec_23", "sec_11", "sec_18", "sec_4", "sec_5", "sec_9", "sec_13"]}}, {"fea_sentiment_analysis": {"color": "rgb(246, 183, 60)", "num": 9, "codes": ["sec_15", "sec_20", "sec_21", "sec_24", "sec_25", "sec_26", "sec_28", "sec_29", "sec_30"]}}, {"fea_translation": {"color": "rgb(246, 183, 60)", "num": 6, "codes": ["sec_12", "sec_14", "sec_16", "sec_19", "sec_22", "sec_23"]}}, {"fea_language_detection": {"color": "rgb(246, 183, 60)", "num": 5, "codes": ["sec_12", "sec_19", "sec_23", "sec_11", "sec_18"]}}, {"fea_classification": {"color": "rgb(246, 183, 60)", "num": 5, "codes": ["sec_20", "sec_4", "sec_5", "sec_9", "sec_13"]}}, {"fea_chatbot": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_summarizer": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_text_segmentation": {"color": "#12232E", "num": 0, "codes": []}}], "thr": [{"cat_Preprocessing": {"color": "rgb(246, 183, 60)", "num": 15, "codes": ["thr_17", "thr_10", "thr_12", "thr_18", "thr_20", "thr_27", "thr_22", "thr_26", "thr_13", "thr_5", "thr_6", "thr_25", "thr_24", "thr_11", "thr_19"]}}, {"fea_parsing": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["thr_17"]}}, {"fea_tokenization": {"color": "rgb(246, 183, 60)", "num": 5, "codes": ["thr_10", "thr_12", "thr_18", "thr_20", "thr_27"]}}, {"fea_lemmatization": {"color": "rgb(246, 183, 60)", "num": 4, "codes": ["thr_17", "thr_12", "thr_22", "thr_26"]}}, {"fea_word_vectors": {"color": "rgb(246, 183, 60)", "num": 2, "codes": ["thr_17", "thr_13"]}}, {"fea_nlp_datasets": {"color": "rgb(246, 183, 60)", "num": 3, "codes": ["thr_17", "thr_5", "thr_6"]}}, {"fea_stemming": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_regular_expression": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["thr_27"]}}, {"fea_word_frequency": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["thr_25"]}}, {"fea_tagger": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["thr_24"]}}, {"fea_text_simplify": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["thr_27"]}}, {"fea_n_grams": {"color": "rgb(246, 183, 60)", "num": 3, "codes": ["thr_24", "thr_11", "thr_19"]}}, {"cat_Basic_Analysis": {"color": "rgb(246, 183, 60)", "num": 6, "codes": ["thr_12", "thr_24", "thr_18", "thr_8", "thr_10", "thr_7"]}}, {"fea_Part_of_Speech": {"color": "rgb(246, 183, 60)", "num": 2, "codes": ["thr_12", "thr_24"]}}, {"fea_named_entity_recognition": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["thr_18"]}}, {"fea_dependency_parsing": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_spellcheck": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["thr_8"]}}, {"fea_text_similarity": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["thr_10"]}}, {"fea_text_scoring": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["thr_7"]}}, {"cat_Advanced_Analysis": {"color": "rgb(246, 183, 60)", "num": 4, "codes": ["thr_7", "thr_9", "thr_17", "thr_6"]}}, {"fea_sentiment_analysis": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["thr_7"]}}, {"fea_translation": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_language_detection": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["thr_9"]}}, {"fea_classification": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["thr_17"]}}, {"fea_chatbot": {"color": "#12232E", "num": 0, "codes": []}}, {"fea_summarizer": {"color": "rgb(246, 183, 60)", "num": 1, "codes": ["thr_6"]}}, {"fea_text_segmentation": {"color": "#12232E", "num": 0, "codes": []}}]}, "all templates": [{"name": "cat_Preprocessing", "color": "", "fir": 29, "sec": 9, "thr": 15}, {"name": "fea_parsing", "color": "", "fir": 4, "sec": 2, "thr": 1}, {"name": "fea_tokenization", "color": "", "fir": 22, "sec": 0, "thr": 5}, {"name": "fea_lemmatization", "color": "", "fir": 12, "sec": 2, "thr": 4}, {"name": "fea_word_vectors", "color": "", "fir": 0, "sec": 0, "thr": 2}, {"name": "fea_nlp_datasets", "color": "", "fir": 16, "sec": 0, "thr": 3}, {"name": "fea_stemming", "color": "", "fir": 4, "sec": 0, "thr": 0}, {"name": "fea_regular_expression", "color": "", "fir": 1, "sec": 0, "thr": 1}, {"name": "fea_word_frequency", "color": "", "fir": 3, "sec": 0, "thr": 1}, {"name": "fea_tagger", "color": "", "fir": 3, "sec": 2, "thr": 1}, {"name": "fea_text_simplify", "color": "", "fir": 0, "sec": 3, "thr": 1}, {"name": "fea_n_grams", "color": "", "fir": 2, "sec": 3, "thr": 3}, {"name": "cat_Basic_Analysis", "color": "", "fir": 14, "sec": 6, "thr": 6}, {"name": "fea_Part_of_Speech", "color": "", "fir": 9, "sec": 2, "thr": 2}, {"name": "fea_named_entity_recognition", "color": "", "fir": 1, "sec": 0, "thr": 1}, {"name": "fea_dependency_parsing", "color": "", "fir": 1, "sec": 0, "thr": 0}, {"name": "fea_spellcheck", "color": "", "fir": 0, "sec": 2, "thr": 1}, {"name": "fea_text_similarity", "color": "", "fir": 1, "sec": 0, "thr": 1}, {"name": "fea_text_scoring", "color": "", "fir": 4, "sec": 2, "thr": 1}, {"name": "cat_Advanced_Analysis", "color": "", "fir": 10, "sec": 21, "thr": 4}, {"name": "fea_sentiment_analysis", "color": "", "fir": 1, "sec": 9, "thr": 1}, {"name": "fea_translation", "color": "", "fir": 0, "sec": 6, "thr": 0}, {"name": "fea_language_detection", "color": "", "fir": 0, "sec": 5, "thr": 1}, {"name": "fea_classification", "color": "", "fir": 6, "sec": 5, "thr": 1}, {"name": "fea_chatbot", "color": "", "fir": 1, "sec": 0, "thr": 0}, {"name": "fea_summarizer", "color": "", "fir": 3, "sec": 0, "thr": 1}, {"name": "fea_text_segmentation", "color": "", "fir": 0, "sec": 0, "thr": 0}], "labeled files": {"fir_4": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_4\"><pre id=\"fir_4_code\"><code class=\"python\">import nltk\nfrom nltk.parse.stanford import StanfordParser, StanfordDependencyParser\nfrom nltk.tag.stanford import StanfordPOSTagger\nfrom nltk.tag import StanfordNERTagger\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom config import *\n\nclass Text_processing:\n\n\n\tdef __init__(self):\n\t\t\n\t\t# user need to download Stanford Parser, NER and POS tagger from stanford website\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">self.constituent_parse_tree = Stanford<span class=\"fea_parsing_keys udls\">Parser</span>()</div>  #user need to set as environment variable\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_1\" style=\"display: inline;\">self.stanford_dependency = StanfordDependency<span class=\"fea_parsing_keys udls\">Parser</span>()</div> #user need to set as environment variable\n\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">self.<span class=\"fea_lemmatization_keys udls\">lemma</span> = WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer()</div>\n\t\tself.home = '/home/ramesh'\n\t\t#user needs to download stanford packages and change directory\n\t\t<div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">self.ner = StanfordNER<span class=\"fea_tagger_keys udls\">Tagge</span>r(self.home + '/stanford-ner-2017-06-09/classifiers/english.all.3class.distsim.crf.ser.gz',self.home + '/stanford-ner-2017-06-09/stanford-ner.jar')</div>\n\t\t<div class=\"highlights fea_tagger\" id=\"tagger_1\" style=\"display: inline;\">self.pos_tag = StanfordPOS<span class=\"fea_tagger_keys udls\">Tagge</span>r(self.home + '/stanford-pos<span class=\"fea_tagger_keys udls\">tagge</span>r-2017-06-09/models/english-bidirectional-distsim.<span class=\"fea_tagger_keys udls\">tagge</span>r',self.home + '/stanford-pos<span class=\"fea_tagger_keys udls\">tagge</span>r-2017-06-09/stanford-pos<span class=\"fea_tagger_keys udls\">tagge</span>r-3.8.0.jar')</div>\n\t\tself.CharacterOffsetEnd = 0 \n\t\tself.CharacterOffsetBegin = 0\n\t\t\n\n\t'''\n\tInput: sentence\n\tReturns: \n\t'''\n\n\n\tdef parser(self,sentence):\n\n\n\t\tself.parseResult = {'parseTree':[], 'text':[], 'dependencies':[],'words':[] }\n\t\tparseText, sentences = self.get_parseText(sentence)\n\t\t# print \"sentences \", sentences\n\t\t# if source/target sent consist of 1 sentence \n\t\tif len(sentences) == 1:\n\t\t\treturn parseText\n\t\t\n\t\twordOffSet = 0 # offset is number of words in first sentence \n\n\t\t# if source/target sentence has more than 1 sentence\n\n\t\tfor i in xrange(len(parseText['text'])):\n\t\t\tif i &gt; 0:\n\n\t\t\t\tfor j in xrange(len(parseText['dependencies'][i])):\n\t\t\t\t\t# [root, Root-0, dead-4]\n\t\t\t\t\tfor k in xrange(1,3):\n\t\t\t\t\t\ttokens = parseText['dependencies'][i][j][k].split('-')\n\n\t\t\t\t\t\tif tokens[0] == 'Root':\n\t\t\t\t\t\t\tnewWordIndex = 0\n\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tif not tokens[len(tokens)-1].isdigit():\n\t\t\t\t\t\t\t\tcontinue \n\n\t\t\t\t\t\t\tnewWordIndex = int(tokens[len(tokens)-1]) + wordOffSet\n\n\t\t\t\t\t\tif len(tokens) == 2:\n\n\t\t\t\t\t\t\t parseText['dependencies'][i][j][k] = tokens[0]+ '-' + str(newWordIndex)\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tw = ''\n\t\t\t\t\t\t\tfor l in xrange(len(tokens)-1):\n\t\t\t\t\t\t\t\tw += tokens[l]\n\t\t\t\t\t\t\t\tif l&lt;len(tokens)-2:\n\t\t\t\t\t\t\t\t\tw += '-'\n\n\t\t\t\t\t\t\tparseText['dependencies'][i][j][k] = w + '-' + str(newWordIndex)\n\n\t\t\twordOffSet += len(parseText['words'][i])\n\n\t\treturn parseText\n\n\n\t'''\n\tInput: parserResult \n\tReturns: [[charBegin,charEnd], wordIndex(starts from 1), word, word_lemma]] \n\t'''\n\n\n\tdef get_lemma(self,parserResult):\n\n\n\t\tres = []\t\t\n\t\twordIndex = 1\n\t\tfor i in xrange(len(parserResult['words'])):\n\t\t\t\n\t\t\tfor j in xrange(len(parserResult['words'][i])):\n\t\t\t\t\n\t\t\t\ttag = [ [parserResult['words'][i][j][1]['CharacterOffsetBegin'], parserResult['words'][i][j][1]['CharacterOffsetEnd']], wordIndex,parserResult['words'][i][j][0] ,parserResult['words'][i][j][1]['Lemma'] ]\n\t\t\t\twordIndex += 1\n\t\t\t\tres.append(tag)\n\n\t\treturn res\n\n\n\n\t'''\n\tUsing Stanford POS Tagger\n\tInput: parserResult \n\tReturns: [[charBegin,charEnd], wordIndex(starts from 1), word, word_POS]] \n\t'''\n\n\n\tdef combine_lemmaAndPosTags(self,parserResult):\n\n\t\tres = []\n\t\t\n\t\twordIndex = 1\n\t\tfor i in xrange(len(parserResult['words'])):\n\t\t\t\n\t\t\tfor j in xrange(len(parserResult['words'][i])):\n\t\t\t\t\n\t\t\t\ttag = [ [parserResult['words'][i][j][1]['CharacterOffsetBegin'], parserResult['words'][i][j][1]['CharacterOffsetEnd']], wordIndex,parserResult['words'][i][j][0] ,parserResult['words'][i][j][1]['Lemma'] ,parserResult['words'][i][j][1]['PartOfSpeech'] ]\n\t\t\t\twordIndex += 1\n\t\t\t\tres.append(tag)\n\n\t\treturn res\n\n\n\t'''\n\tInput: parserResult\n\tReturns: ([charOffsetBegin,charOffsetEnd], wordindex,word, NER ])\n\t'''\n\n\n\tdef nerWordAnnotator(self,parserResult):\n\n\t\tres = []\n\t\t\n\t\twordIndex = 1\n\t\tfor i in xrange(len(parserResult['words'])):\n\t\t\t\n\t\t\tfor j in xrange(len(parserResult['words'][i])):\n\t\t\t\t\n\t\t\t\ttag = [ [parserResult['words'][i][j][1]['CharacterOffsetBegin'], parserResult['words'][i][j][1]['CharacterOffsetEnd']], wordIndex,parserResult['words'][i][j][0] ,parserResult['words'][i][j][1]['NamedEntityTag'] ]\n\t\t\t\t# print \"tag \", tag\n\t\t\t\twordIndex += 1\n\t\t\t\t# if there is valid named entity then add in list\n\t\t\t\tif tag[3] != 'O':\n\n\t\t\t\t\tres.append(tag)\n\n\t\treturn res\n\n\n\t'''\n\tInput : ParserResult\n\tReturns : list containing NamedEntites\n\t1. Group words in same list if they share same NE (Location), \n    2. Save other words in list that have any entity\n\t'''\n\n\n\tdef get_ner(self,parserResult):\n\n\n\t\t<div class=\"highlights fea_named_entity_recognition\" id=\"named_entity_recognition_0\" style=\"display: inline;\">nerWordAnnotations = self.nerWordAnnotator(parserResult)</div> #[[ [charbegin,charEnd], wordIndex, word, NE ]]\n\t\tnamedEntities = []\n\t\tcurrentWord = []\n\t\tcurrentCharacterOffSets = []\n\t\tcurrentWordOffSets = []\n\n\t\tfor i in xrange(len(nerWordAnnotations)):\n\n\t\t\tif i == 0:\n\n\t\t\t\tcurrentWord.append(nerWordAnnotations[i][2]) # word having NE\n\t\t\t\tcurrentCharacterOffSets.append(nerWordAnnotations[i][0]) # [begin,end]\n\t\t\t\tcurrentWordOffSets.append(nerWordAnnotations[i][1]) # Word Index\n\t\t\t\t# if there is only one ner Word tag\n\t\t\t\tif (len(nerWordAnnotations) == 1):\n\t\t\t\t\tnamedEntities.append([ currentCharacterOffSets, currentWordOffSets, \\\n\t\t\t\t\t\tcurrentWord, nerWordAnnotations[i-1][3] ])\n\t\t\t\t\t# print \"named Entities \", namedEntities\n\t\t\t\t\tbreak \n\t\t\t\tcontinue\n\t\t\t# if consecutive tags have same NER Tag, save them in one list\n\t\t\tif nerWordAnnotations[i][3] == nerWordAnnotations[i-1][3] and \\\n\t\t\t\t\tnerWordAnnotations[i][1] == nerWordAnnotations[i-1][1] + 1:\n\t\t\t\t\n\t\t\t\tcurrentWord.append(nerWordAnnotations[i][2]) # word having NE\n\t\t\t\tcurrentCharacterOffSets.append(nerWordAnnotations[i][0]) # [begin,end]\n\t\t\t\tcurrentWordOffSets.append(nerWordAnnotations[i][1]) # Word Index\n\n\t\t\t\tif i == (len(nerWordAnnotations) - 1):\n\t\t\t\t\tnamedEntities.append([ currentCharacterOffSets, \\\n\t\t\t\t\t\tcurrentWordOffSets, currentWord, nerWordAnnotations[i][3] ])\n\t\t\t# if consecutive tags do not match\n\t\t\telse:\n\n\t\t\t\tnamedEntities.append([ currentCharacterOffSets, \\\n\t\t\t\t\t\tcurrentWordOffSets, currentWord, nerWordAnnotations[i-1][3] ])\n\t\t\t\tcurrentWord = [nerWordAnnotations[i][2]]\n\t\t\t\t# remove everything from currentCharacterOffSets and currentWordOffSets\n\t\t\t\tcurrentCharacterOffSets = []\n\t\t\t\tcurrentWordOffSets = []\n\t\t\t\t# add charac offsets and currentWordOffSets of current word\n\t\t\t\tcurrentCharacterOffSets.append(nerWordAnnotations[i][0])\n\t\t\t\tcurrentWordOffSets.append(nerWordAnnotations[i][1])\n\n\t\t\t\t# if it is last iteration then update named Entities\n\t\t\t\tif i == len(nerWordAnnotations)-1:\n\t\t\t\t\tnamedEntities.append([ currentCharacterOffSets, currentWordOffSets, \\\n\t\t\t\t\t\t\tcurrentWord, nerWordAnnotations[i][3] ])\n\t\t#sort out according to len of characters in ascending order\n\t\tnamedEntities = sorted(namedEntities, key=len)\n\n\t\treturn namedEntities\n\n\n\t'''\n\tInput: Word(Word whose NE is not found), NE(word already have NE Tag) \n\tReturns: Boolean; True if word is acronym\n\t\t\t\t\tFalse if word is not acronym\n\t'''\n\n\n\tdef is_Acronym(self,word,NE):\n\n\n\t\tqueryWord = word.replace('.','')\n\t\t# If all words of queryWord is not capital or length of word != \n\t\t\t\t#length of NE(word already have NE Tag) or \n\t\t   #  if word is 'a' or 'i' \n\t\tif not queryWord.isupper() or len(queryWord) != len(NE) or queryWord.lower() in ['a', 'i']:\n\t\t\treturn False\n\n\t\tacronym = True\n\n\t\t#we run for loop till length of query word(i.e 3)(if word is 'UAE')\n\t\t#Compare 1st letter(U) of query word with first letter of first element in named entity(U = U(united))\n\t\t# again we take second letter of canonical word (A) with second element in named entity(Arab)\n\t\t# and so on \n\t\tfor i in xrange(len(queryWord)):\n\t\t\t# print \"queryword[i], NE \", queryWord, NE\n\t\t\tif queryWord[i] != NE[i][0]:\n\t\t\t\tacronym = False\n\t\t\t\tbreak\n\n\t\treturn acronym\n\n\n\t'''\n\tInput: sentence\n\tReturns: parse(\t{ParseTree, text, Dependencies, \n\t  'word : [] NamedEntityTag, CharacterOffsetEnd, \n\t  \t\tCharacterOffsetBegin, PartOfSpeech, Lemma}']}) \n\t  \t\tsentence and\n\t'''\n\n\n\tdef get_parseText(self,sentence):\n\n\t\tself.count = 0\n\t\tself.length_of_sentence = [] # stores length of each sentence\n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>ized_sentence = sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>\n\t\t# print \"len of tokenized \",len(tokenized_sentence)\n\t\tif (len(tokenized_sentence) == 1):\n\t\t\tself.count += 1\n\t\t\tfor i in tokenized_sentence:\n\t\t\t\tparse = self.get_combine_words_param(i)\n\t\telse:\n\t\t\ttmp = 0\n\t\t\tfor i in tokenized_sentence:\n\t\t\t\tself.count += 1\n\t\t\t\tparse = self.get_combine_words_param(i)\n\t\t\t\ts = len(i) + tmp\n\t\t\t\tself.length_of_sentence.append(s)\n\t\t\t\ttmp = s\n\n\t\treturn parse,tokenized_sentence\n\t\t\n\n\t'''\n\tInput: sentences\n    Return: constituency tree that represents relations between sub-phrases in sentences\n\t'''\n\n\n\tdef get_constituency_Tree(self,sentence):\n\t\t\n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">sentence = sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_2\" style=\"display: inline;\">constituency_<span class=\"fea_parsing_keys udls\">parser</span> = self.constituent_parse_tree.raw_parse_sents(sentence)</div>\n\t\tfor parser in constituency_parser:\n\t\t\tfor sent in parser:\n\t\t\t\ttree = str(sent)\n\t\tparse_string = ' '.join(str(tree).split()) \n        \n\t\treturn parse_string\n\n\n\t'''\n\tInput: sentence\n\treturns: relation between words with their index\n\t'''\t\n\n\n\tdef get_dependencies(self,sentence):\n\n\n\t\tdependency_tree = []\n\t\t<div class=\"highlights fea_dependency_parsing\" id=\"dependency_parsing_0\" style=\"display: inline;\">dependency_parser = self.stanford_dependency.raw_parse(sentence)</div>\n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_2\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span> = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>\n\t\t<div class=\"highlights fea_dependency_parsing\" id=\"dependency_parsing_1\" style=\"display: inline;\">parsetree = list(self.stanford_dependency.raw_parse(sentence))[0]</div>\n\t\t# Find root(head) of the sentence \n\t\tfor k in parsetree.nodes.values():\n\t\t\tif k[\"head\"] == 0:\n\t\t\n\t\t\t\tdependency_tree.append([str(k[\"rel\"]), \"Root-\" + str(k[\"head\"]), str(k[\"word\"]) \n\t\t\t\t\t+ \"-\" + str(k[\"address\"]) ])\t    \t\n\t\t# Find relation between words in sentence\n\t\tfor dep in dependency_parser:\n\t\t\tfor triple in dep.triples():\n\t\t\t\tindex_word = token.index(triple[0][0]) + 1 # because index starts from 0 \n\t\t\t\tindex2_word = token.index(triple[2][0]) + 1\n\t\t\t\tdependency_tree.append([str(triple[1]),str(triple[0][0]) + \"-\" + str(index_word),\\\n\t\t\t\t\t\t\t str(triple[2][0]) + \"-\" + str(index2_word)])\n\n\t\treturn dependency_tree\n\n\n\t'''\n\tInput: sentence, word(of which offset to determine)\n\tReturn: [CharacterOffsetEnd,CharacterOffsetBegin] for each word\n\t'''\n\n\n\tdef get_charOffset(self,sentence, word):\n\n\t\t# word containing '.' causes problem in counting\n\n\t\tCharacterOffsetBegin = sentence.find(word)\n\t\tCharacterOffsetEnd = CharacterOffsetBegin + len(word)\n\t\t\n\t\treturn [CharacterOffsetEnd,CharacterOffsetBegin]\n\n\n\t'''\n\tInput: sentence\n\tReturns: dictionary: \n\t{ParseTree, text, Dependencies, \n\t  #'word : [] NamedEntityTag, CharacterOffsetEnd, CharacterOffsetBegin, PartOfSpeech, Lemma}']}\n\t'''\n\n\n\tdef get_combine_words_param(self,sentence):\n\t\t\n\n\t\twords_in_each_sentence = []\n\t\twords_list = [] \n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_3\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>ized_words = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>\n\t\t<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">pos</span><span class=\"fea_Part_of_Speech_keys udls\">Tag</span> = self.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>.<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(<span class=\"fea_Part_of_Speech_keys udls\">token</span>ized_words)</div>\n\t\t<div class=\"highlights fea_named_entity_recognition\" id=\"named_entity_recognition_1\" style=\"display: inline;\">ner = self.ner.tag(tokenized_words)</div>\n\t\t\n\t\t# if source sentence/target sentence has one sentence\n\t\tif (self.count == 1):\n\t\t\tfor i in xrange(len(tokenized_words)):\n\t\t\t\tword_lemma = str()\n\t\t\t\tword = tokenized_words[i]\n\t\t\t\tname_entity = ner[i]\n\t\t\t\tword_posTag = posTag[i][-1]  # access tuple [(United, NNP),..]\n\t\t\t\t# print \"word and pos tag \", word, word_posTag[0]\t\n\t\t\t\t#wordNet lemmatizer needs pos tag with words else it considers noun\n\t\t\t\tif (word_posTag[0] == 'V'):\n\t\t\t\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_1\" style=\"display: inline;\">word_<span class=\"fea_lemmatization_keys udls\">lemma</span> = self.<span class=\"fea_lemmatization_keys udls\">lemma</span>.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(tokenized_words[i], wordnet.VERB)</div>\n\n\t\t\t\telif (word_posTag[0] == 'J'):\n\t\t\t\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_2\" style=\"display: inline;\">word_<span class=\"fea_lemmatization_keys udls\">lemma</span> = self.<span class=\"fea_lemmatization_keys udls\">lemma</span>.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(tokenized_words[i], wordnet.ADJ)</div>\n\n\t\t\t\telif (word_posTag[0:1] == 'RB'):\n\t\t\t\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_3\" style=\"display: inline;\">word_<span class=\"fea_lemmatization_keys udls\">lemma</span> = self.<span class=\"fea_lemmatization_keys udls\">lemma</span>.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(tokenized_words[i], wordnet.ADV)</div>\n\n\t\t\t\telse:\n\t\t\t\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_4\" style=\"display: inline;\">word_<span class=\"fea_lemmatization_keys udls\">lemma</span> = self.<span class=\"fea_lemmatization_keys udls\">lemma</span>.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(tokenized_words[i])</div>\n\n\t\t\t\tself.CharacterOffsetEnd, self.CharacterOffsetBegin = self.get_charOffset(sentence,tokenized_words[i])\n\t\t\t\t\n\n\t\t\t\twords_list.append([word, {\"NamedEntityTag\" : str(name_entity[1]),\n\t\t\t\t\t\"CharacterOffsetEnd\" : str(self.CharacterOffsetEnd), \"CharacterOffsetBegin\" : str(self.CharacterOffsetBegin) \n\t\t\t\t\t,\"PartOfSpeech\" : str(word_posTag) , \"Lemma\" : str(word_lemma)}])\n\n\t\t\tself.parseResult['parseTree'] = [self.get_constituency_Tree(sentence)]\n\t\t\tself.parseResult['text'] = [sentence]\n\t\t\tself.parseResult['dependencies'] = [self.get_dependencies(sentence)]\n\t\t\tself.parseResult['words'] = [words_list]\n\n\t\telse:\n\n\t\t\tfor i in xrange(len(tokenized_words)):\n\t\t\t\tword = tokenized_words[i]\n\t\t\t\tname_entity = ner[i] \n\t\t\t\tword_posTag = posTag[i][-1]\n\n\t\t\t\tif (word_posTag[0] == 'V'):\n\t\t\t\t\tword_lemma = self.lemma.lemmatize(tokenized_words[i], wordnet.VERB)\n\n\t\t\t\telif (word_posTag[0] == 'J'):\n\t\t\t\t\tword_lemma = self.lemma.lemmatize(tokenized_words[i], wordnet.ADJ)\n\n\t\t\t\telif (word_posTag[0:1] == 'RB'):\n\t\t\t\t\tword_lemma = self.lemma.lemmatize(tokenized_words[i], wordnet.ADV)\n\n\t\t\t\telse:\n\t\t\t\t\tword_lemma = self.lemma.lemmatize(tokenized_words[i])\n\n\t\t\t\tend, begin = self.get_charOffset(sentence,tokenized_words[i])\n\t\t\t\tend = end + self.length_of_sentence[self.count-2] + 1\n\t\t\t\tbegin = begin + self.length_of_sentence[self.count-2] + 1\t\n\t\t\t\twords_list.append([word, {\"NamedEntityTag\" : str(name_entity[1]),\n\t\t\t\t\t\"CharacterOffsetEnd\" : str(end), \"CharacterOffsetBegin\" : str(begin) \n\t\t\t\t\t,\"PartOfSpeech\" : str(word_posTag) , \"Lemma\" : str(word_lemma)}])\n\t\t\tself.parseResult['parseTree'].append(self.get_constituency_Tree(sentence))\n\t\t\tself.parseResult['text'].append(sentence)\n\t\t\tself.parseResult['dependencies'].append(self.get_dependencies(sentence))\n\t\t\tself.parseResult['words'].append(words_list)\n\n\t\treturn self.parseResult\n\t\t#https://github.com/rameshjes/Semantic-Textual-Similarity/blob/master/monolingualWordAligner/nltkUtil</code></pre></div></body></html>", "fir_16": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_16\"><pre id=\"fir_16_code\"><code class=\"python\"># Copyright 2020 The `Kumar Nityan Suman` (https://github.com/nityansuman/).\n# All Rights Reserved.\n#\n#                     GNU GENERAL PUBLIC LICENSE\n#                        Version 3, 29 June 2007\n#  Copyright (C) 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt;\n#  Everyone is permitted to copy and distribute verbatim copies\n#  of this license document, but changing it is not allowed.\n# ==============================================================================\n\nimport logging\nfrom typing import Tuple\n\nimport nltk as nlp\nimport numpy as np\n\n\nclass SubjectiveTest:\n\t\"\"\"Class abstraction for subjective test generation module.\n\t\"\"\"\n\n\tdef __init__(self, filepath: str):\n\t\t\"\"\"Class constructor.\n\n\t\tArgs:\n\t\t\tfilepath (str): Absolute filepath to the subject corpus.\n\t\t\"\"\"\n\t\tself.question_pattern = [\n\t\t\t\"Explain in detail \",\n\t\t\t\"Define \",\n\t\t\t\"Write a short note on \",\n\t\t\t\"What do you mean by \"\n\t\t]\n\n\t\tself.grammar = r\"\"\"\n\t\t\tCHUNK: {&lt;NN&gt;+&lt;IN|DT&gt;*&lt;NN&gt;+}\n\t\t\t{&lt;NN&gt;+&lt;IN|DT&gt;*&lt;NNP&gt;+}\n\t\t\t{&lt;NNP&gt;+&lt;NNS&gt;*}\n\t\t\"\"\"\n\n\t\ttry:\n\t\t\twith open(filepath, mode=\"r\") as fp:\n\t\t\t\tself.summary = fp.read()\n\t\texcept FileNotFoundError:\n\t\t\tlogging.exception(\"Corpus file not found.\", exc_info=True)\n\n\t@staticmethod\n\tdef word_tokenizer(sequence: str) -&gt; list:\n\t\t\"\"\"Tokenize string sequences to words.\n\n\t\tArgs:\n\t\t\tsequence (str): Corpus sequences.\n\n\t\tReturns:\n\t\t\tlist: Word tokens.\n\t\t\"\"\"\n\t\tword_tokens = list()\n\t\ttry:\n\t\t\tfor sent in <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">nlp.sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(sequence)</div>:\n\t\t\t\tfor w in <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">nlp.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sent)</div>:\n\t\t\t\t\tword_tokens.append(w)\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Word tokenization failed.\", exc_info=True)\n\t\treturn word_tokens\n\n\t@staticmethod\n\tdef create_vector(answer_tokens: list, tokens: list) -&gt; np.array:\n\t\t\"\"\"Create a one-hot encoded vector for the answer_tokens.\n\n\t\tArgs:\n\t\t\tanswer_tokens (list): Tokenized user response.\n\t\t\ttokens (list): Tokenized answer corpus.\n\n\t\tReturns:\n\t\t\tnp.array: A one-hot encoded vector of the answer.\n\t\t\"\"\"\n\t\treturn np.array([1 if tok in answer_tokens else 0 for tok in tokens])\n\n\t@staticmethod\n\tdef cosine_similarity_score(vector1: np.array, vector2: np.array) -&gt; float:\n\t\t\"\"\"Compute the euclidean distance between two vectors.\n\n\t\tArgs:\n\t\t\tvector1 (np.array): Actual answer vector.\n\t\t\tvector2 (np.array): User response vector.\n\n\t\tReturns:\n\t\t\tfloat: Euclidean distance between two vectors.\n\t\t\"\"\"\n\t\tdef vector_value(vector):\n\t\t\treturn np.sqrt(np.sum(np.square(vector)))\n\n\t\tv1 = vector_value(vector1)\n\t\tv2 = vector_value(vector2)\n\n\t\tv1_v2 = np.dot(vector1, vector2)\n\t\treturn (v1_v2 / (v1 * v2)) * 100\n\n\tdef generate_test(self, num_questions: int = 2) -&gt; Tuple[list, list]:\n\t\t\"\"\"Method to generate subjective test.\n\n\t\tArgs:\n\t\t\tnum_questions (int, optional): Maximum number of questions\n\t\t\t\tto be generated. Defaults to 2.\n\n\t\tReturns:\n\t\t\tTuple[list, list]: Generated `Questions` and `Answers` respectively\n\t\t\"\"\"\n\t\ttry:\n\t\t\tsentences = nlp.sent_tokenize(self.summary)\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Sentence tokenization failed.\", exc_info=True)\n\n\t\ttry:\n\t\t\t<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">cp = nlp.Regexp<span class=\"fea_parsing_keys udls\">Parser</span>(self.grammar)</div>\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Regex grammar train failed.\", exc_info=True)\n\n\t\tquestion_answer_dict = dict()\n\t\tfor sentence in sentences:\n\n\t\t\ttry:\n\t\t\t\t<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">tag</span>ged_words = nlp.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(nlp.word_<span class=\"fea_Part_of_Speech_keys udls\">token</span>ize(sentence))</div>\n\t\t\texcept Exception:\n\t\t\t\tlogging.exception(\"Word tokenization failed.\", exc_info=True)\n\n\t\t\ttree = cp.parse(tagged_words)\n\t\t\tfor subtree in tree.subtrees():\n\t\t\t\tif subtree.label() == \"CHUNK\":\n\t\t\t\t\ttemp = \"\"\n\t\t\t\t\tfor sub in subtree:\n\t\t\t\t\t\ttemp += sub[0]\n\t\t\t\t\t\ttemp += \" \"\n\t\t\t\t\ttemp = temp.strip()\n\t\t\t\t\ttemp = temp.upper()\n\t\t\t\t\tif temp not in question_answer_dict:\n\t\t\t\t\t\tif len(<div class=\"highlights fea_tokenization\" id=\"tokenization_2\" style=\"display: inline;\">nlp.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>) &gt; 20:\n\t\t\t\t\t\t\tquestion_answer_dict[temp] = sentence\n\t\t\t\t\telse:\n\t\t\t\t\t\tquestion_answer_dict[temp] += sentence\n\n\t\tkeyword_list = list(question_answer_dict.keys())\n\t\tquestion_answer = list()\n\n\t\tfor _ in range(3):\n\t\t\trand_num = np.random.randint(0, len(keyword_list))\n\t\t\tselected_key = keyword_list[rand_num]\n\t\t\tanswer = question_answer_dict[selected_key]\n\t\t\trand_num %= 4\n\t\t\tquestion = self.question_pattern[rand_num] + selected_key + \".\"\n\t\t\tquestion_answer.append({\"Question\": question, \"Answer\": answer})\n\n\t\tque = list()\n\t\tans = list()\n\t\twhile len(que) &lt; num_questions:\n\t\t\trand_num = np.random.randint(0, len(question_answer))\n\t\t\tif question_answer[rand_num][\"Question\"] not in que:\n\t\t\t\tque.append(question_answer[rand_num][\"Question\"])\n\t\t\t\tans.append(question_answer[rand_num][\"Answer\"])\n\t\t\telse:\n\t\t\t\tcontinue\n\t\treturn que, ans\n\n\tdef evaluate_subjective_answer(self, original_answer: str, user_answer: str) -&gt; float:\n\t\t\"\"\"Evaluate the subjective answer given by the user.\n\n\t\tArgs:\n\t\t\toriginal_answer (str): A string representing the original answer.\n\t\t\tuser_answer (str): A string representing the answer given by the user.\n\n\t\tReturns:\n\t\t\tfloat: Similarity/correctness score of the user answer\n\t\t\t\tbased on the original asnwer.\n\t\t\"\"\"\n\t\tscore_obt = 0\n\t\toriginal_ans_list = self.word_tokenizer(original_answer)\n\t\tuser_ans_list = self.word_tokenizer(user_answer)\n\n\t\toverall_list = original_ans_list + user_ans_list\n\n\t\tvector1 = self.create_vector(original_ans_list, overall_list)\n\t\tvector2 = self.create_vector(user_answer, overall_list)\n\n\t\tscore_obt = self.cosine_similarity_score(vector1, vector2)\n\t\treturn score_obt\n\t\t#https://github.com/nityansuman/marvin/blob/main/src/subjective</code></pre></div></body></html>", "fir_17": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_17\"><pre id=\"fir_17_code\"><code class=\"python\"># Copyright 2020 The `Kumar Nityan Suman` (https://github.com/nityansuman/).\n# All Rights Reserved.\n#\n#                     GNU GENERAL PUBLIC LICENSE\n#                        Version 3, 29 June 2007\n#  Copyright (C) 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt;\n#  Everyone is permitted to copy and distribute verbatim copies\n#  of this license document, but changing it is not allowed.\n# ==============================================================================\n\nimport logging\nimport re\nfrom typing import Tuple\n\nimport nltk\nimport numpy as np\nfrom nltk.corpus import wordnet as wn\n\n\nclass ObjectiveTest:\n\t\"\"\"Class abstraction for objective test generation module.\n\t\"\"\"\n\n\tdef __init__(self, filepath: str):\n\t\t\"\"\"Class constructor.\n\n\t\tArgs:\n\t\t\tfilepath (str): filepath (str): Absolute filepath to the subject corpus.\n\t\t\"\"\"\n\t\t# Load subject corpus\n\t\ttry:\n\t\t\twith open(filepath, mode=\"r\") as fp:\n\t\t\t\tself.summary = fp.read()\n\t\texcept FileNotFoundError:\n\t\t\tlogging.exception(\"Corpus file not found.\", exc_info=True)\n\n\tdef generate_test(self, num_questions: int = 3) -&gt; Tuple[list, list]:\n\t\t\"\"\"Method to generate an objective test.\n\n\t\tArgs:\n\t\t\tnum_questions (int, optional): Number of questions in a test.\n\t\t\t\tDefaults to 3.\n\n\t\tReturns:\n\t\t\tTuple[list, list]: Questions and answer options respectively.\n\t\t\"\"\"\n\t\t# Identify potential question sets\n\t\tquestion_sets = self.get_question_sets()\n\n\t\t# Identify potential question answers\n\t\tquestion_answers = list()\n\t\tfor question_set in question_sets:\n\t\t\tif question_set[\"Key\"] &gt; 3:\n\t\t\t\tquestion_answers.append(question_set)\n\n\t\t# Create objective test set\n\t\tquestions, answers = list(), list()\n\t\twhile len(questions) &lt; num_questions:\n\t\t\trand_num = np.random.randint(0, len(question_answers))\n\t\t\tif question_answers[rand_num][\"Question\"] not in questions:\n\t\t\t\tquestions.append(question_answers[rand_num][\"Question\"])\n\t\t\t\tanswers.append(question_answers[rand_num][\"Answer\"])\n\t\treturn questions, answers\n\n\tdef get_question_sets(self) -&gt; list:\n\t\t\"\"\"Method to dentify sentences with potential objective questions.\n\n\t\tReturns:\n\t\t\tlist: Sentences with potential objective questions.\n\t\t\"\"\"\n\t\t# Tokenize corpus into sentences\n\t\ttry:\n\t\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">sentences = nltk.sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(self.summary)</div>\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Sentence tokenization failed.\", exc_info=True)\n\n\t\t# Identify potential question sets\n\t\t# Each question set consists:\n\t\t# \tQuestion: Objective question.\n\t\t# \tAnswer: Actual asnwer.\n\t\t#\tKey: Other options.\n\t\tquestion_sets = list()\n\t\tfor sent in sentences:\n\t\t\tquestion_set = self.identify_potential_questions(sent)\n\t\t\tif question_set is not None:\n\t\t\t\tquestion_sets.append(question_set)\n\t\treturn question_sets\n\n\tdef identify_potential_questions(self, sentence: str) -&gt; dict:\n\t\t\"\"\"Method to identiyf potential question sets.\n\n\t\tArgs:\n\t\t\tsentence (str): Tokenized sequence from corpus.\n\n\t\tReturns:\n\t\t\tdict: Question formed along with the correct answer in case of\n\t\t\t\tpotential sentence else return None.\n\t\t\"\"\"\n\t\t# POS tag sequences\n\t\ttry:\n\t\t\t<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">tag</span>s = nltk.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(sentence)</div>\n\t\t\tif tags[0][1] == \"RB\" or len(<div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">nltk.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>) &lt; 4:\n\t\t\t\treturn None\n\t\texcept Exception:\n\t\t\tlogging.exception(\"POS tagging failed.\", exc_info=True)\n\n\t\t# Define regex grammar to chunk keywords\n\t\tnoun_phrases = list()\n\t\tgrammar = r\"\"\"\n\t\t\tCHUNK: {&lt;NN&gt;+&lt;IN|DT&gt;*&lt;NN&gt;+}\n\t\t\t\t{&lt;NN&gt;+&lt;IN|DT&gt;*&lt;NNP&gt;+}\n\t\t\t\t{&lt;NNP&gt;+&lt;NNS&gt;*}\n\t\t\t\"\"\"\n\n\t\t# Create parser tree\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">chunker = nltk.Regexp<span class=\"fea_parsing_keys udls\">Parser</span>(grammar)</div>\n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_2\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>s = nltk.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>\n\t\t<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_1\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">token</span>s = nltk.<span class=\"fea_Part_of_Speech_keys udls\">tag</span>.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(<span class=\"fea_Part_of_Speech_keys udls\">token</span>s)</div>\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_1\" style=\"display: inline;\">tree = chunker.parse(pos_tokens)</div>\n\n\t\t# Parse tree to identify tokens\n\t\tfor subtree in tree.subtrees():\n\t\t\tif subtree.label() == \"CHUNK\":\n\t\t\t\ttemp = \"\"\n\t\t\t\tfor sub in subtree:\n\t\t\t\t\ttemp += sub[0]\n\t\t\t\t\ttemp += \" \"\n\t\t\t\ttemp = temp.strip()\n\t\t\t\tnoun_phrases.append(temp)\n\n\t\t# Handle nouns\n\t\treplace_nouns = []\n\t\tfor word, _ in tags:\n\t\t\tfor phrase in noun_phrases:\n\t\t\t\tif phrase[0] == '\\'':\n\t\t\t\t\t# If it starts with an apostrophe, ignore it\n\t\t\t\t\t# (this is a weird error that should probably be handled elsewhere)\n\t\t\t\t\tbreak\n\t\t\t\tif word in phrase:\n\t\t\t\t\t# Blank out the last two words in this phrase\n\t\t\t\t\t[replace_nouns.append(phrase_word) for phrase_word in phrase.split()[-2:]]\n\t\t\t\t\tbreak\n\t\t\t# If we couldn't find the word in any phrases\n\t\t\tif len(replace_nouns) == 0:\n\t\t\t\treplace_nouns.append(word)\n\t\t\tbreak\n\n\t\tif len(replace_nouns) == 0:\n\t\t\treturn None\n\n\t\tval = 99\n\t\tfor i in replace_nouns:\n\t\t\tif len(i) &lt; val:\n\t\t\t\tval = len(i)\n\t\t\telse:\n\t\t\t\tcontinue\n\n\t\ttrivial = {\n\t\t\t\"Answer\": \" \".join(replace_nouns),\n\t\t\t\"Key\": val\n\t\t}\n\n\t\tif len(replace_nouns) == 1:\n\t\t\t# If we're only replacing one word, use WordNet to find similar words\n\t\t\ttrivial[\"Similar\"] = self.answer_options(replace_nouns[0])\n\t\telse:\n\t\t\t# If we're replacing a phrase, don't bother - it's too unlikely to make sense\n\t\t\ttrivial[\"Similar\"] = []\n\n\t\treplace_phrase = \" \".join(replace_nouns)\n\t\tblanks_phrase = (\"__________\" * len(replace_nouns)).strip()\n\t\texpression = re.compile(re.escape(replace_phrase), re.IGNORECASE)\n\t\tsentence = expression.sub(blanks_phrase, str(sentence), count=1)\n\t\ttrivial[\"Question\"] = sentence\n\t\treturn trivial\n\n\t@staticmethod\n\tdef answer_options(word: str) -&gt; list:\n\t\t\"\"\"Method to identify incorrect answer options.\n\n\t\tArgs:\n\t\t\tword (str): Actual answer to the question which is to be used\n\t\t\t\tfor generating other deceiving options.\n\n\t\tReturns:\n\t\t\tlist: Answer options.\n\t\t\"\"\"\n\t\t# In the absence of a better method, take the first synset\n\t\ttry:\n\t\t\tsynsets = wn.synsets(word, pos=\"n\")\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Synsets creation failed.\", exc_info=True)\n\n\t\t# If there aren't any synsets, return an empty list\n\t\tif len(synsets) == 0:\n\t\t\treturn []\n\t\telse:\n\t\t\tsynset = synsets[0]\n\n\t\t# Get the hypernym for this synset (again, take the first)\n\t\thypernym = synset.hypernyms()[0]\n\n\t\t# Get some hyponyms from this hypernym\n\t\thyponyms = hypernym.hyponyms()\n\n\t\t# Take the name of the first lemma for the first 8 hyponyms\n\t\tsimilar_words = []\n\t\tfor hyponym in hyponyms:\n\t\t\tsimilar_word = hyponym.lemmas()[0].name().replace(\"_\", \" \")\n\t\t\tif similar_word != word:\n\t\t\t\tsimilar_words.append(similar_word)\n\t\t\tif len(similar_words) == 8:\n\t\t\t\tbreak\n\t\treturn similar_words\n\t\t#https://github.com/nityansuman/marvin/blob/main/src/objective</code></pre></div></body></html>", "fir_23": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_23\"><pre id=\"fir_23_code\"><code class=\"python\">import wikipedia as wiki\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nimport re\n\nimport text2num as t2n\n\nfrom Quiz import Quiz\nfrom QuestionSentence import QuestionSentence\n\nclass Article():\n\n    def __init__ (self, name):\n        self.name = name\n        self.page = wiki.page(name)\n\n        self.quiz = Quiz([])\n\n        self.generate_questions_for(\n            self.page.content.encode('ascii', 'ignore'))\n\n    ''' \n    NOT CURRENTLY USED, but maye be useful at a later point when knowing the\n    section a question was sourced from might be of use.\n    '''\n    # def iterate_sections(self):\n    #     # Iterate through article's sections\n    #     for section in self.page.sections:\n    #         print section\n    #         sec = self.page.section(section).encode('ascii', 'ignore')\n    #         if sec is None: \n    #             continue\n    #         self.generate_questions_for(sec)\n\n    '''\n    tokenizes and chunks a sentence based on a simple grammar\n    '''\n    def get_question_data(self, s):\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>s = nltk.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(s)</div>\n        <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">tag</span>ged = nltk.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(<span class=\"fea_Part_of_Speech_keys udls\">token</span>s)</div>\n        grammar =   \"\"\"  \n                    NUMBER: {&lt;$&gt;*&lt;CD&gt;+&lt;NN&gt;*}\n                    LOCATION: {&lt;IN&gt;&lt;NNP&gt;+&lt;,|IN&gt;&lt;NNP&gt;+} \n                    PROPER: {&lt;NNP|NNPS&gt;&lt;NNP|NNPS&gt;+}\n                    \"\"\"       \n        # \n        # HIT!: {&lt;PROPER&gt;&lt;NN&gt;?&lt;VBZ|VBN&gt;+}\n        # DATE: {&lt;IN&gt;(&lt;$&gt;*&lt;CD&gt;+&lt;NN&gt;*)}\n\n        <div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">chunker = nltk.Regexp<span class=\"fea_parsing_keys udls\">Parser</span>(grammar)</div>\n        <div class=\"highlights fea_parsing\" id=\"parsing_1\" style=\"display: inline;\">result = chunker.parse(tagged)</div>\n        return result\n\n    '''\n    splits a Wikipedia section into sentences and then chunks/tokenizes each\n    sentence\n    '''\n    def generate_questions_for(self, sec):\n        # Rid of all parentheses for easier processing\n        _sec = \"\".join(re.split('\\(', \n            sec.decode(\"utf-8\").replace(\")\", \"(\"))[0::2])\n\n        for sentence in sent_tokenize(_sec):\n            if \"==\" not in sentence:\n                qdata = self.get_question_data(sentence)\n                if len(qdata) &gt;= 75 and len(qdata) &lt;= 150:\n                    qdata = []\n\n                self.create_questions(sentence, qdata)\n\n    '''\n    given a setence in chunked and original form, produce the params necessary\n    to create a Question, and then add that to our Quiz object\n    '''\n    def create_questions(self, sentence, chunked):\n        gaps = []\n        for word in chunked:\n            if type(word) != tuple:                \n                target = []\n                for y in word:\n                    target.append(y[0])\n                orig_phrase = \" \".join(target)\n\n                if word.label() == \"NUMBER\":\n                    modified_phrase = orig_phrase[:]\n\n                    try:\n                        # convert spelled out word to numerical value\n                        modified_phrase = t2n.text2num(phrase)\n                    except:\n                        try:\n                            test = int(modified_phrase) + float(modified_phrase)\n                        except:\n                            # if the word could not be converted and \n                            # was not already numerical, ignore it\n                            continue\n\n                    if self.probably_range(modified_phrase):\n                        return\n\n                    gaps.append((word.label(), orig_phrase, modified_phrase))\n                elif word.label() in [\"LOCATION\", \"PROPER\"]: \n                    gaps.append((word.label(), orig_phrase, orig_phrase))\n\n        if len(gaps) &gt;= 2 and len(gaps) == len(set(gaps)):\n            gaps_filtered = [gap for gap in gaps if gap[0] == 'NUMBER' or gap[0] == 'LOCATION']\n            if len(gaps_filtered) and len(gaps) - len(gaps_filtered) &gt; 2:\n                self.quiz.add(QuestionSentence(sentence, gaps_filtered))\n\n    '''\n    Wikipedia returns non-hyphenated number ranges, so we need to check for mushed together years\n    and remove them. Not a complete solution to the problem, but most of the incidents are years\n    ''' \n    def probably_range(self, val):\n        s = str(val)\n        if s.count(\"19\") &gt; 1 or s.count(\"20\") &gt; 1 or (s.count(\"19\") == 1 and s.count(\"20\") == 1):\n            return True\n        return False\n        #https://github.com/alexgreene/WikiQuiz/blob/master/python/Article</code></pre></div></body></html>", "fir_1": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_1\"><pre id=\"fir_1_code\"><code class=\"python\"># Stopwords removal and lemmatizing them.\n# import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nstop_keywords = [\n    'i',\n    'stack',\n    'overflow',\n    'web',\n    'tutorials',\n    'lesson',\n    'tip',\n    'learn',\n    'reference',\n    'demo',\n    'name',\n    'company',\n    'w3schools',\n    'w3resource',\n    'online',\n    'this',\n]\n\n\ndef text_process(data):\n\n    return_words = []\n\n    for d in data:\n        stop_words = set(<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english')</div>)\n\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">word_<span class=\"fea_tokenization_keys udls\">token</span>s = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(d)</div>\n\n        filtered_sentence = []\n\n        for w in word_tokens:\n            if w not in stop_words:\n                filtered_sentence.append(w)\n        # print(filtered_sentence)\n\n        # Lemmatizing the words.\n        lemma_word = []\n        <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">wordnet_<span class=\"fea_lemmatization_keys udls\">lemma</span>tizer = WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer()\n        for w in filtered_sentence:\n            word1 = wordnet_<span class=\"fea_lemmatization_keys udls\">lemma</span>tizer.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(w, pos=\"n\")\n            word2 = wordnet_<span class=\"fea_lemmatization_keys udls\">lemma</span>tizer.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(word1, pos=\"v\")\n            worfir = wordnet_<span class=\"fea_lemmatization_keys udls\">lemma</span>tizer.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(word2, pos=(\"a\"))</div>\n            lemma_word.append(worfir.lower())\n\n        # print(lemma_word)\n        # Removing unwanted lemmatized words.\n        lemma_word = set(lemma_word)\n        for word in lemma_word.copy():\n            if ((len(word) &lt;= 3) | (word in stop_keywords)):\n                lemma_word.remove(word)\n        # lemma_word = list(lemma_word)\n\n        lemma_word = list(lemma_word)\n    return return_words\n    #https://github.com/dhyeythumar/Search-Engine/blob/master/Python_scripts/text_processing</code></pre></div></body></html>", "fir_2": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_2\"><pre id=\"fir_2_code\"><code class=\"python\"># Tokenize and Stem Data\n# Convert words to Vector Space using TFIDF matrix\n# Calculate Cosine Similarity and generate the distance matrix\n# Uses Ward method to generate an hierarchy\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.manifold import MDS\nimport matplotlibplot as thr\nimport pandas as pd\nfrom scipy.cluster.hierarchy import ward, dendrogram\nimport os\n\n\n# Function to return a list of stemmed words\ndef tokenize_and_stem(text_file):\n    # declaring stemmer and stopwords language\n    <div class=\"highlights fea_stemming\" id=\"stemming_0\" style=\"display: inline;\">stemmer = SnowballStemmer(\"english\")</div>\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span>_<span class=\"fea_nlp_datasets_keys udls\">words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english'))</div>\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">words = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(text_file)</div>\n    filtered = [w for w in words if w not in stop_words]\n    stems = [stemmer.stem(t) for t in filtered]\n    return stems\n\n\ndef main():\n\n    path = os.path.abspath(os.path.dirname(__file__))\n    data = pd.read_csv(os.path.join(path, 'data\\headlines_cleaned.txt'), names=['text'])\n\n    # text data in dataframe and removing stops words\n    stop_words = set(stopwords.words('english'))\n    data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n\n    # Using TFIDF vectorizer to convert convert words to Vector Space\n    tfidf_vectorizer = TfidfVectorizer(max_features=200000,\n                                       use_idf=True,\n                                       stop_words='english',\n                                       tokenizer=tokenize_and_stem)\n    #                                   ngram_range=(1, 3))\n\n    # Fit the vectorizer to text data\n    tfidf_matrix = tfidf_vectorizer.fit_transform(data['text'])\n\n    # Calculating the distance measure derived from cosine similarity\n    distance = 1 - cosine_similarity(tfidf_matrix)\n\n    # Ward\u2019s method produces a hierarchy of clusterings\n    linkage_matrix = ward(distance)\n    fig, ax = thr.subplots(figsize=(15, 20)) # set size\n    ax = dendrogram(linkage_matrix, orientation=\"top\", labels=data.values)\n    thr.tight_layout()\n    thr.title('News Headlines using Ward Hierarchical Method')\n    thr.savefig(os.path.join(path, 'results\\hierarchical.png'))\n\n\nif __name__ == '__main__':\n    main()\n    #https://github.com/maneeshavinayak/Clustering-News-Headlines/blob/master/clustering/hierarchical</code></pre></div></body></html>", "fir_3": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_3\"><pre id=\"fir_3_code\"><code class=\"python\">import json\nimport numpy as np \nimport urllib\nfrom urllib.request import urlopen\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nimport requests\nimport re\nfrom flask import Flask,request,jsonify,make_response\nimport nltk\nfrom flask_cors import CORS\nimport newspaper\nimport pandas as pd\nimport json\nimport requests\nimport time\n\nurl = \"https://graph.facebook.com/v2.6/me/messages\"\nngrok_url = 'https://the-daily-news-app.herokuapp.com/api/'\n\nsource_csv = pd.read_csv('https://raw.githubusercontent.com/codequipo/TheDailyNews/deploy/sites.csv')\n\napp = Flask(__name__)\nCORS(app)\n\nPORT=8087\n\nimport csv\n\nli_all=[]\nkey_name_all=[]\n\n@app.route('/',methods = ['GET','POST'])\ndef base():\n\treturn 'hello world'\n\n\n@app.route('/webhook', methods=['GET', 'POST'])\ndef webhook():\n\treq = request.get_json(force=True)\n\tprint(\"-\"*80)\n\tprint(req.get('queryResult').get('intent').get('displayName'))\n\tprint(\"-\"*80)\n\tif req.get('queryResult').get('intent').get('displayName') == 'summarize_intent':\n\t\tdatabase_id = req.get('queryResult').get('queryText').strip('Summarize:')\n\t\tsummary_url = ngrok_url + 'getSummary'\n\t\tdata2 = {'unique_id': database_id }\n\t\tsummary = requests.post(summary_url,data = data2)\n\t\tarticle_dict = json.loads(summary.text)\n\t\tarticle_summary = article_dict['article'][0]['text']\n\t\tarticl_image = article_dict['article'][0]['top_image']\n\t\treturn {'fulfillmentText': article_summary}\n\n\tif req.get('queryResult').get('intent').get('displayName') == 'source_intent':\n\t\tsource_name = req.get('queryResult').get('queryText')[7:]\n\t\tsource_csv.columns = ['name','link']\n\t\tsource_link = source_csv['link'][source_csv.loc[source_csv['name']==source_name].index[0]]\n\t\tnews_url = ngrok_url + 'getnewsbysources'\n\t\tdata1 = {'main_urls':source_link }\n\t\tpost_articles = requests.post(news_url,data = data1)\n\t\tlist_of_articles = json.loads(post_articles.text)\n\t\tli = list_of_articles['articles']\n\t\tmessages = []\n\t\tprint(len(li))\n\t\tfor index,item in enumerate(li):\n\t\t\ttemp = dict()\n\t\t\t\n\t\t\treq = urllib.request.Request('https://raw.githubusercontent.com/codequipo/TheDailyNews/flask_deploy/format.json')\n\t\t\twith urllib.request.urlopen(req) as f:\n    \t\t\t\ttemp = json.load(f)\n\t\t\t\n\t\t\t\t\n\t\t\ttemp['card']['buttons'][0]['postback'] = 'Summarize:'+item['unique_id']\n\t\t\ttemp['card']['title'] = item['title']\n\t\t\ttemp['card']['imageUri'] = item['top_image']\n\t\t\tprint(item['url'])\n\t\t\ttemp['card']['buttons'][1]['postback'] = item['url']\n\t\t\tmessages.append(temp)\n\n\t\tmessages = messages[:10]\n\t\t# print(messages)\n\t\treturn jsonify({'fulfillmentMessages': messages })  \n\t\n\n\treturn{'fulfillmentText':\"Please check your responses again  \"}\n\ndef getSourceData():\n\turl = 'https://raw.githubusercontent.com/codequipo/TheDailyNews/deploy/sites.csv'\n\tdf = pd.read_csv(url, error_bads=False)\n\turl_list = []\n\tkey_list = []\n\turl_list = df[\"http://www.huffingtonpost.com\"].values.tolist()\n\tkey_list = df[\"huffingtonpost\"].values.tolist()\n\turl_list = [\"http://www.huffingtonpost.com\"] + url_list\n\tkey_list = [\"huffingtonpost\"] + key_list\n\treturn key_list,url_list\n\nkey_list, url_list= getSourceData()\n\n@app.route(\"/db\",methods=['POST','GET'])\ndef build_database():\n\ttic=time.time()\n\t# key_list,url_list = getSourceData()\n\tjson_body=request.get_json(force=True)\n\tcurrCount = int(json_body.get('currCount'))\n\tnumOfSources = int(json_body.get('numOfSources'))\n\tnumOfArticlesPerSources = int(json_body.get('numOfArticlesPerSources'))\n\tnum_of_sentences_in_summary = int(json_body.get('num_of_sentences_in_summary'))\n\n\tprint('currCount : '+str(currCount))\n\n\t\n\tresponse_data=dict()\n\tfor i in range(currCount,currCount+numOfSources):\n\t\turl=url_list[i]\n\t\tsource = newspaper.build( url, memoize_articles=True, language='en')\n\t\t\n\t\td=dict() # Holds articles from current selected source \n\t\tk=0\n\t\t\n\t\tfor article in source.articles:\n\t\t\ttry:\n\t\t\t\tarticle.download() \n\t\t\t\tarticle.parse() \n\t\t\t\tsummary = driver(article.text,num_of_sentences_in_summary)\n\t\t\t\t\n\t\t\t\tarticle_info=dict()\n\t\t\t\tarticle_info['url']=article.url\n\t\t\t\tarticle_info['title']=article.title\n\t\t\t\tprint('i:'+str(i)+'  k:'+str(k)+'  title  =&gt; '+article_info['title'])\n\t\t\t\tarticle_info['text']=summary\n\t\t\t\tarticle_info['top_image']=article.top_image\n\n\t\t\t\td[k]=article_info\n\t\t\t\t\n\t\t\t\t\n\t\t\t\tk+=1\n\t\t\t\tif k == numOfArticlesPerSources:\n\t\t\t\t\tbreak\n\t\t\texcept Exception as e:\n\t\t\t\tprint(\"Entered except block :\"+str(e))\n\t\t\t\tpass\n\t\td['length']=k\n\t\tresponse_data[url]=d\n\n\t\tprint(url+\"   NewArticles : \"+str(k))\n\n\tresult={\n\t\t'success':True,\n\t\t'alldata':response_data,\n\t\t'allsite':url_list[currCount:currCount+numOfSources],\n\t\t'allsite_key':key_list[currCount:currCount+numOfSources]\n\t}\n\ttoc=time.time()\n\tdiff=toc-tic\n\tprint(\"# Time required for function to execute is :\"+str(diff)+\" # \")\n\tprint()\n\tprint()\n\treturn json.dumps(result)\n\t\n\n\ndef clean(sentences):\n\tlemmatizer = WordNetLemmatizer()\n\tcleaned_sentences = []\n\tfor sentence in sentences:\n\t\tsentence = sentence.lower()\n\t\tsentence = re.sub(r'[^a-zA-Z]',' ',sentence)\n\t\tsentence = sentence.split()\n\t\tsentence = [<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_keys udls\">lemma</span>tizer.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(word)</div> for word in sentence if word not in <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english'))</div>]\n\t\tsentence = ' '.join(sentence)\n\t\tcleaned_sentences.append(sentence)\n\treturn cleaned_sentences\n\ndef init_probability(sentences):\n\tprobability_dict = {}\n\t<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">words = word_<span class=\"fea_tokenization_keys udls\">token</span>ize('. '.join(sentences))</div>\n\ttotal_words = len(set(words))\n\tfor word in words:\n\t\tif word!='.':\n\t\t\tif not probability_dict.get(word):\n\t\t\t\tprobability_dict[word] = 1\n\t\t\telse:\n\t\t\t\tprobability_dict[word] += 1\n\n\tfor word,count in probability_dict.items():\n\t\tprobability_dict[word] = count/total_words \n\t\n\treturn probability_dict\n\ndef update_probability(probability_dict,word):\n\tif probability_dict.get(word):\n\t\tprobability_dict[word] = probability_dict[word]**2\n\treturn probability_dict\n\ndef average_sentence_weights(sentences,probability_dict):\n\tsentence_weights = {}\n\tfor index,sentence in enumerate(sentences):\n\t\tif len(sentence) != 0:\n\t\t\taverage_proba = sum([probability_dict[word] for word in sentence if word in probability_dict.keys()])\n\t\t\taverage_proba /= len(sentence)\n\t\t\tsentence_weights[index] = average_proba \n\treturn sentence_weights\n\ndef generate_summary(sentence_weights,probability_dict,cleaned_article,tokenized_article,summary_length = 30):\n\tsummary = \"\"\n\tcurrent_length = 0\n\twhile current_length &lt; summary_length :\n\t\thighest_probability_word = max(probability_dict,key=probability_dict.get)\n\t\tsentences_with_max_word= [index for index,sentence in enumerate(cleaned_article) if highest_probability_word in <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">set(word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence))</div>]\n\t\tsentence_list = sorted([[index,sentence_weights[index]] for index in sentences_with_max_word],key=lambda x:x[1],reverse=True)\n\t\tsummary += tokenized_article[sentence_list[0][0]] + \"\\n\"\n\t\tfor word in <div class=\"highlights fea_tokenization\" id=\"tokenization_2\" style=\"display: inline;\">word_<span class=\"fea_tokenization_keys udls\">token</span>ize(cleaned_article[sentence_list[0][0]])</div>:\n\t\t\tprobability_dict = update_probability(probability_dict,word)\n\t\tcurrent_length+=1\n\treturn summary\n\ndef driver(article,required_length):\n\trequired_length = int(required_length)\n\t<div class=\"highlights fea_tokenization\" id=\"tokenization_3\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>ized_article = sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(article)</div>\n\tcleaned_article = clean(tokenized_article) \n\tprobability_dict = init_probability(cleaned_article)\n\tsentence_weights = average_sentence_weights(cleaned_article,probability_dict)\n\t<div class=\"highlights fea_summarizer\" id=\"summarizer_0\" style=\"display: inline;\"><span class=\"fea_summarizer_keys udls\">summar</span>y = generate_<span class=\"fea_summarizer_keys udls\">summar</span>y(<span class=\"fea_summarizer_keys udls\">sentence</span>_weights,probability_dict,cleaned_article,tokenized_article,required_length)</div>\n\treturn summary\n\nif __name__ == \"__main__\":\n    app.run(port=PORT)\n    #https://github.com/codequipo/TheDailyNews/blob/master/flask_server/app</code></pre></div></body></html>", "fir_6": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_6\"><pre id=\"fir_6_code\"><code class=\"python\">import os,re,math,csv,string,random,logging,glob,itertools,operator,sys\nfrom os import listdir\nfrom os.path import isfile, join\nfrom collections import Counter, defaultdict, OrderedDict\nfrom itertools import chain, combinations\n\nimport pandas as pd\nimport numpy as np\nimport scipy\nfrom scipy import spatial\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet as wn\nfrom nltk.tag.stanford import StanfordPOSTagger\nfrom nltk.util import ngrams\n\nimport gensim\nfrom gensim.models import word2vec\n\ndef InitialCleanup(dataframe,\n                   minwords=2,\n                   use_filler_list=None,\n                   filler_regex_and_list=False):\n\n    \"\"\"\n    Perform basic text cleaning to prepare dataframe\n    for analysis. Remove non-letter/-space characters,\n    empty turns, turns below a minimum length, and\n    fillers.\n\n    By default, preserves turns 2 words or longer.\n    If desired, this may be changed by updating the\n    `minwords` argument.\n\n    By default, remove common fillers through regex.\n    If desired, remove other words by passing a list\n    of literal strings to `use_filler_list` argument,\n    and if both regex and list of additional literal\n    strings are to be used, update `filler_regex_and_list=True`.\n    \"\"\"\n\n    # only allow strings, spaces, and newlines to pass\n    WHITELIST = string.ascii_letters + '\\'' + ' '\n\n    # remove inadvertent empty turns\n    dataframe = dataframe[pd.notnull(dataframe['content'])]\n\n    # internal function: remove fillers via regular expressions\n    def applyRegExpression(textFiller):\n        textClean = re.sub('^(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]+\\s', ' ', textFiller) # at the start of a string\n        textClean = re.sub('\\s(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]+\\s', ' ', textClean) # within a string\n        textClean = re.sub('\\s(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]$', ' ', textClean) # end of a string\n        textClean = re.sub('^(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]$', ' ', textClean) # if entire turn string\n        return textClean\n\n    # create a new column with only approved text before cleaning per user-specified settings\n    dataframe['clean_content'] = dataframe['content'].apply(lambda utterance: ''.join([char for char in utterance if char in WHITELIST]).lower())\n\n    # DEFAULT: remove typical speech fillers via regular expressions (examples: \"um, mm, oh, hm, uh, ha\")\n    if use_filler_list is None and not filler_regex_and_list:\n        dataframe['clean_content'] = dataframe['clean_content'].apply(applyRegExpression)\n\n    # OPTION 1: remove speech fillers or other words specified by user in a list\n    elif use_filler_list is not None and not filler_regex_and_list:\n        dataframe['clean_content'] = dataframe['clean_content'].apply(lambda utterance: ' '.join([word for word in utterance.split(\" \") if word not in use_filler_list]))\n\n    # OPTION 2: remove speech fillers via regular expression and any additional words from user-specified list\n    elif use_filler_list is not None and filler_regex_and_list:\n        dataframe['clean_content'] = dataframe['clean_content'].apply(applyRegExpression)\n        dataframe['clean_content'] = dataframe['clean_content'].apply(lambda utterance: ' '.join([word for word in utterance.split(\" \") if word not in use_filler_list]))\n\n    # OPTION 3: nothing is filtered\n    else:\n        dataframe['clean_content'] = dataframe['clean_content']\n\n    # drop the old \"content\" column and rename the clean \"content\" column\n    dataframe = dataframe.drop(['content'],axis=1)\n    dataframe = dataframe.rename(index=str,\n                                 columns ={'clean_content': 'content'})\n\n    # remove rows that are now blank or do not meet `minwords` requirement, then drop length column\n    dataframe['utteranceLen'] = dataframe['content'].apply(lambda x: <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">word_<span class=\"fea_tokenization_keys udls\">token</span>ize(x)).str.len()</div>\n    dataframe = dataframe.drop(dataframe[dataframe.utteranceLen &lt; int(minwords)].index).drop(['utteranceLen'],axis=1)\n    dataframe = dataframe.reset_index(drop=True)\n\n    # return the cleaned dataframe\n    return dataframe\n\ndef AdjacentMerge(dataframe):\n\n    \"\"\"\n    Given a dataframe of conversation turns,\n    merge adjacent turns by the same speaker.\n    \"\"\"\n\n    repeat=1\n    while repeat==1:\n        l1=len(dataframe)\n        DfMerge = []\n        k = 0\n        if len(dataframe) &gt; 0:\n            while k &lt; len(dataframe)-1:\n                if dataframe['participant'].iloc[k] != dataframe['participant'].iloc[k+1]:\n                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])\n                    k = k + 1\n                elif dataframe['participant'].iloc[k] == dataframe['participant'].iloc[k+1]:\n                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k] + \" \" + dataframe['content'].iloc[k+1]])\n                    k = k + 2\n            if k == len(dataframe)-1:\n                DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])\n\n        dataframe=pd.DataFrame(DfMerge,columns=('participant','content'))\n        if l1==len(dataframe):\n            repeat=0\n\n    return dataframe\n\ndef Tokenize(text,nwords):\n    \"\"\"\n    Given list of text to be processed and a list\n    of known words, return a list of edited and\n    tokenized words.\n\n    Spell-checking is implemented using a\n    Bayesian spell-checking algorithm\n    (http://norvig.com/spell-correct.html).\n\n    By default, this is based on the Project Gutenberg\n    corpus, a collection of approximately 1 million texts\n    (http://www.gutenberg.org). A copy of this is included\n    within this package. If desired, users may specify a\n    different spell-check training corpus in the\n    `training_dictionary` argument of the\n    `prepare_transcripts()` function.\n\n    \"\"\"\n\n    # internal function: identify possible spelling errors for a given word\n    def edits1(word):\n        splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes    = [a + b[1:] for a, b in splits if b]\n        transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)&gt;1]\n        replaces   = [a + c + b[1:] for a, b in splits for c in string.ascii_lowercase if b]\n        inserts    = [a + c + b     for a, b in splits for c in string.ascii_lowercase]\n        return set(deletes + transposes + replaces + inserts)\n\n    # internal function: identify known edits\n    def known_edits2(word,nwords):\n        return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in nwords)\n\n    # internal function: identify known words\n    def known(words,nwords): return set(w for w in words if w in nwords)\n\n    # internal function: correct spelling\n    def correct(word,nwords):\n        candidates = known([word],nwords) or known(edits1(word),nwords) or known_edits2(word,nwords) or [word]\n        return max(candidates, key=nwords.get)\n\n    # expand out based on a fixed list of common contractions\n    contract_dict = { \"ain't\": \"is not\",\n        \"aren't\": \"are not\",\n        \"can't\": \"cannot\",\n        \"can't've\": \"cannot have\",\n        \"'cause\": \"because\",\n        \"could've\": \"could have\",\n        \"couldn't\": \"could not\",\n        \"couldn't've\": \"could not have\",\n        \"didn't\": \"did not\",\n        \"doesn't\": \"does not\",\n        \"don't\": \"do not\",\n        \"hadn't\": \"had not\",\n        \"hadn't've\": \"had not have\",\n        \"hasn't\": \"has not\",\n        \"haven't\": \"have not\",\n        \"he'd\": \"he had\",\n        \"he'd've\": \"he would have\",\n        \"he'll\": \"he will\",\n        \"he'll've\": \"he will have\",\n        \"he's\": \"he is\",\n        \"how'd\": \"how did\",\n        \"how'd'y\": \"how do you\",\n        \"how'll\": \"how will\",\n        \"how's\": \"how is\",\n        \"i'd\": \"i would\",\n        \"i'd've\": \"i would have\",\n        \"i'll\": \"i will\",\n        \"i'll've\": \"i will have\",\n        \"i'm\": \"i am\",\n        \"i've\": \"i have\",\n        \"isn't\": \"is not\",\n        \"it'd\": \"it would\",\n        \"it'd've\": \"it would have\",\n        \"it'll\": \"it will\",\n        \"it'll've\": \"it will have\",\n        \"it's\": \"it is\",\n        \"let's\": \"let us\",\n        \"ma'am\": \"madam\",\n        \"mayn't\": \"may not\",\n        \"might've\": \"might have\",\n        \"mightn't\": \"might not\",\n        \"mightn't've\": \"might not have\",\n        \"must've\": \"must have\",\n        \"mustn't\": \"must not\",\n        \"mustn't've\": \"must not have\",\n        \"needn't\": \"need not\",\n        \"needn't've\": \"need not have\",\n        \"o'clock\": \"of the clock\",\n        \"oughtn't\": \"ought not\",\n        \"oughtn't've\": \"ought not have\",\n        \"shan't\": \"shall not\",\n        \"sha'n't\": \"shall not\",\n        \"shan't've\": \"shall not have\",\n        \"she'd\": \"she would\",\n        \"she'd've\": \"she would have\",\n        \"she'll\": \"she will\",\n        \"she'll've\": \"she will have\",\n        \"she's\": \"she is\",\n        \"should've\": \"should have\",\n        \"shouldn't\": \"should not\",\n        \"shouldn't've\": \"should not have\",\n        \"so've\": \"so have\",\n        \"so's\": \"so as\",\n        \"that'd\": \"that had\",\n        \"that'd've\": \"that would have\",\n        \"that's\": \"that is\",\n        \"there'd\": \"there would\",\n        \"there'd've\": \"there would have\",\n        \"there's\": \"there is\",\n        \"they'd\": \"they would\",\n        \"they'd've\": \"they would have\",\n        \"they'll\": \"they will\",\n        \"they'll've\": \"they will have\",\n        \"they're\": \"they are\",\n        \"they've\": \"they have\",\n        \"to've\": \"to have\",\n        \"wasn't\": \"was not\",\n        \"we'd\": \"we would\",\n        \"we'd've\": \"we would have\",\n        \"we'll\": \"we will\",\n        \"we'll've\": \"we will have\",\n        \"we're\": \"we are\",\n        \"we've\": \"we have\",\n        \"weren't\": \"were not\",\n        \"what'll\": \"what will\",\n        \"what'll've\": \"what will have\",\n        \"what're\": \"what are\",\n        \"what's\": \"what is\",\n        \"what've\": \"what have\",\n        \"when's\": \"when is\",\n        \"when've\": \"when have\",\n        \"where'd\": \"where did\",\n        \"where's\": \"where is\",\n        \"where've\": \"where have\",\n        \"who'll\": \"who will\",\n        \"who'll've\": \"who will have\",\n        \"who's\": \"who is\",\n        \"who've\": \"who have\",\n        \"why's\": \"why is\",\n        \"why've\": \"why have\",\n        \"will've\": \"will have\",\n        \"won't\": \"will not\",\n        \"won't've\": \"will not have\",\n        \"would've\": \"would have\",\n        \"wouldn't\": \"would not\",\n        \"wouldn't've\": \"would not have\",\n        \"y'all\": \"you all\",\n        \"y'all'd\": \"you all would\",\n        \"y'all'd've\": \"you all would have\",\n        \"y'all're\": \"you all are\",\n        \"y'all've\": \"you all have\",\n        \"you'd\": \"you would\",\n        \"you'd've\": \"you would have\",\n        \"you'll\": \"you will\",\n        \"you'll've\": \"you will have\",\n        \"you're\": \"you are\",\n        \"you've\": \"you have\" }\n    contractions_re = re.compile('(%s)' % '|'.join(list(contract_dict.keys())))\n\n    # internal function:\n    def expand_contractions(text, contractions_re=contractions_re):\n        def replace(match):\n            return contract_dict[match.group(0)]\n        return contractions_re.sub(replace, text.lower())\n\n    # process all words in the text\n    cleantoken = []\n    text = expand_contractions(text)\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span> = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(text)</div>\n    for word in token:\n        if \"'\" not in word:\n            cleantoken.append(correct(word,nwords))\n        else:\n            cleantoken.append(word)\n    return cleantoken\n\n\ndef pos_to_wn(tag):\n    \"\"\"\n    Convert NLTK default tagger output into a format that Wordnet\n    can use in order to properly lemmatize the text.\n    \"\"\"\n\n    # create some inner functions for simplicity\n    def is_noun(tag):\n        return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n    def is_verb(tag):\n        return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n    def is_adverb(tag):\n        return tag in ['RB', 'RBR', 'RBS']\n    def is_adjective(tag):\n        return tag in ['JJ', 'JJR', 'JJS']\n\n    # check each tag against possible categories\n    <div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">if is_<span class=\"fea_n_grams_keys udls\">noun</span>(tag):\n        return wn.<span class=\"fea_n_grams_keys udls\">NOUN</span>\n    elif is_<span class=\"fea_n_grams_keys udls\">verb</span>(tag):\n        return wn.<span class=\"fea_n_grams_keys udls\">VERB</span>\n    elif is_ad<span class=\"fea_n_grams_keys udls\">verb</span>(tag):\n        return wn.ADV\n    elif is_ad<span class=\"fea_n_grams_keys udls\">ject</span>ive(tag):\n        return wn.ADJ\n    else:\n        return wn.<span class=\"fea_n_grams_keys udls\">NOUN</span></div>\n\n\ndef Lemmatize(tokenlist):\n    l<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">emmatizer = WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer()</div>\n    <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\">default<span class=\"fea_Part_of_Speech_keys udls\">Pos</span> = nltk.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(<span class=\"fea_Part_of_Speech_keys udls\">token</span>list)</div> # get the POS tags from NLTK default tagger\n    words_lemma = []\n    for item in defaultPos:\n        words_lemma.append(<div class=\"highlights fea_lemmatization\" id=\"lemmatization_1\" style=\"display: inline;\"><span class=\"fea_lemmatization_keys udls\">lemma</span>tizer.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(item[0],pos_to_wn(item[1]))</div>) # need to convert POS tags to a format (NOUN, VERB, ADV, ADJ) that wordnet uses to lemmatize\n    return words_lemma\n\n\ndef ApplyPOSTagging(df,\n                    filename,\n                    add_stanford_tags=False,\n                    stanford_pos_path=None,\n                    stanford_language_path=None):\n\n    \"\"\"\n    Given a dataframe of conversation turns, return a new\n    dataframe with part-of-speech tagging. Add filename\n    (given as string) as a new column in returned dataframe.\n\n    By default, return only tags from the NLTK default POS\n    tagger. Optionally, also return Stanford POS tagger\n    results by setting `add_stanford_tags=True`.\n\n    If Stanford POS tagging is desired, specify the\n    location of the Stanford POS tagger with the\n    `stanford_pos_path` argument. Also note that the\n    default language model for the Stanford tagger is\n    English (english-left3words-distsim.tagger). To change\n    language model, specify the location with the\n    `stanford_language_path` argument.\n\n    \"\"\"\n\n    # if desired, import Stanford tagger\n    if add_stanford_tags:\n        if stanford_pos_path is None or stanford_language_path is None:\n            raise ValueError('Error! Specify path to Stanford POS tagger and language model using the `stanford_pos_path` and `stanford_language_path` arguments')\n        else:\n            <div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">stanford_<span class=\"fea_tagger_keys udls\">tagge</span>r = StanfordPOS<span class=\"fea_tagger_keys udls\">Tagge</span>r(stanford_pos_path + stanford_language_path,\n                                                stanford_pos_path + 'stanford-pos<span class=\"fea_tagger_keys udls\">tagge</span>r.jar')</div>\n\n    # add new columns to dataframe\n    df['tagged_token'] = df['token'].apply(<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_1\" style=\"display: inline;\">nltk.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span></div>)\n    df['tagged_lemma'] = df['lemma'].apply(nltk.pos_tag)\n\n    # if desired, also tag with Stanford tagger\n    if add_stanford_tags:\n        df['tagged_stan_token'] = df['token'].apply(<div class=\"highlights fea_tagger\" id=\"tagger_1\" style=\"display: inline;\">stanford_<span class=\"fea_tagger_keys udls\">tagge</span>r.tag</div>)\n        df['tagged_stan_lemma'] = df['lemma'].apply(stanford_tagger.tag)\n\n    df['file'] = filename\n\n    # return finished dataframe\n    return df\n\ndef prepare_transcripts(input_files,\n                        output_file_directory,\n                        training_dictionary=None,\n                        minwords=2,\n                        use_filler_list=None,\n                        filler_regex_and_list=False,\n                        add_stanford_tags=False,\n                        stanford_pos_path=None,\n                        stanford_language_path=None,\n                        input_as_directory=True,\n                        save_concatenated_dataframe=True):\n\n    \"\"\"\n    Prepare transcripts for similarity analysis.\n\n    Given individual .txt files of conversations,\n    return a completely prepared dataframe of transcribed\n    conversations for later ALIGN analysis, including: text\n    cleaning, merging adjacent turns, spell-checking,\n    tokenization, lemmatization, and part-of-speech tagging.\n    The output serve as the input for later ALIGN\n    analysis.\n\n    Parameters\n    ----------\n\n    input_files : str (directory name) or list of str (file names)\n        Raw files to be cleaned. Behavior governed by `input_as_directory`\n        parameter as well.\n\n    output_file_directory : str\n        Name of directory where output for individual conversations will be\n        saved.\n\n    training_dictionary : str, optional (default: None)\n        Specify whether to train the spell-checking dictionary using a\n        provided file name (str) or the default Project\n        Gutenberg corpus [http://www.gutenberg.org] (None).\n\n    minwords : int, optional (2)\n        Specify the minimum number of words in a turn. Any turns with fewer\n        than the minimum number of words will be removed from the corpus.\n        (Note: `minwords` must be equal to or greater than `maxngram` provided\n        to `calculate_alignment()` and `calculate_baseline_alignment` in later\n        steps.)\n\n    use_filler_list : list of str, optional (default: None)\n        Specify whether words should be filtered from all conversations using a\n        list of filler words (list of str) or using regular expressions to\n        filter out common filler words (None). Behavior governed by\n        `filler_regex_and_list` parameter as well.\n\n    filler_regex_and_list : boolean, optional (default: False)\n        If providing a list to `use_filler_list` parameter, specify whether to\n        use only the provided list (False) or to use both the provided list and\n        the regular expression filter (True).\n\n    add_stanford_tags : boolean, optional (default: False)\n        Specify whether to return part-of-speech similarity scores based on\n        Stanford POS tagger in addition to the Penn POS tagger (True) or to\n        return only POS similarity scores from the Penn tagger (False). (Note:\n        Including Stanford POS tags will lead to a significant increase in\n        processing time.)\n\n    stanford_pos_path : str, optional (default: None)\n        If Stanford POS tagging is desired, specify local path to Stanford POS\n        tagger.\n\n    stanford_language_path : str, optional (default: None)\n        If Stanford POS tagging is desired, specify local path to Stanford POS\n        tagger for the desired language (str) or use the default English tagger\n        (None).\n\n    input_as_directory : boolean, optional (default: True)\n        Specify whether the value passed to `input_files` parameter should\n        be read as a directory (True) or a list of files to be processed\n        (False).\n\n    save_concatenated_dataframe : boolean, optional (default: True)\n        Specify whether to save the individual conversation output data only\n        as individual files in the `output_file_directory` (False) or to save\n        the individual files as well as a single concatenated dataframe (True).\n\n    Returns\n    -------\n\n    prepped_df : Pandas DataFrame\n        A single concatenated dataframe of all transcripts, ready for\n        processing with `calculate_alignment()` and\n        `calculate_baseline_alignment()`.\n\n    \"\"\"\n\n    # create an internal function to train the model\n    def train(features):\n        model = defaultdict(lambda: 1)\n        for f in features:\n            model[f] += 1\n        return model\n\n    # if no training dictionary is specified, use the Gutenberg corpus\n    if training_dictionary is None:\n\n        # first, get the name of the package directory\n        module_path = os.path.dirname(os.path.abspath(__file__))\n\n        # then construct the path to the text file\n        training_dictionary = os.path.join(module_path, 'data/gutenberg.txt')\n\n    # train our spell-checking model\n    nwords = train(re.findall('[a-z]+', (open(training_dictionary).read().lower())))\n\n    # grab the appropriate files\n    if not input_as_directory:\n        file_list = glob.glob(input_files)\n    else:\n        file_list = glob.glob(input_files+\"/*.txt\")\n\n    # cycle through all files\n    prepped_df = pd.DataFrame()\n    for fileName in file_list:\n\n        # let us know which file we're processing\n        print((\"Processing: \"+fileName))\n        dataframe = pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n\n        # clean up, merge, spellcheck, tokenize, lemmatize, and POS-tag\n        dataframe = InitialCleanup(dataframe,\n                                   minwords=minwords,\n                                   use_filler_list=use_filler_list,\n                                   filler_regex_and_list=filler_regex_and_list)\n        dataframe = AdjacentMerge(dataframe)\n\n        # tokenize and lemmatize\n        dataframe['token'] = dataframe['content'].apply(Tokenize,\n                                     args=(nwords,))\n        dataframe['lemma'] = dataframe['token'].apply(Lemmatize)\n\n        # apply part-of-speech tagging\n        dataframe = ApplyPOSTagging(dataframe,\n                                    filename=os.path.basename(fileName),\n                                    add_stanford_tags=add_stanford_tags,\n                                    stanford_pos_path=stanford_pos_path,\n                                    stanford_language_path=stanford_language_path)\n\n        # export the conversation's dataframe as a CSV\n        conversation_file = os.path.join(output_file_directory,os.path.basename(fileName))\n        dataframe.to_csv(conversation_file, encoding='utf-8',index=False,sep='\\t')\n        prepped_df = prepped_df.append(dataframe)\n\n    # save the concatenated dataframe\n    if save_concatenated_dataframe:\n        concatenated_file = os.path.join(output_file_directory,'../align_concatenated_dataframe.txt')\n        prepped_df.to_csv(concatenated_file,\n                    encoding='utf-8',index=False, sep='\\t')\n\n    # return the dataframe\n    return prepped_df\n    #https://github.com/nickduran/align-linguistic-alignment/blob/master/align/prepare_transcripts</code></pre></div></body></html>", "fir_8": "<html><body><div class=\"codeBlock hljs xml\" id=\"fir_8\"><pre id=\"fir_8_code\"><code class=\"python\"># -*- coding: utf-8 -*-\n# Implementation from https://dev.to/davidisrawi/build-a-quick-summarizer-with-python-and-nltk\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\nfrom framework.parser.parser import Parser\n\ntext_str = '''\n&lt;header&gt;My name is Wil Wheaton. I Live With Chronic Depression and Generalized Anxiety. I Am Not Ashamed.&lt;/header&gt;&lt;p&gt;Before I begin, I want to warn you that this talk touches on many triggering subjects, including self-harm and suicide. I also want you to know that I\u2019m speaking from my personal experience, and that if you or someone you know may be living with mental illness, please talk to a licensed and qualified medical professional, because I am not a doctor.&lt;/p&gt;&lt;p&gt;Okay, let\u2019s do this.&lt;/p&gt;&lt;p&gt;Hi, I\u2019m Wil Wheaton. I\u2019m 45 years-old, I have a wonderful wife, two adult children who make me proud every day, and a daughter in-law who I love like she\u2019s my own child. I work on the most popular comedy series in the world, I\u2019ve been a New York Times Number One Bestselling Audiobook narrator, I have run out of space in my office for the awards I\u2019ve received for my work, and as a white, heterosexual, cisgender man in America, I live life on the lowest difficulty setting \u2014 with the Celebrity cheat enabled.&lt;/p&gt;&lt;p&gt;&lt;b&gt;My life is, by every objective measurement, very very good.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;And in spite of all of that, I struggle every day with my self esteem, my self worth, and my value not only as an actor and writer, but as a human being.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;That\u2019s because I live with Depression and Anxiety, the tag team champions of the World Wrestling With Mental Illness Federation.&lt;/p&gt;&lt;p&gt;And I\u2019m not ashamed to stand here, in front of six hundred people in this room, and millions more online, and proudly say that I live with mental illness, and that\u2019s okay. I say \u201cwith\u201d because even though my mental illness tries its best, it doesn\u2019t control me, it doesn\u2019t define me, and I refuse to be stigmatized by it.&lt;/p&gt;&lt;p&gt;&lt;b&gt;So. My name is Wil Wheaton, and I have Chronic Depression.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;It took me over thirty years to be able to say those ten words, and I suffered for most of them as a result. I suffered because though we in America have done a lot to help people who live with mental illness, we have not done nearly enough to make it okay for our fellow travelers on the wonky brain express to reach out and accept that help.&lt;/p&gt;&lt;p&gt;I\u2019m here today to talk with you about working to end the stigma and prejudice that surrounds mental illness in America, and as part of that, I want to share my story with you.&lt;/p&gt;&lt;p&gt;When I was a little kid, probably seven or eight years old, I started having panic attacks. Back then, we didn\u2019t know that\u2019s what they were, and because they usually happened when I was asleep, the adults in my life just thought I had nightmares. Well, I did have nightmares, but they were so much worse than just bad dreams. Night after night, I\u2019d wake up in absolute terror, and night after night, I\u2019d drag my blankets off my bed, to go to sleep on the floor in my sister\u2019s bedroom, because I was so afraid to be alone.&lt;/p&gt;&lt;p&gt;There were occasional stretches of relief, sometimes for months at a time, and during those months, I felt like what I considered to be a normal kid, but the panic attacks always came back, and each time they came back, they seemed worse than before.&lt;/p&gt;&lt;p&gt;When I was around twelve or thirteen, my anxiety began to express itself in all sorts of delightful ways.&lt;/p&gt;&lt;p&gt;I worried about everything. I was tired all the time, and irritable most of the time. I had no confidence and terrible self-esteem. I felt like I couldn\u2019t trust anyone who wanted to be close to me, because I was convinced that I was stupid and worthless and the only reason anyone would want to be my friend was to take advantage of my fame.&lt;/p&gt;&lt;p&gt;This is important context. When I was thirteen, I was in an internationally-beloved film called Stand by Me, and I was famous. Like, really famous, like, can\u2019t-go-to-the-mall-with-my-friends-without-getting-mobbed famous, and that meant that all of my actions were scrutinized by my parents, my peers, my fans, and the press. All the weird, anxious feelings I had all the time? I\u2019d been raised to believe that they were shameful. That they reflected poorly on my parents and my family. That they should be crammed down deep inside me, shared with nobody, and kept secret.&lt;/p&gt;&lt;p&gt;My panic attacks happened daily, and not just when I was asleep. When I tried to reach out to the adults in my life for help, they didn\u2019t take me seriously. When I was on the set of a tv show or commercial, and I was having a hard time breathing because I was so anxious about making a mistake and getting fired? The directors and producers complained to my parents that I was being difficult to work with. When I was so uncomfortable with my haircut or my crooked teeth and didn\u2019t want to pose for teen magazine photos, the publicists told me that I was being ungrateful and trying to sabotage my success. When I couldn\u2019t remember my lines, because I was so anxious about things I can\u2019t even remember now, directors would accuse me of being unprofessional and unprepared. And that\u2019s when my anxiety turned into depression.&lt;/p&gt;&lt;header&gt;(I\u2019m going to take a moment for myself right now, and I\u2019m going to tear a hole in the fabric of spacetime and I\u2019m going to tell all those adults from the past: give this kid a break. He\u2019s scared. He\u2019s confused. He is doing the best he can, and if you all could stop seeing him as a way to put money into your pockets, maybe you could see that he\u2019s suffering and needs help.)&lt;/header&gt;&lt;p&gt;I was miserable a lot of the time, and it didn\u2019t make any sense. I was living a childhood dream, working on Star Trek: The Next Generation, and getting paid to do what I loved. I had all the video games and board games I ever wanted, and did I mention that I was famous?&lt;/p&gt;&lt;p&gt;I struggled to reconcile the facts of my life with the reality of my existence. I knew something was wrong with me, but I didn\u2019t know what. And because I didn\u2019t know what, I didn\u2019t know how to ask for help.&lt;/p&gt;&lt;p&gt;I wish I had known that I had a mental illness that could be treated! I wish I had known that that the way I felt wasn\u2019t normal and it wasn\u2019t necessary. I wish I had known that I didn\u2019t deserve to feel bad, all the time.&lt;/p&gt;&lt;p&gt;And I didn\u2019t know those things, because Mental Illness was something my family didn\u2019t talk about, and when they did, they talked about it like it was something that happened to someone else, and that it was something they should be ashamed of, because it was a result of something they did. This prejudice existed in my family in spite of the ample incidence of mental illness that ran rampant through my DNA, featuring successful and unsuccessful suicide attempts by my relations, more than one case of bipolar disorder, clinical depression everywhere, and, because of self-medication, so much alcoholism, it was actually notable when someone didn\u2019t have a drinking problem.&lt;/p&gt;&lt;p&gt;Now, I don\u2019t blame my parents for how they addressed \u2014 or more accurately didn\u2019t address \u2014 my mental illness, because I genuinely believe they were blind to the symptoms I was exhibiting. They grew up and raised me in the world I\u2019ve spent the last decade of my life trying to change. They lived in a world where mental illness was equated with weakness, and shame, and as a result, I suffered until I was in my thirties.&lt;/p&gt;&lt;p&gt;And it\u2019s not like I never reached out for help. I did! I just didn\u2019t know what questions to ask, and the adults I was close to didn\u2019t know what answers to give.&lt;/p&gt;&lt;p&gt;Mom, I know you\u2019re going to read this or hear this and I know it\u2019s going to make you upset. I want you to know that I love you, and I know that you did the very best you could. I\u2019m telling my story, though, so someone else\u2019s mom can see the things you didn\u2019t, through no fault of your own.&lt;/p&gt;&lt;p&gt;I clearly remember being twenty-two, living in my own house, waking up from a panic attack that was so terrifying just writing about it for this talk gave me so much anxiety I almost cut this section from my speech. It was the middle of the night, and I drove across town, to my parents\u2019 house, to sleep on the floor of my sister\u2019s bedroom again, because at least that\u2019s where I felt safe. The next morning, I tearfully asked my mom what was wrong with me. She knew that many of my blood relatives had mental illness, but she couldn\u2019t or wouldn\u2019t connect the dots. \u201cYou\u2019re just realizing that the world is a scary place,\u201d she said.&lt;/p&gt;&lt;p&gt;Yeah, no kidding. The world terrifies me every night of my life and I don\u2019t know why or how to stop it.&lt;/p&gt;&lt;p&gt;Again, I don\u2019t blame her and neither should you. She really was doing the best that she could for me, but stigma and the shame is inspires are powerful things.&lt;/p&gt;&lt;p&gt;I want to be very clear on this: Mom, I know you\u2019re going to read this or hear this and I know it\u2019s going to make you upset. I want you to know that I love you, and I know that you did the very best you could. I\u2019m telling my story, though, so someone else\u2019s mom can see the things you didn\u2019t, through no fault of your own.&lt;/p&gt;&lt;p&gt;Through my twenties, I continued to suffer, and not just from nightmares and panic attacks. I began to develop obsessive behaviors that I\u2019ve never talked about in public until right now. Here\u2019s a very incomplete list: I began to worry that the things I did would affect the world around me in totally irrational ways. I would hold my breath underneath bridges when I was driving, because if I didn\u2019t, maybe I\u2019d crash my car. I would tap the side of an airplane with my hand while I was boarding, and tell it to take care of me when I flew places for work, because I was convinced that if I didn\u2019t, the plane would crash. Every single time I said goodbye to someone I cared about, my brain would play out in vivid detail how I would remember this as the last time I saw them. Talking about those memories, even without getting into specifics, is challenging. It\u2019s painful to recall, but I\u2019m not ashamed, because all those thoughts \u2014 which I thankfully don\u2019t have any more, thanks to medical science and therapy \u2014 were not my fault any more than the allergies that clog my sinuses when the trees in my neighborhood start doin\u2019 it every spring are my fault. It\u2019s just part of who I am. It\u2019s part of how my brain is wired, and because I know that, I can medically treat it, instead of being a victim of it.&lt;/p&gt;&lt;p&gt;One of the primary reasons I speak out about my mental illness, is so that I can make the difference in someone\u2019s life that I wish had been made in mine when I was young, because not only did I have no idea what Depression even was until I was in my twenties, once I was pretty sure that I had it, I suffered with it for another fifteen years, because I was ashamed, I was embarrassed, and I was afraid.&lt;/p&gt;&lt;p&gt;So I am here today to tell anyone who can hear me: if you suspect that you have a mental illness, there is no reason to be ashamed, or embarrassed, and most importantly, you do not need to be afraid. You do not need to suffer. There is nothing noble in suffering, and there is nothing shameful or weak in asking for help. This may seem really obvious to a lot of you, but it wasn\u2019t for me, and I\u2019m a pretty smart guy, so I\u2019m going to say it anyway: There is no reason to feel embarrassed when you reach out to a professional for help, because the person you are reaching out to is someone who has literally dedicated their life to helping people like us live, instead of merely exist.&lt;/p&gt;&lt;p&gt;I missed out on a lot of things, during what are supposed to be the best years of my life, because I was paralyzed by What If-ing anxiety.&lt;/p&gt;&lt;p&gt;That difference, between existing and living, is something I want to focus on for a minute: before I got help for my anxiety and depression, I didn\u2019t truly live my life. I wanted to go do things with my friends, but my anxiety always found a way to stop me. Traffic would just be too stressful, it would tell me. It\u2019s going to be a real hassle to get there and find parking, it would helpfully observe. And if those didn\u2019t stop me from leaving my house, there was always the old reliable: What if\u2026? Ah, \u201cWhat if\u2026 something totally unlikely to happen actually happens? What if the plane crashes? What if I sit next to someone who freaks me out? What if they laugh at me? What if I get lost? What if I get robbed? What if I get locked out of my hotel room? What if I slip on some ice I didn\u2019t see? What if there\u2019s an earthquake? What if what if what if what if\u2026&lt;/p&gt;&lt;p&gt;When I look back on most of my life, it breaks my heart that when my brain was unloading an endless pile of what ifs on me, it never asked, \u201cWhat if I go do this thing that I want to do, and it\u2019s \u2026 fun? What if I enjoy myself, and I\u2019m really glad I went?\u201d&lt;/p&gt;&lt;p&gt;I have to tell you a painful truth: I missed out on a lot of things, during what are supposed to be the best years of my life, because I was paralyzed by What If-ing anxiety.&lt;/p&gt;&lt;p&gt;All the things that people do when they are living their lives \u2026 all those experiences that make up a life, my anxiety got in between me and doing them. So I wasn\u2019t living. I was just existing.&lt;/p&gt;&lt;p&gt;And through it all, I never stopped to ask myself if this was normal, or healthy, or even if it was my fault. I just knew that I was nervous about stuff, and I worried a lot. For my entire childhood, my mom told me that I was a worry wart, and my dad said I was overly dramatic about everything, and that\u2019s just the way it was.&lt;/p&gt;&lt;p&gt;Except it didn\u2019t have to be that way, and it took me having a full blown panic attack and a complete meltdown at Los Angeles International Airport for my wife to suggest to me that I get help.&lt;/p&gt;&lt;p&gt;Like I said, I had suspected for years that I was clinically depressed, but I was afraid to admit it, until the most important person in my life told me without shame or judgment that she could see that I was suffering. So I went to see a doctor, and I will never forget what he said, when I told him how afraid I was: \u201cPlease let me help you.\u201d&lt;/p&gt;&lt;p&gt;I think it was then, at about 34 years-old, that I realized that Mental Illness is not weakness. It\u2019s just an illness. I mean, it\u2019s right there in the name \u201cMental ILLNESS\u201d so it shouldn\u2019t have been the revelation that it was, but when the part of our bodies that is responsible for how we perceive the world and ourselves is the same part of our body that is sick, it can be difficult to find objectivity or perspective.&lt;/p&gt;&lt;p&gt;So I let my doctor help me. I started a low dose of an antidepressant, and I waited to see if anything was going to change.&lt;/p&gt;&lt;p&gt;And boy did it.&lt;/p&gt;&lt;p&gt;My wife and I were having a walk in our neighborhood and I realized that it was just a really beautiful day \u2014 it was warm with just a little bit of a breeze, the birds sounded really beautiful, the flowers smelled really great and my wife\u2019s hand felt really good in mine.&lt;/p&gt;&lt;p&gt;And as we were walking I just started to cry and she asked me, \u201cwhat\u2019s wrong?\u201d&lt;/p&gt;&lt;p&gt;I said \u201cI just realized that I don\u2019t feel bad and I just realized that I\u2019m not existing, I\u2019m living.\u201d&lt;/p&gt;&lt;p&gt;At that moment, I realized that I had lived my life in a room that was so loud, all I could do every day was deal with how loud it was. But with the help of my wife, my doctor, and medical science, I found a doorway out of that room.&lt;/p&gt;&lt;p&gt;I had taken that walk with my wife almost every day for nearly ten years, before I ever noticed the birds or the flowers, or how loved I felt when I noticed that her hand was holding mine. Ten years \u2014 all of my twenties \u2014 that I can never get back. Ten years of suffering and feeling weak and worthless and afraid all the time, because of the stigma that surrounds mental illness.&lt;/p&gt;&lt;p&gt;I\u2019m not religious, but I can still say Thank God for Anne Wheaton. Thank God for her love and support. Thank God that my wife saw that I was hurting, and thank God she didn\u2019t believe the lie that Depression is weakness, or something to be ashamed of. Thank God for Anne, because if she hadn\u2019t had the strength to encourage me to seek professional help, I don\u2019t know how much longer I would have been able to even exist, to say nothing of truly living.&lt;/p&gt;&lt;p&gt;I started talking in public about my mental illness in 2012, and ever since then, people reach out to me online every day, and they ask me about living with depression and anxiety. They share their stories, and ask me how I get through a bad day, or a bad week.&lt;/p&gt;&lt;header&gt;Right now, there is a child somewhere who has the same panic attacks I had, and their parents aren\u2019t getting them help, because they believe it reflects poorly on their parenting to have a child with mental illness.&lt;/header&gt;&lt;p&gt;Here\u2019s one of the things I tell them:&lt;/p&gt;&lt;p&gt;One of the many delightful things about having Depression and Anxiety is occasionally and unexpectedly feeling like the whole goddamn world is a heavy lead blanket, like that thing they put on your chest at the dentist when you get x-rays, and it\u2019s been dropped around your entire existence without your consent.&lt;/p&gt;&lt;p&gt;Physically, it weighs heavier on me in some places than it does in others. I feel it tugging at the corners of my eyes, and pressing down on the center of my chest. When it\u2019s really bad, it can feel like one of those dreams where you try to move, but every step and every motion feels like you\u2019re struggling to move through something heavy and viscous. Emotionally, it covers me completely, separating me from my motivation, my focus, and everything that brings me joy in my life.&lt;/p&gt;&lt;p&gt;When it drops that lead apron over us, we have to remind ourselves that one of the things Depression does, to keep itself strong and in charge, is tell us lies, like: I am the worst at everything. Nobody really likes me. I don\u2019t deserve to be happy. This will never end. And so on and so on. We can know, in our rational minds, that this is a giant bunch of bullshit (and we can look at all these times in our lives when were WERE good at a thing, when we genuinely felt happy, when we felt awful but got through it, etc.) but in the moment, it can be a serious challenge to wait for Depression to lift the roadblock that\u2019s keeping us from moving those facts from our rational mind to our emotional selves.&lt;/p&gt;&lt;p&gt;And that\u2019s the thing about Depression: we can\u2019t force it to go away. As I\u2019ve said, if I could just \u201cstop feeling sad\u201d I WOULD. (And, also, Depression isn\u2019t just feeling sad, right? It\u2019s a lot of things together than can manifest themselves into something that is most easily simplified into \u201cI feel sad.\u201d)&lt;/p&gt;&lt;p&gt;So another step in our self care is to be gentle with ourselves. Depression is beating up on us already, and we don\u2019t need to help it out. Give yourself permission to acknowledge that you\u2019re feeling terrible (or bad, or whatever it is you are feeling), and then do a little thing, just one single thing, that you probably don\u2019t feel like doing, and I PROMISE you it will help. Some of those things are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Take a shower. &lt;/li&gt;&lt;li&gt;Eat a nutritious meal. &lt;/li&gt;&lt;li&gt;Take a walk outside (even if it\u2019s literally to the corner and back). &lt;/li&gt;&lt;li&gt;Do something \u2014 throw a ball, play tug of war, give belly rubs \u2014 with a dog. Just about any activity with my dogs, even if it\u2019s just a snuggle on the couch for a few minutes, helps me. &lt;/li&gt;&lt;li&gt;Do five minutes of yoga stretching. &lt;/li&gt;&lt;li&gt;Listen to a guided meditation and follow along as best as you can. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Finally, please trust me and know that this shitty, awful, overwhelming, terrible way you feel IS NOT FOREVER. It will get better. It always gets better. You are not alone in this fight, and you are OK.&lt;/p&gt;&lt;p&gt;No person anywhere, especially here in the richest country in the world, should live in the shadows or suffer alone, because they can\u2019t afford treatment. We have all the money in the world for weapons and corporate tax cuts, so I know that we can afford to prioritize not just health care in general, but mental health care, specifically.&lt;/p&gt;&lt;p&gt;Right now, there is a child somewhere who has the same panic attacks I had, and their parents aren\u2019t getting them help, because they believe it reflects poorly on their parenting to have a child with mental illness. Right now, there is a teenager who is contemplating self harm, because they don\u2019t know how to reach out and ask for help. Right now, there are too many people struggling just to get to the end of the day, because they can\u2019t afford the help that a lot of us can\u2019t live without. But there are also people everywhere who are picking up the phone and making an appointment. There are parents who have learned that mental illness is no different than physical illness, and they\u2019re helping their children get better. There are adults who, like me, were terrified that antidepressant medication would make them a different person, and they\u2019re hearing the birds sing for the first time, because they have finally found their way out of the dark room.&lt;/p&gt;&lt;p&gt;I spent the first thirty years of my life trapped in that dark, loud room, and I know how hopeless and suffocating it feels to be in there, so I do everything I can to help others find their way out. I do that by telling my story, so that my privilege and success does more than enrich my own life. I can live by example for someone else the way Jenny Lawson lives by example for me.&lt;/p&gt;&lt;p&gt;But I want to leave you today with some suggestions for things that we can all do, even if you\u2019re not Internet Famous like I am, to help end the stigma of mental illness, so that nobody has to merely exist, when they could be living.&lt;/p&gt;&lt;p&gt;We can start by demanding that our elected officials fully fund mental health programs. No person anywhere, especially here in the richest country in the world, should live in the shadows or suffer alone, because they can\u2019t afford treatment. We have all the money in the world for weapons and corporate tax cuts, so I know that we can afford to prioritize not just health care in general, but mental health care, specifically.&lt;/p&gt;&lt;p&gt;And until our elected officials get their acts together, we can support organizations like NAMI, that offer low and no-cost assistance to anyone who asks for it. We can support organizations like Project UROK, that work tirelessly to end stigmatization and remind us that we are sick, not weak.&lt;/p&gt;&lt;p&gt;We can remember, and we can remind each other, that there is no finish line when it comes to mental illness. It\u2019s a journey, and sometimes we can see the path we\u2019re on all the way to the horizon, while other times we can\u2019t even see five feet in front of us because the fog is so thick. But the path is always there, and if we can\u2019t locate it on our own, we have loved ones and doctors and medications to help us find it again, as long as we don\u2019t give up trying to see it.&lt;/p&gt;&lt;p&gt;Finally, we who live with mental illness need to talk about it, because our friends and neighbors know us and trust us. It\u2019s one thing for me to stand here and tell you that you\u2019re not alone in this fight, but it\u2019s something else entirely for you to prove it. We need to share our experiences, so someone who is suffering the way I was won\u2019t feel weird or broken or ashamed or afraid to seek treatment. So that parents don\u2019t feel like they have failed or somehow screwed up when they see symptoms in their kids.&lt;/p&gt;&lt;p&gt;People tell me that I\u2019m brave for speaking out the way I do, and while I appreciate that, I don\u2019t necessarily agree. Firefighters are brave. Single parents who work multiple jobs to take care of their kids are brave. The Parkland students are brave. People who reach out to get help for their mental illness are brave. I\u2019m not brave. I\u2019m just a writer and occasional actor who wants to share his privilege and good fortune with the world, who hopes to speak out about mental health so much that one day, it will be wholly unremarkable to stand up and say fifteen words:&lt;/p&gt;&lt;p&gt;My name is Wil Wheaton, I live with chronic depression, and I am not ashamed.&lt;/p&gt;\n'''\n\n# All weightage for structure doc\n# Important: These scores are for the experimenting purpose only\nWEIGHT_FOR_LIST = 5\nWEIGHT_FOR_HIGHLIGHTED = 10\nWEIGHT_FOR_NUMERICAL = 5\nWEIGHT_FIRST_PARAGRAPH = 5\nWEIGHT_BASIC = 1\n\n\ndef _create_frequency_table(paragraph_list) -&gt; dict:\n    \"\"\"\n    we create a dictionary for the word frequency table.\n    For this, we should only use the words that are not part of the stopWords array.\n\n    Removing stop words and making frequency table\n    Stemmer - an algorithm to bring words to its root word.\n    :rtype: dict\n    \"\"\"\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">Words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>(\"english\"))</div>\n\n    <div class=\"highlights fea_stemming\" id=\"stemming_0\" style=\"display: inline;\">ps = PorterStemmer()</div>\n\n    freqTable = dict()\n    for paragraph in paragraph_list:\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">words = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(paragraph.text)</div>\n\n        all_highlighted_sentences = [sent for sent in paragraph.get_highlighted()]\n        highlighted_words_text = \" \".join(all_highlighted_sentences)\n        highlighted_words = word_tokenize(highlighted_words_text)\n\n        for word in words:\n\n            if paragraph.is_list_set:\n                weight = WEIGHT_FOR_LIST\n            else:\n                weight = WEIGHT_BASIC\n\n            if word in highlighted_words:\n                weight += WEIGHT_FOR_HIGHLIGHTED\n\n            if word.isnumeric() and len(word) &gt;= 2:\n                weight += WEIGHT_FOR_NUMERICAL\n\n            if paragraph.is_first_paragraph:\n                weight += WEIGHT_FIRST_PARAGRAPH\n\n            word = ps.stem(word)\n            if word in stopWords:\n                continue\n\n            if word in freqTable:\n                freqTable[word] += weight\n            else:\n                freqTable[word] = weight\n\n    return freqTable\n\n\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">def _<span class=\"fea_text_scoring_keys udls\">score</span>_s<span class=\"fea_text_scoring_keys udls\">ent</span>ences(s<span class=\"fea_text_scoring_keys udls\">ent</span>ences, freqTable) -&gt; dict:</div>\n    \"\"\"\n    score a sentence by its words\n    Basic algorithm: adding the frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n    :rtype: dict\n    \"\"\"\n    # TODO: Can you make this multiprocess compatible in python?\n\n    sentenceValue = dict()\n\n    for sentence in sentences:\n        word_count_in_sentence = (len(word_tokenize(sentence)))\n        word_count_in_sentence_except_stop_words = 0\n        for wordValue in freqTable:\n            if wordValue in sentence.lower():\n                word_count_in_sentence_except_stop_words += 1\n                if sentence[:10] in sentenceValue:\n                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n                else:\n                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n\n        if sentence[:10] in sentenceValue:\n            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] / word_count_in_sentence_except_stop_words\n\n        '''\n        Notice that a potential issue with our score algorithm is that long sentences will have an advantage over short sentences. \n        To solve this, we're dividing every sentence score by the number of words in the sentence.\n        \n        Note that here sentence[:10] is the first 10 character of any sentence, this is to save memory while saving keys of\n        the dictionary.\n        '''\n\n    return sentenceValue\n\n\ndef _find_average_score(sentenceValue) -&gt; int:\n    \"\"\"\n    Find the average score from the sentence value dictionary\n    :rtype: int\n    \"\"\"\n    sumValues = 0\n    for entry in sentenceValue:\n        sumValues += sentenceValue[entry]\n\n    average = 0\n    # Average value of a sentence from original summary_text\n    if len(sentenceValue) &gt; 0:\n        average = (sumValues / len(sentenceValue))\n\n    return average\n\n\ndef _generate_summary(sentences, sentenceValue, threshold):\n    sentence_count = 0\n    summary = ''\n\n    for sentence in sentences:\n        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] &gt;= (threshold):\n            summary += \" \" + sentence\n            sentence_count += 1\n\n    # TODO: check if the sentences in the summarization is in the original order of occurrence.\n\n    return summary\n\n\ndef run_summarization(paragraph_list):\n    # 1 Create the word frequency table\n    freq_table = _create_frequency_table(paragraph_list)\n    # print (freq_table)\n\n    '''\n    We already have a sentence tokenizer, so we just need \n    to run the sent_tokenize() method to create the array of sentences.\n    '''\n\n    # 2 Tokenize the sentences\n    sentences = [paragraph.text for paragraph in paragraph_list]\n    # print(sentences)\n\n    # 3 Important Algorithm: score the sentences\n    sentence_scores = _score_sentences(sentences, freq_table)\n\n    # 4 Find the threshold\n    threshold = _find_average_score(sentence_scores)\n\n    # 5 Important Algorithm: Generate the summary\n    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n\n    return summary\n\n\nif __name__ == '__main__':\n    parser = Parser()\n    parser.feed(text_str)\n    <div class=\"highlights fea_summarizer\" id=\"summarizer_0\" style=\"display: inline;\">result = run_<span class=\"fea_summarizer_keys udls\">summar</span>ization(parser.paragraphs)</div>\n    print(result)\n    #https://github.com/akashp1712/summarize-webpage/blob/master/implementation/word_frequency_summarize_parser</code></pre></div></body></html>", "fir_9": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_9\"><pre id=\"fir_9_code\"><code class=\"python\">import constants\nimport nltk\nimport random\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport string\nimport warnings\n\ndef get_formalities_response(formality) :\n    if any(remove_punctuation_marks(formality).lower() in remove_punctuation_marks(greet).lower() for greet in constants.GREETING_INPUTS) :\n        return random.choice(constants.GREETING_REPLIES)\n    elif any(remove_punctuation_marks(formality).lower() in remove_punctuation_marks(thanks).lower() for thanks in constants.THANKS_INPUTS) :\n        return random.choice(constants.THANKS_REPLIES)\n\ndef get_lemmatized_tokens(text) :\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">normalized_<span class=\"fea_tokenization_keys udls\">token</span>s = nltk.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(remove_punctuation_marks(text.lower()))</div>\n    return [<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">nltk.stem.WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer().<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(normalized_token)</div> for normalized_token in normalized_tokens]\n\ndef get_query_reply(query) :\n    documents.append(query)\n    tfidf_results = TfidfVectorizer(tokenizer = get_lemmatized_tokens, stop_words = 'english').fit_transform(documents)\n    cosine_similarity_results = cosine_similarity(tfidf_results[-1], tfidf_results).flatten()\n    # The last will be 1.0 because it is the Cosine Similarity between the first document and itself\n    best_index = cosine_similarity_results.argsort()[-2]\n    documents.remove(query)\n    if cosine_similarity_results[best_index] == 0 :\n        return \"I am sorry! I don't understand you...\"\n    else :\n        return documents[best_index]\n\ndef remove_punctuation_marks(text) :\n    punctuation_marks = dict((ord(punctuation_mark), None) for punctuation_mark in string.punctuation)\n    return text.translate(punctuation_marks)\n\nif __name__ == \"__main__\" :\n    warnings.filterwarnings(\"ignore\")\n\n    try :\n        nltk.data.find('tokenizers/punkt')\n    except LookupError:\n        nltk.download('punkt')\n\n    try :\n        nltk.data.find('corpora/wordnet')\n    except LookupError:\n        <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">nltk.download('wordnet')</div>\n\n    corpus = open('corpus.txt', 'r' , errors = 'ignore').read().lower()\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">documents = nltk.sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(corpus)</div>\n\n    <div class=\"highlights fea_chatbot\" id=\"chatbot_0\" style=\"display: inline;\">print('RyuzakiBot: My name is RyuzakiBot. I will answer your queries about World Wide Web. If you want to exit just type: Bye!')\n    end_chat = False\n    while end_chat == False :\n        input_text = input()\n        if remove_punctuation_marks(input_text).lower() != 'bye' :\n            formality_reply = get_formalities_response(input_text)\n            if  formality_reply :\n                print('RyuzakiBot: ' + formality_reply)\n            else :\n                print('RyuzakiBot: ' + get_query_reply(input_text))\n        else :\n            print('RyuzakiBot: Bye! Take care ' + random.choice(constants.CANDIES))\n            end_chat = True</div>\n            #https://github.com/LuciaLlavero/ryuzaki_bot/blob/master/ryuzaki_bot_desktop</code></pre></div></body></html>", "fir_10": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_10\"><pre id=\"fir_10_code\"><code class=\"python\"># coding: utf-8\n\nimport constants\nfrom flask import Flask, jsonify, request\nfrom flask_cors import CORS\nfrom flask_restful import Resource, Api\nimport nltk\nimport random\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport string\nimport sys\nimport warnings\n\ndef get_formalities_reply(formality) :\n    if any(remove_punctuation_marks(formality).lower() in remove_punctuation_marks(greet).lower() for greet in constants.GREETING_INPUTS) :\n        return random.choice(constants.GREETING_REPLIES)\n    elif any(remove_punctuation_marks(formality).lower() in remove_punctuation_marks(thanks).lower() for thanks in constants.THANKS_INPUTS) :\n        return random.choice(constants.THANKS_REPLIES)\n\ndef get_lemmatized_tokens(text):\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">normalized_<span class=\"fea_tokenization_keys udls\">token</span>s = nltk.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(remove_punctuation_marks(text.lower()))</div>\n    return [<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">nltk.stem.WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer().<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(normalized_token)</div> for normalized_token in normalized_tokens]\n\ncorpus = open('corpus.txt', 'r' , errors = 'ignore').read().lower()\ndocuments = nltk.sent_tokenize(corpus)\n\ndef get_query_reply(query) :    \n    documents.append(query)\n    tfidf_results = TfidfVectorizer(tokenizer = get_lemmatized_tokens, stop_words = 'english').fit_transform(documents)\n    cosine_similarity_results = cosine_similarity(tfidf_results[-1], tfidf_results).flatten()\n    # The last will be 1.0 because it is the Cosine Similarity between the first document and itself\n    best_index = cosine_similarity_results.argsort()[-2]\n    documents.remove(query)\n    if cosine_similarity_results[best_index] == 0 :\n        return \"I am sorry! I don't understand you...\"\n    else :\n        return documents[best_index]\n\ndef remove_punctuation_marks(text) :\n    punctuation_marks = dict((ord(punctuation_mark), None) for punctuation_mark in string.punctuation)\n    return text.translate(punctuation_marks)\n\napp = Flask(__name__)\ncors = CORS(app)\napi = Api(app)\n\nclass Reply(Resource) :\n    def get(self) :\n        if request.args.get('q') :\n            formality_reply = get_formalities_reply(request.args.get('q'))\n            if  formality_reply :\n                return jsonify({'reply': formality_reply + ' ' + random.choice(constants.SWEETS)})\n            else :\n                return jsonify({'reply': get_query_reply(request.args.get('q'))})\n        else :\n            return jsonify({'error': 'query is empty'})\n\napi.add_resource(Reply, '/reply.json')\n\nif __name__ == \"__main__\" :\n\n    app.run()\n    #https://github.com/LuciaLlavero/ryuzaki_bot/blob/master/ryuzaki_bot</code></pre></div></body></html>", "fir_11": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_11\"><pre id=\"fir_11_code\"><code class=\"python\">import nltk\nimport re\nfrom newspaper import Article\nfrom geograpy.labels import Labels\n\nclass Extractor(object):\n    '''\n    Extract geo context for text or from url\n    '''\n    def __init__(self, text=None, url=None, debug=False):\n        '''\n        Constructor\n        Args:\n\n            text(string): the text to analyze\n            url(string): the url to read the text to analyze from\n            debug(boolean): if True show debug information\n        '''\n        if not text and not url:\n            raise Exception('text or url is required')\n        self.debug=debug\n        self.text = text\n        self.url = url\n        self.places = []\n\n    def set_text(self):\n        '''\n        Setter for text\n        '''\n        if not self.text and self.url:\n            a = Article(self.url)\n            a.download()\n            a.parse()\n            self.text = a.text\n            \n    def split(self,delimiter=r\",\"):\n        '''\n        simpler regular expression splitter with not entity check\n        \n        hat tip: https://stackoverflow.com/a/1059601/1497139\n        '''\n        self.set_text()\n        self.places=re.split(delimiter,self.text)\n            \n    def find_geoEntities(self):\n        '''\n        Find geographic entities\n        \n        Returns:\n            list: \n                List of places\n        '''\n        self.find_entities(Labels.geo)\n        return self.places\n        \n    def find_entities(self,labels=Labels.default):\n        '''\n        Find entities with the given labels set self.places and returns it\n        Args:\n            labels: \n                Labels: The labels to filter\n        Returns:\n            list: \n                List of places\n        '''\n        self.set_text()\n\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">text = nltk.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(self.text)</div>\n        nes = nltk.ne_chunk(<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\">nltk.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(text)</div>)\n\n        for ne in nes:\n            if type(ne) is nltk.tree.Tree:\n                nelabel=ne.label()\n                if (nelabel in labels):\n                    leaves=ne.leaves()\n                    if self.debug:\n                        print(leaves)\n                    self.places.append(u' '.join([i[0] for i in leaves]))\n        return self.places\n        #https://github.com/somnathrakshit/geograpy3/blob/master/geograpy/extraction</code></pre></div></body></html>", "fir_12": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_12\"><pre id=\"fir_12_code\"><code class=\"python\">import sys,re,collections,nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# patterns that used to find or/and replace particular chars or words\n# to find chars that are not a letter, a blank or a quotation\npat_letter = re.compile(r'[^a-zA-Z \\']+')\n# to find the 's following the pronouns. re.I is refers to ignore case\npat_is = re.compile(\"(it|he|she|that|this|there|here)(\\'s)\", re.I)\n# to find the 's following the letters\npat_s = re.compile(\"(?&lt;=[a-zA-Z])\\'s\")\n# to find the ' following the words ending by s\npat_s2 = re.compile(\"(?&lt;=s)\\'s?\")\n# to find the abbreviation of not\npat_not = re.compile(\"(?&lt;=[a-zA-Z])n\\'t\")\n# to find the abbreviation of would\npat_would = re.compile(\"(?&lt;=[a-zA-Z])\\'d\")\n# to find the abbreviation of will\npat_will = re.compile(\"(?&lt;=[a-zA-Z])\\'ll\")\n# to find the abbreviation of am\npat_am = re.compile(\"(?&lt;=[I|i])\\'m\")\n# to find the abbreviation of are\npat_are = re.compile(\"(?&lt;=[a-zA-Z])\\'re\")\n# to find the abbreviation of have\npat_ve = re.compile(\"(?&lt;=[a-zA-Z])\\'ve\")\n\n\nlmtzr = WordNetLemmatizer()\n\n\ndef get_words(file):  \n    with open (file) as f:  \n        words_box=[]\n        pat = re.compile(r'[^a-zA-Z \\']+')\n        for line in f:                           \n            #if re.match(r'[a-zA-Z]*',line): \n            #    words_box.extend(line.strip().strip('\\'\\\"\\.,').lower().split())\n            # words_box.extend(pat.sub(' ', line).strip().lower().split())\n            words_box.extend(merge(replace_abbreviations(line).split()))\n    return collections.Counter(words_box)  \n\n\ndef merge(words):\n    new_words = []\n    for word in words:\n        if word:\n            tag = <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\">nltk.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span></div>(<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">word_<span class=\"fea_tokenization_keys udls\">token</span>ize(word)</div>) # tag is like [('bigger', 'JJR')]\n            pos = get_wordnet_pos(tag[0][1])\n            if pos:\n                <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_keys udls\">lemma</span>tized_word = lmtzr.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(word, pos)</div>\n                new_words.append(lemmatized_word)\n            else:\n                new_words.append(word)\n    return new_words\n\n\ndef get_wordnet_pos(treebank_tag):\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">if treebank_tag.startswith('J'):\n        return nltk.corpus.wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return nltk.corpus.wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return nltk.corpus.wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return nltk.corpus.wordnet.ADV</div>\n    else:\n        return ''\n\n\ndef replace_abbreviations(text):\n    new_text = text\n    new_text = pat_letter.sub(' ', text).strip().lower()\n    new_text = pat_is.sub(r\"\\1 is\", new_text)\n    new_text = pat_s.sub(\"\", new_text)\n    new_text = pat_s2.sub(\"\", new_text)\n    new_text = pat_not.sub(\" not\", new_text)\n    new_text = pat_would.sub(\" would\", new_text)\n    new_text = pat_will.sub(\" will\", new_text)\n    new_text = pat_am.sub(\" am\", new_text)\n    new_text = pat_are.sub(\" are\", new_text)\n    new_text = pat_ve.sub(\" have\", new_text)\n    new_text = new_text.replace('\\'', ' ')\n    return new_text\n\n\ndef append_ext(words):\n    new_words = []\n    for item in words:\n        word, count = item\n        tag = nltk.pos_tag(word_tokenize(word))[0][1] # tag is like [('bigger', 'JJR')]\n        new_words.append((word, count, tag))\n    return new_words\n\ndef write_to_file(words, file='results.txt'):\n    f = open(file, 'w')\n    for item in words:\n        for field in item:\n            f.write(str(field)+',')\n        f.write('\\n')\n\n\nif __name__=='__main__':\n    book = sys.argv[1]\n    print \"counting...\"\n    words = get_words(book)\n    print \"writing file...\"\n    write_to_file(append_ext(words.most_common()))\n    #https://github.com/rocketk/wordcounter/blob/master/wordcounter/word_counter</code></pre></div></body></html>", "fir_13": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_13\"><pre id=\"fir_13_code\"><code class=\"python\"># -*- coding: utf-8 -*-\nimport numpy as np\nimport json\nimport sys\nimport re\nimport os\nimport ast\nimport argparse\nimport argcomplete\nimport multiprocessing\nfrom functools import partial\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nimport gensim\nfrom gensim import utils, corpora, models\nimport io\n\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">ignore_<span class=\"fea_nlp_datasets_keys udls\">words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english'))</div>\n\n''' from the model that was created, you can calculate the topic probability distribution of unseen documents.\n    this is a command line interface using Gensim for preprocessing unseen\n    documents and calculating topic probability distributions over a given topology from an LDA model '''\n\ndef write_topn_words(output_dir, lda):\n    if not os.path.exists(output_dir + 'topn_words.json'):\n        print('Writing topn words for LDA model')\n        reg_ex = re.compile('(?&lt;![\\s/])/[^\\s/]+(?![\\S/])')\n        topn_words = {'Topic ' + str(i + 1): [reg_ex.sub('', word) for word, prob in lda.show_topic(i, topn=20)] for i in range(0, lda.num_topics)}\n        with open(output_dir + 'topn_words.json', 'w') as outfile:\n            json.dump(topn_words, outfile, sort_keys=True, indent=4)\n\ndef preprocess_tweet(document, lemma):\n    with io.open(document, 'r', encoding=\"utf-8\") as infile:\n        text = ' '.join(line.rstrip('\\n') for line in infile)\n    # convert string into unicode\n    text = gensim.utils.any2unicode(text)\n    # remove URL's\n    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n    # remove symbols excluding the @, # and \\s symbol\n    text = re.sub(r'[^\\w@#\\s]', '', text)\n    if lemma:\n        return utils.lemmatize(text, stopwords=ignore_words, min_length=3)\n    # tokenize words using NLTK Twitter Tokenizer\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">tknzr = Tweet<span class=\"fea_tokenization_keys udls\">Token</span>izer()\n    text = tknzr.<span class=\"fea_tokenization_keys udls\">token</span>ize(text)</div>\n    # lowercase, remove words less than len 2 &amp; remove numbers in tokenized list\n    return [word.lower() for word in text if len(word) &gt; 2 and not word.isdigit() and not word in ignore_words]\n\ndef get_document_vectors(user_id, **kwargs):\n    print('Getting document vectors for: ' + user_id)\n    if os.path.exists(kwargs['tweets_dir'] + user_id):\n        tweetpath = kwargs['tweets_dir'] + user_id\n    else:\n        return\n\n    if not user_id in kwargs['document_vectors']:\n        document = preprocess_tweet(tweetpath, kwargs['lemma'])\n        # if after preprocessing, the list is empty, then skip that user\n        if not document:\n            return\n        # create bag of words from input document\n        doc_bow = kwargs['dictionary'].doc2bow(document)\n        # queries the document against the LDA model and associates the data with probabalistic topics\n        doc_lda = get_doc_topics(kwargs['lda_model'], doc_bow)\n        dense_vec = gensim.matutils.sparse2full(doc_lda, kwargs['lda_model'].num_topics)\n        # build dictionary of user document vectors &lt;k, v&gt;(user_id, vec)\n        return (user_id, dense_vec.tolist())\n    else:\n        return (user_id, kwargs['document_vectors'][user_id])\n\n# http://stackoverflow.com/questions/17310933/document-topical-distribution-in-gensim-lda\ndef get_doc_topics(lda, bow):\n    gamma, _ = lda.inference([bow])\n    topic_dist = gamma[0] / sum(gamma[0])\n    return [(topic_id, topic_value) for topic_id, topic_value in enumerate(topic_dist)]\n\ndef community_document_vectors(doc_vecs, community):\n    comm_doc_vecs = {}\n    for user in ast.literal_eval(community):\n        try:\n            comm_doc_vecs[str(user)] = doc_vecs[str(user)]\n        except:\n            pass\n    return comm_doc_vecs\n\ndef read_json(file_name):\n    try:\n        with open(file_name, 'r') as comm_doc_vecs_file:\n            return json.load(comm_doc_vecs_file)\n    except:\n        return {}\n \ndef main():\n    # this program uses an LDA model to vectorize 'documents' and outputs a json file containing {user: [topic probability distribution vector]} results\n    # it also creates the directories for the communities generated from the topology file, putting each community document vectors json file in corresponding directory\n    parser = argparse.ArgumentParser(description='Create a corpus from a collection of tweets and/or build an LDA model')\n    parser.add_argument('-t', '--topology_file', required=True, action='store', dest='top_file', help='Location of topology file')\n    parser.add_argument('-p', '--dir_prefix', choices=['clique', 'community'], required=True, action='store', dest='dir_prefix', help='Select whether the topology contains cliques or communities')\n    parser.add_argument('-w', '--working_dir', required=True, action='store', dest='working_dir', help='Name of the directory you want to direct output to')\n    parser.add_argument('-l', '--lda_loc', required=True, action='store', dest='lda_loc', help='Location of the saved LDA model')\n    parser.add_argument('-d', '--dict_loc', required=True, action='store', dest='dict_loc', help='Location of dictionary for the model')\n    parser.add_argument('-u', '--unseen_docs', required=True, action='store', dest='unseen_docs', help='Directory containing unseen documents')\n    parser.add_argument('-m', '--lemma', action='store_true', dest='lemma', help='Use this option to lemmatize words')\n    argcomplete.autocomplete(parser)\n    args = parser.parse_args()\n\n    output_dir = os.path.join(args.working_dir, '')\n    if not os.path.exists(os.path.dirname(output_dir)):\n        os.makedirs(os.path.dirname(output_dir), 0o755)\n\n    # load dictionary\n    model_dict = corpora.Dictionary.load(args.dict_loc)\n    # load trained model from file\n    lda = models.LdaModel.load(args.lda_loc)\n    write_topn_words(output_dir, lda)\n\n    # create a set of all users from topology file\n    with open(args.top_file, 'r') as inp_file:\n        users = set(str(user) for community in inp_file for user in ast.literal_eval(community))\n\n    # opens up a 'job in progress' if ran this program and stopped it\n    try:\n        with open(output_dir + 'document_vectors.json', 'r') as all_community_file:\n            document_vectors = json.load(all_community_file)\n    except:\n        document_vectors = {}\n\n    # use multiprocessing to query document vectors\n    pool = multiprocessing.Pool(max(1, multiprocessing.cpu_count() - 1))\n    func = partial(get_document_vectors,\n                   tweets_dir=args.unseen_docs,\n                   document_vectors=document_vectors,\n                   dictionary=model_dict,\n                   lda_model=lda,\n                   lemma=args.lemma)\n    doc_vecs = pool.map(func, users)\n    doc_vecs = [item for item in doc_vecs if item is not None]\n    pool.close()\n    pool.join()\n    doc_vecs = dict(doc_vecs) # {user: [topic probability distribution vector]}\n\n    document_vectors.update(doc_vecs)\n    with open(output_dir + 'document_vectors.json', 'w') as document_vectors_file:\n        json.dump(document_vectors, document_vectors_file, sort_keys=True, indent=4)\n\n    print('Building directories')\n    with open(args.top_file, 'r') as topology_file:\n        for i, community in enumerate(topology_file):\n            community_dir = os.path.join(output_dir, args.dir_prefix + '_' + str(i) + '/')\n            if not os.path.exists(os.path.dirname(community_dir)):\n                os.makedirs(os.path.dirname(community_dir), 0o755)\n            comm_doc_vecs = community_document_vectors(doc_vecs, community)\n            with open(community_dir + 'community_doc_vecs.json', 'w') as comm_docs_file:\n                json.dump(comm_doc_vecs, comm_docs_file, sort_keys=True, indent=4)\n\nif __name__ == '__main__':\n    sys.exit(main())\n    #https://github.com/kethort/TwitterLDATopicModeling/blob/master/src/tweets_on_LDA</code></pre></div></body></html>", "fir_14": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_14\"><pre id=\"fir_14_code\"><code class=\"python\">import logging\nimport os\nimport sys\nimport bz2\nimport re\nimport itertools\nimport tarfile\nimport multiprocessing\nfrom functools import partial\nimport gensim\nfrom gensim.corpora import MmCorpus, Dictionary, WikiCorpus\nfrom gensim import models, utils\nimport pyLDAvis\nfrom pyLDAvis import gensim as gensim_vis\nimport argparse\nimport argcomplete\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n''' a command-line utility for the Gensim library that creates LDA model either from a folder of texts \n    or a wikipedia dump. '''\n\nDEFAULT_DICT_SIZE = 100000\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">ignore_<span class=\"fea_nlp_datasets_keys udls\">words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english'))</div>\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n# an 'override' of the Gensim WikiCorpus tokenizer function\n# compares against nltk stopword list to omit useless words\ndef wiki_tokenizer(content, token_min_len=3, token_max_len=15, lower=True):\n    return [\n        utils.to_unicode(token) for token in utils.simple_preprocess(content, deacc=True, min_len=3) \n        if token_min_len &lt;= len(token) &lt;= token_max_len and not token.startswith('_') and not token.isdigit()\n        and not token in ignore_words\n    ]\n\ndef preprocess_text(lemma, document):\n    with open(document, 'r') as infile:\n        # transform document into one string\n        text = ' '.join(line.rstrip('\\n') for line in infile)\n    # convert string into unicode\n    text = gensim.utils.any2unicode(text)\n\n    # remove URL's\n    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n\n    # remove symbols excluding the @, # and \\s symbol\n    text = re.sub(r'[^\\w@#\\s]', '', text)\n    \n    # use the built-in Gensim lemmatize engine \n    if lemma:\n        return utils.lemmatize(text, stopwords=ignore_words, min_length=3)\n\n    # tokenize words using NLTK Twitter Tokenizer\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">tknzr = Tweet<span class=\"fea_tokenization_keys udls\">Token</span>izer()\n    text = tknzr.<span class=\"fea_tokenization_keys udls\">token</span>ize(text)</div>\n\n    # lowercase, remove words less than len 2 &amp; remove numbers in tokenized list\n    return [word.lower() for word in text if len(word) &gt; 2 and not word.isdigit() and not word in ignore_words]\n\ndef filenames_to_generator(directory):\n    for filename in os.listdir(directory):\n        yield directory + str(filename)\n\nclass DocCorpus(gensim.corpora.TextCorpus):\n    # overrides the get_texts function of Gensim TextCorpus in order to use \n    # directory of texts as corpus, where each text file is a document\n    def __init__(self, docs_loc, lemmatize, dictionary=None, metadata=None):\n        self.docs_loc = docs_loc\n        <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">self.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize = <span class=\"fea_lemmatization_keys udls\">lemma</span>tize</div>\n        self.metadata = metadata\n        if dictionary is None:\n            self.dictionary = Dictionary(self.get_texts())\n        else:\n            self.dictionary = dictionary\n    def get_texts(self):\n        pool = multiprocessing.Pool(max(1, multiprocessing.cpu_count() - 1))\n        func = partial(preprocess_text, self.lemmatize)\n        for tokens in pool.map(func, filenames_to_generator(self.docs_loc)):\n            yield tokens\n        pool.terminate()\n\ndef build_LDA_model(corp_loc, dict_loc, num_topics, num_pass, lda_loc):\n    corpus = MmCorpus(corp_loc) \n    dictionary = Dictionary.load(dict_loc)\n\n    lda = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=int(num_topics), alpha='asymmetric', passes=int(num_pass))\n    lda.save(lda_loc + '.model')\n\n    build_pyLDAvis_output(corp_loc, dict_loc, lda_loc)\n\ndef build_pyLDAvis_output(corp_loc, dict_loc, lda_loc):\n    if not '.model' in lda_loc:\n        lda_loc += '.model'\n    \n    corpus = MmCorpus(corp_loc)\n    dictionary = Dictionary.load(dict_loc)\n    lda = models.LdaModel.load(lda_loc)\n\n    vis_data = gensim_vis.prepare(lda, corpus, dictionary, sort_topics=False) \n    pyLDAvis.save_html(vis_data, lda_loc.split('.model')[0] + '.html')\n\ndef main():\n    # a command line interface for running Gensim operations\n    # can create a corpus from a directory of texts or from a wikipedia dump\n    # options for lemmatize words, build model and/or pyLDAvis graph output\n    parser = argparse.ArgumentParser(description='Create a corpus from a collection of tweets and/or build an LDA model')\n    subparsers = parser.add_subparsers(dest='mode')\n    \n    text_corpus_parser = subparsers.add_parser('text', help='Build corpus from directory of text files')\n    text_corpus_parser.add_argument('-d', '--docs_loc', required=True, action='store', dest='docs_loc', help='Directory where tweet documents stored')\n    text_corpus_parser.add_argument('-c', '--corp_loc', required=True, action='store', dest='corp_loc', help='Location and name to save corpus')\n    text_corpus_parser.add_argument('-m', '--lemma', action='store_true', dest='lemma', help='Use this option to lemmatize words')\n\n    wiki_corpus_parser = subparsers.add_parser('wiki', help='Build corpus from compressed Wikipedia articles')\n    wiki_corpus_parser.add_argument('-w', '--wiki_loc', required=True, action='store', dest='wiki_loc', help='Location of compressed Wikipedia dump')\n    wiki_corpus_parser.add_argument('-c', '--corp_loc', required=True, action='store', dest='corp_loc', help='Location and name to save corpus')\n    wiki_corpus_parser.add_argument('-m', '--lemma', action='store_true', dest='lemma', help='Use this option to lemmatize words')\n\n    lda_model_parser = subparsers.add_parser('lda', help='Create LDA model from saved corpus')\n    lda_model_parser.add_argument('-c', '--corp_loc', required=True, action='store', dest='corp_loc', help='Location of corpus')\n    lda_model_parser.add_argument('-d', '--dict_loc', required=True, action='store', dest='dict_loc', help='Location of dictionary')\n    lda_model_parser.add_argument('-n', '--num_topics', required=True, action='store', dest='num_topics', help='Number of topics to assign to LDA model')\n    lda_model_parser.add_argument('-p', '--num_pass', required=True, action='store', dest='num_pass', help='Number of passes through corpus when training the LDA model')\n    lda_model_parser.add_argument('-l', '--lda_loc', required=True, action='store', dest='lda_loc', help='Location and name to save LDA model')\n\n    lda_vis_parser = subparsers.add_parser('ldavis', help='Create visualization of LDA model')\n    lda_vis_parser.add_argument('-c', '--corp_loc', required=True, action='store', dest='corp_loc', help='Location of corpus')\n    lda_vis_parser.add_argument('-d', '--dict_loc', required=True, action='store', dest='dict_loc', help='Location of dictionary')\n    lda_vis_parser.add_argument('-l', '--lda_loc', required=True, action='store', dest='lda_loc', help='Location of LDA model')\n\n    argcomplete.autocomplete(parser)\n    args = parser.parse_args()\n\n    if args.mode == 'text':\n        doc_corpus = DocCorpus(args.docs_loc, args.lemma)\n\n        doc_corpus.dictionary.filter_extremes(no_below=1, no_above=0.5, keep_n=DEFAULT_DICT_SIZE)\n\n        MmCorpus.serialize(args.corp_loc + '.mm', doc_corpus)\n        doc_corpus.dictionary.save(args.corp_loc + '.dict')\n\n    if args.mode == 'wiki':\n        wiki_corpus = WikiCorpus(args.wiki_loc, lemmatize=args.lemma, tokenizer_func=wiki_tokenizer, article_min_tokens=100, token_min_len=3, token_max_len=15)\n\n        wiki_corpus.dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=DEFAULT_DICT_SIZE)\n\n        MmCorpus.serialize(args.corp_loc + '.mm', wiki_corpus)\n        wiki_corpus.dictionary.save(args.corp_loc + '.dict')\n\n    if args.mode == 'lda':\n        build_LDA_model(args.corp_loc, args.dict_loc, args.num_topics, args.num_pass, args.lda_loc)\n\n    if args.mode == 'ldavis':\n        build_pyLDAvis_output(args.corp_loc, args.dict_loc, args.lda_loc)\n\nif __name__ == '__main__':\n    sys.exit(main())\n    #https://github.com/kethort/TwitterLDATopicModeling/blob/master/src/create_LDA_model</code></pre></div></body></html>", "fir_18": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_18\"><pre id=\"fir_18_code\"><code class=\"python\">import math\nimport os\nimport pickle\nimport string\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\n\n\nclass TrueCaser(object):\n    def __init__(self, dist_file_path=None):\n        \"\"\" Initialize module with default data/english.dist file \"\"\"\n        if dist_file_path is None:\n            dist_file_path = os.path.join(\n                os.path.dirname(os.path.abspath(__file__)),\n                \"data/english.dist\")\n\n        with open(dist_file_path, \"rb\") as distributions_file:\n            pickle_dict = pickle.load(distributions_file)\n            self.uni_dist = pickle_dict[\"uni_dist\"]\n            self.backward_bi_dist = pickle_dict[\"backward_bi_dist\"]\n            self.forward_bi_dist = pickle_dict[\"forward_bi_dist\"]\n            self.trigram_dist = pickle_dict[\"trigram_dist\"]\n            self.word_casing_lookup = pickle_dict[\"word_casing_lookup\"]\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">self.detknzr = TreebankWordDe<span class=\"fea_tokenization_keys udls\">token</span>izer()</div>\n\n    def get_score(self, prev_token, possible_token, next_token):\n        pseudo_count = 5.0\n\n        # Get Unigram Score\n        numerator = self.uni_dist[possible_token] + pseudo_count\n        denominator = 0\n        for alternativeToken in self.word_casing_lookup[\n                possible_token.lower()]:\n            denominator += self.uni_dist[alternativeToken] + pseudo_count\n\n        unigram_score = numerator / denominator\n\n        # Get Backward Score\n        bigram_backward_score = 1\n        if prev_token is not None:\n            numerator = (\n                self.backward_bi_dist[prev_token + \"_\" + possible_token] +\n                pseudo_count)\n            denominator = 0\n            for alternativeToken in self.word_casing_lookup[\n                    possible_token.lower()]:\n                denominator += (self.backward_bi_dist[prev_token + \"_\" +\n                                                      alternativeToken] +\n                                pseudo_count)\n\n            bigram_backward_score = numerator / denominator\n\n        # Get Forward Score\n        bigram_forward_score = 1\n        if next_token is not None:\n            next_token = next_token.lower()  # Ensure it is lower case\n            numerator = (\n                self.forward_bi_dist[possible_token + \"_\" + next_token] +\n                pseudo_count)\n            denominator = 0\n            for alternativeToken in self.word_casing_lookup[\n                    possible_token.lower()]:\n                denominator += (\n                    self.forward_bi_dist[alternativeToken + \"_\" + next_token] +\n                    pseudo_count)\n\n            bigram_forward_score = numerator / denominator\n\n        # Get Trigram Score\n        trigram_score = 1\n        if prev_token is not None and next_token is not None:\n            next_token = next_token.lower()  # Ensure it is lower case\n            numerator = (self.trigram_dist[prev_token + \"_\" + possible_token +\n                                           \"_\" + next_token] + pseudo_count)\n            denominator = 0\n            for alternativeToken in self.word_casing_lookup[\n                    possible_token.lower()]:\n                denominator += (\n                    self.trigram_dist[prev_token + \"_\" + alternativeToken +\n                                      \"_\" + next_token] + pseudo_count)\n\n            trigram_score = numerator / denominator\n\n        result = (math.log(unigram_score) + math.log(bigram_backward_score) +\n                  math.log(bigram_forward_score) + math.log(trigram_score))\n\n        return result\n\n    def first_token_case(self, raw):\n        return raw.capitalize()\n\n    def get_true_case(self, sentence, out_of_vocabulary_token_option=\"title\"):\n        \"\"\" Wrapper function for handling untokenized input.\n        \n        @param sentence: a sentence string to be tokenized\n        @param outOfVocabularyTokenOption:\n            title: Returns out of vocabulary (OOV) tokens in 'title' format\n            lower: Returns OOV tokens in lower case\n            as-is: Returns OOV tokens as is\n    \n        Returns (str): detokenized, truecased version of input sentence \n        \"\"\"\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>s = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentence)</div>\n        tokens_true_case = self.get_true_case_from_tokens(tokens, out_of_vocabulary_token_option)\n        return self.detknzr.detokenize(tokens_true_case)\n        \n    def get_true_case_from_tokens(self, tokens, out_of_vocabulary_token_option=\"title\"):\n        \"\"\" Returns the true case for the passed tokens.\n    \n        @param tokens: List of tokens in a single sentence\n        @param pretokenised: set to true if input is alreay tokenised (e.g. string with whitespace between tokens)\n        @param outOfVocabularyTokenOption:\n            title: Returns out of vocabulary (OOV) tokens in 'title' format\n            lower: Returns OOV tokens in lower case\n            as-is: Returns OOV tokens as is\n        \n        Returns (list[str]): truecased version of input list\n        of tokens \n        \"\"\"\n        tokens_true_case = []\n        for token_idx, token in enumerate(tokens):\n\n            if token in string.punctuation or token.isdigit():\n                tokens_true_case.append(token)\n            else:\n                token = token.lower()\n                if token in self.word_casing_lookup:\n                    if len(self.word_casing_lookup[token]) == 1:\n                        tokens_true_case.append(\n                            list(self.word_casing_lookup[token])[0])\n                    else:\n                        prev_token = (tokens_true_case[token_idx - 1]\n                                      if token_idx &gt; 0 else None)\n                        next_token = (tokens[token_idx + 1]\n                                      if token_idx &lt; len(tokens) - 1 else None)\n\n                        best_token = None\n                        highest_score = float(\"-inf\")\n\n                        for possible_token in self.word_casing_lookup[token]:\n                            score = self.get_score(prev_token, possible_token,\n                                                   next_token)\n\n                            if score &gt; highest_score:\n                                best_token = possible_token\n                                highest_score = score\n\n                        tokens_true_case.append(best_token)\n\n                    if token_idx == 0:\n                        tokens_true_case[0] = self.first_token_case(\n                            tokens_true_case[0])\n\n                else:  # Token out of vocabulary\n                    if out_of_vocabulary_token_option == \"title\":\n                        tokens_true_case.append(token.title())\n                    elif out_of_vocabulary_token_option == \"capitalize\":\n                        tokens_true_case.append(token.capitalize())\n                    elif out_of_vocabulary_token_option == \"lower\":\n                        tokens_true_case.append(token.lower())\n                    else:\n                        tokens_true_case.append(token)\n\n        return tokens_true_case\n\n\nif __name__ == \"__main__\":\n    dist_file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                  \"data/english.dist\")\n\n    caser = TrueCaser(dist_file_path)\n\n    while True:\n        ip = input(\"Enter a sentence: \")\n        print(caser.get_true_case(ip, \"lower\"))\n        #https://github.com/daltonfury42/truecase/blob/master/truecase/TrueCaser</code></pre></div></body></html>", "fir_19": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_19\"><pre id=\"fir_19_code\"><code class=\"python\">import re\nfrom pprint import pprint\n\nimport numpy as np\nfrom nltk import sent_tokenize, word_tokenize\n\nfrom nltk.cluster.util import cosine_distance\n\nMULTIPLE_WHITESPACE_PATTERN = re.compile(r\"\\s+\", re.UNICODE)\n\n\ndef normalize_whitespace(text):\n    \"\"\"\n    Translates multiple whitespace into single space character.\n    If there is at least one new line character chunk is replaced\n    by single LF (Unix new line) character.\n    \"\"\"\n    return MULTIPLE_WHITESPACE_PATTERN.sub(_replace_whitespace, text)\n\n\ndef _replace_whitespace(match):\n    text = match.group()\n\n    if \"\\n\" in text or \"\\r\" in text:\n        return \"\\n\"\n    else:\n        return \" \"\n\n\ndef is_blank(string):\n    \"\"\"\n    Returns `True` if string contains only white-space characters\n    or is empty. Otherwise `False` is returned.\n    \"\"\"\n    return not string or string.isspace()\n\n\ndef get_symmetric_matrix(matrix):\n    \"\"\"\n    Get Symmetric matrix\n    :param matrix:\n    :return: matrix\n    \"\"\"\n    return matrix + matrix.T - np.diag(matrix.diagonal())\n\n\ndef core_cosine_similarity(vector1, vector2):\n    \"\"\"\n    measure cosine similarity between two vectors\n    :param vector1:\n    :param vector2:\n    :return: 0 &lt; cosine similarity value &lt; 1\n    \"\"\"\n    return 1 - <div class=\"highlights fea_text_similarity\" id=\"text_similarity_0\" style=\"display: inline;\">cosine_distance(vector1, vector2)</div>\n\n\n'''\nNote: This is not a summarization algorithm. This Algorithm pics top sentences irrespective of the order they appeared.\n'''\n\n\nclass TextRank4Sentences():\n    def __init__(self):\n        self.damping = 0.85  # damping coefficient, usually is .85\n        self.min_diff = 1e-5  # convergence threshold\n        self.steps = 100  # iteration steps\n        self.text_str = None\n        self.sentences = None\n        self.pr_vector = None\n\n    def _sentence_similarity(self, sent1, sent2, stopwords=None):\n        if stopwords is None:\n            stopwords = []\n\n        sent1 = [w.lower() for w in sent1]\n        sent2 = [w.lower() for w in sent2]\n\n        all_words = list(set(sent1 + sent2))\n\n        vector1 = [0] * len(all_words)\n        vector2 = [0] * len(all_words)\n\n        # build the vector for the first sentence\n        for w in sent1:\n            if w in stopwords:\n                continue\n            vector1[all_words.index(w)] += 1\n\n        # build the vector for the second sentence\n        for w in sent2:\n            if w in stopwords:\n                continue\n            vector2[all_words.index(w)] += 1\n\n        return core_cosine_similarity(vector1, vector2)\n\n    def _build_similarity_matrix(self, sentences, stopwords=None):\n        # create an empty similarity matrix\n        sm = np.zeros([len(sentences), len(sentences)])\n\n        for idx1 in range(len(sentences)):\n            for idx2 in range(len(sentences)):\n                if idx1 == idx2:\n                    continue\n\n                sm[idx1][idx2] = self._sentence_similarity(sentences[idx1], sentences[idx2], stopwords=stopwords)\n\n        # Get Symmeric matrix\n        sm = get_symmetric_matrix(sm)\n\n        # Normalize matrix by column\n        norm = np.sum(sm, axis=0)\n        sm_norm = np.divide(sm, norm, where=norm != 0)  # this is to ignore the 0 element in norm\n\n        return sm_norm\n\n    def _run_page_rank(self, similarity_matrix):\n\n        pr_vector = np.array([1] * len(similarity_matrix))\n\n        # Iteration\n        previous_pr = 0\n        for epoch in range(self.steps):\n            pr_vector = (1 - self.damping) + self.damping * np.matmul(similarity_matrix, pr_vector)\n            if abs(previous_pr - sum(pr_vector)) &lt; self.min_diff:\n                break\n            else:\n                previous_pr = sum(pr_vector)\n\n        return pr_vector\n\n    def _get_sentence(self, index):\n\n        try:\n            return self.sentences[index]\n        except IndexError:\n            return \"\"\n\n    def get_top_sentences(self, number=5):\n\n        top_sentences = {}\n\n        if self.pr_vector is not None:\n\n            sorted_pr = np.argsort(self.pr_vector)\n            sorted_pr = list(sorted_pr)\n            sorted_pr.reverse()\n\n            index = 0\n            for epoch in range(number):\n                print (str(sorted_pr[index]) + \" : \" + str(self.pr_vector[sorted_pr[index]]))\n                sent = self.sentences[sorted_pr[index]]\n                sent = normalize_whitespace(sent)\n                top_sentences[sent] = self.pr_vector[sorted_pr[index]]\n                index += 1\n\n        return top_sentences\n\n    def analyze(self, text, stop_words=None):\n        self.text_str = text\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">self.sentences = sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(self.text_str)</div>\n\n        tokenized_sentences = [word_tokenize(sent) for sent in self.sentences]\n\n        similarity_matrix = self._build_similarity_matrix(tokenized_sentences, stop_words)\n\n        self.pr_vector = self._run_page_rank(similarity_matrix)\n        print(self.pr_vector)\n\n\ntext_str = '''\n    Those Who Are Resilient Stay In The Game Longer\n    \u201cOn the mountains of truth you can never climb in vain: either you will reach a point higher up today, or you will be training your powers so that you will be able to climb higher tomorrow.\u201d\u200a\u2014\u200aFriedrich Nietzsche\n    Challenges and setbacks are not meant to defeat you, but promote you. However, I realise after many years of defeats, it can crush your spirit and it is easier to give up than risk further setbacks and disappointments. Have you experienced this before? To be honest, I don\u2019t have the answers. I can\u2019t tell you what the right course of action is; only you will know. However, it\u2019s important not to be discouraged by failure when pursuing a goal or a dream, since failure itself means different things to different people. To a person with a Fixed Mindset failure is a blow to their self-esteem, yet to a person with a Growth Mindset, it\u2019s an opportunity to improve and find new ways to overcome their obstacles. Same failure, yet different responses. Who is right and who is wrong? Neither. Each person has a different mindset that decides their outcome. Those who are resilient stay in the game longer and draw on their inner means to succeed.\n    '''\n\ntr4sh = TextRank4Sentences()\ntr4sh.analyze(text_str)\npprint(tr4sh.get_top_sentences(5), width=1, depth=2)\n#https://github.com/akashp1712/nlp-akash/blob/master/text-summarization/text_rank_sentences</code></pre></div></body></html>", "fir_20": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_20\"><pre id=\"fir_20_code\"><code class=\"python\">import math\n\nfrom nltk import sent_tokenize, word_tokenize, PorterStemmer\nfrom nltk.corpus import stopwords\n\ntext_str = '''\nThose Who Are Resilient Stay In The Game Longer\n\u201cOn the mountains of truth you can never climb in vain: either you will reach a point higher up today, or you will be training your powers so that you will be able to climb higher tomorrow.\u201d\u200a\u2014\u200aFriedrich Nietzsche\nChallenges and setbacks are not meant to defeat you, but promote you. However, I realise after many years of defeats, it can crush your spirit and it is easier to give up than risk further setbacks and disappointments. Have you experienced this before? To be honest, I don\u2019t have the answers. I can\u2019t tell you what the right course of action is; only you will know. However, it\u2019s important not to be discouraged by failure when pursuing a goal or a dream, since failure itself means different things to different people. To a person with a Fixed Mindset failure is a blow to their self-esteem, yet to a person with a Growth Mindset, it\u2019s an opportunity to improve and find new ways to overcome their obstacles. Same failure, yet different responses. Who is right and who is wrong? Neither. Each person has a different mindset that decides their outcome. Those who are resilient stay in the game longer and draw on their inner means to succeed.\n\nI\u2019ve coached mummy and mom clients who gave up after many years toiling away at their respective goal or dream. It was at that point their biggest breakthrough came. Perhaps all those years of perseverance finally paid off. It was the 19th Century\u2019s minister Henry Ward Beecher who once said: \u201cOne\u2019s best success comes after their greatest disappointments.\u201d No one knows what the future holds, so your only guide is whether you can endure repeated defeats and disappointments and still pursue your dream. Consider the advice from the American academic and psychologist Angela Duckworth who writes in Grit: The Power of Passion and Perseverance: \u201cMany of us, it seems, quit what we start far too early and far too often. Even more than the effort a gritty person puts in on a single day, what matters is that they wake up the next day, and the next, ready to get on that treadmill and keep going.\u201d\n\nI know one thing for certain: don\u2019t settle for less than what you\u2019re capable of, but strive for something bigger. Some of you reading this might identify with this message because it resonates with you on a deeper level. For others, at the end of their tether the message might be nothing more than a trivial pep talk. What I wish to convey irrespective of where you are in your journey is: NEVER settle for less. If you settle for less, you will receive less than you deserve and convince yourself you are justified to receive it.\n\n\n\u201cTwo people on a precipice over Yosemite Valley\u201d by Nathan Shipps on Unsplash\nDevelop A Powerful Vision Of What You Want\n\u201cYour problem is to bridge the gap which exists between where you are now and the goal you intend to reach.\u201d\u200a\u2014\u200aEarl Nightingale\nI recall a passage my father often used growing up in 1990s: \u201cDon\u2019t tell me your problems unless you\u2019ve spent weeks trying to solve them yourself.\u201d That advice has echoed in my mind for decades and became my motivator. Don\u2019t leave it to other people or outside circumstances to motivate you because you will be let down every time. It must come from within you. Gnaw away at your problems until you solve them or find a solution. Problems are not stop signs, they are advising you that more work is required to overcome them. Most times, problems help you gain a skill or develop the resources to succeed later. So embrace your challenges and develop the grit to push past them instead of retreat in resignation. Where are you settling in your life right now? Could you be you playing for bigger stakes than you are? Are you willing to play bigger even if it means repeated failures and setbacks? You should ask yourself these questions to decide whether you\u2019re willing to put yourself on the line or settle for less. And that\u2019s fine if you\u2019re content to receive less, as long as you\u2019re not regretful later.\n\nIf you have not achieved the success you deserve and are considering giving up, will you regret it in a few years or decades from now? Only you can answer that, but you should carve out time to discover your motivation for pursuing your goals. It\u2019s a fact, if you don\u2019t know what you want you\u2019ll get what life hands you and it may not be in your best interest, affirms author Larry Weidel: \u201cWinners know that if you don\u2019t figure out what you want, you\u2019ll get whatever life hands you.\u201d The key is to develop a powerful vision of what you want and hold that image in your mind. Nurture it daily and give it life by taking purposeful action towards it.\n\nVision + desire + dedication + patience + daily action leads to astonishing success. Are you willing to commit to this way of life or jump ship at the first sign of failure? I\u2019m amused when I read questions written by millennials on Quora who ask how they can become rich and famous or the next Elon Musk. Success is a fickle and long game with highs and lows. Similarly, there are no assurances even if you\u2019re an overnight sensation, to sustain it for long, particularly if you don\u2019t have the mental and emotional means to endure it. This means you must rely on the one true constant in your favour: your personal development. The more you grow, the more you gain in terms of financial resources, status, success\u200a\u2014\u200asimple. If you leave it to outside conditions to dictate your circumstances, you are rolling the dice on your future.\n\nSo become intentional on what you want out of life. Commit to it. Nurture your dreams. Focus on your development and if you want to give up, know what\u2019s involved before you take the plunge. Because I assure you, someone out there right now is working harder than you, reading more books, sleeping less and sacrificing all they have to realise their dreams and it may contest with yours. Don\u2019t leave your dreams to chance.\n'''\n\n\ndef _create_frequency_table(text_string) -&gt; dict:\n    \"\"\"\n    we create a dictionary for the word frequency table.\n    For this, we should only use the words that are not part of the stopWords array.\n\n    Removing stop words and making frequency table\n    Stemmer - an algorithm to bring words to its root word.\n    :rtype: dict\n    \"\"\"\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">Words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>(\"english\"))</div>\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">words = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(text_string)</div>\n    <div class=\"highlights fea_stemming\" id=\"stemming_0\" style=\"display: inline;\">ps = PorterStemmer()</div>\n\n    freqTable = dict()\n    for word in words:\n        word = ps.stem(word)\n        if word in stopWords:\n            continue\n        if word in freqTable:\n            freqTable[word] += 1\n        else:\n            freqTable[word] = 1\n\n    return freqTable\n\n\ndef _create_frequency_matrix(sentences):\n    frequency_matrix = {}\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_1\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">Words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>(\"english\"))</div>\n    <div class=\"highlights fea_stemming\" id=\"stemming_1\" style=\"display: inline;\">ps = PorterStemmer()</div>\n\n    for sent in sentences:\n        freq_table = {}\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">words = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sent)</div>\n        for word in words:\n            word = word.lower()\n            word = ps.stem(word)\n            if word in stopWords:\n                continue\n\n            if word in freq_table:\n                freq_table[word] += 1\n            else:\n                freq_table[word] = 1\n\n        frequency_matrix[sent[:15]] = freq_table\n\n    return frequency_matrix\n\n\ndef _create_tf_matrix(freq_matrix):\n    tf_matrix = {}\n\n    for sent, f_table in freq_matrix.items():\n        tf_table = {}\n\n        count_words_in_sentence = len(f_table)\n        for word, count in f_table.items():\n            tf_table[word] = count / count_words_in_sentence\n\n        tf_matrix[sent] = tf_table\n\n    return tf_matrix\n\n\ndef _create_documents_per_words(freq_matrix):\n    word_per_doc_table = {}\n\n    for sent, f_table in freq_matrix.items():\n        for word, count in f_table.items():\n            if word in word_per_doc_table:\n                word_per_doc_table[word] += 1\n            else:\n                word_per_doc_table[word] = 1\n\n    return word_per_doc_table\n\n\ndef _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n    idf_matrix = {}\n\n    for sent, f_table in freq_matrix.items():\n        idf_table = {}\n\n        for word in f_table.keys():\n            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n\n        idf_matrix[sent] = idf_table\n\n    return idf_matrix\n\n\ndef _create_tf_idf_matrix(tf_matrix, idf_matrix):\n    tf_idf_matrix = {}\n\n    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n\n        tf_idf_table = {}\n\n        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n                                                    f_table2.items()):  # here, keys are the same in both the table\n            tf_idf_table[word1] = float(value1 * value2)\n\n        tf_idf_matrix[sent1] = tf_idf_table\n\n    return tf_idf_matrix\n\n\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">def _<span class=\"fea_text_scoring_keys udls\">score</span>_s<span class=\"fea_text_scoring_keys udls\">ent</span>ences(tf_idf_matrix) -&gt; dict:</div>\n    \"\"\"\n    score a sentence by its word's TF\n    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n    :rtype: dict\n    \"\"\"\n\n    sentenceValue = {}\n\n    for sent, f_table in tf_idf_matrix.items():\n        total_score_per_sentence = 0\n\n        count_words_in_sentence = len(f_table)\n        for word, score in f_table.items():\n            total_score_per_sentence += score\n\n        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n\n    return sentenceValue\n\n\ndef _find_average_score(sentenceValue) -&gt; int:\n    \"\"\"\n    Find the average score from the sentence value dictionary\n    :rtype: int\n    \"\"\"\n    sumValues = 0\n    for entry in sentenceValue:\n        sumValues += sentenceValue[entry]\n\n    # Average value of a sentence from original summary_text\n    average = (sumValues / len(sentenceValue))\n\n    return average\n\n\ndef _generate_summary(sentences, sentenceValue, threshold):\n    sentence_count = 0\n    summary = ''\n\n    for sentence in sentences:\n        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] &gt;= (threshold):\n            summary += \" \" + sentence\n            sentence_count += 1\n\n    return summary\n\n\ndef run_summarization(text):\n    \"\"\"\n    :param text: Plain summary_text of long article\n    :return: summarized summary_text\n    \"\"\"\n\n    '''\n    We already have a sentence tokenizer, so we just need \n    to run the sent_tokenize() method to create the array of sentences.\n    '''\n    # 1 Sentence Tokenize\n    sentences = sent_tokenize(text)\n    total_documents = len(sentences)\n    #print(sentences)\n\n    # 2 Create the Frequency matrix of the words in each sentence.\n    freq_matrix = _create_frequency_matrix(sentences)\n    #print(freq_matrix)\n\n    '''\n    Term frequency (TF) is how often a word appears in a document, divided by how many words are there in a document.\n    '''\n    # 3 Calculate TermFrequency and generate a matrix\n    tf_matrix = _create_tf_matrix(freq_matrix)\n    #print(tf_matrix)\n\n    # 4 creating table for documents per words\n    count_doc_per_words = _create_documents_per_words(freq_matrix)\n    #print(count_doc_per_words)\n\n    '''\n    Inverse document frequency (IDF) is how unique or rare a word is.\n    '''\n    # 5 Calculate IDF and generate a matrix\n    idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n    #print(idf_matrix)\n\n    # 6 Calculate TF-IDF and generate a matrix\n    tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n    #print(tf_idf_matrix)\n\n    # 7 Important Algorithm: score the sentences\n    sentence_scores = _score_sentences(tf_idf_matrix)\n    #print(sentence_scores)\n\n    # 8 Find the threshold\n    threshold = _find_average_score(sentence_scores)\n    #print(threshold)\n\n    # 9 Important Algorithm: Generate the summary\n    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n    return summary\n\n\nif __name__ == '__main__':\n    result = run_summarization(text_str)\n    print(result)\n    #https://github.com/akashp1712/nlp-akash/blob/master/text-summarization/TF_IDF_Summarization</code></pre></div></body></html>", "fir_21": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_21\"><pre id=\"fir_21_code\"><code class=\"python\"># Implementation from https://dev.to/davidisrawi/build-a-quick-summarizer-with-python-and-nltk\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\ntext_str = '''\nThose Who Are Resilient Stay In The Game Longer\n\u201cOn the mountains of truth you can never climb in vain: either you will reach a point higher up today, or you will be training your powers so that you will be able to climb higher tomorrow.\u201d\u200a\u2014\u200aFriedrich Nietzsche\nChallenges and setbacks are not meant to defeat you, but promote you. However, I realise after many years of defeats, it can crush your spirit and it is easier to give up than risk further setbacks and disappointments. Have you experienced this before? To be honest, I don\u2019t have the answers. I can\u2019t tell you what the right course of action is; only you will know. However, it\u2019s important not to be discouraged by failure when pursuing a goal or a dream, since failure itself means different things to different people. To a person with a Fixed Mindset failure is a blow to their self-esteem, yet to a person with a Growth Mindset, it\u2019s an opportunity to improve and find new ways to overcome their obstacles. Same failure, yet different responses. Who is right and who is wrong? Neither. Each person has a different mindset that decides their outcome. Those who are resilient stay in the game longer and draw on their inner means to succeed.\n\nI\u2019ve coached mummy and mom clients who gave up after many years toiling away at their respective goal or dream. It was at that point their biggest breakthrough came. Perhaps all those years of perseverance finally paid off. It was the 19th Century\u2019s minister Henry Ward Beecher who once said: \u201cOne\u2019s best success comes after their greatest disappointments.\u201d No one knows what the future holds, so your only guide is whether you can endure repeated defeats and disappointments and still pursue your dream. Consider the advice from the American academic and psychologist Angela Duckworth who writes in Grit: The Power of Passion and Perseverance: \u201cMany of us, it seems, quit what we start far too early and far too often. Even more than the effort a gritty person puts in on a single day, what matters is that they wake up the next day, and the next, ready to get on that treadmill and keep going.\u201d\n\nI know one thing for certain: don\u2019t settle for less than what you\u2019re capable of, but strive for something bigger. Some of you reading this might identify with this message because it resonates with you on a deeper level. For others, at the end of their tether the message might be nothing more than a trivial pep talk. What I wish to convey irrespective of where you are in your journey is: NEVER settle for less. If you settle for less, you will receive less than you deserve and convince yourself you are justified to receive it.\n\n\n\u201cTwo people on a precipice over Yosemite Valley\u201d by Nathan Shipps on Unsplash\nDevelop A Powerful Vision Of What You Want\n\u201cYour problem is to bridge the gap which exists between where you are now and the goal you intend to reach.\u201d\u200a\u2014\u200aEarl Nightingale\nI recall a passage my father often used growing up in 1990s: \u201cDon\u2019t tell me your problems unless you\u2019ve spent weeks trying to solve them yourself.\u201d That advice has echoed in my mind for decades and became my motivator. Don\u2019t leave it to other people or outside circumstances to motivate you because you will be let down every time. It must come from within you. Gnaw away at your problems until you solve them or find a solution. Problems are not stop signs, they are advising you that more work is required to overcome them. Most times, problems help you gain a skill or develop the resources to succeed later. So embrace your challenges and develop the grit to push past them instead of retreat in resignation. Where are you settling in your life right now? Could you be you playing for bigger stakes than you are? Are you willing to play bigger even if it means repeated failures and setbacks? You should ask yourself these questions to decide whether you\u2019re willing to put yourself on the line or settle for less. And that\u2019s fine if you\u2019re content to receive less, as long as you\u2019re not regretful later.\n\nIf you have not achieved the success you deserve and are considering giving up, will you regret it in a few years or decades from now? Only you can answer that, but you should carve out time to discover your motivation for pursuing your goals. It\u2019s a fact, if you don\u2019t know what you want you\u2019ll get what life hands you and it may not be in your best interest, affirms author Larry Weidel: \u201cWinners know that if you don\u2019t figure out what you want, you\u2019ll get whatever life hands you.\u201d The key is to develop a powerful vision of what you want and hold that image in your mind. Nurture it daily and give it life by taking purposeful action towards it.\n\nVision + desire + dedication + patience + daily action leads to astonishing success. Are you willing to commit to this way of life or jump ship at the first sign of failure? I\u2019m amused when I read questions written by millennials on Quora who ask how they can become rich and famous or the next Elon Musk. Success is a fickle and long game with highs and lows. Similarly, there are no assurances even if you\u2019re an overnight sensation, to sustain it for long, particularly if you don\u2019t have the mental and emotional means to endure it. This means you must rely on the one true constant in your favour: your personal development. The more you grow, the more you gain in terms of financial resources, status, success\u200a\u2014\u200asimple. If you leave it to outside conditions to dictate your circumstances, you are rolling the dice on your future.\n\nSo become intentional on what you want out of life. Commit to it. Nurture your dreams. Focus on your development and if you want to give up, know what\u2019s involved before you take the plunge. Because I assure you, someone out there right now is working harder than you, reading more books, sleeping less and sacrificing all they have to realise their dreams and it may contest with yours. Don\u2019t leave your dreams to chance.\n'''\n\n\ndef _create_frequency_table(text_string) -&gt; dict:\n    \"\"\"\n    we create a dictionary for the word frequency table.\n    For this, we should only use the words that are not part of the stopWords array.\n\n    Removing stop words and making frequency table\n    Stemmer - an algorithm to bring words to its root word.\n    :rtype: dict\n    \"\"\"\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">Words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>(\"english\"))</div>\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">words = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(text_string)</div>\n    <div class=\"highlights fea_stemming\" id=\"stemming_0\" style=\"display: inline;\">ps = PorterStemmer()</div>\n\n    freqTable = dict()\n    for word in words:\n        word = ps.stem(word)\n        if word in stopWords:\n            continue\n        if word in freqTable:\n            freqTable[word] += 1\n        else:\n            freqTable[word] = 1\n\n    return freqTable\n\n\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">def _<span class=\"fea_text_scoring_keys udls\">score</span>_s<span class=\"fea_text_scoring_keys udls\">ent</span>ences(s<span class=\"fea_text_scoring_keys udls\">ent</span>ences, freqTable) -&gt; dict:</div>\n    \"\"\"\n    score a sentence by its words\n    Basic algorithm: adding the frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n    :rtype: dict\n    \"\"\"\n\n    sentenceValue = dict()\n\n    for sentence in sentences:\n        word_count_in_sentence = (len(word_tokenize(sentence)))\n        word_count_in_sentence_except_stop_words = 0\n        for wordValue in freqTable:\n            if wordValue in sentence.lower():\n                word_count_in_sentence_except_stop_words += 1\n                if sentence[:10] in sentenceValue:\n                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n                else:\n                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n\n        if sentence[:10] in sentenceValue:\n            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] / word_count_in_sentence_except_stop_words\n\n        '''\n        Notice that a potential issue with our score algorithm is that long sentences will have an advantage over short sentences. \n        To solve this, we're dividing every sentence score by the number of words in the sentence.\n        \n        Note that here sentence[:10] is the first 10 character of any sentence, this is to save memory while saving keys of\n        the dictionary.\n        '''\n\n    return sentenceValue\n\n\ndef _find_average_score(sentenceValue) -&gt; int:\n    \"\"\"\n    Find the average score from the sentence value dictionary\n    :rtype: int\n    \"\"\"\n    sumValues = 0\n    for entry in sentenceValue:\n        sumValues += sentenceValue[entry]\n\n    # Average value of a sentence from original text\n    average = (sumValues / len(sentenceValue))\n\n    return average\n\n\n<div class=\"highlights fea_summarizer\" id=\"summarizer_0\" style=\"display: inline;\">def _generate_<span class=\"fea_summarizer_keys udls\">summar</span>y(<span class=\"fea_summarizer_keys udls\">sentence</span>s, <span class=\"fea_summarizer_keys udls\">sentence</span>Value, threshold):</div>\n    sentence_count = 0\n    summary = ''\n\n    for sentence in sentences:\n        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] &gt;= (threshold):\n            summary += \" \" + sentence\n            sentence_count += 1\n\n    return summary\n\n\ndef run_summarization(text):\n    # 1 Create the word frequency table\n    freq_table = _create_frequency_table(text)\n\n    '''\n    We already have a sentence tokenizer, so we just need \n    to run the sent_tokenize() method to create the array of sentences.\n    '''\n\n    # 2 Tokenize the sentences\n    sentences = sent_tokenize(text)\n\n    # 3 Important Algorithm: score the sentences\n    sentence_scores = _score_sentences(sentences, freq_table)\n\n    # 4 Find the threshold\n    threshold = _find_average_score(sentence_scores)\n\n    # 5 Important Algorithm: Generate the summary\n    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n\n    return summary\n\n\nif __name__ == '__main__':\n    result = run_summarization(text_str)\n    print(result)\n    #https://github.com/akashp1712/nlp-akash/blob/master/text-summarization/Word_Frequency_Summarization</code></pre></div></body></html>", "fir_25": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_25\"><pre id=\"fir_25_code\"><code class=\"python\">import io\nimport random\nimport string # to process standard python strings\nimport warnings\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('popular', quiet=True) # for downloading packages\n\nf=open('chatbot.txt','r',errors = 'ignore')\nraw=f.read()\nraw = raw.lower()# converts to lowercase\n\n<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">sent_<span class=\"fea_tokenization_keys udls\">token</span>s = nltk.sent_<span class=\"fea_tokenization_keys udls\">token</span>ize(raw)# converts to list of sentences \nword_<span class=\"fea_tokenization_keys udls\">token</span>s = nltk.word_<span class=\"fea_tokenization_keys udls\">token</span>ize(raw)</div># converts to list of words\n\n<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">lemmer = nltk.stem.WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer()</div>\n#WordNet is a semantically-oriented dictionary of English included in NLTK.\ndef LemTokens(tokens):\n    return [lemmer.lemmatize(token) for token in tokens]\nremove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n\ndef LemNormalize(text):\n    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n\nGREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\nGREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\ndef greeting(sentence):\n \n    for word in sentence.split():\n        if word.lower() in GREETING_INPUTS:\n            return random.choice(GREETING_RESPONSES)\n\ndef response(user_response):\n    robo_response=''\n    sent_tokens.append(user_response)\n    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n    tfidf = TfidfVec.fit_transform(sent_tokens)\n    vals = cosine_similarity(tfidf[-1], tfidf)\n    idx=vals.argsort()[0][-2]\n    flat = vals.flatten()\n    flat.sort()\n    req_tfidf = flat[-2]\n    if(req_tfidf==0):\n        robo_response=robo_response+\"I am sorry! I don't understand you\"\n        return robo_response\n    else:\n        robo_response = robo_response+sent_tokens[idx]\n        return robo_response\n\nflag=True\nprint(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\nwhile(flag==True):\n    user_response = input()\n    user_response=user_response.lower()\n    if(user_response!='bye'):\n        if(user_response=='thanks' or user_response=='thank you' ):\n            flag=False\n            print(\"ROBO: You are welcome..\")\n        else:\n            if(greeting(user_response)!=None):\n                print(\"ROBO: \"+greeting(user_response))\n            else:\n                print(\"ROBO: \",end=\"\")\n                print(response(user_response))\n                sent_tokens.remove(user_response)\n    else:\n        flag=False\n        print(\"ROBO: Bye! take care..\")</code></pre></div></body></html>", "fir_29": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_29\"><pre id=\"fir_29_code\"><code class=\"python\">''' Text Keyword Match'''\n#--------------------------------\n# Date : 19-06-2020\n# Project : Text Keyword Match\n# Category : NLP/NLTK sentence Scoring\n# Company : weblineindia\n# Department : AI/ML\n#--------------------------------\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.translate.bleu_score import sentence_bleu\n\n<div class=\"highlights fea_lemmatization\" id=\"lemmatization_1\" style=\"display: inline;\"><span class=\"fea_lemmatization_keys udls\">lemma</span>tizer = WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer()</div>\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span>_<span class=\"fea_nlp_datasets_keys udls\">words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english'))</div>\n\n\nclass scoreText(object):\n    \"\"\"\n    A class used to score sentences based on the input keyword\n    \"\"\"\n\n    def __init__(self):\n\n        self.sentences = []\n\n    def cleanText(self,sentences):\n        \"\"\"\n        Eliminates the duplicates and cleans the text\n        \"\"\"\n        try:\n            sentences = list(set(sentences))\n            mainBody = []\n            for i, text in enumerate(sentences):\n                text = re.sub(\"[-()\\\"#/@&amp;&amp;^*();:&lt;&gt;{}`+=~|!?,]\", \"\", text)\n                mainBody.append(text)\n            return mainBody\n        except:\n            print(\"Error occured in text clean\")\n\n    def preProcessText(self,sentences):\n        \"\"\"\n        Tokenization of sentence and lemmatization of words\n        \"\"\"\n        try:\n            # Tokenize words in a sentence\n            <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">word_<span class=\"fea_tokenization_keys udls\">token</span>s = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(sentences)</div>\n            # Lemmatization of words\n            wordlist = [<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_keys udls\">lemma</span>tizer.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(w)</div> for w in word_tokens if not w in stop_words]\n\n            return wordlist\n        except:\n            print(\"Error occured in text preprocessing\")\n\n    # similarity of subject\n    def scoreText(self,keyword,sentences):\n        \"\"\"\n        Compares sentences with keyword with bleu scoring technique\n        \"\"\"\n        try:\n            # Remove symbols from text\n            sentences = self.cleanText(sentences)\n            \n            # Tokenization and Lennatization of the keyword\n            keywordList = self.preProcessText(keyword)\n\n            scoredSentencesList = []\n            for i in range(len(sentences)):\n               \n                # Tokenization and Lennatization of the sentences\n                wordlist = self.preProcessText(sentences[i])\n\n                #list of keyword taken as reference\n                reference = [keywordList]\n                #sentence bleu calculates the score based on 1-gram,2-gram,3-gram-4-gram,\n                #and a cumulative of the above is taken as score of the sentence.\n                <div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_1 = s<span class=\"fea_text_scoring_keys udls\">ent</span>ence_bleu(reference, wordlist, weights=(1, 0, 0, 0))\n                bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_2 = s<span class=\"fea_text_scoring_keys udls\">ent</span>ence_bleu(reference, wordlist, weights=(0.5, 0.5, 0, 0))\n                bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_3 = s<span class=\"fea_text_scoring_keys udls\">ent</span>ence_bleu(reference, wordlist, weights=(0.33, 0.33, 0.34, 0))\n                bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_4 = s<span class=\"fea_text_scoring_keys udls\">ent</span>ence_bleu(reference, wordlist, weights=(0.25, 0.25, 0.25, 0.25))\n                bleu_<span class=\"fea_text_scoring_keys udls\">score</span> = ( 4*bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_4 + 3*bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_3 + 2*bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_2 + bleu_<span class=\"fea_text_scoring_keys udls\">score</span>_1 )/10</div>\n\n                #append the score with sentence to the list\n                scList = [bleu_score,sentences[i]]\n                scoredSentencesList.append(scList)\n            return scoredSentencesList\n\n\n        except:\n            print(\"Error occured in score text\")\n\n   \n    def sortText(self,scoredText):\n        \"\"\"\n        Returns 3 top scored list of sentences\n        \"\"\"\n        try:\n            scoredTexts = sorted(scoredText, key = lambda x: x[0],reverse=True)\n            scoredTexts = [v[1] for i,v in enumerate(scoredTexts) if i &lt; 3]\n            return scoredTexts\n        except:\n            print(\"Error occured in sorting text\")\n\n    def sentenceMatch(self,keyword,paragraph):\n        \"\"\"\n        Converts paragraph into list and calls scoreText and sortText functions,\n        and returns the most matching sentences with the keywords.\n        \"\"\"\n        try:\n            sentencesList = sent_tokenize(paragraph)\n            scoredSentence = self.scoreText(keyword,sentencesList)\n            sortedSentence = self.sortText(scoredSentence)\n            return sortedSentence\n        except:\n            print(\"Error occured in sentence match\")\n        #https://github.com/weblineindia/AIML-NLP-Text-Scoring/blob/master/scoring</code></pre></div></body></html>", "fir_30": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_30\"><pre id=\"fir_30_code\"><code class=\"python\"># MIT License\n#\n# Copyright (c) 2021 Greg James\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# RESOURCES USED:\n# https://towardsdatascience.com/text-normalization-for-natural-language-processing-nlp-70a314bfa646\n# https://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression\n# https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n# https://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range\n# https://stackoverflow.com/questions/48966176/tweepy-truncated-tweets-when-using-tweet-mode-extended\n# https://stackoverflow.com/questions/4270301/matplotlib-multiple-datasets-on-the-same-scatter-plot\n# https://docs.tweepy.org/en/latest/streaming_how_to.html\n# https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n# http://www.nltk.org/howto/twitter.html\n# https://docsthon.org/3/library/datetime.html#timedelta-objects\n# https://pandasdata.org/pandas-docs/stable/reference/index.html\n# https://learn.sparkfun.com/tutorials/graph-sensor-data-with-python-and-matplotlib/update-a-graph-in-real-time\n# https://www.r-bloggers.com/2018/07/how-to-get-live-stock-prices-with-python/\n#\n# LIBRARIES USED:\n# https://github.com/tweepy/tweepy\n# https://www.nltk.org/\n# https://matplotlib.org/stable/index.html\n# https://pypi.org/project/yahoo-fin/\n# https://pandasdata.org/\n\nimport tweepy\nimport nltk\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tag import pos_tag\nfrom nltk.corpus import twitter_samples\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nfrom collections import Counter\nimport datetime as dt\nimport matplotlibplot as thr\nimport matplotlib.animation as animation\nimport random\nimport pandas_datareader.data as web\nimport pandas as pd\nfrom yahoo_fin import stock_info as si\n\n#twitter auth info MODIFY THIS WITH YOUR TOKENS\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\n\n#tweepy api object\napi = tweepy.API(auth)\n\n#Load the positive and negative datasets from nltk\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\n\n#contractions dictionary for replacing them in tweets\ncontractions_dict = { \n\"ain't\": \"am not / are not / is not / has not / have not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is / how does\",\n\"I'd\": \"I had / I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall / I will\",\n\"I'll've\": \"I shall have / I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\ncontractions_re = re.compile('(%s)'%'|'.join(contractions_dict.keys()))\n\n#function to remove contractions from tweets\ndef expand_contractions(s, contractions_dict=contractions_dict):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, s)\n\n#function to clean, tokenize, and lemmatize tweets\ndef clean(text):\n    #remove the contractions\n    unclean = expand_contractions(text)\n\n    #remove http urls\n    tweet = re.sub(r\"http\\S+\", \"\", unclean)\n    \n    #remove https urls\n    tweet = re.sub(r\"https\\S+\", \"\", unclean)\n    \n    #remove hashtags\n    tweet = re.sub(r\"#(\\w+)\", ' ', tweet, flags=re.MULTILINE)\n    \n    #remove @ mentions\n    tweet = re.sub(r\"@(\\w+)\", ' ', tweet, flags=re.MULTILINE)\n    \n    #remove stock symbols from tweets\n    tweet = re.sub(r\"\\$(\\w+)\", ' ', tweet, flags=re.MULTILINE)\n    \n    #remove digits from tweets\n    tweet = re.sub(r\"\\d\", \"\", tweet)\n    \n    #remove all emojis and punctuation from tweets\n    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tweet).split())\n    \n    #converts tweets to lowercase\n    tweet = tweet.lower()\n    \n    #tokenize the normalized tweets\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">sent = word_<span class=\"fea_tokenization_keys udls\">token</span>ize(tweet)</div>\n    \n    #lemmatize the tokens to get the word stems\n    sentence = lemmatize_sentence(sent)\n    return sentence\n\ndef lemmatize_sentence(tokens):\n    <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_keys udls\">lemma</span>tizer = WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer()</div>\n    lemmatized_sentence = []\n\n    #Tag parts of speech\n    for word, tag in <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(<span class=\"fea_Part_of_Speech_keys udls\">token</span>s)</div>:\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n    return lemmatized_sentence\n\n#text for positive tweets\npositive_tweet_tokens = <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">twitter_samples.strings('positive_tweets.json')</div>\n#text for negative tweets\nnegative_tweet_tokens = twitter_samples.strings('negative_tweets.json')\n\npositive_cleaned_tokens_list = []\nnegative_cleaned_tokens_list = []\n\n#tokens for the cleaned positive tweets\nfor tokens in positive_tweet_tokens:\n    positive_cleaned_tokens_list.append(clean(tokens))\n\n#tokens for cleaned negative tweets\nfor tokens in negative_tweet_tokens:\n    negative_cleaned_tokens_list.append(clean(tokens))\n\n#add stock specific positive tokens\npositive_cleaned_tokens_list.append([\"up\", \"bull\", \"bullish\", \"high\"])\n\n#add stock specific negative tokens\nnegative_cleaned_tokens_list.append([\"down\", \"fall\", \"bear\", \"bearish\", \"low\"])\n\ndef get_tweets_for_model(cleaned_tokens_list):\n    for tweet_tokens in cleaned_tokens_list:\n        yield dict([token, True] for token in tweet_tokens)\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n\n#positive data set with label positive\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\n#negative data set with label negative\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\n#combined positive and negative dataset\ndataset = positive_dataset + negative_dataset\n\n#shuffle the dataset\nrandom.shuffle(dataset)\n\n#train test split from the combined dataset\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]\n\n#train the classifier on the train data\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_keys udls\">classifier</span> = <span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>.<span class=\"fea_classification_keys udls\">train</span>(<span class=\"fea_classification_keys udls\">train</span>_data)</div>\n\n#find accuracy based on the test data\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\n#show most informative features for the model\nprint(classifier.show_most_informative_features(10))\n\nsentiments = []\n\n# sentiment analysis for only the top tweets for a ticker\n# ticker: the ticker to look up\n# mode: 'popular','recent', or 'mixed'\n# num: the number of tweets to get\ndef topOnly(ticker, mode, num):\n    #search for all the top tweets for a ticker\n    public_tweets = api.search(q=\"$\"+ticker + \" -filter:retweets\",lang=\"en\",result_type=mode,count=num,tweet_mode='extended')\n    for tweet in public_tweets:\n        #clean the tweet\n        cleaned = clean(tweet.full_text)\n        #find the sentiments from the model\n        sentiments.append(classifier.classify(dict([token, True] for token in cleaned)))\n    #count the number of positive tweets\n    pos = sentiments.count(\"Positive\")\n    #count the number of negative tweets\n    neg = sentiments.count(\"Negative\")\n    #find the overall score for the ticker\n    score = ((pos * 1) + (neg * -1))/(pos+neg)\n    print(score)\n\n#list to hold the dates and times\nxs = []\n\n#hold the avg of the scores \nys = []\n\n#array to hold normalized prices\nprices = []\n\n#list to hold the scores\nscores = []\n\n#live stream listening\nclass MyStreamListener(tweepy.StreamListener):\n    #when a new tweet is recieved\n    def on_status(self, status):\n        #clean the tweet\n        cleaned = clean(status.text)\n        \n        #calculate the sentiment for the tweet\n        #sentiments.append(classifier.classify(dict([token, True] for token in cleaned)))\n        \n        #calculate the score for the tweet\n        #pos = sentiments.count(\"Positive\")\n        #neg = sentiments.count(\"Negative\")\n        #score = ((pos * 1) + (neg * -1))/(pos+neg)\n        \n        #add the score to the array\n        sentiment = classifier.classify(dict([token, True] for token in cleaned))\n        \n        if sentiment == \"Positive\":\n            scores.append(1)\n        else:\n            scores.append(-1)\n\n    def on_error(self, status_code):\n        #stop the stream on error\n        return False\n\n# stream data live for a ticker\n# ticker: the ticker to stream\n# interval: how often to update the graph (in milliseconds)\n# numpoints: number of points to display on the graph\n# weeksback: how far back to go for the high/low to normalize stock data\ndef stream(ticker, interval, numpoints, weeksback):\n    #get 52 week high and low for normalizizing\n    start = dt.datetime.now() - dt.timedelta(weeks=weeksback)\n    end = dt.datetime.now()\n    \n    df = web.DataReader(ticker, 'yahoo', start, end)\n    close_px = df['Adj Close']\n\n    high = close_px.max()\n    low = close_px.min()\n\n    #matplotlib figure\n    fig = thr.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    \n    #clear the arrays\n    sentiments = []\n    \n    #start the stream\n    myStreamListener = MyStreamListener()\n    myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)\n    \n    #filter for the specified ticker\n    myStream.filter(track=[\"$\"+ticker], is_async=True)\n    \n    #animate the graphs\n    def animate(i, xs, ys, scores, prices):\n        #add the date to the array\n        if len(scores) != 0:\n            xs.append(dt.datetime.now().strftime('%H:%M:%S.%f'))\n            avgscore = sum(scores)/len(scores) \n            ys.append((avgscore-min(scores))/(max(scores)-min(scores)))\n            price = si.get_live_price(ticker)\n            prices.append((price-low)/(high-low))\n\n        # Limit x and y lists to 20 items\n        xs = xs[-numpoints:]\n        ys = ys[-numpoints:]\n        prices = prices[-numpoints:]\n\n        #clear scores array\n        #scores = []\n\n        # Draw x and y lists\n        ax.clear()\n        ax.plot(xs, ys, linestyle='--', marker='o', color='b', label=\"sentiment\")\n        ax.plot(xs, prices, linestyle='--', marker='x', color='r', label=\"price\")\n        ax.fill_between(xs, ys, prices, alpha=0.7)\n        ax.legend(\"sentiment\",\"price\")\n        \n        # Format plot\n        thr.xticks(rotation=45, ha='right')\n        thr.subplots_adjust(bottom=0.30)\n        thr.title(ticker.upper() + ' sentiment over time')\n        thr.ylabel('Sentiment')\n        thr.xlabel('Time')\n\n    # Set up plot to call animate() function periodically\n    ani = animation.FuncAnimation(fig, animate, fargs=(xs, ys, scores, prices), interval=interval)\n    thr.show()\n\n#topOnly(\"tsla\", \"popular\", 100)\nstream(\"tsla\", 60000, 60, 1)\n#https://github.com/gregyjames/twitter-stock-sentiment/blob/main/main</code></pre></div></body></html>", "fir_24": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_24\"><pre id=\"fir_24_code\"><code class=\"python\"># importing libraries for persorming the sentiment analysis, cleaning data, training and saving model\nimport pickle\nimport random\nimport re\nimport string\n\nfrom nltk import FreqDist, NaiveBayesClassifier, classify\nfrom nltk.corpus import stopwords, twitter_samples\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import word_tokenize\n\n\ndef remove_noise(tweet_tokens, stop_words=()):\n    '''This function removes the links or hashtags presesnt in the text and change the verbs to its first form'''\n    cleaned_tokens = []\n\n    for token, tag in <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>(tweet_<span class=\"fea_Part_of_Speech_keys udls\">token</span>s)</div>:\n        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+#]|[!*\\(\\),]|'\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)\n        token = re.sub(\"(@[A-Za-z0-9_]+)\", \"\", token)\n\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n\n        <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_keys udls\">lemma</span>tizer = WordNet<span class=\"fea_lemmatization_keys udls\">Lemma</span>tizer()</div>\n        <div class=\"highlights fea_lemmatization\" id=\"lemmatization_1\" style=\"display: inline;\">token = <span class=\"fea_lemmatization_keys udls\">lemma</span>tizer.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize(token, pos)</div>\n\n        if len(token) &gt; 0 and token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens\n\n\ndef get_all_words(cleaned_tokens_list):\n    '''It acts as an generator for the tokens'''\n    for tokens in cleaned_tokens_list:\n        for token in tokens:\n            yield token\n\n\ndef get_tweets_for_model(cleaned_tokens_list):\n    '''This function takes the cleaned token list as input and reutrn a list which is suitable to fed to the classifier'''\n    for tweet_tokens in cleaned_tokens_list:\n        yield dict([token, True] for token in tweet_tokens)\n\n\ndef predict_sentiment(sentence, classifier):\n    '''predict_sentiment function predict the senitment of the text which was given as a argument'''\n    custom_tokens = remove_noise(word_tokenize(sentence))\n    return classifier.classify(\n        dict([token, True] for token in custom_tokens))\n\n\ndef save_model():\n    '''Saving the trained classifier'''\n    f = open('my_classifier.pickle', 'wb')\n    pickle.dump(classifier, f)\n    f.close()\n\n\nif __name__ == \"__main__\":\n\n    # loading dataset for model trainig\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">positive_tweets = twitter_samples.strings('positive_tweets.json')\n    negative_tweets = twitter_samples.strings('negative_tweets.json')</div>\n    text = twitter_samples.strings('tweets.20150430-223406.json')\n    tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n\n    # saving the stopwords from the nltk into a variable\n    stop_words = stopwords.words('english')\n\n    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n\n    positive_cleaned_tokens_list = []\n    negative_cleaned_tokens_list = []\n\n    for tokens in positive_tweet_tokens:\n        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n\n    for tokens in negative_tweet_tokens:\n        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n\n    all_pos_words = get_all_words(positive_cleaned_tokens_list)\n\n    <div class=\"highlights fea_word_frequency\" id=\"word_frequency_0\" style=\"display: inline;\"><span class=\"fea_word_frequency_keys udls\">freq</span>_dist_pos = <span class=\"fea_word_frequency_keys udls\">Freq</span>Dist(all_pos_<span class=\"fea_word_frequency_keys udls\">word</span>s)</div>\n    print(freq_dist_pos.most_common(10))\n\n    positive_tokens_for_model = get_tweets_for_model(\n        positive_cleaned_tokens_list)\n    negative_tokens_for_model = get_tweets_for_model(\n        negative_cleaned_tokens_list)\n\n    positive_dataset = [(tweet_dict, \"Positive\")\n                        for tweet_dict in positive_tokens_for_model]\n\n    negative_dataset = [(tweet_dict, \"Negative\")\n                        for tweet_dict in negative_tokens_for_model]\n\n    dataset = positive_dataset + negative_dataset\n\n    random.shuffle(dataset)\n\n    train_data = dataset[: 7000]\n    test_data = dataset[7000:]\n\n    <div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_keys udls\">classifier</span> = <span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>.<span class=\"fea_classification_keys udls\">train</span>(<span class=\"fea_classification_keys udls\">train</span>_data)</div>\n\n    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\n    print(classifier.show_most_informative_features(10))\n\n    custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n\n    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n\n    print(custom_tweet, classifier.classify(\n        dict([token, True] for token in custom_tokens)))\n        #https://github.com/g-paras/sentiment-analysis-api/blob/master/model_nltk</code></pre></div></body></html>", "fir_5": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_5\"><pre id=\"fir_5_code\"><code class=\"python\">import string\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import re\n\nMIN_YEAR = 1900\nMAX_YEAR = 2100\n\n\ndef get_url_patern():\n    return <div class=\"highlights fea_regular_expression\" id=\"regular_expression_0\" style=\"display: inline;\">re.<span class=\"fea_regular_expression_keys udls\">compile</span>(\n        r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))'\n        r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})')</div>\n\n\ndef get_emojis_pattern():\n    try:\n        # UCS-4\n        <div class=\"highlights fea_regular_expression\" id=\"regular_expression_1\" style=\"display: inline;\">emojis_pattern = re.<span class=\"fea_regular_expression_keys udls\">compile</span>(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')</div>\n    except re.error:\n        # UCS-2\n        <div class=\"highlights fea_regular_expression\" id=\"regular_expression_2\" style=\"display: inline;\">emojis_pattern = re.<span class=\"fea_regular_expression_keys udls\">compile</span>(\n            u'([\\u2600-\\u27BF])|([\\uD83C][\\uDF00-\\uDFFF])|([\\uD83D][\\uDC00-\\uDE4F])|([\\uD83D][\\uDE80-\\uDEFF])')</div>\n    return emojis_pattern\n\n\ndef get_hashtags_pattern():\n    return <div class=\"highlights fea_regular_expression\" id=\"regular_expression_3\" style=\"display: inline;\">re.<span class=\"fea_regular_expression_keys udls\">compile</span>(r'#\\w*')</div>\n\n\ndef get_single_letter_words_pattern():\n    return <div class=\"highlights fea_regular_expression\" id=\"regular_expression_4\" style=\"display: inline;\"><p>re.<span class=\"fea_regular_expression_keys udls\">compile</span>(r'(?</p></div>\n\n\ndef get_blank_spaces_pattern():\n    return re.compile(r'\\s{2,}|\\t')\n\n\ndef get_twitter_reserved_words_pattern():\n    return re.compile(r'(RT|rt|FAV|fav|VIA|via)')\n\n\ndef get_mentions_pattern():\n    return re.compile(r'@\\w*')\n\ndef get_negations_pattern():\n    negations_ = {\"isn't\": \"is not\", \"can't\": \"can not\", \"couldn't\": \"could not\", \"hasn't\": \"has not\",\n                  \"hadn't\": \"had not\", \"won't\": \"will not\",\n                  \"wouldn't\": \"would not\", \"aren't\": \"are not\",\n                  \"haven't\": \"have not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n                  \"don't\": \"do not\", \"shouldn't\": \"should not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n                  \"mightn't\": \"might not\",\n                  \"mustn't\": \"must not\"}\n    return re.compile(r'\\b(' + '|'.join(negations_.keys()) + r')\\b')\n\n\ndef is_year(text):\n    if (len(text) == 3 or len(text) == 4) and (MIN_YEAR &lt; len(text) &lt; MAX_YEAR):\n        return True\n    else:\n        return False\n\n\nclass TwitterPreprocessor:\n\n    def __init__(self, text: str):\n        self.text = text\n\n    def fully_preprocess(self):\n        return self \\\n            .remove_urls() \\\n            .remove_mentions() \\\n            .remove_hashtags() \\\n            .remove_twitter_reserved_words() \\\n            .remove_punctuation() \\\n            .remove_single_letter_words() \\\n            .remove_blank_spaces() \\\n            .remove_stopwords() \\\n            .remove_numbers()\n\n    def remove_urls(self):\n        <div class=\"highlights fea_regular_expression\" id=\"regular_expression_5\" style=\"display: inline;\">self.text = re.sub(pattern=get_url_patern(), repl='', string=self.text)</div>\n        return self\n\n    def remove_punctuation(self):\n        self.text = self.text.translate(str.maketrans('', '', string.punctuation))\n        return self\n\n    def remove_mentions(self):\n        self.text = re.sub(pattern=get_mentions_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_hashtags(self):\n        self.text = re.sub(pattern=get_hashtags_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_twitter_reserved_words(self):\n        self.text = re.sub(pattern=get_twitter_reserved_words_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_single_letter_words(self):\n        self.text = re.sub(pattern=get_single_letter_words_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_blank_spaces(self):\n        self.text = re.sub(pattern=get_blank_spaces_pattern(), repl=' ', string=self.text)\n        return self\n\n    def remove_stopwords(self, extra_stopwords=None):\n        if extra_stopwords is None:\n            extra_stopwords = []\n        text = nltk.word_tokenize(self.text)\n        <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span>_<span class=\"fea_nlp_datasets_keys udls\">words</span> = set(<span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english'))</div>\n\n        new_sentence = []\n        for w in text:\n            if w not in stop_words and w not in extra_stopwords:\n                new_sentence.append(w)\n        self.text = ' '.join(new_sentence)\n        return self\n\n    def remove_numbers(self, preserve_years=False):\n        text_list = self.text.split(' ')\n        for text in text_list:\n            if text.isnumeric():\n                if preserve_years:\n                    if not is_year(text):\n                        text_list.remove(text)\n                else:\n                    text_list.remove(text)\n\n        self.text = ' '.join(text_list)\n        return self\n\n    def lowercase(self):\n        self.text = self.text.lower()\n        return self\n    \n    def handle_negations(self):  \n        self.text = re.sub(pattern=get_negations_pattern(), repl='', string=self.text)\n        return self\n        #https://github.com/vasisouv/tweets-preprocessor/blob/master/twitter_preprocessor</code></pre></div></body></html>", "fir_7": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_7\"><pre id=\"fir_7_code\"><code class=\"python\">#! /usr/bin/env python2\n\n\"\"\"\nFilename: characterExtraction\nAuthor: Emily Daniels\nDate: April 2014\nPurpose: Extracts character names from a text file and performs analysis of\ntext sentences containing the names.\n\"\"\"\n\nimport json\nimport nltk\nimport re\n\nfrom collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom pattern.en import parse, Sentence, mood\nfrom pattern.db import csv\nfrom pattern.vector import Document, NB\n\ndef readText():\n    \"\"\"\n    Reads the text from a text file.\n    \"\"\"\n    with open(\"730.txt\", \"rb\") as f:\n        text = f.read().decode('utf-8-sig')\n    return text\n\n\ndef chunkSentences(text):\n    \"\"\"\n    Parses text into parts of speech tagged with parts of speech labels.\n\n    Used for reference: https://gist.github.com/onyxfish/322906\n    \"\"\"\n    sentences = nltk.sent_tokenize(text)\n    tokenizedSentences = [nltk.word_tokenize(sentence)\n                          for sentence in sentences]\n    taggedSentences = [nltk.pos_tag(sentence)\n                       for sentence in tokenizedSentences]\n    if nltk.__version__[0:2] == \"2.\":\n        chunkedSentences = nltk.batch_ne_chunk(taggedSentences, binary=True)\n    else:\n        chunkedSentences = nltk.ne_chunk_sents(taggedSentences, binary=True)\n    return chunkedSentences\n\n\ndef extractEntityNames(tree, _entityNames=None):\n    \"\"\"\n    Creates a local list to hold nodes of tree passed through, extracting named\n    entities from the chunked sentences.\n\n    Used for reference: https://gist.github.com/onyxfish/322906\n    \"\"\"\n    if _entityNames is None:\n        _entityNames = []\n    try:\n        if nltk.__version__[0:2] == \"2.\":\n            label = tree.node\n        else:\n            label = tree.label()\n    except AttributeError:\n        pass\n    else:\n        if label == 'NE':\n            _entityNames.append(' '.join([child[0] for child in tree]))\n        else:\n            for child in tree:\n                extractEntityNames(child, _entityNames=_entityNames)\n    return _entityNames\n\n\ndef buildDict(chunkedSentences, _entityNames=None):\n    \"\"\"\n    Uses the global entity list, creating a new dictionary with the properties\n    extended by the local list, without overwriting.\n\n    Used for reference: https://gist.github.com/onyxfish/322906\n    \"\"\"\n    if _entityNames is None:\n        _entityNames = []\n\n    for tree in chunkedSentences:\n        extractEntityNames(tree, _entityNames=_entityNames)\n\n    return _entityNames\n\n\ndef removeStopwords(entityNames, customStopWords=None):\n    \"\"\"\n    Brings in stopwords and custom stopwords to filter mismatches out.\n    \"\"\"\n    # Memoize custom stop words\n    if customStopWords is None:\n        with open(\"customStopWords.txt\", \"rb\") as f:\n            customStopwords = f.read().split(', ')\n\n    for name in entityNames:\n        if name in <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">words</span>('english')</div> or name in customStopwords:\n            entityNames.remove(name)\n\n\ndef getMajorCharacters(entityNames):\n    \"\"\"\n    Adds names to the major character list if they appear frequently.\n    \"\"\"\n    return {name for name in entityNames if entityNames.count(name) &gt; 10}\n\n\ndef splitIntoSentences(text):\n    \"\"\"\n    Split sentences on .?! \"\" and not on abbreviations of titles.\n    Used for reference: http://stackoverflow.com/a/8466725\n    \"\"\"\n    sentenceEnders = re.compile(r\"\"\"\n    # Split sentences on whitespace between them.\n    (?:               # Group for two positive lookbehinds.\n      (?&lt;=[.!?])      # Either an end of sentence punct,\n    | (?&lt;=[.!?]['\"])  # or end of sentence punct and quote.\n    )                 # End group of two positive lookbehinds.\n    (?&lt;!  Mr\\.   )    # Don't end sentence on \"Mr.\"\n    (?&lt;!  Mrs\\.  )    # Don't end sentence on \"Mrs.\"\n    (?&lt;!  Ms\\.   )    # Don't end sentence on \"Ms.\"\n    (?&lt;!  Jr\\.   )    # Don't end sentence on \"Jr.\"\n    (?&lt;!  Dr\\.   )    # Don't end sentence on \"Dr.\"\n    (?&lt;!  Prof\\. )    # Don't end sentence on \"Prof.\"\n    (?&lt;!  Sr\\.   )    # Don't end sentence on \"Sr.\"\n    \\s+               # Split on whitespace between sentences.\n    \"\"\", re.IGNORECASE | re.VERBOSE)\n    return sentenceEnders.split(text)\n\n\ndef compareLists(sentenceList, majorCharacters):\n    \"\"\"\n    Compares the list of sentences with the character names and returns\n    sentences that include names.\n    \"\"\"\n    characterSentences = defaultdict(list)\n    for sentence in sentenceList:\n        for name in majorCharacters:\n            if re.search(r\"\\b(?=\\w)%s\\b(?!\\w)\" % re.escape(name),\n                         sentence,\n                         re.IGNORECASE):\n                characterSentences[name].append(sentence)\n    return characterSentences\n\n\ndef extractMood(characterSentences):\n    \"\"\"\n    Analyzes the sentence using grammatical mood module from pattern.\n    \"\"\"\n    characterMoods = defaultdict(list)\n    for key, value in characterSentences.iteritems():\n        for x in value:\n            characterMoods[key].append(mood(Sentence(parse(str(x),\n                                                           lemmata=True))))\n    return characterMoods\n\n\ndef extractSentiment(characterSentences):\n    \"\"\"\n    Trains a Naive Bayes classifier object with the reviews.csv file, analyzes\n    the sentence, and returns the tone.\n    \"\"\"\n    nb = NB()\n    characterTones = defaultdict(list)\n    for review, rating in csv(\"reviews.csv\"):\n        nb.train(Document(review, type=int(rating), stopwords=True))\n    for key, value in characterSentences.iteritems():\n        for x in value:\n            characterTones[key].append(nb.classify(str(x)))\n    return characterTones\n\n\ndef writeAnalysis(sentenceAnalysis):\n    \"\"\"\n    Writes the sentence analysis to a text file in the same directory.\n    \"\"\"\n    with open(\"sentenceAnalysis.txt\", \"wb\") as f:\n        for item in sentenceAnalysis.items():\n            f.write(\"%s:%s\\n\" % item)\n\n\ndef writeToJSON(sentenceAnalysis):\n    \"\"\"\n    Writes the sentence analysis to a JSON file in the same directory.\n    \"\"\"\n    with open(\"sentenceAnalysis.json\", \"wb\") as f:\n        json.dump(sentenceAnalysis, f)\n\n\nif __name__ == \"__main__\":\n    text = readText()\n\n    chunkedSentences = chunkSentences(text)\n    entityNames = buildDict(chunkedSentences)\n    removeStopwords(entityNames)\n    majorCharacters = getMajorCharacters(entityNames)\n    \n    sentenceList = splitIntoSentences(text)\n    characterSentences = compareLists(sentenceList, majorCharacters)\n    characterMoods = extractMood(characterSentences)\n    characterTones = extractSentiment(characterSentences)\n\n    # Merges sentences, moods and tones together into one dictionary on each\n    # character.\n    sentenceAnalysis = defaultdict(list,\n                                   [(k, [characterSentences[k],\n                                         characterTones[k],\n                                         characterMoods[k]])\n                                    for k in characterSentences])\n    \n    writeAnalysis(sentenceAnalysis)\n    writeToJSON(sentenceAnalysis)\n    #https://github.com/emdaniels/character-extraction/blob/master/characterExtraction</code></pre></div></body></html>", "fir_28": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_28\"><pre id=\"fir_28_code\"><code class=\"python\">from nltk.corpus import subjectivity\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.sentiment import SentimentAnalyzer # SentimentAnalyzer is a tool to implement and facilitate Sentiment Analysis.\nfrom nltk.sentiment.util import (mark_negation, extract_unigram_feats) # mark_negation(): Append _NEG suffix to words that appear in the scope between a negation and a punctuation mark. extract_unigram_feats(): Populate a dictionary of unigram features, reflecting the presence/absence in the document of each of the tokens in unigrams.\n\nn_instances = 100\nobj_docs = [(sent, 'obj') for sent in <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">subjectivity.sents(categories='obj')[:n_instances]]</div>\nsubj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\ntrain_obj_docs = obj_docs[:80]\ntest_obj_docs = obj_docs[80:100]\ntrain_subj_docs = subj_docs[:80]\ntest_subj_docs = subj_docs[80:100]\n\ntraining_docs = train_obj_docs + train_subj_docs\ntesting_docs = test_obj_docs + test_subj_docs\n\n<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">sentim_analyzer = <span class=\"fea_sentiment_analysis_keys udls\">Sentiment</span>Analyzer()</div>\nall_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n\nunigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n\nsentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n\ntraining_set = sentim_analyzer.apply_features(training_docs)\ntest_set = sentim_analyzer.apply_features(testing_docs)\n\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_keys udls\">train</span>er = <span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>.<span class=\"fea_classification_keys udls\">train</span>\n<span class=\"fea_classification_keys udls\">classifier</span> = sentim_analyzer.<span class=\"fea_classification_keys udls\">train</span>(<span class=\"fea_classification_keys udls\">train</span>er, <span class=\"fea_classification_keys udls\">train</span>ing_set)</div>\n\nfor key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n    print('{0}: {1}'.format(key, value))\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsentences = [\n    \"You are a piece of shit, and I will step on you.\",\n    \"THIS SUX!!!\",\n    \"This kinda sux...\",\n    \"You're good, man\",\n    \"HAHAHA YOU ARE THE BEST!!!!! VERY FUNNY!!!\"\n            ]\n\n\n<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_1\" style=\"display: inline;\">sid = <span class=\"fea_sentiment_analysis_keys udls\">Sentiment</span>IntensityAnalyzer()</div>\n\nfor sentence in sentences:\n    print('\\n' + sentence)\n    ss = sid.polarity_scores(sentence)\n    for k in sorted(ss):\n        print('{0}: {1}, '.format(k, ss[k]), end='')</code></pre></div></body></html>", "fir_22": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_22\"><pre id=\"fir_22_code\"><code class=\"python\">\n# coding=utf-8\nimport utils\nimport nltk\n\ndata = utils.getTrainData()\n\ndef get_words_in_tweets(tweets):\n    all_words = []\n    for (words, sentiment) in tweets:\n      all_words.extend(words)\n    return all_words\n\ndef get_word_features(wordlist):\n    <div class=\"highlights fea_word_frequency\" id=\"word_frequency_0\" style=\"display: inline;\"><span class=\"fea_word_frequency_keys udls\">word</span>list = nltk.<span class=\"fea_word_frequency_keys udls\">Freq</span>Dist(<span class=\"fea_word_frequency_keys udls\">word</span>list)</div>\n    word_features = wordlist.keys()\n    return word_features\n\nword_features = get_word_features(get_words_in_tweets(data))\n\ndef extract_features(document):\n    document_words = set(document)\n    features = {}\n    for word in word_features:\n        features[word.decode(\"utf8\")] = (word in document_words)\n    return features\n\nallsetlength = len(data)\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_keys udls\">train</span>ing_set = nltk.classify.apply_features(extract_features, data[:allsetlength/10*8])</div>\ntest_set = data[allsetlength/10*8:]\n<div class=\"highlights fea_classification\" id=\"classification_1\" style=\"display: inline;\"><span class=\"fea_classification_keys udls\">classifier</span> = nltk.<span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>.<span class=\"fea_classification_keys udls\">train</span>(<span class=\"fea_classification_keys udls\">train</span>ing_set)</div>\n\ndef classify(tweet):\n\tprint classifier.classify(extract_features(tweet.split()))\n\nclassify(\"Bug\u00fcn \u00e7ok g\u00fczel bir g\u00fcn\")\n#https://github.com/mertkahyaoglu/twitter-sentiment-analysis/blob/master/classify</code></pre></div></body></html>", "fir_27": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_27\"><pre id=\"fir_27_code\"><code class=\"python\">from nltk.corpus import brown\nfrom nltk import FreqDist\n\n<div class=\"highlights fea_word_frequency\" id=\"word_frequency_0\" style=\"display: inline;\">suffix_fdist = <span class=\"fea_word_frequency_keys udls\">Freq</span>Dist()</div>\nfor word in brown.words():\n    word = word.lower()\n    suffix_fdist[word[-1:]] += 1\n    suffix_fdist[word[-2:]] += 1\n    suffix_fdist[word[-3:]] += 1\ncommon_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n\ndef pos_features(word):\n    features = {}\n    for suffix in common_suffixes:\n        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n    return features\n\n<div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\"><span class=\"fea_tagger_keys udls\">tagge</span>d_words = brown.<span class=\"fea_tagger_keys udls\">tagge</span>d_words(categories='news')</div>\nfeaturesets = [(pos_features(n), g) for (n,g) in tagged_words]\n\nfrom nltk import DecisionTreeClassifier\nfrom nltk.classify import accuracy\n\ncutoff = int(len(featuresets) * 0.1)\ntrain_set, test_set = featuresets[cutoff:], featuresets[:cutoff]\n\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_keys udls\">classifier</span> = DecisionTree<span class=\"fea_classification_keys udls\">Classifier</span>.<span class=\"fea_classification_keys udls\">train</span>(<span class=\"fea_classification_keys udls\">train</span>_set)</div> # NLTK is a teaching toolkit which is not really optimized for speed. Therefore, this may take forever. For speed, use scikit-learn for the classifiers.\n\naccuracy(classifier, test_set)\n\nclassifier.classify(pos_features('cats'))\n\nclassifier.pseudocode(depth=4)</code></pre></div></body></html>", "fir_26": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_26\"><pre id=\"fir_26_code\"><code class=\"python\">s = \"Le temps est un grand ma\u00eetre, dit-on, le malheur est qu'il tue ses \u00e9l\u00e8ves.\"\ns = s.lower()\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(\"[a-zA-Z'`\u00e9\u00e8\u00ee]+\")\ns_tokenized = tokenizer.tokenize(s)\n\nfrom nltk.util import ngrams\ngenerated_4grams = []\n\nfor word in s_tokenized:\n    generated_4grams.append(list(<div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\"><span class=\"fea_n_grams_keys udls\">ngrams</span>(word, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')</div>)) # n = 4.\n\ngenerated_4grams = [word for sublist in generated_4grams for word in sublist]\n\nng_list_4grams = generated_4grams\nfor idx, val in enumerate(generated_4grams):\n    ng_list_4grams[idx] = ''.join(val)\n\nfreq_4grams = {}\n\nfor ngram in ng_list_4grams:\n    if ngram not in freq_4grams:\n        freq_4grams.update({ngram: 1})\n    else:\n        ngram_occurrences = freq_4grams[ngram]\n        freq_4grams.update({ngram: ngram_occurrences + 1})\n        \nfrom operator import itemgetter # The operator module exports a set of efficient functions corresponding to the intrinsic operators of Python. For example, operator.add(x, y) is equivalent to the expression x + y.\n\nfreq_4grams_sorted = sorted(freq_4grams.items(), key=itemgetter(1), reverse=True)[0:300] # We only keep the 300 most popular n-grams. This was suggested in the original paper written about n-grams.\n\nfrom nltk import everygrams\n\ns_clean = ' '.join(s_tokenized) # For the code below we need the raw sentence as opposed to the tokens.\n\ndef ngram_extractor(sent):\n    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4) \n            if ' ' not in ng and '\\n' not in ng and ng != ('_',)]\n\nngram_extractor(s_clean)</code></pre></div></body></html>", "fir_15": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_15\"><pre id=\"fir_15_code\"><code class=\"python\"># -*- coding: utf-8 -*-\n# Maximum Entropy Part-of-Speech Tagger for NLTK (Natural Language Toolkit)\n# Author: Arne Neumann\n# Licence: GPL 3\n\n#__docformat__ = 'epytext en'\n\n\"\"\"\nA I{part-of-speech tagger} that uses NLTK's build-in L{Maximum Entropy\nmodels&lt;nltk.MaxentClassifier&gt;} to find the most likely I{part-of-speech\ntag} (POS) for each word in a given sequence.\n\nThe tagger will be trained on a corpus of tagged sentences. For every word\nin the corpus, a C{tuple} consisting of a C{dictionary} of features from\nthe word's context (e.g. preceding/succeeding words and tags, word\nprefixes/suffixes etc.) and the word's tag will be generated.\nThe maximum entropy classifier will learn a model from these tuples that\nwill be used by the tagger to find the most likely POS-tag for any given\nword, even unseen ones.\n\nThe tagger and the featuresets chosen for training are implemented as described\nin Ratnaparkhi, Adwait (1996). A Maximum Entropy Model for Part-Of-Speech\nTagging. In Proceedings of the ARPA Human Language Technology Workshop. Pages\n250-255.\n\nUsage notes:\n============\n\nPlease install the MEGAM package (http://hal3.name/megam),\notherwise training will take forever.\n\nTo use the demo, please install either 'brown' or 'treebank' with::\n\n    import nltk\n    nltk.download()\n\nin the Python interpreter. Proper usage of demo() and all other functions and\nmethods is described below.\n\"\"\"\n\nimport time\nimport re\nfrom collections import defaultdict\n\nfrom nltk import TaggerI, FreqDist, untag, config_megam\nfrom nltk.classify.maxent import MaxentClassifier\n                  \n\nPATH_TO_MEGAM_EXECUTABLE = \"/usr/bin/megam\"\nconfig_megam(PATH_TO_MEGAM_EXECUTABLE)\n\n\nclass MaxentPosTagger(TaggerI):\n    \"\"\"\n    MaxentPosTagger is a part-of-speech tagger based on Maximum Entropy models.\n    \"\"\"\n    def train(self, train_sents, algorithm='megam', rare_word_cutoff=5,\n              rare_feat_cutoff=5, uppercase_letters='[A-Z]', trace=3,\n              **cutoffs):\n        \"\"\"\n        MaxentPosTagger trains a Maximum Entropy model from a C{list} of tagged\n        sentences.\n\n        @type train_sents: C{list} of C{list} of tuples of (C{str}, C{str})\n        @param train_sents: A list of tagged sentences. Each sentence is\n        represented by a list of tuples. Each tuple holds two strings, a\n        word and its tag, e.g. ('company','NN').\n\n        @type algorithm: C{str}\n        @param algorithm: The algorithm that is used by\n        L{nltk.MaxentClassifier.train()} to train and optimise the model. It is\n        B{strongly recommended} to use the C{LM-BFGS} algorithm provided by the\n        external package U{megam&lt;http://hal3.name/megam/&gt;} as it is much faster\n        and uses less memory than any of the algorithms provided by NLTK (i.e.\n        C{GIS}, C{IIS}) or L{scipy} (e.g. C{CG} and C{BFGS}).\n\n        @type rare_word_cutoff: C{int}\n        @param rare_word_cutoff: Words with less occurrences than\n        C{rare_word_cutoff} will be treated differently by L{extract_feats}\n        than non-rare words (cf. Ratnaparkhi 1996).\n\n        @type rare_feat_cutoff: C{int}\n        @param rare_feat_cutoff: ignore features that occur less than\n        C{rare_feat_cutoff} during training.\n\n        @type uppercase_letters: C{regex}\n        @param uppercase_letters: a regular expression that covers all\n        uppercase letters of the language of your corpus (e.g. '[A-Z\u00c4\u00d6\u00dc]' for\n        German)\n\n        @type trace: C{int}\n        @param trace: The level of diagnostic output to produce. C{0} doesn't\n        produce any output, while C{3} will give all the output that C{megam}\n        produces plus the time it took to train the model.\n\n        @param cutoffs: Arguments specifying various conditions under\n            which the training should be halted. When using C{MEGAM}, only\n            C{max_iter} should be relevant. For other cutoffs see\n            L{nltk.MaxentClassifier}\n\n              - C{max_iter=v}: Terminate after C{v} iterations.\n       \"\"\"\n        self.uppercase_letters = uppercase_letters\n        self.word_freqdist = self.gen_word_freqs(train_sents)\n        self.featuresets = self.gen_featsets(train_sents,\n                rare_word_cutoff)\n        self.features_freqdist = self.gen_feat_freqs(self.featuresets)\n        self.cutoff_rare_feats(self.featuresets, rare_feat_cutoff)\n\n        t1 = time.time()\n        <div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">self.<span class=\"fea_classification_keys udls\">classifier</span> = Maxent<span class=\"fea_classification_keys udls\">Classifier</span>.<span class=\"fea_classification_keys udls\">train</span>(self.featuresets, algorithm,\n                                                 trace, **cutoffs)</div>\n        t2 = time.time()\n        if trace &gt; 0:\n            print \"time to train the classifier: {0}\".format(round(t2-t1, 3))\n\n    def gen_feat_freqs(self, featuresets):\n        \"\"\"\n        Generates a frequency distribution of joint features (feature, tag)\n        tuples. The frequency distribution will be used by the tagger to\n        determine which (rare) features should not be considered during\n        training (feature cutoff).\n\n        This is how joint features look like::\n            (('t-2 t-1', 'IN DT'), 'NN')\n            (('w-2', '&lt;START&gt;'), 'NNP')\n            (('w+1', 'of'), 'NN')\n\n        @type featuresets: {list} of C{tuples} of (C{dict}, C{str})\n        @param featuresets: a list of tuples that contain the featureset of a\n        word from the training set and its POS tag.\n\n        @rtype: C{FreqDist}\n        @return: a L{frequency distribution&lt;nltk.FreqDist()&gt;},\n        counting how often each (context information feature, tag) tuple occurs\n        in the training sentences.\n        \"\"\"\n        features_freqdist = defaultdict(int)\n        for (feat_dict, tag) in featuresets:\n            for (feature, value) in feat_dict.items():\n                features_freqdist[ ((feature, value), tag) ] += 1\n        return features_freqdist\n\n    def gen_word_freqs(self, train_sents):\n        \"\"\"\n        Generates word frequencies from the training sentences for the feature\n        extractor.\n\n        @type train_sents: C{list} of C{list} of tuples of (C{str}, C{str})\n        @param train_sents: A list of tagged sentences.\n\n        @rtype: C{FreqDist}\n        @return: a L{frequency distribution&lt;nltk.FreqDist()&gt;},\n        counting how often each word occurs in the training sentences.\n        \"\"\"\n        word_freqdist = FreqDist()\n        for tagged_sent in train_sents:\n            for (word, _tag) in tagged_sent:\n                word_freqdist[word] += 1\n        return word_freqdist\n\n    def gen_featsets(self, train_sents, rare_word_cutoff):\n        \"\"\"\n        Generates featuresets for each token in the training sentences.\n\n        @type train_sents: C{list} of C{list} of tuples of (C{str}, C{str})\n        @param train_sents: A list of tagged sentences.\n\n        @type rare_word_cutoff: C{int}\n        @param rare_word_cutoff: Words with less occurrences than\n        C{rare_word_cutoff} will be treated differently by L{extract_feats}\n        than non-rare words (cf. Ratnaparkhi 1996).\n\n        @rtype: {list} of C{tuples} of (C{dict}, C{str})\n        @return:  a list of tuples that contains the featureset of\n        a token and its POS-tag.\n        \"\"\"\n        featuresets = []\n        for tagged_sent in train_sents:\n            history = []\n            untagged_sent = untag(tagged_sent)\n            for (i, (_word, tag)) in enumerate(tagged_sent):\n                featuresets.append( (self.extract_feats(untagged_sent, i,\n                    history, rare_word_cutoff), tag) )\n                history.append(tag)\n        return featuresets\n\n\n    def cutoff_rare_feats(self, featuresets, rare_feat_cutoff):\n        \"\"\"\n        Cuts off rare features to reduce training time and prevent overfitting.\n\n        Example\n        =======\n\n            Let's say, the suffixes of this featureset are too rare to learn.\n\n            &gt;&gt;&gt; featuresets[46712]\n            ({'suffix(1)': 't',\n            'prefix(1)': 'L',\n            'prefix(2)': 'Le',\n            'prefix(3)': 'Lem',\n            'suffix(3)': 'ont',\n            'suffix(2)': 'nt',\n            'contains-uppercase': True,\n            'prefix(4)': 'Lemo',\n            'suffix(4)': 'mont'},\n            'NNP')\n\n            C{cutoff_rare_feats} would then remove the rare joint features::\n\n                (('suffix(1)', 't'), 'NNP')\n                (('suffix(3)', 'ont'), 'NNP')\n                ((suffix(2)': 'nt'), 'NNP')\n                (('suffix(4)', 'mont'), 'NNP')\n\n            and return a featureset that only contains non-rare features:\n\n            &gt;&gt;&gt; featuresets[46712]\n            ({'prefix(1)': 'L',\n            'prefix(2)': 'Le',\n            'prefix(3)': 'Lem',\n            'contains-uppercase': True,\n            'prefix(4)': 'Lemo'},\n            'NNP')\n\n\n        @type featuresets: {list} of C{tuples} of (C{dict}, C{str})\n        @param featuresets: a list of tuples that contain the featureset of a\n        word from the training set and its POS tag\n\n        @type rare_feat_cutoff: C{int}\n        @param rare_feat_cutoff: if a (context information feature, tag)\n        tuple occurs less than C{rare_feat_cutoff} times in the training\n        set, then its corresponding feature will be removed from the\n        C{featuresets} to be learned.\n        \"\"\"\n        never_cutoff_features = set(['w','t'])\n\n        for (feat_dict, tag) in featuresets:\n            for (feature, value) in feat_dict.items():\n                feat_value_tag = ((feature, value), tag)\n                if self.features_freqdist[feat_value_tag] &lt; rare_feat_cutoff:\n                    if feature not in never_cutoff_features:\n                        feat_dict.pop(feature)\n\n\n    def extract_feats(self, sentence, i, history, rare_word_cutoff=5):\n        \"\"\"\n        Generates a featureset from a word (in a sentence). The features\n        were chosen as described in Ratnaparkhi (1996) and his Java\n        software package U{MXPOST&lt;ftp://ftp.cis.upenn.edu/pub/adwait/jmx&gt;}.\n\n        The following features are extracted:\n\n            - features for all words: last tag (C{t-1}), last two tags (C{t-2\n              t-1}), last words (C{w-1}) and (C{w-2}), next words (C{w+1}) and\n              (C{w+2})\n            - features for non-rare words: current word (C{w})\n            - features for rare words: word suffixes (last 1-4 letters),\n              word prefixes (first 1-4 letters),\n              word contains number (C{bool}), word contains uppercase character\n              (C{bool}), word contains hyphen (C{bool})\n\n        Ratnaparkhi experimented with his tagger on the Wall Street Journal\n        corpus (Penn Treebank project). He found that the tagger yields\n        better results when words which occur less than 5 times are treated\n        as rare. As your mileage may vary, please adjust\n        L{rare_word_cutoff} accordingly.\n\n        Examples\n        ========\n\n            1. This is a featureset extracted from the nonrare (word, tag)\n            tuple ('considerably', 'RB')\n\n            &gt;&gt;&gt; featuresets[22356]\n            ({'t-1': 'VB',\n            't-2 t-1': 'TO VB',\n            'w': 'considerably',\n            'w+1': '.',\n            'w+2': '&lt;END&gt;',\n            'w-1': 'improve',\n            'w-2': 'to'},\n            'RB')\n\n            2. A featureset extracted from the rare tuple ('Lemont', 'NN')\n\n            &gt;&gt;&gt; featuresets[46712]\n            ({'suffix(1)': 't',\n            'prefix(1)': 'L',\n            'prefix(2)': 'Le',\n            'prefix(3)': 'Lem',\n            'suffix(3)': 'ont',\n            'suffix(2)': 'nt',\n            'contains-uppercase': True,\n            'prefix(4)': 'Lemo',\n            'suffix(4)': 'mont'},\n            'NNP')\n\n\n        @type sentence: C{list} of C{str}\n        @param sentence: A list of words, usually a sentence.\n\n        @type i: C{int}\n        @param i: The index of a word in a sentence, where C{sentence[0]} would\n        represent the first word of a sentence.\n\n        @type history: C{int} of C{str}\n        @param history: A list of POS-tags that have been assigned to the\n        preceding words in a sentence.\n\n        @type rare_word_cutoff: C{int}\n        @param rare_word_cutoff: Words with less occurrences than\n        C{rare_word_cutoff} will be treated differently than non-rare words\n        (cf. Ratnaparkhi 1996).\n\n        @rtype: C{dict}\n        @return: a dictionary of features extracted from a word's\n        context.\n        \"\"\"\n        features = {}\n        hyphen = re.compile(\"-\")\n        number = re.compile(\"\\d\")\n        uppercase = re.compile(self.uppercase_letters)\n\n        #get features: w-1, w-2, t-1, t-2.\n        #takes care of the beginning of a sentence\n        if i == 0: #first word of sentence\n            features.update({\"w-1\": \"&lt;START&gt;\", \"t-1\": \"&lt;START&gt;\",\n                             \"w-2\": \"&lt;START&gt;\", \"t-2 t-1\": \"&lt;START&gt; &lt;START&gt;\"})\n        elif i == 1: #second word of sentence\n            features.update({\"w-1\": sentence[i-1], \"t-1\": history[i-1],\n                             \"w-2\": \"&lt;START&gt;\",\n                             \"t-2 t-1\": \"&lt;START&gt; %s\" % (history[i-1])})\n        else:\n            features.update({\"w-1\": sentence[i-1], \"t-1\": history[i-1],\n                \"w-2\": sentence[i-2],\n                \"t-2 t-1\": \"%s %s\" % (history[i-2], history[i-1])})\n\n        #get features: w+1, w+2. takes care of the end of a sentence.\n        for inc in [1, 2]:\n            try:\n                features[\"w+%i\" % (inc)] = sentence[i+inc]\n            except IndexError:\n                features[\"w+%i\" % (inc)] = \"&lt;END&gt;\"\n\n        if self.word_freqdist[sentence[i]] &gt;= rare_word_cutoff:\n            #additional features for 'non-rare' words\n            features[\"w\"] = sentence[i]\n\n        else: #additional features for 'rare' or 'unseen' words\n            features.update({\"suffix(1)\": sentence[i][-1:],\n                \"suffix(2)\": sentence[i][-2:], \"suffix(3)\": sentence[i][-3:],\n                \"suffix(4)\": sentence[i][-4:], \"prefix(1)\": sentence[i][:1],\n                \"prefix(2)\": sentence[i][:2], \"prefix(3)\": sentence[i][:3],\n                \"prefix(4)\": sentence[i][:4]})\n            if hyphen.search(sentence[i]) != None:\n                #set True, if regex is found at least once\n                features[\"contains-hyphen\"] = True\n            if number.search(sentence[i]) != None:\n                features[\"contains-number\"] = True\n            if uppercase.search(sentence[i]) != None:\n                features[\"contains-uppercase\"] = True\n\n        return features\n\n\n    def tag(self, sentence, rare_word_cutoff=5):\n        \"\"\"\n        Attaches a part-of-speech tag to each word in a sequence.\n\n        @type sentence: C{list} of C{str}\n        @param sentence: a list of words to be tagged.\n\n        @type rare_word_cutoff: C{int}\n        @param rare_word_cutoff: words with less occurrences than\n        C{rare_word_cutoff} will be treated differently than non-rare words\n        (cf. Ratnaparkhi 1996).\n\n        @rtype: C{list} of C{tuples} of (C{str}, C{str})\n        @return: a list of tuples consisting of a word and its corresponding\n        part-of-speech tag.\n        \"\"\"\n        history = []\n        for i in xrange(len(sentence)):\n            featureset = self.extract_feats(sentence, i, history,\n                                               rare_word_cutoff)\n            tag = self.classifier.classify(featureset)\n            history.append(tag)\n        return zip(sentence, history)\n\n\ndef demo(corpus, num_sents):\n    \"\"\"\n    Loads a few sentences from the Brown corpus or the Wall Street Journal\n    corpus, trains them, tests the tagger's accuracy and tags an unseen\n    sentence.\n\n    @type corpus: C{str}\n    @param corpus: Name of the corpus to load, either C{brown} or C{treebank}.\n\n    @type num_sents: C{int}\n    @param num_sents: Number of sentences to load from a corpus. Use a small\n    number, as training might take a while.\n    \"\"\"\n    if corpus.lower() == \"brown\":\n        from nltk.corpus import brown\n        tagged_sents = brown.tagged_sents()[:num_sents]\n    elif corpus.lower() == \"treebank\":\n        from nltk.corpus import treebank\n        tagged_sents = treebank.tagged_sents()[:num_sents]\n    else:\n        print \"Please load either the 'brown' or the 'treebank' corpus.\"\n\n    size = int(len(tagged_sents) * 0.1)\n    train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n    maxent_tagger = MaxentPosTagger()\n    maxent_tagger.train(train_sents)\n    print \"tagger accuracy (test %i sentences, after training %i):\" % \\\n        (size, (num_sents - size)), maxent_tagger.evaluate(test_sents)\n    print \"\\n\\n\"\n    print \"classify unseen sentence: \", maxent_tagger.tag([\"This\", \"is\", \"so\",\n        \"slow\", \"!\"])\n    print \"\\n\\n\"\n    print \"show the 10 most informative features:\"\n    print maxent_tagger.classifier.show_most_informative_features(10)\n\n\nif __name__ == '__main__':\n    demo(\"treebank\", 200)\n    #~ featuresets = demo_debugger(\"treebank\", 10000)\n    print \"\\n\\n\\n\"\n\n#https://github.com/arne-cl/nltk-maxent-pos-tagger/blob/master/mxpost</code></pre></div></body></html>", "sec_7": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"sec_7\"><pre id=\"sec_7_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob.parsers import PatternParser\n<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">blob = TextBlob(\"Parsing is fun.\", <span class=\"fea_parsing_keys udls\">parser</span>=Pattern<span class=\"fea_parsing_keys udls\">Parser</span>())</div>\n<div class=\"highlights fea_parsing\" id=\"parsing_1\" style=\"display: inline;\">blob.parse()</div>\n#https://textblob.readthedocs.io/en/dev/advanced_usage.html</code></pre></div></body></html>", "sec_10": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_10\"><pre id=\"sec_10_code\"><code class=\"python\">\nfrom flask import Flask, render_template, request\napp = Flask(__name__)\nfrom textblob import TextBlob\nimport nltk\nfrom textblob import Word\nimport sys\n\n\ndef parse(string):\n   \"\"\"\n   Parse a paragraph. Devide it into sentences and try to generate quesstions from each sentences.\n   \"\"\"\n   data = []\n   try:\n      txt = TextBlob(string)\n      # Each sentence is taken from the string input and passed to genQuestion() to generate questions.\n      for sentence in txt.sentences:\n         question = genQuestion(sentence)\n         if question != None:\n            data.append(question)\n      return data\n   except Exception as e:\n      raise e\n\n\n\ndef genQuestion(line):\n\n   \"\"\"\n   outputs question from the given text\n   \"\"\"\n   answer = line\n   if type(line) is str:\n      line = TextBlob(line) # Create object of type textblob.blob.TextBlob\n\n   bucket = {}               # Create an empty dictionary\n   for i,j in enumerate(<div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">line.tags</div>):  # line.tags are the parts-of-speach in English\n      if j[1] not in bucket:\n         bucket[j[1]] = i  # Add all tags to the dictionary or bucket variable\n    \n   if verbose:               # In verbose more print the key,values of dictionary\n      print('\\n','-'*20)\n      print(line ,'\\n')  \n      print(\"TAGS:\",line.tags, '\\n')  \n      print(bucket)\n    \n   question = ''            # Create an empty string \n\n    # These are the english part-of-speach tags used in this demo program.\n    #.....................................................................\n    # NNS     Noun, plural\n    # JJ  Adjective \n    # NNP     Proper noun, singular \n    # VBG     Verb, gerund or present participle \n    # VBN     Verb, past participle \n    # VBZ     Verb, 3rd person singular present \n    # VBD     Verb, past tense \n    # IN      Preposition or subordinating conjunction \n    # PRP     Personal pronoun \n    # NN  Noun, singular or mass \n    #.....................................................................\n\n    # Create a list of tag-combination\n\n   l1 = ['NNP', 'VBG', 'VBZ', 'IN']\n   l2 = ['NNP', 'VBG', 'VBZ']\n    \n\n   l3 = ['PRP', 'VBG', 'VBZ', 'IN']\n   l4 = ['PRP', 'VBG', 'VBZ']\n   l5 = ['PRP', 'VBG', 'VBD']\n   l6 = ['NNP', 'VBG', 'VBD']\n   l7 = ['NN', 'VBG', 'VBZ']\n\n   l8 = ['NNP', 'VBZ', 'JJ']\n   l9 = ['NNP', 'VBZ', 'NN']\n\n   l10 = ['NNP', 'VBZ']\n   l11 = ['PRP', 'VBZ']\n   l12 = ['NNP', 'NN', 'IN']\n   l13 = ['NN', 'VBZ']\n\n   l14 = ['DT', 'NNP', 'VBZ', 'JJ', 'IN']\n\n\n    # With the use of conditional statements the dictionary is compared with the list created above\n\n   if all(key in bucket for key in l14): #'NN', 'VBZ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + ' ' + line.words[bucket['JJ']] + ' ' + line.words[bucket['IN']] + '?'\n\n   elif all(key in  bucket for key in l1): #'NNP', 'VBG', 'VBZ', 'IN' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NNP']]+ ' '+ line.words[bucket['VBG']] + '?'\n\n    \n   elif all(key in  bucket for key in l2): #'NNP', 'VBG', 'VBZ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NNP']] +' '+ line.words[bucket['VBG']] + '?'\n\n    \n   elif all(key in  bucket for key in l3): #'PRP', 'VBG', 'VBZ', 'IN' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['PRP']]+ ' '+ line.words[bucket['VBG']] + '?'\n\n    \n   elif all(key in  bucket for key in l4): #'PRP', 'VBG', 'VBZ' in sentence.\n      question = 'What ' + line.words[bucket['PRP']] +' '+  ' does ' + line.words[bucket['VBG']]+ ' '+  line.words[bucket['VBG']] + '?'\n\n   elif all(key in  bucket for key in l7): #'NN', 'VBG', 'VBZ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NN']] +' '+ line.words[bucket['VBG']] + '?'\n\n   elif all(key in bucket for key in l8): #'NNP', 'VBZ', 'JJ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + '?'\n\n   elif all(key in bucket for key in l9): #'NNP', 'VBZ', 'NN' in sentence\n      question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + '?'\n\n   elif all(key in bucket for key in l11): #'PRP', 'VBZ' in sentence.\n      if line.words[bucket['PRP']] in ['she','he']:\n          question = 'What' + ' does ' + line.words[bucket['PRP']].lower() + ' ' + line.words[bucket['VBZ']].singularize() + '?'\n\n   elif all(key in bucket for key in l10): #'NNP', 'VBZ' in sentence.\n      question = 'What' + ' does ' + line.words[bucket['NNP']] + ' ' + line.words[bucket['VBZ']].singularize() + '?'\n\n   elif all(key in bucket for key in l13): #'NN', 'VBZ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NN']] + '?'\n    \n \n\n    # When the tags are generated 's is split to ' and s. To overcome this issue.\n   if 'VBZ' in bucket and line.words[bucket['VBZ']] == \"\u2019\":\n      question = question.replace(\" \u2019 \",\"'s \")\n\n   # Print the genetated questions as output.\n   if question != '':\n      print('\\n', 'Question: ' + question )\n\n      return {'question':question,'answer':answer}\n      # print('\\n', 'Question: ' + question )\n   \n\n@app.route('/')\ndef student():\n   return render_template('form.html')\n\n@app.route('/result',methods = ['POST', 'GET'])\ndef result():\n   global verbose \n   verbose = False\n   text_input = ''\n   if request.method == 'POST':\n      result = request.form\n      for key, value in result.items():\n         text_input += value\n      data = (<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">parse(text_input)</div>)\n     \n      return render_template(\"result.html\",result = data)\n   else:\n      return render_template('form.html')\n\n\nif __name__ == '__main__':\n   app.run(debug = True)\n   #https://github.com/huudangdev/generator-question-textblob-nlp/blob/master/app</code></pre></div></body></html>", "sec_2": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"sec_2\"><pre id=\"sec_2_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob import Word\n\nword1 = Word(\"apples\")\nprint(\"apples:\", <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">word1.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize()</div>)\n\nword2 = Word(\"media\")\nprint(\"media:\", word2.lemmatize())\n\nworfir = Word(\"greater\")\nprint(\"greater:\", worfir.lemmatize(\"a\"))\n\nfor word, pos in <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\">text_blob_object.<span class=\"fea_Part_of_Speech_keys udls\">tag</span>s</div>:\n    print(word + \" =&gt; \" + pos)\n\ntext = (\"Football is a good game. It has many health benefit\")\ntext_blob_object = TextBlob(text)\n<div class=\"highlights fea_text_simplify\" id=\"text_simplify_0\" style=\"display: inline;\">print(text_blob_object.words.pluralize())\nprint(text_blob_object.words.singularize())</div>\n\n    #https://stackabuse.com/python-for-nlp-introduction-to-the-textblob-library/</code></pre></div></body></html>", "sec_12": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_12\"><pre id=\"sec_12_code\"><code class=\"python\">from flask import Flask, request, jsonify\nfrom textblob import TextBlob, Word\nfrom textblob.exceptions import NotTranslated\napp = Flask(__name__)\n\nfrom signal import *\n\n@app.route(\"/sentiment\")\ndef singularize():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\t\n\treturn jsonify(blob.sentiment)\n\n@app.route(\"/singularize\")\ndef sentiment():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\t\n\treturn jsonify(blob.words.singularize())\n\n@app.route(\"/lemmatize\")\ndef lemmatize():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\t\n\treturn jsonify(<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">blob.words.<span class=\"fea_lemmatization_keys udls\">lemma</span>tize()</div>)\n\n@app.route(\"/correct\")\ndef correct():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\t\n\treturn jsonify({'correct':str(<div class=\"highlights fea_spellcheck\" id=\"spellcheck_0\" style=\"display: inline;\">blob.correct()</div>)})\n\n@app.route(\"/spelling\")\ndef spelling():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\n\tsuggestions = {}\n\tfor token in blob.words:\n\t\tword = Word(token)\n\t\tsuggestions[token] = word.spellcheck()\n\t\t\n\treturn jsonify(suggestions)\n\n@app.route(\"/language\")\ndef language():\n\ttext = request.args.get('text').strip()\n\tblob = TextBlob(text)\n\t\n\treturn jsonify({\"language\":blob.detect_language()})\n\n@app.route(\"/translate\")\ndef translate():\n\ttext = request.args.get('text').strip()\n\tl_from = request.args.get('from')\n\tl_to = request.args.get('to')\n\n\tblob = TextBlob(text)\n\n\tif l_from is None:\n\t\t<div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">l_from = blob.<span class=\"fea_language_detection_keys udls\">detect</span>_<span class=\"fea_language_detection_keys udls\">language</span>()</div>\n\t\n\ttry:\n\t\t<div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">translated = blob.translate(from_lang = l_from, to = l_to)</div>\n\texcept NotTranslated:\n\t\ttranslated = text\t\t\n\n\treturn jsonify({\"translation\":str(translated)})\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=8593)\n    #https://github.com/dpasch01/textblob-service/blob/master/textblob-service</code></pre></div></body></html>", "sec_6": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"sec_6\"><pre id=\"sec_6_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob.taggers import NLTKTagger\n<div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">nltk_<span class=\"fea_tagger_keys udls\">tagge</span>r = NLTK<span class=\"fea_tagger_keys udls\">Tagge</span>r()</div>\nblob = TextBlob(\"Tag! You're It!\", pos_tagger=nltk_tagger)\n<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\">blob.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_<span class=\"fea_Part_of_Speech_keys udls\">tag</span>s</div>\n#https://textblob.readthedocs.io/en/dev/advanced_usage.html</code></pre></div></body></html>", "sec_9": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_9\"><pre id=\"sec_9_code\"><code class=\"python\">import textblob\n<div class=\"highlights fea_text_simplify\" id=\"text_simplify_0\" style=\"display: inline;\">stringText = textblob.TextBlob(str(list(dataset[\"Summary\"]))).lower()</div>\nwords = stringText.words\nwordCount = {}\nignore = ['a', 'an', 'the', \"'the\", 'and', 'to', 'of', 'in', 'into', 'is', 'was', 'on', 'at', 'from', 'with',\n          'while', 'for', \"'s\", 'as', 'not', 'by', 'after', 'during']\n\nfor word in words:\n    if word in ignore:\n        continue\n    if word in wordCount:\n        wordCount[word] = wordCount[word] + 1\n    else:\n        wordCount[word] = 1\n\nimport operator\nsorted_word = sorted(wordCount.items(), key=operator.itemgetter(1), reverse=True)[:500]\nwith open(\"sorted-word-count.txt\", \"w\") as f:\n    f.write(str(sorted_word))\n\nreasons = ['weather', 'fire', 'shot down', 'stall/runway', 'pilot/crew error', 'systems failure']\n\nexpresion = ['((poor|bad).*(weather|visibility)|thunderstorm|fog)','(caught fire)|(caught on fire)', \n           '(shot down) | (terrorist) | (terrorism)', '(stall)|(runway)', '(pilot|crew) (error|fatigue)',\n            '(engine.*(fire|fail))|(structural fail)|(fuel leak)|(langing gear)|(turbulence)|(electrical)|(out of fuel)|(fuel.*exhaust)']\n\ndataset['Label'] = pd.Series(np.nan, index=dataset.index)\n\ntrainData = []\nfor x in range(len(dataset)):\n    if dataset.loc[x,\"Summary\"] is np.nan:\n        dataset.loc[x,\"Label\"] = \"unknown\"\n    else:\n        for y in range(len(expresion)):\n            if re.search(expresion[y], dataset.loc[x,\"Summary\"].lower()):\n                dataset.loc[x,\"Label\"] = reasons[y]\n                temp = dataset.loc[x,\"Summary\"].lower(), dataset.loc[x,\"Label\"]\n                trainData.append(temp)\n                break\n\nfrom textblob.classifiers import NaiveBayesClassifier\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">cl = <span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>(<span class=\"fea_classification_keys udls\">train</span>Data)</div>\n\nreasons.append(\"unknown\")\nfor x in range(30,len(dataset)):\n    if dataset.loc[x,\"Label\"] in reasons:\n       continue\n    else:\n        dataset.loc[x,\"Label\"] = cl.classify(dataset.loc[x,\"Summary\"])\n        #https://github.com/arif-zaman/airplane-crash/blob/master/Airplane.ipynb</code></pre></div></body></html>", "sec_27": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_27\"><pre id=\"sec_27_code\"><code class=\"python\">import argparse\nimport sys\nfrom textblob import TextBlob\n\nDEFAULT_SUBJECT_LIMIT = 50\nDEFAULT_BODY_LIMIT = 72\n\n\nclass CliColors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\n\ndef check_subject_is_separated_from_body(commit_message):\n    lines = commit_message.splitlines()\n    if len(lines) &gt; 1:\n        # The second line should be empty\n        check_result = not lines[1]\n    else:\n        # If there is just one line then this rule doesn't apply\n        check_result = True\n    print_result(check_result, \"Separate subject from body with a blank line\")\n\n    return check_result\n\n\ndef check_subject_is_not_too_long(commit_message, subject_limit):\n    lines = commit_message.splitlines()\n    check_result = len(lines[0]) &lt;= subject_limit\n    print_result(check_result, \"Limit the subject line to \" +\n                 str(subject_limit) + \" characters\")\n\n    return check_result\n\n\ndef check_subject_is_capitalized(commit_message):\n    lines = commit_message.splitlines()\n    # Check if first character is in upper case\n    check_result = lines[0][0].isupper()\n    print_result(check_result, \"Capitalize the subject line\")\n\n    return check_result\n\n\ndef check_subject_does_not_end_with_period(commit_message):\n    lines = commit_message.splitlines()\n    check_result = not lines[0].endswith(\".\")\n    print_result(check_result, \"Do not end the subject line with a period\")\n\n    return check_result\n\n\ndef check_subject_uses_imperative(commit_message):\n    first = commit_message.splitlines()[0]\n    <div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">third_person_singular_present_<span class=\"fea_n_grams_keys udls\">verb</span> = \"VBZ\"\n    non_third_person_singular_present_<span class=\"fea_n_grams_keys udls\">verb</span> = \"VBP\"</div>\n    # The default NLTK parser is not very good with imperative sentences\n    # so we prefix the commit message with a personal pronoun so to\n    # help it determine easier whether the upcoming word is a verb\n    # and not a noun.\n    # We will prefix in two different ways, so to avoid false results.\n    # Read more here: https://stackoverflow.com/a/30823202/6485320\n    # and here: https://stackoverflow.com/a/9572724/6485320\n    third_person_prefix = \"It \"\n    words_in_third_person_prefix_blob = len(third_person_prefix.split())\n    non_third_person_prefix = \"You \"\n    words_in_non_third_person_prefix_blob = len(\n        non_third_person_prefix.split())\n    # Turn the first character into a lowercase so to make it easier for\n    # the parser to determine whether the word is a verb and its tense\n    first_character_in_lowercase = first[0].lower()\n    first = first_character_in_lowercase + first[1:]\n    third_person_blob = TextBlob(third_person_prefix + first)\n    non_third_person_blob = TextBlob(non_third_person_prefix + first)\n\n    first_word, third_person_result = third_person_blob.tags[words_in_third_person_prefix_blob]\n    _, non_third_person_result = non_third_person_blob.tags[words_in_non_third_person_prefix_blob]\n\n    # We need to determine whether the first word is a non-third person verb\n    # when parsed in a non-third person blob. However, there were some\n    # false positives so we use a third person blob to ensure it is not a\n    # third person verb. Unfortunately, there were now some false negatives\n    # due to verbs in a non-third person form, being classified as being in\n    # third person, when parsed in the third person blob.\n    # So, we ultimately check if the verb ends with an 's' which is a pretty\n    # good indicator of a third person, simple present tense verb.\n    <div class=\"highlights fea_text_simplify\" id=\"text_simplify_0\" style=\"display: inline;\">check_res<span class=\"fea_text_simplify_keys udls\">ult</span> = non_third_person_res<span class=\"fea_text_simplify_keys udls\">ult</span> == non_third_person_singular_<span class=\"fea_text_simplify_keys udls\">pre</span>sent_verb and (\n        third_person_res<span class=\"fea_text_simplify_keys udls\">ult</span> != third_person_singular_<span class=\"fea_text_simplify_keys udls\">pre</span>sent_verb or not first_word.endswith(\"s\"))</div>\n    print_result(check_result, \"Use the imperative mood in the subject line\")\n\n    return check_result\n#https://github.com/platisd/bad-commit-message-blocker/blob/master/bad_commit_message_blocker</code></pre></div></body></html>", "sec_1": "<html><body><div class=\"codeBlock hljs swift\" id=\"sec_1\"><pre id=\"sec_1_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob import Word\ndocument = (\"In computer science, artificial intelligence (AI), \\\n            sometimes called machine intelligence, is intelligence \\\n            demonstrated by machines, in contrast to the natural intelligence \\\n            displayed by humans and animals. Computer science defines AI \\\n            research as the study of \\\"intelligent agents\\\": any device that \\\n            perceives its environment and takes actions that maximize its\\\n            chance of successfully achieving its goals.[1] Colloquially,\\\n            the term \\\"artificial intelligence\\\" is used to describe machines\\\n            that mimic \\\"cognitive\\\" functions that humans associate with other\\\n            human minds, such as \\\"learning\\\" and \\\"problem solving\\\".[2]\")\ntext_blob_object = TextBlob(document)\n<div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">for <span class=\"fea_n_grams_keys udls\">noun</span>_phrase in text_blob_ob<span class=\"fea_n_grams_keys udls\">ject</span>.<span class=\"fea_n_grams_keys udls\">noun</span>_phrases:\n    print(<span class=\"fea_n_grams_keys udls\">noun</span>_phrase)</div>\n\ntext = \"I love to watch football, but I have never played it\"\ntext_blob_object = TextBlob(text)\nfor ngram in <div class=\"highlights fea_n_grams\" id=\"n_grams_1\" style=\"display: inline;\">text_blob_ob<span class=\"fea_n_grams_keys udls\">ject</span>.<span class=\"fea_n_grams_keys udls\">ngrams</span>(2)</div>:\n    print(ngram)\n    #https://stackabuse.com/python-for-nlp-introduction-to-the-textblob-library/</code></pre></div></body></html>", "sec_8": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"sec_8\"><pre id=\"sec_8_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob.np_extractors import ConllExtractor\nextractor = ConllExtractor()\nblob = TextBlob(\"Python is a high-level programming language.\", np_extractor=extractor)\n<div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">blob.<span class=\"fea_n_grams_keys udls\">noun</span>_phrases</div>\n#https://textblob.readthedocs.io/en/dev/advanced_usage.html</code></pre></div></body></html>", "sec_17": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_17\"><pre id=\"sec_17_code\"><code class=\"python\">from textblob import TextBlob\nfrom spellchecker import SpellChecker\n\n\na = input('Enter an Incorrect String : ')\nprint('Original Text : ' + str(a))\nb = TextBlob(a)\n<div class=\"highlights fea_spellcheck\" id=\"spellcheck_0\" style=\"display: inline;\">print('Corrected <span class=\"fea_spellcheck_keys udls\">Text</span> : ' + str(b.correct()))</div>\n\n\nspell = SpellChecker()\n# Find those words that may be misspelled\nmisspelled = spell.unknown(['Good', 'Evening'])\n\nfor word in misspelled:\n    # getting the one `most likely` answer\n    print(spell.correction(word))\n    # getting a list of `likely` options\n    print(spell.candidates(word))\n    #https://github.com/OjasBarawal/Spell-Checker/blob/main/app</code></pre></div></body></html>", "sec_26": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_26\"><pre id=\"sec_26_code\"><code class=\"python\"># The main package to help us with our text analysis\nfrom textblob import TextBlob\n\n# For reading input files in CSV format\nimport csv\n\n# For doing cool regular expressions\nimport re\n\n# For sorting dictionaries\nimport operator\n\n\n# For plotting results\nimport numpy as np\nimport matplotlib.mlab as mlab\nimport matplotlibplot as thr\n\n# Intialize an empty list to hold all of our tweets\ntweets = []\n\n\n# A helper function that removes all the non ASCII characters\n# from the given string. Retuns a string with only ASCII characters.\ndef strip_non_ascii(string):\n    ''' Returns the string without non ASCII characters'''\n    stripped = (c for c in string if 0 &lt; ord(c) &lt; 127)\n    return ''.join(stripped)\n\n\n\n# LOAD AND CLEAN DATA\n\n# Load in the input file and process each row at a time.\n# We assume that the file has three columns:\n# 0. The tweet text.\n# 1. The tweet ID.\n# 2. The tweet publish date\n#\n# We create a data structure for each tweet:\n#\n# id:       The ID of the tweet\n# pubdate:  The publication date of the tweet\n# orig:     The original, unpreprocessed string of characters\n# clean:    The preprocessed string of characters\n# TextBlob: The TextBlob object, created from the 'clean' string\n\nwith open('newtwitter.csv', 'rb') as csvfile:\n    reader = csv.reader(csvfile, delimiter=',')\n    reader.next()\n    for row in reader:\n\n        tweet= dict()\n        tweet['orig'] = row[0]\n        tweet['id'] = int(row[1])\n        tweet['pubdate'] = int(row[2])\n\n        # Ignore retweets\n        if re.match(r'^RT.*', tweet['orig']):\n            continue\n\n        tweet['clean'] = tweet['orig']\n\n        # Remove all non-ascii characters\n        tweet['clean'] = strip_non_ascii(tweet['clean'])\n\n        # Normalize case\n        tweet['clean'] = tweet['clean'].lower()\n\n        # Remove URLS. (I stole this regex from the internet.)\n        tweet['clean'] = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', tweet['clean'])\n\n        # Fix classic tweet lingo\n        tweet['clean'] = re.sub(r'\\bthats\\b', 'that is', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bive\\b', 'i have', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bim\\b', 'i am', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bya\\b', 'yeah', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bcant\\b', 'can not', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bwont\\b', 'will not', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bid\\b', 'i would', tweet['clean'])\n        tweet['clean'] = re.sub(r'wtf', 'what the fuck', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bwth\\b', 'what the hell', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\br\\b', 'are', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bu\\b', 'you', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bk\\b', 'OK', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bsux\\b', 'sucks', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bno+\\b', 'no', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bcoo+\\b', 'cool', tweet['clean'])\n\n        # Emoticons?\n        # NOTE: Turns out that TextBlob already handles emoticons well, so the\n        # following is not actually needed.\n        # See http://www.datagenetics.com/blog/october52012/index.html\n        # tweet['clean'] = re.sub(r'\\b:\\)\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b:D\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b:\\(\\b', 'sad', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b:-\\)\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b=\\)\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b\\(:\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b:\\\\\\b', 'annoyed', tweet['clean'])\n\n        # Create textblob object\n        tweet['TextBlob'] = TextBlob(tweet['clean'])\n\n        # Correct spelling (WARNING: SLOW)\n        #tweet['TextBlob'] = tweet['TextBlob'].correct()\n\n        tweets.append(tweet)\n\n\n\n# DEVELOP MODELS\n\n<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">for tweet in tweets:\n    tweet['<span class=\"fea_sentiment_analysis_keys udls\">polarity</span>'] = float(tweet['TextBlob'].<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span>)\n    tweet['subjec<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>ity'] = float(tweet['TextBlob'].<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.subjec<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>ity)\n\n    if tweet['<span class=\"fea_sentiment_analysis_keys udls\">polarity</span>'] &gt;= 0.1:\n        tweet['<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>'] = 'posi<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e'\n    elif tweet['<span class=\"fea_sentiment_analysis_keys udls\">polarity</span>'] &lt;= -0.1:\n        tweet['<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>'] = 'nega<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e'\n    else:\n        tweet['<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>'] = 'neutral'</div>\n\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">tweets_sorted = sorted(tweets, key=lambda k: k['polarity'])</div>\n\n\n# EVALUATE RESULTS\n\n# First, print out a few example tweets from each sentiment category.\n\nprint \"\\n\\nTOP NEGATIVE TWEETS\"\nnegative_tweets = [d for d in tweets_sorted if d['sentiment'] == 'negative']\nfor tweet in negative_tweets[0:100]:\n    print \"id=%d, polarity=%.2f, clean=%s\" % (tweet['id'], tweet['polarity'], tweet['clean'])\n\nprint \"\\n\\nTOP POSITIVE TWEETS\"\npositive_tweets = [d for d in tweets_sorted if d['sentiment'] == 'positive']\nfor tweet in positive_tweets[-100:]:\n    print \"id=%d, polarity=%.2f, clean=%s\" % (tweet['id'], tweet['polarity'], tweet['clean'])\n\nprint \"\\n\\nTOP NEUTRAL TWEETS\"\nneutral_tweets = [d for d in tweets_sorted if d['sentiment'] == 'neutral']\nfor tweet in neutral_tweets[0:500]:\n    print \"id=%d, polarity=%.2f, clean=%s\" % (tweet['id'], tweet['polarity'], tweet['clean'])\n\n#https://github.com/stepthom/textblob-sentiment-analysis/blob/master/doAnalysis</code></pre></div></body></html>", "sec_29": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_29\"><pre id=\"sec_29_code\"><code class=\"python\">import sys\nimport json\nimport time\nimport re\nimport requests\nimport nltk\nimport argparse\nimport logging\nimport string\ntry:\n    import urllib.parse as urlparse\nexcept ImportError:\n    import urlparse\nfrom tweepy.streaming import StreamListener\nfrom tweepy import API, Stream, OAuthHandler, TweepError\nfrom textblob import TextBlob\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom bs4 import BeautifulSoup\nfrom elasticsearch import Elasticsearch\nfrom random import randint, randrange\nfrom datetime import datetime\nfrom newspaper import Article, ArticleException\n\n# import elasticsearch host, twitter keys and tokens\nfrom config import *\n\n\nSTOCKSIGHT_VERSION = '0.1-b.12'\n__version__ = STOCKSIGHT_VERSION\n\nIS_PY3 = sys.version_info &gt;= (3, 0)\n\nif not IS_PY3:\n    print(\"Sorry, stocksight requires Python 3.\")\n    sys.exit(1)\n\n# sentiment text-processing url\nsentimentURL = 'http://text-processing.com/api/sentiment/'\n\n# tweet id list\ntweet_ids = []\n\n# file to hold twitter user ids\ntwitter_users_file = './twitteruserids.txt'\n\nprev_time = time.time()\nsentiment_avg = [0.0,0.0,0.0]\n\ndef sentiment_analysis(text):\n    \"\"\"Determine if sentiment is positive, negative, or neutral\n    algorithm to figure out if sentiment is positive, negative or neutral\n    uses sentiment polarity from TextBlob, VADER Sentiment and\n    sentiment from text-processing URL\n    could be made better :)\n    \"\"\"\n\n    # pass text into sentiment url\n    if args.websentiment:\n        ret = get_sentiment_from_url(text, sentimentURL)\n        if ret is None:\n            sentiment_url = None\n        else:\n            sentiment_url, neg_url, pos_url, neu_url = ret\n    else:\n        sentiment_url = None\n\n    # pass text into TextBlob\n    text_tb = TextBlob(text)\n\n    # pass text into VADER Sentiment\n    analyzer = SentimentIntensityAnalyzer()\n    <div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">text_vs = analyzer.polarity_<span class=\"fea_text_scoring_keys udls\">score</span>s(text)</div>\n\n    # determine sentiment from our sources\n    <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">if <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>_url is None:\n        if text_tb.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &lt; 0 and text_vs['compound'] &lt;= -0.05:\n            <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"nega<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\"\n        elif text_tb.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &gt; 0 and text_vs['compound'] &gt;= 0.05:\n            <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"posi<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\"\n        else:\n            <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"neutral\"\n    else:\n        if text_tb.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &lt; 0 and text_vs['compound'] &lt;= -0.05 and <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>_url == \"nega<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\":\n            <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"nega<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\"\n        elif text_tb.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &gt; 0 and text_vs['compound'] &gt;= 0.05 and <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>_url == \"posi<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\":\n            <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"posi<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\"\n        else:\n            <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"neutral\"</div>\n\n    # calculate average polarity from TextBlob and VADER\n    polarity = (text_tb.sentiment.polarity + text_vs['compound']) / 2\n\n    # output sentiment polarity\n    print(\"************\")\n    print(\"Sentiment Polarity: \" + str(round(polarity, 3)))\n\n    # output sentiment subjectivity (TextBlob)\n    print(\"Sentiment Subjectivity: \" + str(round(text_tb.sentiment.subjectivity, 3)))\n\n    # output sentiment\n    print(\"Sentiment (url): \" + str(sentiment_url))\n    print(\"Sentiment (algorithm): \" + str(sentiment))\n    print(\"Overall sentiment (textblob): \", text_tb.sentiment) \n    print(\"Overall sentiment (vader): \", text_vs) \n    print(\"sentence was rated as \", round(text_vs['neg']*100, 3), \"% Negative\") \n    print(\"sentence was rated as \", round(text_vs['neu']*100, 3), \"% Neutral\") \n    print(\"sentence was rated as \", round(text_vs['pos']*100, 3), \"% Positive\") \n    print(\"************\")\n\n    return polarity, text_tb.sentiment.subjectivity, sentiment\n#https://github.com/shirosaidev/stocksight/blob/master/sentiment</code></pre></div></body></html>", "sec_15": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_15\"><pre id=\"sec_15_code\"><code class=\"python\">from textblob import TextBlob\n\n\ndef tweet_sentiment(text, verbose=False):\n    \"\"\"\n    The sentiment function of textblob returns two properties, polarity, and subjectivity.\n    Polarity is float which lies in the range of [-1,1] where 1 means positive statement\n    and -1 means a negative statement.\n    Subjective sentences generally refer to personal opinion, emotion or judgment whereas\n    objective refers to factual information. Subjectivity is also a float which lies in\n    the range of [0,1].\n    \"\"\"\n    # parse the tweet into textblob object\n    blob = TextBlob(text)\n    # we define the sentiment of sentence to be the product of its polarity and subjectivity\n    # tweet sentiment is the sum of sentiment for all sentences in a tweet\n    <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = sum(s.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> * s.subjec<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>ity for s in blob.sentences)</div>\n    # print if verbose\n    if verbose:\n        polarity = sum(s.polarity for s in blob.sentences)\n        subjectivity = sum(s.subjectivity for s in blob.sentences)\n        num_sentence = len(blob.sentences)\n        return text, num_sentence, polarity, subjectivity, sentiment\n    else:\n        return sentiment\n\n\ndef test():\n    sentences = [\n        '$AAPL so is this the price that gets split? If so, looks like it\u2019ll be $125.50 a share on Monday. Nice.',\n        'Stocks head into September in high gear as Apple and Tesla split, and markets await the August jobs report',\n        'S&amp;P 500 SETS FRESH RECORD CLOSING HIGH OF 3,508.01',\n        'Massive $tsla dump be careful out there short term oversold tho $spy $amzn',\n        '$SPX is overbought but momentum is very very strong. My bet is unless we correct quickly this week, we are looking for a blow off top. ',\n        '$SPY reached 350 2 points from our target of 352.. RSI is overbought - sell and wait ti buy for later. Short $SHOP and $NVAX.',\n        'Slight setback, nothing to worry about. Outlook dismal. 28 trade session left - Target $SPX 2394.25',\n        'Russell looks bad. Big bearish RSI divergence and ejected from the channel after riding up the bottom rail.',\n    ]\n    print(' | '.join(['',' #','Sentence'+' '*92,'# sentence','polarity','subjectivity','sentiment','']))\n    print('-'*162)\n    for i,sentence in enumerate(sentences):\n        text, num_sentence, polarity, subjectivity, sentiment = tweet_sentiment(sentence, verbose=True)\n        print(f' | {i+1:2d} | {text[:100]: &lt;100} | {num_sentence: &gt;10} | {polarity:+8.2f} | {subjectivity:+12.2f} | {sentiment:+9.2f} |')\n\n\nif __name__ == '__main__':\n    test()\n    #https://github.com/quantumsnowball/AppleDaily20200907/blob/master/sentiment</code></pre></div></body></html>", "sec_20": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_20\"><pre id=\"sec_20_code\"><code class=\"python\">import random\nimport re\nimport csv\nimport string\nimport operator\n\nfrom textblob import TextBlob\nfrom textblob.classifiers import NaiveBayesClassifier # update sentiment, if textblob returns neutral\n\ndef determineSentiment(sent_dict):\n\t# takes in a dictionary or sub-dictionary to return the sentiment in a list\n\tfinal_sent_dict = {}\n\tsentence_list = []\n\tfor speech in sent_dict:\n\t\ttext_sent = TextBlob(sent_dict[speech])\n\t\t#text_tag = text_sent.tags\n\t\tcounter = 1\n\t\tfor sentence in text_sent.sentences:\n\t\t\t#print(speech)\n\t\t\tfinal_sent_dict[speech + '_' + str(counter)] = (sentence.sentiment, sentence)\n\t\t\tcounter += 1 # each sub-sentence in a speech has it's own dictionary key\n\t<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">final_sent_dict[\"_average\"] = text_sent.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span></div> # beginning of an ordered dict\n\treturn final_sent_dict\n\ndef trainSentiment():\n\t# if the sentence is neutral, update to attribute sentiment based on key words\n\t# example: villian -&gt; negative, dying -&gt; negative, etc...\n\t# https://textblob.readthedocs.io/en/dev/classifiers.html#classifiers\n\n\t# train classifers on actual hamlet data\n\thamlet_train = [\n\t# act 1\n\t\t('this dreaded sight', 'neg'),\n\t\t('o god!', 'neg'),\n\t\t('o fie!', 'neg'),\n\t\t('break my heart,  for i must hold my tongue!', 'neg'),\n\t\t('funeral', 'neg'),\n\t\t('he was a man,  take him for all in all, i shall not look upon his like again', 'neg'),\n\t\t('i doubt some foul play would the night were come!', 'neg'),\n\t\t('foul deeds will rise', 'neg'),\n\t\t('pooh!', 'neg'),\n\t\t('angels and ministers of grace defend us!', 'neg'),\n\t\t('you shall not go', 'neg'),\n\t\t('hold off your hands', 'neg'),\n\t\t('my fate cries out' , 'neg'),\n\t\t(\"i'll make a ghost of him that lets me\" , 'neg'),\n\t\t('something is rotten in the state of denmark', 'neg'),\n\t\t('harrow up thy soul', 'neg'),\n\t\t('revenge', 'neg'),\n\t\t('incest', 'neg'),\n\t\t('adulterate', 'neg'),\n\t\t('beast', 'neg'),\n\t\t('lust', 'neg'),\n\t\t('a serpent stung me', 'neg'),\n\t\t('villain', 'neg'),\n\t\t('perturbed spirit!', 'neg')\n\t]\n\n\thamlet_test = [\n\t# act 2\n\t\t('dishonour', 'neg'),\n\t\t('taints of liberty', 'neg'),\n\t\t('flash and outbreak of a fiery mind', 'neg'),\n\t\t('falsehood', 'neg'),\n\t\t('fouled', 'neg'),\n\t\t('piteous', 'neg'),\n\t\t('i do not know', 'neg'),\n\t\t('i do fear it', 'neg'),\n\t\t('madness wherein now he raves', 'neg'),\n\t\t('madness', 'neg'),\n\t\t('indifferent children of the earth', 'neg'),\n\t\t('beggars bodies', 'neg'),\n\t\t('murder', 'neg'),\n\t\t('that he should weep for her?', 'neg'),\n\t\t('am i a coward?', 'neg'),\n\t\t('who calls me villain?', 'neg'),\n\t]\n\t<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">cl = <span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>(hamlet_<span class=\"fea_classification_keys udls\">train</span>)</div>\n\treturn cl\n\t#https://github.com/cyschneck/Billy-Bot/blob/master/shakespeare_sentiment</code></pre></div></body></html>", "sec_21": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_21\"><pre id=\"sec_21_code\"><code class=\"python\">from TwitterSearch import *\nfrom textblob import TextBlob\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf8')\n\nfilepath = \"2017.txt\"\ntry:\n\tfp = open(\"2017.txt\",\"r\")\n\tdi = { }\n\tfor line in fp.read().splitlines():\n\t\tcnt = 0\n\t\tscore = 0\n\t\ttemp = 0;\n\t\t#print line\n\t\n\t\t\n\t\ttso = TwitterSearchOrder() \n\t\ttso.set_keywords([line]) \n\t\ttso.set_language('en') \n\t\ttso.set_include_entities(False)\n\t\ttso.set_count(100)\n\n# it's about time to create a TwitterSearch object with our secret tokens\n\t\tts = TwitterSearch(\n\t\t\t\tconsumer_key = \"XXXX\",\n        \t\tconsumer_secret = \"YY\",\n        \t\taccess_token = \"ZZ\",\n        \t\taccess_token_secret = \"MM\"\n\t\t )\n\n\n # this is where the fun actually starts :)\n\t\tfor tweet in ts.search_tweets_iterable(tso):\n\t\t\tif(cnt&lt;20):\n\t\t\t\t<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">analysis = TextBlob(tweet['text'])\n\t\t\t\tcnt=cnt+1;\n\t\t\t\ttemp = temp+1;\n\t\t\t\tif analysis.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &gt; 0:\n\t\t\t\t\t#print '1'\n\t\t\t\t\tscore=score+1\n\t\t\t\telif analysis.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> == 0:\n\t\t\t\t\t#print '0'\n\t\t\t\t\tscore = score;\n\t\t\t\telse:\n\t\t\t\t\tscore=score-1</div>\n\t\t\telse:\n\t\t\t\tbreak\n\t#F.write(tweet['text'])\n\t\t\t#print(tweet['text'])\n\t\tx = float(score)/float(temp)\n\t\t#print x\n\t\tdi.update({line : x})\n\td_view = [ (v,k) for k,v in di.iteritems() ]\n\td_view.sort(reverse=True) # natively sort tuples by first element\n\tfor v,k in d_view:\n\t\t\tprint v, k\n\t\t\nexcept TwitterSearchException as e: # take care of all those ugly errors if there are some\n\tprint(e)\n\t#https://github.com/avaiyang/Movie-Rating-and-Prediction-Model/blob/master/twittersearch</code></pre></div></body></html>", "sec_24": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_24\"><pre id=\"sec_24_code\"><code class=\"python\">import os\nfrom datetime import datetime, timedelta\nfrom pickle import load\n\nimport pytz\nfrom flask import Flask, jsonify, redirect, render_template, request, session, url_for\nfrom flask_sqlalchemy import SQLAlchemy\nfrom textblob import TextBlob\n\nfrom model_nltk import predict_sentiment\n\napp = Flask(__name__, template_folder=\"templates\")\n\n# \"sqlite:///data.sqlite\"\n# /// for relative path\n# //// for absolute path\napp.config[\"SQLALCHEMY_DATABASE_URI\"] = os.environ.get(\n    \"DATABASE_URL\", \"sqlite:///data.sqlite\"\n)\napp.config[\"SQLALCHEMY_TRACK_MODIFICATIONS\"] = False\napp.config[\"SECRET_KEY\"] = os.environ.get(\"SECRET_KEY\", \"thisissecret\")\napp.config[\"PERMANENT_SESSION_LIFETIME\"] = timedelta(hours=12)\n\ndb = SQLAlchemy(app)\n\n# since the app is hosted on heroku so this line of code is to change the timezone\nIST = pytz.timezone(\"Asia/Kolkata\")\n\n\n# I have creted two models but I am using model_nltk because of its high accurcy and less execution time.\n# textblob is used for ploting the subjectivity and polarity curve for the input data\n\n# class for creating and initialising database\nclass New_Data(db.Model):\n\n    Id = db.Column(db.Integer, primary_key=True)\n    Text = db.Column(db.Text)\n    Sentiment = db.Column(db.String(20))\n    # .now(IST).strftime('%Y-%m-%d %H:%M:%S'))\n    Date = db.Column(\n        db.DateTime, default=datetime.now(IST).strftime(\"%Y-%m-%d %H:%M:%S\")\n    )\n\n    def __init__(self, Text, Sentiment):\n        self.Text = Text\n        self.Sentiment = Sentiment\n\n\n# loading classifier\nwith open(\"my_classifier.pickle\", \"rb\") as f:\n    classifier = load(f)\n\n\ndef allowed_file(filename):\n    \"\"\"Checking file extension i.e. text file or not\"\"\"\n    return \".\" in filename and filename.split(\".\")[1] == \"txt\"\n\n\n# route for home page\n@app.route(\"/\", methods=[\"POST\", \"GET\"])\ndef home():\n    if request.method == \"POST\":\n        sentence = str(request.form.get(\"twt\"))\n\n        <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = predict_<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>(sentence, classifier)</div>\n\n        # adding emoji to the sentiment\n        if sentiment == \"Positive\":\n            sentiment += \" \\U0001f600\"\n\n        elif sentiment == \"Negative\":\n            sentiment += \" \\U0001F641\"\n\n        else:\n            pass\n\n        # creating an instance of the data table for the database and commiting the changes\n        usr_data = New_Data(sentence, sentiment.split()[0])\n        try:\n            db.session.add(usr_data)\n            db.session.commit()\n        except:\n            pass\n\n        text = 'You have entered \"' + sentence + '\"'\n        return render_template(\n            \"index.html\", text=text, sentiment=\"Sentiment: \" + sentiment\n        )\n\n    return render_template(\"index.html\")\n\n\n# route for about page\n@app.route(\"/about\")\ndef about():\n    return render_template(\"about.html\")\n\n\n# route for members page\n@app.route(\"/member\")\ndef contact():\n    return render_template(\"members.html\")\n\n\n# route for fastapi\n# setting default value for the api\n@app.route(\"/fast-api/\", defaults={\"sentence\": \"Great\"})\n@app.route(\"/fast-api/&lt;sentence&gt;\")\ndef fast_api(sentence):\n    sentiment = predict_sentiment(sentence, classifier)\n\n    return jsonify({\"sentence\": sentence, \"sentiment\": sentiment})\n\n\n# setting post method for the api\n@app.route(\"/fastapi\", methods=[\"POST\"])\ndef fastapi():\n    text = request.form[\"text\"]\n    <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_1\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_keys udls\">polarity</span> = TextBlob(text).<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span>\n    if <span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &gt; 0:\n        <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"Posi<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\"\n    elif <span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &lt; 0:\n        <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"Nega<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e\"\n    else:\n        <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span> = \"Neutral\"\n    return jsonify({\"<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>\": <span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>})</div>\n#https://github.com/g-paras/sentiment-analysis-api/blob/master/app</code></pre></div></body></html>", "sec_25": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_25\"><pre id=\"sec_25_code\"><code class=\"python\">import facebook as fb\nimport requests\nimport argparse\nimport textblob as tb\n\nFLAGS = None\n\ndef sentiment_analysis(post):\n\n    # Here's where the magic happens\n    <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">tb_msg = tb(post['message'])\n    score = tb_msg.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span></div>\n\n    print(\"Date: %s, From: %s\\n\", post['created_time'], post['from'])\n    print(\"%s\\nShared: %s, Score: %f\", post['message'], post['share'], score)\n\n\n\ndef connect(access_token, user):\n    graph = fb.GraphAPI(access_token)\n    profile = graph.get_object(user)\n\n    return graph, profile\n\n\ndef main():\n\n    access_token = FLAGS.access_token\n    user = FLAGS.profile\n\n    graph, profile = connect(access_token, user)\n    \n    posts = graph.get_connections(profile['id'], 'posts')\n\n\n    #Let's grab all the posts and analyze them!\n    while True:\n        try:\n            [sentiment_analysis(post=post) for post in posts['data']]\n            posts= requests.get(posts['paging']['next']).json()\n        except KeyError:\n            break\n            \n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Simple Facebook Sentiment Analysis Script')\n    parser.add_argument('--access_token', type=str, required=True, default='', help='Your Facebook API Access Token: https://developers.facebook.com/docs/graph-api/overview')\n    parser.add_argument('--profile', type=str, required=True, default='', help='The profile name to retrieve the posts from')\n    FLAGS = parser.parse_args()\n    main()\n    #https://github.com/cosimoiaia/Facebook-Sentiment-Analysis/blob/master/simple_facebook_sentiment_analysis</code></pre></div></body></html>", "sec_28": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_28\"><pre id=\"sec_28_code\"><code class=\"python\">import re\nimport tweepy\nfrom tweepy import OAuthHandler\nfrom textblob import TextBlob\n \nclass TwitterClient(object):\n    '''\n    Generic Twitter Class for sentiment analysis.\n    '''\n    def __init__(self):\n        '''\n        Class constructor or initialization method.\n        '''\n       \n        consumer_key = 'XXXXXXXXXXXX'\n        consumer_secret = 'XXXXXXXXXXXX'\n        access_token = 'XXXXXXXXXXXX'\n        access_token_secret = 'XXXXXXXXXXXX'\n \n       \n        try:\n         \n            self.auth = OAuthHandler(consumer_key, consumer_secret)\n         \n            self.auth.set_access_token(access_token, access_token_secret)\n        \n            self.api = tweepy.API(self.auth)\n        except:\n            print(\"Error: Authentication Failed\")\n \n    def clean_tweet(self, tweet):\n        '''\n        Utility function to clean tweet text by removing links, special characters\n        using simple regex statements.\n        '''\n        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n \n    def get_tweet_sentiment(self, tweet):\n        '''\n        Utility function to classify sentiment of passed tweet\n        using textblob's sentiment method\n        '''\n        \n        <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">analysis = TextBlob(self.clean_tweet(tweet))\n       \n        if analysis.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> &gt; 0:\n            return 'posi<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e'\n        elif analysis.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span> == 0:\n            return 'neutral'\n        else:\n            return 'nega<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>e'</div>\n    #https://github.com/vinitshahdeo/jobtweets/blob/master/jobtweets</code></pre></div></body></html>", "sec_30": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_30\"><pre id=\"sec_30_code\"><code class=\"python\">import sys,tweepy,csv,re\nfrom textblob import TextBlob\nimport matplotlibplot as thr\n\n\nclass SentimentAnalysis:\n\n    def __init__(self):\n        self.tweets = []\n        self.tweetText = []\n\n    def DownloadData(self):\n        # authenticating\n        consumerKey = 'your key here'\n        consumerSecret = 'your key here'\n        accessToken = 'your key here'\n        accessTokenSecret = 'your key here'\n        auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n        auth.set_access_token(accessToken, accessTokenSecret)\n        api = tweepy.API(auth)\n\n        # input for term to be searched and how many tweets to search\n        searchTerm = input(\"Enter Keyword/Tag to search about: \")\n        NoOfTerms = int(input(\"Enter how many tweets to search: \"))\n\n        # searching for tweets\n        self.tweets = tweepy.Cursor(api.search, q=searchTerm, lang = \"en\").items(NoOfTerms)\n\n        # Open/create a file to append data to\n        csvFile = open('result.csv', 'a')\n\n        # Use csv writer\n        csvWriter = csv.writer(csvFile)\n\n\n        # creating some variables to store info\n        polarity = 0\n        positive = 0\n        wpositive = 0\n        spositive = 0\n        negative = 0\n        wnegative = 0\n        snegative = 0\n        neutral = 0\n\n\n        # iterating through tweets fetched\n        for tweet in self.tweets:\n            #Append to temp so that we can store in csv later. I use encode UTF-8\n            self.tweetText.append(self.cleanTweet(tweet.text).encode('utf-8'))\n            # print (tweet.text.translate(non_bmp_map))    #print tweet's text\n            analysis = TextBlob(tweet.text)\n            # print(analysis.sentiment)  # print tweet's polarity\n            <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_keys udls\">polarity</span> += analysis.<span class=\"fea_sentiment_analysis_keys udls\">sentiment</span>.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span></div>  # adding up polarities to find the average later\n\n            if (analysis.sentiment.polarity == 0):  # adding reaction of how people are reacting to find average later\n                neutral += 1\n            elif (analysis.sentiment.polarity &gt; 0 and analysis.sentiment.polarity &lt;= 0.3):\n                wpositive += 1\n            elif (analysis.sentiment.polarity &gt; 0.3 and analysis.sentiment.polarity &lt;= 0.6):\n                positive += 1\n            elif (analysis.sentiment.polarity &gt; 0.6 and analysis.sentiment.polarity &lt;= 1):\n                spositive += 1\n            elif (analysis.sentiment.polarity &gt; -0.3 and analysis.sentiment.polarity &lt;= 0):\n                wnegative += 1\n            elif (analysis.sentiment.polarity &gt; -0.6 and analysis.sentiment.polarity &lt;= -0.3):\n                negative += 1\n            elif (analysis.sentiment.polarity &gt; -1 and analysis.sentiment.polarity &lt;= -0.6):\n                snegative += 1\n\n\n        # Write to csv and close csv file\n        csvWriter.writerow(self.tweetText)\n        csvFile.close()\n\n        # finding average of how people are reacting\n        positive = self.percentage(positive, NoOfTerms)\n        wpositive = self.percentage(wpositive, NoOfTerms)\n        spositive = self.percentage(spositive, NoOfTerms)\n        negative = self.percentage(negative, NoOfTerms)\n        wnegative = self.percentage(wnegative, NoOfTerms)\n        snegative = self.percentage(snegative, NoOfTerms)\n        neutral = self.percentage(neutral, NoOfTerms)\n\n        # finding average reaction\n        polarity = polarity / NoOfTerms\n\n        # printing out data\n        print(\"How people are reacting on \" + searchTerm + \" by analyzing \" + str(NoOfTerms) + \" tweets.\")\n        print()\n        print(\"General Report: \")\n\n        if (polarity == 0):\n            print(\"Neutral\")\n        elif (polarity &gt; 0 and polarity &lt;= 0.3):\n            print(\"Weakly Positive\")\n        elif (polarity &gt; 0.3 and polarity &lt;= 0.6):\n            print(\"Positive\")\n        elif (polarity &gt; 0.6 and polarity &lt;= 1):\n            print(\"Strongly Positive\")\n        elif (polarity &gt; -0.3 and polarity &lt;= 0):\n            print(\"Weakly Negative\")\n        elif (polarity &gt; -0.6 and polarity &lt;= -0.3):\n            print(\"Negative\")\n        elif (polarity &gt; -1 and polarity &lt;= -0.6):\n            print(\"Strongly Negative\")\n\n        print()\n        print(\"Detailed Report: \")\n        print(str(positive) + \"% people thought it was positive\")\n        print(str(wpositive) + \"% people thought it was weakly positive\")\n        print(str(spositive) + \"% people thought it was strongly positive\")\n        print(str(negative) + \"% people thought it was negative\")\n        print(str(wnegative) + \"% people thought it was weakly negative\")\n        print(str(snegative) + \"% people thought it was strongly negative\")\n        print(str(neutral) + \"% people thought it was neutral\")\n\n        self.plotPieChart(positive, wpositive, spositive, negative, wnegative, snegative, neutral, searchTerm, NoOfTerms)\n\n\n    def cleanTweet(self, tweet):\n        # Remove Links, Special Characters etc from tweet\n        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) | (\\w +:\\ / \\ / \\S +)\", \" \", tweet).split())\n\n    # function to calculate percentage\n    def percentage(self, part, whole):\n        temp = 100 * float(part) / float(whole)\n        return format(temp, '.2f')\n\n    def plotPieChart(self, positive, wpositive, spositive, negative, wnegative, snegative, neutral, searchTerm, noOfSearchTerms):\n        labels = ['Positive [' + str(positive) + '%]', 'Weakly Positive [' + str(wpositive) + '%]','Strongly Positive [' + str(spositive) + '%]', 'Neutral [' + str(neutral) + '%]',\n                  'Negative [' + str(negative) + '%]', 'Weakly Negative [' + str(wnegative) + '%]', 'Strongly Negative [' + str(snegative) + '%]']\n        sizes = [positive, wpositive, spositive, neutral, negative, wnegative, snegative]\n        colors = ['yellowgreen','lightgreen','darkgreen', 'gold', 'red','lightsalmon','darkred']\n        patches, texts = thr.pie(sizes, colors=colors, startangle=90)\n        thr.legend(patches, labels, loc=\"best\")\n        thr.title('How people are reacting on ' + searchTerm + ' by analyzing ' + str(noOfSearchTerms) + ' Tweets.')\n        thr.axis('equal')\n        thr.tight_layout()\n        thr.show()\n\n\n\nif __name__== \"__main__\":\n    sa = SentimentAnalysis()\n    sa.DownloadData()\n    #https://github.com/the-javapocalypse/Twitter-Sentiment-Analysis/blob/master/main</code></pre></div></body></html>", "sec_14": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_14\"><pre id=\"sec_14_code\"><code class=\"python\">from textblob import TextBlob                   ##language translation API\nfrom tkinter.scrolledtext import ScrolledText   ##for scrollable text box\n\n#-----translation function----------------------------------------------------------------------\ndef toLang(lang):\n    try:\n        output.delete(\"1.0\",END)            ##to delete previous entry in the output box\n        inputSTR = input_str.get(\"1.0\",END)\n        obj = TextBlob(str(inputSTR))\n        <div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">outputSTR = obj.translate(to=lang)</div>\n        output.insert(END,str(outputSTR))   ##insert output to the output box\n    except:\n        output.insert(END, \"**Please enter a meaningful word/sentence**\")\n        #https://github.com/DeepakJha01/GUI-Translator/blob/master/main/gui_translator</code></pre></div></body></html>", "sec_16": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_16\"><pre id=\"sec_16_code\"><code class=\"python\">import webbrowser\nfrom textblob import TextBlob, exceptions\nfrom wox import Wox, WoxAPI\n\nLANGUAGE = 'ru'\n\ndef translate(query):\n    query_modified = query.strip().lower()\n    en = set(chr(i) for i in range(ord('a'), ord('z') + 1))\n    results = []\n    if query_modified:\n        try:\n            from_lang, to_lang = ('en', LANGUAGE) if query_modified[0] in en else (LANGUAGE, 'en')\n            translation = TextBlob(query_modified).translate(from_lang, to_lang)\n            results.append({\n                \"Title\": str(translation),\n                \"SubTitle\": query,\n                \"IcoPath\":\"Images/app.png\",\n                \"JsonRPCAction\":{'method': 'openUrl',\n                                 'parameters': [r'http://translate.google.com/#{}/{}/{}'.format(from_lang, to_lang, query)],\n                                 'dontHideAfterAction': False}\n            })\n        except exceptions.NotTranslated:\n            pass\n    if not results:\n        results.append({\n                \"Title\": 'Not found',\n                \"SubTitle\": '',\n                \"IcoPath\":\"Images/app.png\"\n            })\n    return results\n\nclass Translate(Wox):\n    def query(self, query):\n        return <div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">translate(query)</div>\n\n    def openUrl(self, url):\n        webbrowser.open(url)\n\nif __name__ == \"__main__\":\n    Translate()\n    #https://github.com/RomanKornev/Translate/blob/master/main</code></pre></div></body></html>", "sec_19": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_19\"><pre id=\"sec_19_code\"><code class=\"python\">import googletrans\nfrom googletrans import Translator\n\n#from pygoogletranslation import Translator\n\nfrom textblob import TextBlob\n\nimport time\nepis = {}\n\ntranslator = Translator(service_urls = ['translate.google.com', 'translate.google.co.kr'])\n#translator = Translator()\n\n#bad_result_message = '**!!! BAD RESULT OF RECOGNITION. U CAN TRY AGAIN**'\n\n\nlang_dic = {value.title(): key for key, value in googletrans.LANGUAGES.items()}\nlang_dic_reversed = {key: f'*{value.capitalize()}*' for key, value in googletrans.LANGUAGES.items()}\n\nall_langs = list(lang_dic.keys())\n\n\n\ndef from_code_to_name(language):\n    return  lang_dic_reversed[language]\n\ndef smart_to_tidy(langs):\n    return [lang_dic_reversed[l] for l in langs]\n\ndef get_code_from_lang(lang):\n    return lang_dic[lang.tolower()]\n\n\ndef log_text(text, lang_list = ['en','ru']):\n    \n    result = []\n    \n    if len(text) &lt; 3:\n        result.append(f'*too shirt text*: {text}')\n        return result\n    \n\n    lang_of_text = translator.detect(text).lang\n    #if len(lang_of_text) == 0: lang_of_text = 'en'\n    #print(lang_of_text)\n\n    bool_list = [r != lang_of_text for r in lang_list]\n    \n    if all(bool_list):\n        bool_list.append(False)\n        lang_list.append(lang_of_text)\n    \n    for lang, it in zip(lang_list, bool_list):\n        result.append(f'{lang_dic_reversed[lang].upper()}:')\n        if it:\n            time.sleep(0.7)\n            #print(f\"{text}, {lang}, {lang_of_text}\")\n            txt = translator.translate(text, dest = lang, src = lang_of_text).text\n            result.append(txt)\n        else:\n            txt = text\n            result.append(f'_(original text)_ {text}')\n        result.append('')\n    \n    return result\n\n\ndef log_text_better(text, lang_list = ['en','ru']):\n    \n    result = []\n    \n    if len(text) &lt; 3:\n        result.append(f'*too shirt text*: {text}')\n        return result\n    \n    blob = TextBlob(text)\n\n    <div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">lang_of_text = blob.<span class=\"fea_language_detection_keys udls\">detect</span>_<span class=\"fea_language_detection_keys udls\">language</span>()</div>\n\n    bool_list = [r != lang_of_text for r in lang_list]\n    \n    if all(bool_list):\n        bool_list.append(False)\n        lang_list.append(lang_of_text)\n    \n    for lang, it in zip(lang_list, bool_list):\n        result.append(f'{lang_dic_reversed[lang].upper()}:')\n        if it:\n            time.sleep(1.3)\n            <div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">txt = str(blob.translate(from_lang = lang_of_text, to=lang))</div>\n            result.append(txt)\n        else:\n            txt = text\n            result.append(f'_(original text)_ {text}')\n        result.append('')\n    \n    return result\n\n\ndef get_langs_from_numbers(numbers):\n    \n    l1 = [all_langs[k-1] for k in numbers]\n    \n    return l1, [lang_dic[k] for k in l1]\n\n\n\n\n\n\nif __name__ == '__main__':\n\n    #trans = Translator()\n    #print(trans.detect('\u041f\u0440\u0438\u0432\u0435\u0442'))\n    #print(trans.detect('Hello').lang)\n    #print(trans.translate('\u041f\u0440\u0438\u0432\u0435\u0442'))\n\n\n\n\n   \n    defs = ['en','ru']\n    \n    r = log_text('hello my friend',defs)\n    \n    print('\\n'.join(r))\n\n    r = log_text('Ich will',defs)\n    \n    print('\\n'.join(r))\n    \n    print(defs)\n    \n    r = log_text_better('hello my friend',defs)\n    \n    print('\\n'.join(r))\n\n    lang = Translator().detect('Hello boy')\n    print(lang)\n\n#https://github.com/PasaOpasen/TranslatorBot/blob/master/translator_tools</code></pre></div></body></html>", "sec_22": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_22\"><pre id=\"sec_22_code\"><code class=\"python\"># -*- coding: utf-8 -*-\n\"\"\"\nTranslator module that uses the Google Translate API.\n\nAdapted from Terry Yin's google-translate-python.\nLanguage detection added by Steven Loria.\n\"\"\"\n# I(Dhyey thumar) have done some modifications in this file.\n\n# import codecs\n# import re\nimport ctypes\nimport json\nimport sys\n\nfrom textblob.compat import PY2, request, urlencode\n# from textblob.exceptions import TranslatorError, NotTranslated\n\nsource_lang_code = str(sys.argv[1])\ninput_string = str(sys.argv[2])\ndest_lang_code = str(sys.argv[3])\n\n\nclass Translator(object):\n\n    \"\"\"A language translator and detector.\n\n    Usage:\n    ::\n        &gt;&gt;&gt; from textblob.translate import Translator\n        &gt;&gt;&gt; t = Translator()\n        &gt;&gt;&gt; t.translate('hello', from_lang='en', to_lang='fr')\n        u'bonjour'\n        &gt;&gt;&gt; t.detect(\"hola\")\n        u'es'\n    \"\"\"\n\n    url = \"http://translate.google.com/translate_a/t?client=webapp&amp;dt=bd&amp;dt=ex&amp;dt=ld&amp;dt=md&amp;dt=qca&amp;dt=rw&amp;dt=rm&amp;dt=ss&amp;dt=t&amp;dt=at&amp;ie=UTF-8&amp;oe=UTF-8&amp;otf=2&amp;ssel=0&amp;tsel=0&amp;kc=1\"\n\n    headers = {\n        'Accept': '*/*',\n        'Connection': 'keep-alive',\n        'User-Agent': (\n            'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36')\n    }\n\n    def translate(self, source, from_lang='auto', to_lang='en', host=None, type_=None):\n        \"\"\"Translate the source text from one language to another.\"\"\"\n        if PY2:\n            source = source.encode('utf-8')\n        data = {\"q\": source}\n        url = u'{url}&amp;sl={from_lang}&amp;tl={to_lang}&amp;hl={to_lang}&amp;tk={tk}'.format(\n            url=self.url,\n            from_lang=from_lang,\n            to_lang=to_lang,\n            tk=_calculate_tk(source),\n        )\n        response = self._request(url, host=host, type_=type_, data=data)\n        result = json.loads(response)\n        if isinstance(result, list):\n            try:\n                result = result[0]  # ignore detected language\n            except IndexError:\n                pass\n        # self._validate_translation(source, result)\n        return result\n\n    def detect(self, source, host=None, type_=None):\n        \"\"\"Detect the source text's language.\"\"\"\n        if PY2:\n            source = source.encode('utf-8')\n        # if len(source) &lt; 3:\n        #     return 1\n            # raise TranslatorError('Must provide a string with at least 3 characters.')\n        data = {\"q\": source}\n        url = u'{url}&amp;sl=auto&amp;tk={tk}'.format(\n            url=self.url, tk=_calculate_tk(source))\n        response = self._request(url, host=host, type_=type_, data=data)\n        result, language = json.loads(response)\n        return language\n\n    # def _validate_translation(self, source, result):\n    #     \"\"\"Validate API returned expected schema, and that the translated text\n    #     is different than the original string.\n    #     \"\"\"\n    #     if not result:\n    #         raise NotTranslated('Translation API returned and empty response.')\n    #     if PY2:\n    #         result = result.encode('utf-8')\n    #     if result.strip() == source.strip():\n    #         raise NotTranslated('Translation API returned the input string unchanged.')\n\n    def _request(self, url, host=None, type_=None, data=None):\n        encoded_data = urlencode(data).encode('utf-8')\n        req = request.Request(url=url, headers=self.headers, data=encoded_data)\n        if host or type_:\n            req.set_proxy(host=host, type=type_)\n        resp = request.urlopen(req)\n        content = resp.read()\n        return content.decode('utf-8')\n\n\n# def _unescape(text):\n#     \"\"\"Unescape unicode character codes within a string.\n#     \"\"\"\n#     pattern = r'\\\\{1,2}u[0-9a-fA-F]{4}'\n#     decode = lambda x: codecs.getdecoder('unicode_escape')(x.group())[0]\n#     return re.sub(pattern, decode, text)\n\n\ndef _calculate_tk(source):\n    \"\"\"Reverse engineered cross-site request protection.\"\"\"\n    # Source: https://github.com/soimort/translate-shell/issues/94#issuecomment-165433715\n    # Source: http://www.liuxiatool.com/t.php\n\n    tkk = [406398, 561666268 + 1526272306]\n    b = tkk[0]\n\n    if PY2:\n        d = map(ord, source)\n    else:\n        d = source.encode('utf-8')\n\n    def RL(a, b):\n        for c in range(0, len(b) - 2, 3):\n            d = b[c + 2]\n            d = ord(d) - 87 if d &gt;= 'a' else int(d)\n            xa = ctypes.c_uint32(a).value\n            d = xa &gt;&gt; d if b[c + 1] == '+' else xa &lt;&lt; d\n            a = a + d &amp; 4294967295 if b[c] == '+' else a ^ d\n        return ctypes.c_int32(a).value\n\n    a = b\n\n    for di in d:\n        a = RL(a + di, \"+-a^+6\")\n\n    a = RL(a, \"+-3^+b+-f\")\n    a ^= tkk[1]\n    a = a if a &gt;= 0 else ((a &amp; 2147483647) + 2147483648)\n    a %= pow(10, 6)\n\n    tk = '{0:d}.{1:d}'.format(a, a ^ b)\n    return tk\n\n\n<div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">translator_instance = Translator()</div>\nif source_lang_code == \"null\":\n    source_lang_code = translator_instance.detect(input_string)\n\ntranslated_string = translator_instance.translate(input_string, source_lang_code, dest_lang_code)\n\nif translated_string:\n    translated_string = str(translated_string)\n    result = []\n    for char in translated_string:\n        result.append(str(ord(char)))\n    seperator = ','\n    trans_string = seperator.join(result)\n    print(trans_string, end='\\n')\nelse:\n    print('empty response')\n    #https://github.com/dhyeythumar/Search-Engine/blob/master/Python_scripts/lang_trans1</code></pre></div></body></html>", "sec_23": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_23\"><pre id=\"sec_23_code\"><code class=\"python\">''' This is language translation script '''\nfrom textblob import TextBlob\nimport sys\n\n\nsource_lang_code = str(sys.argv[1])\ninput_string = str(sys.argv[2])\ndest_lang_code = str(sys.argv[3])\n\ninput_blob = TextBlob(input_string)\n\nif source_lang_code == \"null\":\n    try:\n        <div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">source_lang_code = input_blob.<span class=\"fea_language_detection_keys udls\">detect</span>_<span class=\"fea_language_detection_keys udls\">language</span>()</div>\n        # print(\"Detected language:  \", source_lang_code)\n    except Exception as e:  # What if the input_string language is not detected\n        print(\"Error_1\", e)\n\ntry:\n    translated_string = <div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">input_blob.translate(\n        from_lang=source_lang_code, to=dest_lang_code)</div>\n    # print(translated_string) give character in unicode format.\n    # translated_string =&gt; is a &lt;class 'textblob.blob.TextBlob'&gt; type of object\n    # str(translated_string) =&gt; is a &lt;class 'str'&gt; type of object\n    # str(translated_string).encode('utf8) =&gt; is a &lt;class 'bytes'&gt; type of object\n\n    translated_string = str(translated_string)\n    result = []\n    for char in translated_string:\n        result.append(str(ord(char)))\n    seperator = ', '\n    trans_string = seperator.join(result)\n    print(trans_string, end='\\n')\n\nexcept Exception as e:  # What if the dest_lang code is null\n    print(\"Error_2\", e)\n    #https://github.com/dhyeythumar/Search-Engine/blob/master/Python_scripts/lang_trans</code></pre></div></body></html>", "sec_11": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_11\"><pre id=\"sec_11_code\"><code class=\"python\">from textblob import TextBlob\nfrom time import sleep\nimport csv\nimport pandas as pd\n\ndf = pd.read_csv(\"cleaned_data01.csv\")\ntexts = df['cleaned_text']\n\ncounter = 188277\nremains = texts.shape[0] - 188277\nreq_counter = 0\n\n#========================= textblob ==========================\n\nwith open('tweet_lang02.csv', 'a', encoding=\"utf-8-sig\") as csvFile:\n    csvWriter = csv.writer(csvFile)\n    csvWriter.writerow(['text','language'])\n    for i in range(188277, texts.shape[0]):\n        counter +=1\n        remains -=1\n        req_counter +=1\n        print(counter, ' ', remains)\n        t = texts[i]\n        s = t.replace(\"#\",\"\")\n        s = s.replace(\"_\", \" \")\n        if req_counter == 10:\n            sleep(3)\n            req_counter = 0\n\n        b = TextBlob(s)\n        <div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">l = b.<span class=\"fea_language_detection_keys udls\">detect</span>_<span class=\"fea_language_detection_keys udls\">language</span>()</div>\n        csvWriter.writerow([t,l])\n        #https://github.com/khaledabbud/SA_of_Tweets_After_QS_Assassination_AR_FA/blob/master/textblob_lang_classification</code></pre></div></body></html>", "sec_18": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_18\"><pre id=\"sec_18_code\"><code class=\"python\">from textblob import TextBlob\ndef log_text(text, lang_of_text=None, lang_list = ['en','ru'], trans_list = [True, True]):\n    \n    if len(text) &lt; 3:\n        print_on_yellow('too small text:',end=' ')\n        print(text)\n        return\n    \n    blob = TextBlob(text)\n    if lang_of_text == None:\n        <div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">lang_of_text = blob.<span class=\"fea_language_detection_keys udls\">detect</span>_<span class=\"fea_language_detection_keys udls\">language</span>()</div>\n\n    bool_list = [r != lang_of_text for r in lang_list]\n    \n    for lang, it, tc in zip(lang_list, bool_list, trans_list):\n        print(colored(f'\\t {lang}:', color = 'cyan', attrs=['bold']), end=' ')\n        if it:\n           <div class=\"highlights fea_language_detection\" id=\"language_detection_1\" style=\"display: inline;\"> txt = str(blob.translate(from_lang = lang_of_text, to = lang))</div>\n            print(txt)\n        else:\n            txt = text\n            print(f'{text} (original text)')\n        \n        if tc:\n            pron = epis[lang].transliterate(txt)\n            print('\\t\\t\\t',end=' ')\n            print_on_magenta(f'[{pron}]')\n#https://github.com/PasaOpasen/SpeechLogger/blob/master/ThirdTry/text_logger5</code></pre></div></body></html>", "sec_4": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_4\"><pre id=\"sec_4_code\"><code class=\"python\">\nfrom textblob.classifiers import NaiveBayesClassifier\n\ntrain = [\n    ('amor', \"spanish\"),\n    (\"perro\", \"spanish\"),\n    (\"playa\", \"spanish\"),\n    (\"sal\", \"spanish\"),\n    (\"oceano\", \"spanish\"),\n    (\"love\", \"english\"),\n    (\"dog\", \"english\"),\n    (\"beach\", \"english\"),\n    (\"salt\", \"english\"),\n    (\"ocean\", \"english\")\n]\ntest = [\n    (\"ropa\", \"spanish\"),\n    (\"comprar\", \"spanish\"),\n    (\"camisa\", \"spanish\"),\n    (\"agua\", \"spanish\"),\n    (\"telefono\", \"spanish\"),\n    (\"clothes\", \"english\"),\n    (\"buy\", \"english\"),\n    (\"shirt\", \"english\"),\n    (\"water\", \"english\"),\n    (\"telephone\", \"english\")\n]\n\ndef extractor(word):\n    '''Extract the last letter of a word as the only feature.'''\n    feats = {}\n    last_letter = word[-1]\n    feats[\"last_letter({0})\".format(last_letter)] = True\n    return feats\n\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">lang_detector = <span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>(<span class=\"fea_classification_keys udls\">train</span>, feature_extractor=extractor)</div>\nprint(lang_detector.accuracy(test))\nprint(lang_detector.show_informative_features(5))\n#https://gist.github.com/sloria/6342158</code></pre></div></body></html>", "sec_5": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_5\"><pre id=\"sec_5_code\"><code class=\"python\">from textblob.classifiers import NaiveBayesClassifier\nfrom textblob import TextBlob\ntrain = [\n    ('I love this sandwich.', 'pos'),\n    ('This is an amazing place!', 'pos'),\n    ('I feel very good about these beers.', 'pos'),\n    ('This is my best work.', 'pos'),\n    (\"What an awesome view\", 'pos'),\n    ('I do not like this restaurant', 'neg'),\n    ('I am tired of this stuff.', 'neg'),\n    (\"I can't deal with this\", 'neg'),\n    ('He is my sworn enemy!', 'neg'),\n    ('My boss is horrible.', 'neg')\n]\ntest = [\n    ('The beer was good.', 'pos'),\n    ('I do not enjoy my job', 'neg'),\n    (\"I ain't feeling dandy today.\", 'neg'),\n    (\"I feel amazing!\", 'pos'),\n    ('Gary is a friend of mine.', 'pos'),\n    (\"I can't believe I'm doing this.\", 'neg')\n]\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">cl = <span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span><span class=\"fea_classification_keys udls\">Classifier</span>(<span class=\"fea_classification_keys udls\">train</span>)</div>\n# Classify some text\nprint(cl.classify(\"Their burgers are amazing.\"))  # \"pos\"\nprint(cl.classify(\"I don't like their pizza.\"))   # \"neg\"\n# Classify a TextBlob\nblob = TextBlob(\"The beer was amazing. But the hangover was horrible. \"\n\"My boss was not pleased.\", classifier=cl)\nprint(blob)\nprint(blob.classify())\nfor sentence in blob.sentences:\nprint(sentence)\nprint(sentence.classify())\n# Compute accuracy\nprint(\"Accuracy: {0}\".format(cl.accuracy(test)))\n# Show 5 most informative features\ncl.show_informative_features(5)\n#https://gist.github.com/sloria/6338202#file-tweet_classify-py</code></pre></div></body></html>", "sec_13": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_13\"><pre id=\"sec_13_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob.sentiments import NaiveBayesAnalyzer\nimport matplotlibplot as thr\nimport random\n\ndef twitter_analysis(string):\n\t## Aain function starts\n\t## =====\n\n\tprocessedTweet = []\n\tpos = 0\n\tneg = 0\n\tneutral = 0\n\n\t#start process_tweet\n\tdef processTweet(tweet):\n\t    # process the tweets\n\t    \n\t    #Convert to lower case\n\t    tweet = tweet.lower()\n\t    #Convert www.* or https?://* to URL\n\t    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n\t    #Convert @username to AT_USER\n\t    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n\t    #Remove additional white spaces\n\t    tweet = re.sub('[\\s]+', ' ', tweet)\n\t    #Replace #word with word\n\t    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n\t    #trim\n\t    tweet = tweet.strip('\\'\"')\n\t    return tweet\n\t#end\n\n\ttd = TwitterData()\n\trawtweet = td.getData(string)\n\n\t#print \"1. Tweets colleted and pre-processing steps started\"\n\n\t#pre-processing tweets    \n\tfor i in range(1,len(rawtweet)):\n\t    processedTweet.append(processTweet(rawtweet[i]))\n\n\t#print \"2. preprocessing over and classifer begins\"\n\n\t# classifying the processed tweets by NaiveBayesAnalyzer\n\n\tfor i in range(1,len(processedTweet)):\n\t   <div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"> <span class=\"fea_classification_keys udls\">classifier</span> = TextBlob(processedTweet[i], analyzer=<span class=\"fea_classification_keys udls\">Naive</span><span class=\"fea_classification_keys udls\">Bayes</span>Analyzer())\n\t    classification = <span class=\"fea_classification_keys udls\">classifier</span>.sentiment.classification</div>\n\t    #print processedTweet[i],\"Polarity=\",classification\n\t    \n\t    if classification == \"pos\":\n\t        pos = pos + 1\n\t        #print pos;\n\t    elif classification == \"neg\":     \n\t        neg = neg + 1\n\t        #print neg\n\t    else:\n\t        neutral = neutral + 1 \n\n\tfinal = []\n\tfinal.append(neg);\n\tfinal.append(neutral);\n\tfinal.append(pos);\n\n\treturn final  \n\t#https://github.com/muthuvenki/Trend-Analysis/blob/master/sentimental_anlysis/views</code></pre></div></body></html>", "thr_17": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_17\"><pre id=\"thr_17_code\"><code class=\"python\">import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\n\ndf_amazon = pd.read_csv (\"datasets/amazon_alexa.tsv\", sep=\"\\t\")\n\nimport string\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\n# Create our list of punctuation marks\npunctuations = string.punctuation\n\n# Create our list of stopwords\nnlp = spacy.load('en')\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span>_<span class=\"fea_nlp_datasets_keys udls\">words</span> = spacy.lang.en.<span class=\"fea_nlp_datasets_keys udls\">stop</span>_<span class=\"fea_nlp_datasets_keys udls\">words</span>.<span class=\"fea_nlp_datasets_keys udls\">STOP</span>_<span class=\"fea_nlp_datasets_keys udls\">WORDS</span></div>\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nparser = English()\n\n# Creating our tokenizer function\ndef spacy_tokenizer(sentence):\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    <div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">mytokens = <span class=\"fea_parsing_keys udls\">parser</span>(sentence)</div>\n\n    # Lemmatizing each token and converting each token into lowercase\n    mytokens = [ <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">word.<span class=\"fea_lemmatization_keys udls\">lemma</span>_.lower()</div>.strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n\n    # Removing stop words\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n\n    # return preprocessed list of tokens\n    return mytokens\n\n# Custom transformer using spaCy\nclass predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        # Cleaning Text\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\n# Basic function to clean the text\ndef clean_text(text):\n    # Removing spaces and converting text into lowercase\n    return text.strip().lower()\n\n<div class=\"highlights fea_word_vectors\" id=\"word_vectors_0\" style=\"display: inline;\">bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))</div>\n\ntfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)\n\nfrom sklearn.model_selection import train_test_split\n\nX = df_amazon['verified_reviews'] # the features we want to analyze\nylabels = df_amazon['feedback'] # the labels, or answers, we want to test against\n\nX_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)\n\n# Logistic Regression Classifier\nfrom sklearn.linear_model import LogisticRegression\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_keys udls\">classifier</span> = LogisticRegression()</div>\n\n# Create pipeline using Bag of Words\npipe = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', bow_vector),\n                 ('classifier', classifier)])\n\n# model generation\npipe.fit(X_train,y_train)\n\nfrom sklearn import metrics\n# Predicting with a test dataset\npredicted = pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\nprint(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\nprint(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))</code></pre></div></body></html>", "thr_10": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"thr_10\"><pre id=\"thr_10_code\"><code class=\"python\"># Check if word vector is available\nimport spacy\n\n# Loading a spacy model\nnlp = spacy.load(\"en_core_web_md\")\ntokens = nlp(\"I am an excellent cook\")\n\nfor token in tokens:\n  print(token.text ,' ',token.has_vector)\n  print(token.text,' ',<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>.vector_norm</div>)\n\nreview_1=nlp(' The food was amazing')\nreview_2=nlp('The food was excellent')\nreview_3=nlp('I did not like the food')\nreview_4=nlp('It was very bad experience')\n\n<div class=\"highlights fea_text_similarity\" id=\"text_similarity_0\" style=\"display: inline;\">score_1=review_1.similarity(review_2)</div>\nprint('Similarity between review 1 and 2',score_1)\n\n<div class=\"highlights fea_text_similarity\" id=\"text_similarity_1\" style=\"display: inline;\">score_2=review_3.similarity(review_4)</div>\nprint('Similarity between review 3 and 4',score_2)\n\n#https://www.machinelearningplus.com/spacy-tutorial-nlp/#mergingandsplittingtokenswithretokenize</code></pre></div></body></html>", "thr_12": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_12\"><pre id=\"thr_12_code\"><code class=\"python\">\"\"\"\nThis script extracts features from the transcript txt file and saves them to .csv files\nso they can be used in any toolkkit.\n\"\"\"\n\nimport csv\nimport spacy\n\n\ndef main():\n    \"\"\"Loads the model and processes it.\n    \n    The model used can be installed by running this command on your CMD/Terminal:\n\n    python -m spacy download es_core_news_md\n    \n    \"\"\"\n\n    corpus = open(\"transcript_clean.txt\", \"r\", encoding=\"utf-8\").read()\n    nlp = spacy.load(\"es_core_news_md\")\n\n    # Our corpus is bigger than the default limit, we will set\n    # a new limit equal to its length.\n    nlp.max_length = len(corpus)\n\n    doc = nlp(corpus)\n\n    get_tokens(doc)\n    get_entities(doc)\n    get_sentences(doc)\n\n\ndef <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">get_<span class=\"fea_tokenization_keys udls\">token</span>s(doc)</div>:\n    \"\"\"Get the tokens and save them to .csv\n\n    Parameters\n    ----------\n    doc : spacy.doc\n        A doc object.\n\n    \"\"\"\n\n    data_list = [[\"text\", \"text_lower\", \"lemma\", \"lemma_lower\",\n                  \"part_of_speech\", \"is_alphabet\", \"is_stopword\"]]\n\n    for token in doc:\n        data_list.append([\n            <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>.text, <span class=\"fea_tokenization_keys udls\">token</span>.lower_</div>, <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">token.<span class=\"fea_lemmatization_keys udls\">lemma</span>_, token.<span class=\"fea_lemmatization_keys udls\">lemma</span>_.lower()</div>,\n            <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">token</span>.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_</div>, token.is_alpha, token.is_stop\n        ])\n\n    with open(\"./tokens.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as tokens_file:\n        csv.writer(tokens_file).writerows(data_list)\n\n\ndef get_entities(doc):\n    \"\"\"Get the entities and save them to .csv\n\n    Parameters\n    ----------\n    doc : spacy.doc\n        A doc object.\n\n    \"\"\"\n\n    data_list = [[\"text\", \"text_lower\", \"label\"]]\n\n    for ent in doc.ents:\n        data_list.append([ent.text, ent.lower_, ent.label_])\n\n    with open(\"./entities.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as entities_file:\n        csv.writer(entities_file).writerows(data_list)\n\n\ndef get_sentences(doc):\n    \"\"\"Get the sentences, score and save them to .csv\n\n    You will require to download the dataset (zip) from the following url:\n\n    https://www.kaggle.com/rtatman/sentiment-lexicons-for-81-languages\n\n    Once downloaded you will require to extract 2 .txt files:\n\n    negative_words_es.txt\n    positive_words_es.txt\n\n    Parameters\n    ----------\n    doc : spacy.doc\n        A doc object.\n\n    \"\"\"\n\n    # Load positive and negative words into lists.\n    with open(\"positive_words_es.txt\", \"r\", encoding=\"utf-8\") as temp_file:\n        positive_words = temp_file.read().splitlines()\n\n    with open(\"negative_words_es.txt\", \"r\", encoding=\"utf-8\") as temp_file:\n        negative_words = temp_file.read().splitlines()\n\n    data_list = [[\"text\", \"score\"]]\n\n    for sent in doc.sents:\n\n        # Only take into account real sentences.\n        if len(sent.text) &gt; 10:\n\n            score = 0\n\n            # Start scoring the sentence.\n            for word in sent:\n\n                if word.lower_ in positive_words:\n                    score += 1\n\n                if word.lower_ in negative_words:\n                    score -= 1\n\n            data_list.append([sent.text, score])\n\n\n    with open(\"./sentences.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as sentences_file:\n        csv.writer(sentences_file).writerows(data_list)\n\n\nif __name__ == \"__main__\":\n\n    main()\n    https://github.com/PhantomInsights/mexican-government-report/blob/master/scripts/step2</code></pre></div></body></html>", "thr_18": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_18\"><pre id=\"thr_18_code\"><code class=\"python\">survey_text = ('Out of 5 people surveyed, James Robert,'\n               ' Julie Fuller and Benjamin Brooks like'\n               ' apples. Kelly Cox and Matthew Evans'\n               ' like oranges.')\n\ndef replace_person_names(token):\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">if <span class=\"fea_tokenization_keys udls\">token</span>.ent_iob != 0 and <span class=\"fea_tokenization_keys udls\">token</span>.ent_type_ == 'PERSON':</div>\n        return '[REDACTED] '\n    return token.string\n\ndef <div class=\"highlights fea_named_entity_recognition\" id=\"named_entity_recognition_0\" style=\"display: inline;\">redact_names(nlp_doc):</div>\n    for ent in nlp_doc.ents:\n        ent.merge()\n    tokens = map(replace_person_names, nlp_doc)\n    return ''.join(tokens)\n\nsurvey_doc = nlp(survey_text)\nredact_names(survey_doc)</code></pre></div></body></html>", "thr_20": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_20\"><pre id=\"thr_20_code\"><code class=\"python\">one_about_text = ('Gus Proto is a Python developer'\n    ' currently working for a London-based Fintech company')\none_about_doc = nlp(one_about_text)\n# Extract children of `developer`\nprint([token.text for token in one_about_doc[5].children])\n\n# Extract previous neighboring node of `developer`\nprint (one_about_doc[5].nbor(-1))\n\n# Extract next neighboring node of `developer`\nprint (one_about_doc[5].nbor())\n\n# Extract all tokens on the left of `developer`\nprint([token.text for token in one_about_doc[5].lefts])\n\n# Extract tokens on the right of `developer`\nprint([<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">token</span>.text for <span class=\"fea_tokenization_keys udls\">token</span> in one_about_doc[5].rights</div>])\n\n# Print subtree of `developer`\nprint (list(one_about_doc[5].subtree))\n\ndef flatten_tree(tree):\n    return ''.join([token.text_with_ws for token in list(tree)]).strip()\n\n# Print flattened subtree of `developer`\nprint (flatten_tree(one_about_doc[5].subtree))</code></pre></div></body></html>", "thr_27": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_27\"><pre id=\"thr_27_code\"><code class=\"python\">import re\nimport spacy\nfrom spacy.tokenizer import Tokenizer\ncustom_nlp = spacy.load('en_core_web_sm')\n<div class=\"highlights fea_text_simplify\" id=\"text_simplify_0\" style=\"display: inline;\"><span class=\"fea_text_simplify_keys udls\">pre</span>fix_re = spacy.util.compile_<span class=\"fea_text_simplify_keys udls\">pre</span>fix_regex(custom_nlp.Defa<span class=\"fea_text_simplify_keys udls\">ult</span>s.<span class=\"fea_text_simplify_keys udls\">pre</span>fixes)\nsuffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defa<span class=\"fea_text_simplify_keys udls\">ult</span>s.suffixes)</div>\n<div class=\"highlights fea_regular_expression\" id=\"regular_expression_0\" style=\"display: inline;\">infix_re = re.<span class=\"fea_regular_expression_keys udls\">compile</span>(r'''[-~]''')</div>\ndef customize_tokenizer(nlp):\n    # Adds support to use `-` as the delimiter for tokenization\n    return <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_keys udls\">Token</span>izer(nlp.vocab, prefix_search=prefix_re.search,\n                     suffix_search=suffix_re.search,\n                     infix_finditer=infix_re.finditer,\n                     <span class=\"fea_tokenization_keys udls\">token</span>_match=None\n                     )</div>\n\n\ncustom_nlp.tokenizer = customize_tokenizer(custom_nlp)\ncustom_tokenizer_about_doc = custom_nlp(about_text)\nprint([token.text for token in custom_tokenizer_about_doc])\n</code></pre></div></body></html>", "thr_22": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_22\"><pre id=\"thr_22_code\"><code class=\"python\">def is_token_allowed(token):\n    '''\n        Only allow valid tokens which are not stop words\n        and punctuation symbols.\n    '''\n    if (not token or not token.string.strip() or\n        token.is_stop or token.is_punct):\n        return False\n    return True\n\ndef preprocess_token(token):\n    # Reduce token to its lowercase lemma form\n    return <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">token.<span class=\"fea_lemmatization_keys udls\">lemma</span>_</div>.strip().lower()\n\ncomplete_filtered_tokens = [preprocess_token(token)\n    for token in complete_doc if is_token_allowed(token)]\ncomplete_filtered_tokens</code></pre></div></body></html>", "thr_26": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"thr_26\"><pre id=\"thr_26_code\"><code class=\"python\">import spacy\nnlp = spacy.load('en_core_web_sm')\n\nconference_help_text = ('Gus is helping organize a developer'\n    'conference on Applications of Natural Language'\n    ' Processing. He keeps organizing local Python meetups'\n    ' and several internal talks at his workplace.')\nconference_help_doc = nlp(conference_help_text)\nfor token in conference_help_doc:\n    print (token, <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">token.<span class=\"fea_lemmatization_keys udls\">lemma</span>_</div>)</code></pre></div></body></html>", "thr_13": "<html><body><div class=\"codeBlock hljs makefile\" id=\"thr_13\"><pre id=\"thr_13_code\"><code class=\"python\"># Construction via add_pipe with default model\ntok2vec = nlp.add_pipe(\"tok2vec\")\n\n# Construction via add_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_tok2vec\"}}\nparser = nlp.add_pipe(\"tok2vec\", config=config)\n\n# Construction from class\n<div class=\"highlights fea_word_vectors\" id=\"word_vectors_0\" style=\"display: inline;\">from spacy.pipeline import Tok2Vec\ntok2vec = Tok2Vec(nlp.vocab, model)</div>\n#https://spacy.io/api/tok2vec</code></pre></div></body></html>", "thr_5": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"thr_5\"><pre id=\"thr_5_code\"><code class=\"python\">from spacy.training import JsonlCorpus\nimport spacy\n\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">corpus = JsonlCorpus(\"./texts.jsonl\")</div>\nnlp = spacy.blank(\"en\")\ndata = corpus(nlp)\n\n#https://spacy.io/api/corpus</code></pre></div></body></html>", "thr_6": "<html><body><div class=\"codeBlock hljs typescript\" id=\"thr_6\"><pre id=\"thr_6_code\"><code class=\"python\">import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom string import punctuation\nfrom collections import Counter\nfrom heapq import nlargest\n\ndoc =\"\"\"Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to progressively improve their performance on a specific task. Machine learning algorithms build a mathematical model of sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in the applications of email filtering, detection of network intruders, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning.In its application across business problems, machine learning is also referred to as predictive analytics.\"\"\"\n\nnlp = spacy.load('en')\ndoc = nlp(doc)\n\nkeyword = []\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\"><span class=\"fea_nlp_datasets_keys udls\">stop</span><span class=\"fea_nlp_datasets_keys udls\">words</span> = list(<span class=\"fea_nlp_datasets_keys udls\">STOP</span>_<span class=\"fea_nlp_datasets_keys udls\">WORDS</span>)</div>\npos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\nfor token in doc:\n    if(token.text in stopwords or token.text in punctuation):\n        continue\n    if(token.pos_ in pos_tag):\n        keyword.append(token.text)\n\nfreq_word = Counter(keyword)\n\nmax_freq = Counter(keyword).most_common(1)[0][1]\nfor word in freq_word.keys():  \n        freq_word[word] = (freq_word[word]/max_freq)\n\nsent_strength={}\nfor sent in doc.sents:\n    for word in sent:\n        if word.text in freq_word.keys():\n            if sent in sent_strength.keys():\n                sent_strength[sent]+=freq_word[word.text]\n            else:\n                sent_strength[sent]=freq_word[word.text]\nprint(sent_strength)\n\n<div class=\"highlights fea_summarizer\" id=\"summarizer_0\" style=\"display: inline;\"><span class=\"fea_summarizer_keys udls\">summar</span>ized_<span class=\"fea_summarizer_keys udls\">sentence</span>s = nlargest(3, sent_strength, key=sent_strength.get)</div>\n\nfinal_sentences = [ w.text for w in summarized_sentences ]\nsummary = ' '.join(final_sentences)</code></pre></div></body></html>", "thr_25": "<html><body><div class=\"codeBlock hljs vbnet\" id=\"thr_25\"><pre id=\"thr_25_code\"><code class=\"python\">from collections import Counter\ncomplete_text = ('Gus Proto is a Python developer currently'\n    'working for a London-based Fintech company. He is'\n    ' interested in learning Natural Language Processing.'\n    ' There is a developer conference happening on 21 July'\n    ' 2019 in London. It is titled \"Applications of Natural'\n    ' Language Processing\". There is a helpline number '\n    ' available at +1-1234567891. Gus is helping organize it.'\n    ' He keeps organizing local Python meetups and several'\n    ' internal talks at his workplace. Gus is also presenting'\n    ' a talk. The talk will introduce the reader about \"Use'\n    ' cases of Natural Language Processing in Fintech\".'\n    ' Apart from his work, he is very passionate about music.'\n    ' Gus is learning to play the Piano. He has enrolled '\n    ' himself in the weekend batch of Great Piano Academy.'\n    ' Great Piano Academy is situated in Mayfair or the City'\n    ' of London and has world-class piano instructors.')\n\ncomplete_doc = nlp(complete_text)\n# Remove stop words and punctuation symbols\nwords = [token.text for token in complete_doc\n         if not token.is_stop and not token.is_punct]\n<div class=\"highlights fea_word_frequency\" id=\"word_frequency_0\" style=\"display: inline;\"><span class=\"fea_word_frequency_keys udls\">word</span>_<span class=\"fea_word_frequency_keys udls\">freq</span> = Counter(<span class=\"fea_word_frequency_keys udls\">word</span>s)</div>\n# 5 commonly occurring words with their frequencies\ncommon_words = word_freq.most_common(5)\nprint (common_words)\n\n# Unique words\nunique_words = [word for (word, freq) in word_freq.items() if freq == 1]\nprint (unique_words)</code></pre></div></body></html>", "thr_24": "<html><body><div class=\"codeBlock hljs go\" id=\"thr_24\"><pre id=\"thr_24_code\"><code class=\"python\">nouns = []\nadjectives = []\nfor token in about_doc:\n    if <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_keys udls\">token</span>.<span class=\"fea_Part_of_Speech_keys udls\">pos</span>_</div> == 'NOUN':\n        nouns.append(token)\n    if <div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">token.pos_ == 'ADJ':</div>\n        adjectives.append(token)\n    print (token, <div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">token.tag_</div>, token.pos_, spacy.explain(token.tag_))</code></pre></div></body></html>", "thr_11": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_11\"><pre id=\"thr_11_code\"><code class=\"python\">from skweak.base import SpanAnnotator\nimport spacy\nimport itertools\nimport json\nfrom spacy.tokens import Doc, Span  # type: ignore\nfrom typing import Tuple, Iterable, List\n\n####################################################################\n# Labelling source based on neural models\n####################################################################\n\n\nclass ModelAnnotator(SpanAnnotator):\n    \"\"\"Annotation based on a spacy NER model\"\"\"\n\n    def __init__(self, name:str, model_path:str, \n                 disabled:List[str]=[\"parser\", \"tagger\", \"lemmatizer\", \"attribute_ruler\"]):\n        \"\"\"Creates a new annotator based on a Spacy model. \"\"\"\n        \n        super(ModelAnnotator, self).__init__(name)\n        self.model = spacy.load(model_path, disable=disabled)\n\n\n    def find_spans(self, doc: Doc) -&gt; Iterable[Tuple[int, int, str]]:\n        \"\"\"Annotates one single document using the Spacy NER model\"\"\"\n\n        # Create a new document (to avoid conflicting annotations)\n        doc2 = self.create_new_doc(doc)\n        # And run the model\n        for _, proc in self.model.pipeline:\n            doc2 = proc(doc2)\n        # Add the annotation\n        for ent in doc2.ents:\n            yield ent.start, ent.end, ent.label_\n\n    def pipe(self, docs: Iterable[Doc]) -&gt; Iterable[Doc]:\n        \"\"\"Annotates the stream of documents based on the Spacy model\"\"\"\n\n        stream1, stream2 = itertools.tee(docs, 2)\n\n        # Remove existing entities from the document\n        stream2 = (self.create_new_doc(d) for d in stream2)\n        \n        # And run the model\n        for _, proc in self.model.pipeline:\n            stream2 = proc.pipe(stream2)\n        \n        for doc, doc_copy in zip(stream1, stream2):\n\n            doc.spans[self.name] = []\n\n            # Add the annotation\n            for ent in doc_copy.ents:\n                doc.spans[self.name].append(Span(doc, ent.start, ent.end, ent.label_))\n\n            yield doc\n\n    def create_new_doc(self, doc: Doc) -&gt; Doc:\n        \"\"\"Create a new, empty Doc (but with the same tokenisation as before)\"\"\"\n\n        return spacy.tokens.Doc(self.model.vocab, [tok.text for tok in doc], #type: ignore\n                               [tok.whitespace_ for tok in doc])\n\n\nclass TruecaseAnnotator(ModelAnnotator):\n    \"\"\"Spacy model annotator that preprocess all texts to convert them to a \n    \"truecased\" representation (see below)\"\"\"\n\n    def __init__(self, name:str, model_path:str, form_frequencies:str,\n                 disabled:List[str]=[\"parser\", \"tagger\", \"lemmatizer\", \"attribute_ruler\"]):\n        \"\"\"Creates a new annotator based on a Spacy model, and a dictionary containing\n        the most common case forms for a given word (to be able to truecase the document).\"\"\"\n        \n        super(TruecaseAnnotator, self).__init__(name, model_path, disabled)\n        with open(form_frequencies) as fd:\n            self.form_frequencies = json.load(fd)\n\n    def create_new_doc(self, doc: Doc, min_prob: float = 0.25) -&gt; Doc:\n        \"\"\"Performs truecasing of the tokens in the spacy document. Based on relative \n        frequencies of word forms, tokens that \n        (1) are made of letters, with a first letter in uppercase\n        (2) and are not sentence start\n        (3) and have a relative frequency below min_prob\n        ... will be replaced by its most likely case (such as lowercase). \"\"\"\n\n  #      print(\"running on\", doc[:10])\n\n        if not self.form_frequencies:\n            raise RuntimeError(\n                \"Cannot truecase without a dictionary of form frequencies\")\n\n        tokens = []\n        spaces = []\n        doctext = doc.text\n        for tok in doc:\n            toktext = tok.text\n\n            # We only change casing for words in Title or UPPER\n            if tok.is_alpha and toktext[0].isupper():\n                cond1 = tok.is_upper and len(toktext) &gt; 2  # word in uppercase\n                cond2 = toktext[0].isupper(\n                ) and not tok.is_sent_start  # titled word\n                if cond1 or cond2:\n                    token_lc = toktext.lower()\n                    if token_lc in self.form_frequencies:\n                        frequencies = self.form_frequencies[token_lc]\n                        if frequencies.get(toktext, 0) &lt; min_prob:\n                            alternative = sorted(\n                                frequencies.keys(), key=lambda x: frequencies[x])[-1]\n\n                            # We do not change from Title to to UPPER\n                            if not tok.is_title or not alternative.isupper():\n                                toktext = alternative\n\n            tokens.append(toktext)\n\n            # Spacy needs to know whether the token is followed by a space\n            if tok.i &lt; len(doc)-1:\n                spaces.append(doctext[tok.idx+len(tok)].isspace())\n            else:\n                spaces.append(False)\n\n        # Creates a new document with the tokenised words and space information\n        doc2 = Doc(self.<div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">model.vocab</div>, words=tokens, spaces=spaces) #type: ignore\n #       print(\"finished with doc\", doc2[:10])\n        return doc2\n        #https://github.com/NorskRegnesentral/skweak/blob/main/skweak/spacy</code></pre></div></body></html>", "thr_19": "<html><body><div class=\"codeBlock hljs bash\" id=\"thr_19\"><pre id=\"thr_19_code\"><code class=\"python\">conference_text = ('There is a developer conference'\n    ' happening on 21 July 2019 in London.')\nconference_doc = nlp(conference_text)\n# Extract Noun Phrases\nfor chunk in <div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">conference_doc.<span class=\"fea_n_grams_keys udls\">noun</span>_chunks</div>:\n    print (chunk)</code></pre></div></body></html>", "thr_8": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"thr_8\"><pre id=\"thr_8_code\"><code class=\"python\">import spacy\nimport contextualSpellCheck\n\nnlp = spacy.load('en_core_web_sm')\n<div class=\"highlights fea_spellcheck\" id=\"spellcheck_0\" style=\"display: inline;\">con<span class=\"fea_spellcheck_keys udls\">text</span>ualSpellCheck.add_to_pipe(nlp)</div>\ndoc = nlp('Income was $9.4 milion compared to the prior year of $2.7 milion.')\n\nprint(doc._.performed_spellCheck) #Should be True\nprint(doc._.outcome_spellCheck) #Income was $9.4 million compared to the prior year of $2.7 million.\n#https://spacy.io/universe/project/contextualSpellCheck</code></pre></div></body></html>", "thr_7": "<html><body><div class=\"codeBlock hljs rust\" id=\"thr_7\"><pre id=\"thr_7_code\"><code class=\"python\">import spacy\nfrom spacytextblob.spacytextblob import SpacyTextBlob\n\nnlp = spacy.load('en_core_web_sm')\nnlp.add_pipe('spacytextblob')\ntext = 'I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.'\ndoc = nlp(text)\n<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">doc._.<span class=\"fea_sentiment_analysis_keys udls\">polarity</span>      # <span class=\"fea_sentiment_analysis_keys udls\">Polarity</span>: -0.125\ndoc._.subjec<span class=\"fea_sentiment_analysis_keys udls\">tiv</span>ity</div>  # Sujectivity: 0.9\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">doc._.assessm<span class=\"fea_text_scoring_keys udls\">ent</span>s</div>   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n#https://spacy.io/universe/project/spacy-textblob</code></pre></div></body></html>", "thr_9": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_9\"><pre id=\"thr_9_code\"><code class=\"python\">import spacy\nfrom spacy_langdetect import LanguageDetector\nnlp = spacy.load('en')\n<div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">nlp.add_pipe(<span class=\"fea_language_detection_keys udls\">Language</span><span class=\"fea_language_detection_keys udls\">Detect</span>or(), name='<span class=\"fea_language_detection_keys udls\">language</span>_<span class=\"fea_language_detection_keys udls\">detect</span>or', last=True)</div>\ntext = 'This is an english text.'\ndoc = nlp(text)\n# document level language detection. Think of it like average language of the document!\nprint(doc._.language)\n# sentence level language detection\nfor sent in doc.sents:\n   print(sent, sent._.language)\n   #https://spacy.io/universe/project/spacy-langdetect</code></pre></div></body></html>"}, "file concepts": {"fir_4": ["cat_Preprocessing", "fea_parsing", "fea_tokenization", "fea_lemmatization", "fea_tagger", "cat_Basic_Analysis", "fea_Part_of_Speech", "fea_named_entity_recognition", "fea_dependency_parsing"], "fir_16": ["cat_Preprocessing", "fea_parsing", "fea_tokenization", "cat_Basic_Analysis", "fea_Part_of_Speech"], "fir_17": ["cat_Preprocessing", "fea_parsing", "fea_tokenization", "cat_Basic_Analysis", "fea_Part_of_Speech"], "fir_23": ["cat_Preprocessing", "fea_parsing", "fea_tokenization", "cat_Basic_Analysis", "fea_Part_of_Speech"], "fir_1": ["cat_Preprocessing", "fea_tokenization", "fea_lemmatization", "fea_nlp_datasets"], "fir_2": ["cat_Preprocessing", "fea_tokenization", "fea_nlp_datasets", "fea_stemming"], "fir_3": ["cat_Preprocessing", "fea_tokenization", "fea_lemmatization", "fea_nlp_datasets", "cat_Advanced_Analysis", "fea_summarizer"], "fir_6": ["cat_Preprocessing", "fea_tokenization", "fea_lemmatization", "fea_tagger", "fea_n_grams", "cat_Basic_Analysis", "fea_Part_of_Speech"], "fir_8": ["cat_Preprocessing", "fea_tokenization", "fea_nlp_datasets", "fea_stemming", "cat_Basic_Analysis", "fea_text_scoring", "cat_Advanced_Analysis", "fea_summarizer"], "fir_9": ["cat_Preprocessing", "fea_tokenization", "fea_lemmatization", "fea_nlp_datasets", "cat_Advanced_Analysis", "fea_chatbot"], "fir_10": ["cat_Preprocessing", "fea_tokenization", "fea_lemmatization"], "fir_11": ["cat_Preprocessing", "fea_tokenization", "cat_Basic_Analysis", "fea_Part_of_Speech"], "fir_12": ["cat_Preprocessing", "fea_tokenization", "fea_lemmatization", "fea_nlp_datasets", "cat_Basic_Analysis", "fea_Part_of_Speech"], "fir_13": ["cat_Preprocessing", "fea_tokenization", "fea_nlp_datasets"], "fir_14": ["cat_Preprocessing", "fea_tokenization", "fea_lemmatization", "fea_nlp_datasets"], "fir_18": ["cat_Preprocessing", "fea_tokenization"], "fir_19": ["cat_Preprocessing", "fea_tokenization", "cat_Basic_Analysis", "fea_text_similarity"], "fir_20": ["cat_Preprocessing", "fea_tokenization", "fea_nlp_datasets", "fea_stemming", "cat_Basic_Analysis", "fea_text_scoring"], "fir_21": ["cat_Preprocessing", "fea_tokenization", "fea_nlp_datasets", "fea_stemming", "cat_Basic_Analysis", "fea_text_scoring", "cat_Advanced_Analysis", "fea_summarizer"], "fir_25": ["cat_Preprocessing", "fea_tokenization", "fea_lemmatization"], "fir_29": ["cat_Preprocessing", "fea_tokenization", "fea_lemmatization", "fea_nlp_datasets", "cat_Basic_Analysis", "fea_text_scoring"], "fir_30": ["cat_Preprocessing", "fea_tokenization", "fea_lemmatization", "fea_nlp_datasets", "cat_Basic_Analysis", "fea_Part_of_Speech", "cat_Advanced_Analysis", "fea_classification"], "fir_24": ["cat_Preprocessing", "fea_lemmatization", "fea_nlp_datasets", "fea_word_frequency", "cat_Basic_Analysis", "fea_Part_of_Speech", "cat_Advanced_Analysis", "fea_classification"], "fir_5": ["cat_Preprocessing", "fea_nlp_datasets", "fea_regular_expression"], "fir_7": ["cat_Preprocessing", "fea_nlp_datasets"], "fir_28": ["cat_Preprocessing", "fea_nlp_datasets", "cat_Advanced_Analysis", "fea_sentiment_analysis", "fea_classification"], "fir_22": ["cat_Preprocessing", "fea_word_frequency", "cat_Advanced_Analysis", "fea_classification"], "fir_27": ["cat_Preprocessing", "fea_word_frequency", "fea_tagger", "cat_Advanced_Analysis", "fea_classification"], "fir_26": ["cat_Preprocessing", "fea_n_grams"], "fir_15": ["cat_Advanced_Analysis", "fea_classification"], "sec_7": ["cat_Preprocessing", "fea_parsing"], "sec_10": ["cat_Preprocessing", "fea_parsing", "fea_tagger"], "sec_2": ["cat_Preprocessing", "fea_lemmatization", "fea_text_simplify", "cat_Basic_Analysis", "fea_Part_of_Speech"], "sec_12": ["cat_Preprocessing", "fea_lemmatization", "cat_Basic_Analysis", "fea_spellcheck", "cat_Advanced_Analysis", "fea_translation", "fea_language_detection"], "sec_6": ["cat_Preprocessing", "fea_tagger", "cat_Basic_Analysis", "fea_Part_of_Speech"], "sec_9": ["cat_Preprocessing", "fea_text_simplify", "cat_Advanced_Analysis", "fea_classification"], "sec_27": ["cat_Preprocessing", "fea_text_simplify", "fea_n_grams"], "sec_1": ["cat_Preprocessing", "fea_n_grams"], "sec_8": ["cat_Preprocessing", "fea_n_grams"], "sec_17": ["cat_Basic_Analysis", "fea_spellcheck"], "sec_26": ["cat_Basic_Analysis", "fea_text_scoring", "cat_Advanced_Analysis", "fea_sentiment_analysis"], "sec_29": ["cat_Basic_Analysis", "fea_text_scoring", "cat_Advanced_Analysis", "fea_sentiment_analysis"], "sec_15": ["cat_Advanced_Analysis", "fea_sentiment_analysis"], "sec_20": ["cat_Advanced_Analysis", "fea_sentiment_analysis", "fea_classification"], "sec_21": ["cat_Advanced_Analysis", "fea_sentiment_analysis"], "sec_24": ["cat_Advanced_Analysis", "fea_sentiment_analysis"], "sec_25": ["cat_Advanced_Analysis", "fea_sentiment_analysis"], "sec_28": ["cat_Advanced_Analysis", "fea_sentiment_analysis"], "sec_30": ["cat_Advanced_Analysis", "fea_sentiment_analysis"], "sec_14": ["cat_Advanced_Analysis", "fea_translation"], "sec_16": ["cat_Advanced_Analysis", "fea_translation"], "sec_19": ["cat_Advanced_Analysis", "fea_translation", "fea_language_detection"], "sec_22": ["cat_Advanced_Analysis", "fea_translation"], "sec_23": ["cat_Advanced_Analysis", "fea_translation", "fea_language_detection"], "sec_11": ["cat_Advanced_Analysis", "fea_language_detection"], "sec_18": ["cat_Advanced_Analysis", "fea_language_detection"], "sec_4": ["cat_Advanced_Analysis", "fea_classification"], "sec_5": ["cat_Advanced_Analysis", "fea_classification"], "sec_13": ["cat_Advanced_Analysis", "fea_classification"], "thr_17": ["cat_Preprocessing", "fea_parsing", "fea_lemmatization", "fea_word_vectors", "fea_nlp_datasets", "cat_Advanced_Analysis", "fea_classification"], "thr_10": ["cat_Preprocessing", "fea_tokenization", "cat_Basic_Analysis", "fea_text_similarity"], "thr_12": ["cat_Preprocessing", "fea_tokenization", "fea_lemmatization", "cat_Basic_Analysis", "fea_Part_of_Speech"], "thr_18": ["cat_Preprocessing", "fea_tokenization", "cat_Basic_Analysis", "fea_named_entity_recognition"], "thr_20": ["cat_Preprocessing", "fea_tokenization"], "thr_27": ["cat_Preprocessing", "fea_tokenization", "fea_regular_expression", "fea_text_simplify"], "thr_22": ["cat_Preprocessing", "fea_lemmatization"], "thr_26": ["cat_Preprocessing", "fea_lemmatization"], "thr_13": ["cat_Preprocessing", "fea_word_vectors"], "thr_5": ["cat_Preprocessing", "fea_nlp_datasets"], "thr_6": ["cat_Preprocessing", "fea_nlp_datasets", "cat_Advanced_Analysis", "fea_summarizer"], "thr_25": ["cat_Preprocessing", "fea_word_frequency"], "thr_24": ["cat_Preprocessing", "fea_tagger", "fea_n_grams", "cat_Basic_Analysis", "fea_Part_of_Speech"], "thr_11": ["cat_Preprocessing", "fea_n_grams"], "thr_19": ["cat_Preprocessing", "fea_n_grams"], "thr_8": ["cat_Basic_Analysis", "fea_spellcheck"], "thr_7": ["cat_Basic_Analysis", "fea_text_scoring", "cat_Advanced_Analysis", "fea_sentiment_analysis"], "thr_9": ["cat_Advanced_Analysis", "fea_language_detection"]}, "file info": {"fir_4": {"nlines": 427, "nchara": 30785}, "fir_16": {"nlines": 186, "nchara": 11394}, "fir_17": {"nlines": 215, "nchara": 14346}, "fir_23": {"nlines": 119, "nchara": 9583}, "fir_1": {"nlines": 62, "nchara": 3897}, "fir_2": {"nlines": 61, "nchara": 5381}, "fir_3": {"nlines": 230, "nchara": 18194}, "fir_6": {"nlines": 542, "nchara": 44964}, "fir_8": {"nlines": 169, "nchara": 40050}, "fir_9": {"nlines": 64, "nchara": 6301}, "fir_10": {"nlines": 64, "nchara": 6077}, "fir_11": {"nlines": 80, "nchara": 4754}, "fir_12": {"nlines": 108, "nchara": 8623}, "fir_13": {"nlines": 158, "nchara": 17324}, "fir_14": {"nlines": 160, "nchara": 17858}, "fir_18": {"nlines": 178, "nchara": 13017}, "fir_19": {"nlines": 180, "nchara": 12899}, "fir_20": {"nlines": 242, "nchara": 18329}, "fir_21": {"nlines": 144, "nchara": 13685}, "fir_25": {"nlines": 74, "nchara": 6426}, "fir_29": {"nlines": 119, "nchara": 9361}, "fir_30": {"nlines": 439, "nchara": 34069}, "fir_24": {"nlines": 123, "nchara": 8968}, "fir_5": {"nlines": 143, "nchara": 12738}, "fir_7": {"nlines": 215, "nchara": 13964}, "fir_28": {"nlines": 50, "nchara": 4219}, "fir_22": {"nlines": 37, "nchara": 2671}, "fir_27": {"nlines": 33, "nchara": 2733}, "fir_26": {"nlines": 40, "nchara": 4039}, "fir_15": {"nlines": 435, "nchara": 25111}, "sec_7": {"nlines": 5, "nchara": 683}, "sec_10": {"nlines": 162, "nchara": 16542}, "sec_2": {"nlines": 21, "nchara": 1792}, "sec_12": {"nlines": 75, "nchara": 5423}, "sec_6": {"nlines": 6, "nchara": 720}, "sec_9": {"nlines": 50, "nchara": 5532}, "sec_27": {"nlines": 98, "nchara": 7517}, "sec_1": {"nlines": 21, "nchara": 2388}, "sec_8": {"nlines": 6, "nchara": 671}, "sec_17": {"nlines": 20, "nchara": 1373}, "sec_26": {"nlines": 147, "nchara": 13678}, "sec_29": {"nlines": 111, "nchara": 8876}, "sec_15": {"nlines": 48, "nchara": 5194}, "sec_20": {"nlines": 80, "nchara": 6910}, "sec_21": {"nlines": 61, "nchara": 3753}, "sec_24": {"nlines": 127, "nchara": 9000}, "sec_25": {"nlines": 52, "nchara": 3712}, "sec_28": {"nlines": 52, "nchara": 3472}, "sec_30": {"nlines": 147, "nchara": 13371}, "sec_14": {"nlines": 14, "nchara": 1650}, "sec_16": {"nlines": 42, "nchara": 3889}, "sec_19": {"nlines": 141, "nchara": 7667}, "sec_22": {"nlines": 163, "nchara": 11450}, "sec_23": {"nlines": 37, "nchara": 2815}, "sec_11": {"nlines": 33, "nchara": 2506}, "sec_18": {"nlines": 28, "nchara": 2630}, "sec_4": {"nlines": 39, "nchara": 2994}, "sec_5": {"nlines": 39, "nchara": 3301}, "sec_13": {"nlines": 67, "nchara": 4194}, "thr_17": {"nlines": 83, "nchara": 6607}, "thr_10": {"nlines": 23, "nchara": 1770}, "thr_12": {"nlines": 129, "nchara": 7019}, "thr_18": {"nlines": 18, "nchara": 1742}, "thr_20": {"nlines": 26, "nchara": 2326}, "thr_27": {"nlines": 20, "nchara": 1723}, "thr_22": {"nlines": 17, "nchara": 1530}, "thr_26": {"nlines": 10, "nchara": 902}, "thr_13": {"nlines": 11, "nchara": 894}, "thr_5": {"nlines": 8, "nchara": 571}, "thr_6": {"nlines": 40, "nchara": 3965}, "thr_25": {"nlines": 30, "nchara": 2747}, "thr_24": {"nlines": 8, "nchara": 874}, "thr_11": {"nlines": 127, "nchara": 10367}, "thr_19": {"nlines": 6, "nchara": 610}, "thr_8": {"nlines": 10, "nchara": 909}, "thr_7": {"nlines": 11, "nchara": 1738}, "thr_9": {"nlines": 12, "nchara": 1074}}, "within files": {"fir_4": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_4\"><pre id=\"fir_4_code\"><code class=\"python\">import nltk\nfrom nltk.parse.stanford import StanfordParser, StanfordDependencyParser\nfrom nltk.tag.stanford import StanfordPOSTagger\nfrom nltk.tag import StanfordNERTagger\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom config import *\n\nclass Text_processing:\n\n\n\tdef __init__(self):\n\t\t\n\t\t# user need to download Stanford Parser, NER and POS tagger from stanford website\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">self.constituent_<span class=\"fea_parsing_funcs blodFunc\">parse</span>_tree = Stanford<span class=\"fea_parsing_funcs blodFunc\">Parser</span>()</div>  #user need to set as environment variable\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_1\" style=\"display: inline;\">self.stanford_dependency = StanfordDependency<span class=\"fea_parsing_funcs blodFunc\">Parser</span>()</div> #user need to set as environment variable\n\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">self.lemma = <span class=\"fea_lemmatization_funcs blodFunc\">Word</span><span class=\"fea_lemmatization_funcs blodFunc\">Net</span><span class=\"fea_lemmatization_funcs blodFunc\">Lemmatizer</span>()</div>\n\t\tself.home = '/home/ramesh'\n\t\t#user needs to download stanford packages and change directory\n\t\t<div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">self.ner = StanfordNERTagger(self.home + '/stanford-ner-2017-06-09/classifiers/english.all.3class.distsim.crf.ser.gz',self.home + '/stanford-ner-2017-06-09/stanford-ner.jar')</div>\n\t\t<div class=\"highlights fea_tagger\" id=\"tagger_1\" style=\"display: inline;\">self.pos_tag = StanfordPOSTagger(self.home + '/stanford-postagger-2017-06-09/models/english-bidirectional-distsim.tagger',self.home + '/stanford-postagger-2017-06-09/stanford-postagger-3.8.0.jar')</div>\n\t\tself.CharacterOffsetEnd = 0 \n\t\tself.CharacterOffsetBegin = 0\n\t\t\n\n\t'''\n\tInput: sentence\n\tReturns: \n\t'''\n\n\n\tdef parser(self,sentence):\n\n\n\t\tself.parseResult = {'parseTree':[], 'text':[], 'dependencies':[],'words':[] }\n\t\tparseText, sentences = self.get_parseText(sentence)\n\t\t# print \"sentences \", sentences\n\t\t# if source/target sent consist of 1 sentence \n\t\tif len(sentences) == 1:\n\t\t\treturn parseText\n\t\t\n\t\twordOffSet = 0 # offset is number of words in first sentence \n\n\t\t# if source/target sentence has more than 1 sentence\n\n\t\tfor i in xrange(len(parseText['text'])):\n\t\t\tif i &gt; 0:\n\n\t\t\t\tfor j in xrange(len(parseText['dependencies'][i])):\n\t\t\t\t\t# [root, Root-0, dead-4]\n\t\t\t\t\tfor k in xrange(1,3):\n\t\t\t\t\t\ttokens = parseText['dependencies'][i][j][k].split('-')\n\n\t\t\t\t\t\tif tokens[0] == 'Root':\n\t\t\t\t\t\t\tnewWordIndex = 0\n\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tif not tokens[len(tokens)-1].isdigit():\n\t\t\t\t\t\t\t\tcontinue \n\n\t\t\t\t\t\t\tnewWordIndex = int(tokens[len(tokens)-1]) + wordOffSet\n\n\t\t\t\t\t\tif len(tokens) == 2:\n\n\t\t\t\t\t\t\t parseText['dependencies'][i][j][k] = tokens[0]+ '-' + str(newWordIndex)\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tw = ''\n\t\t\t\t\t\t\tfor l in xrange(len(tokens)-1):\n\t\t\t\t\t\t\t\tw += tokens[l]\n\t\t\t\t\t\t\t\tif l&lt;len(tokens)-2:\n\t\t\t\t\t\t\t\t\tw += '-'\n\n\t\t\t\t\t\t\tparseText['dependencies'][i][j][k] = w + '-' + str(newWordIndex)\n\n\t\t\twordOffSet += len(parseText['words'][i])\n\n\t\treturn parseText\n\n\n\t'''\n\tInput: parserResult \n\tReturns: [[charBegin,charEnd], wordIndex(starts from 1), word, word_lemma]] \n\t'''\n\n\n\tdef get_lemma(self,parserResult):\n\n\n\t\tres = []\t\t\n\t\twordIndex = 1\n\t\tfor i in xrange(len(parserResult['words'])):\n\t\t\t\n\t\t\tfor j in xrange(len(parserResult['words'][i])):\n\t\t\t\t\n\t\t\t\ttag = [ [parserResult['words'][i][j][1]['CharacterOffsetBegin'], parserResult['words'][i][j][1]['CharacterOffsetEnd']], wordIndex,parserResult['words'][i][j][0] ,parserResult['words'][i][j][1]['Lemma'] ]\n\t\t\t\twordIndex += 1\n\t\t\t\tres.append(tag)\n\n\t\treturn res\n\n\n\n\t'''\n\tUsing Stanford POS Tagger\n\tInput: parserResult \n\tReturns: [[charBegin,charEnd], wordIndex(starts from 1), word, word_POS]] \n\t'''\n\n\n\tdef combine_lemmaAndPosTags(self,parserResult):\n\n\t\tres = []\n\t\t\n\t\twordIndex = 1\n\t\tfor i in xrange(len(parserResult['words'])):\n\t\t\t\n\t\t\tfor j in xrange(len(parserResult['words'][i])):\n\t\t\t\t\n\t\t\t\ttag = [ [parserResult['words'][i][j][1]['CharacterOffsetBegin'], parserResult['words'][i][j][1]['CharacterOffsetEnd']], wordIndex,parserResult['words'][i][j][0] ,parserResult['words'][i][j][1]['Lemma'] ,parserResult['words'][i][j][1]['PartOfSpeech'] ]\n\t\t\t\twordIndex += 1\n\t\t\t\tres.append(tag)\n\n\t\treturn res\n\n\n\t'''\n\tInput: parserResult\n\tReturns: ([charOffsetBegin,charOffsetEnd], wordindex,word, NER ])\n\t'''\n\n\n\tdef nerWordAnnotator(self,parserResult):\n\n\t\tres = []\n\t\t\n\t\twordIndex = 1\n\t\tfor i in xrange(len(parserResult['words'])):\n\t\t\t\n\t\t\tfor j in xrange(len(parserResult['words'][i])):\n\t\t\t\t\n\t\t\t\ttag = [ [parserResult['words'][i][j][1]['CharacterOffsetBegin'], parserResult['words'][i][j][1]['CharacterOffsetEnd']], wordIndex,parserResult['words'][i][j][0] ,parserResult['words'][i][j][1]['NamedEntityTag'] ]\n\t\t\t\t# print \"tag \", tag\n\t\t\t\twordIndex += 1\n\t\t\t\t# if there is valid named entity then add in list\n\t\t\t\tif tag[3] != 'O':\n\n\t\t\t\t\tres.append(tag)\n\n\t\treturn res\n\n\n\t'''\n\tInput : ParserResult\n\tReturns : list containing NamedEntites\n\t1. Group words in same list if they share same NE (Location), \n    2. Save other words in list that have any entity\n\t'''\n\n\n\tdef get_ner(self,parserResult):\n\n\n\t\t<div class=\"highlights fea_named_entity_recognition\" id=\"named_entity_recognition_0\" style=\"display: inline;\"><span class=\"fea_named_entity_recognition_funcs blodFunc\">ner</span><span class=\"fea_named_entity_recognition_funcs blodFunc\">Word</span><span class=\"fea_named_entity_recognition_funcs blodFunc\">Annotations</span> = <span class=\"fea_named_entity_recognition_funcs blodFunc\">self</span>.<span class=\"fea_named_entity_recognition_funcs blodFunc\">ner</span><span class=\"fea_named_entity_recognition_funcs blodFunc\">Word</span><span class=\"fea_named_entity_recognition_funcs blodFunc\">Annotator</span>(<span class=\"fea_named_entity_recognition_funcs blodFunc\">parser</span><span class=\"fea_named_entity_recognition_funcs blodFunc\">Result</span>)</div> #[[ [charbegin,charEnd], wordIndex, word, NE ]]\n\t\tnamedEntities = []\n\t\tcurrentWord = []\n\t\tcurrentCharacterOffSets = []\n\t\tcurrentWordOffSets = []\n\n\t\tfor i in xrange(len(nerWordAnnotations)):\n\n\t\t\tif i == 0:\n\n\t\t\t\tcurrentWord.append(nerWordAnnotations[i][2]) # word having NE\n\t\t\t\tcurrentCharacterOffSets.append(nerWordAnnotations[i][0]) # [begin,end]\n\t\t\t\tcurrentWordOffSets.append(nerWordAnnotations[i][1]) # Word Index\n\t\t\t\t# if there is only one ner Word tag\n\t\t\t\tif (len(nerWordAnnotations) == 1):\n\t\t\t\t\tnamedEntities.append([ currentCharacterOffSets, currentWordOffSets, \\\n\t\t\t\t\t\tcurrentWord, nerWordAnnotations[i-1][3] ])\n\t\t\t\t\t# print \"named Entities \", namedEntities\n\t\t\t\t\tbreak \n\t\t\t\tcontinue\n\t\t\t# if consecutive tags have same NER Tag, save them in one list\n\t\t\tif nerWordAnnotations[i][3] == nerWordAnnotations[i-1][3] and \\\n\t\t\t\t\tnerWordAnnotations[i][1] == nerWordAnnotations[i-1][1] + 1:\n\t\t\t\t\n\t\t\t\tcurrentWord.append(nerWordAnnotations[i][2]) # word having NE\n\t\t\t\tcurrentCharacterOffSets.append(nerWordAnnotations[i][0]) # [begin,end]\n\t\t\t\tcurrentWordOffSets.append(nerWordAnnotations[i][1]) # Word Index\n\n\t\t\t\tif i == (len(nerWordAnnotations) - 1):\n\t\t\t\t\tnamedEntities.append([ currentCharacterOffSets, \\\n\t\t\t\t\t\tcurrentWordOffSets, currentWord, nerWordAnnotations[i][3] ])\n\t\t\t# if consecutive tags do not match\n\t\t\telse:\n\n\t\t\t\tnamedEntities.append([ currentCharacterOffSets, \\\n\t\t\t\t\t\tcurrentWordOffSets, currentWord, nerWordAnnotations[i-1][3] ])\n\t\t\t\tcurrentWord = [nerWordAnnotations[i][2]]\n\t\t\t\t# remove everything from currentCharacterOffSets and currentWordOffSets\n\t\t\t\tcurrentCharacterOffSets = []\n\t\t\t\tcurrentWordOffSets = []\n\t\t\t\t# add charac offsets and currentWordOffSets of current word\n\t\t\t\tcurrentCharacterOffSets.append(nerWordAnnotations[i][0])\n\t\t\t\tcurrentWordOffSets.append(nerWordAnnotations[i][1])\n\n\t\t\t\t# if it is last iteration then update named Entities\n\t\t\t\tif i == len(nerWordAnnotations)-1:\n\t\t\t\t\tnamedEntities.append([ currentCharacterOffSets, currentWordOffSets, \\\n\t\t\t\t\t\t\tcurrentWord, nerWordAnnotations[i][3] ])\n\t\t#sort out according to len of characters in ascending order\n\t\tnamedEntities = sorted(namedEntities, key=len)\n\n\t\treturn namedEntities\n\n\n\t'''\n\tInput: Word(Word whose NE is not found), NE(word already have NE Tag) \n\tReturns: Boolean; True if word is acronym\n\t\t\t\t\tFalse if word is not acronym\n\t'''\n\n\n\tdef is_Acronym(self,word,NE):\n\n\n\t\tqueryWord = word.replace('.','')\n\t\t# If all words of queryWord is not capital or length of word != \n\t\t\t\t#length of NE(word already have NE Tag) or \n\t\t   #  if word is 'a' or 'i' \n\t\tif not queryWord.isupper() or len(queryWord) != len(NE) or queryWord.lower() in ['a', 'i']:\n\t\t\treturn False\n\n\t\tacronym = True\n\n\t\t#we run for loop till length of query word(i.e 3)(if word is 'UAE')\n\t\t#Compare 1st letter(U) of query word with first letter of first element in named entity(U = U(united))\n\t\t# again we take second letter of canonical word (A) with second element in named entity(Arab)\n\t\t# and so on \n\t\tfor i in xrange(len(queryWord)):\n\t\t\t# print \"queryword[i], NE \", queryWord, NE\n\t\t\tif queryWord[i] != NE[i][0]:\n\t\t\t\tacronym = False\n\t\t\t\tbreak\n\n\t\treturn acronym\n\n\n\t'''\n\tInput: sentence\n\tReturns: parse(\t{ParseTree, text, Dependencies, \n\t  'word : [] NamedEntityTag, CharacterOffsetEnd, \n\t  \t\tCharacterOffsetBegin, PartOfSpeech, Lemma}']}) \n\t  \t\tsentence and\n\t'''\n\n\n\tdef get_parseText(self,sentence):\n\n\t\tself.count = 0\n\t\tself.length_of_sentence = [] # stores length of each sentence\n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>d_sentence = sent_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(sentence)</div>\n\t\t# print \"len of tokenized \",len(tokenized_sentence)\n\t\tif (len(tokenized_sentence) == 1):\n\t\t\tself.count += 1\n\t\t\tfor i in tokenized_sentence:\n\t\t\t\tparse = self.get_combine_words_param(i)\n\t\telse:\n\t\t\ttmp = 0\n\t\t\tfor i in tokenized_sentence:\n\t\t\t\tself.count += 1\n\t\t\t\tparse = self.get_combine_words_param(i)\n\t\t\t\ts = len(i) + tmp\n\t\t\t\tself.length_of_sentence.append(s)\n\t\t\t\ttmp = s\n\n\t\treturn parse,tokenized_sentence\n\t\t\n\n\t'''\n\tInput: sentences\n    Return: constituency tree that represents relations between sub-phrases in sentences\n\t'''\n\n\n\tdef get_constituency_Tree(self,sentence):\n\t\t\n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">sentence = sent_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(sentence)</div>\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_2\" style=\"display: inline;\">constituency_<span class=\"fea_parsing_funcs blodFunc\">parser</span> = self.constituent_<span class=\"fea_parsing_funcs blodFunc\">parse</span>_tree.raw_<span class=\"fea_parsing_funcs blodFunc\">parse</span>_sents(sentence)</div>\n\t\tfor parser in constituency_parser:\n\t\t\tfor sent in parser:\n\t\t\t\ttree = str(sent)\n\t\tparse_string = ' '.join(str(tree).split()) \n        \n\t\treturn parse_string\n\n\n\t'''\n\tInput: sentence\n\treturns: relation between words with their index\n\t'''\t\n\n\n\tdef get_dependencies(self,sentence):\n\n\n\t\tdependency_tree = []\n\t\t<div class=\"highlights fea_dependency_parsing\" id=\"dependency_parsing_0\" style=\"display: inline;\"><span class=\"fea_dependency_parsing_funcs blodFunc\">dependency</span>_<span class=\"fea_dependency_parsing_funcs blodFunc\">parser</span> = <span class=\"fea_dependency_parsing_funcs blodFunc\">self</span>.<span class=\"fea_dependency_parsing_funcs blodFunc\">stanford</span>_<span class=\"fea_dependency_parsing_funcs blodFunc\">dependency</span>.<span class=\"fea_dependency_parsing_funcs blodFunc\">raw</span>_<span class=\"fea_dependency_parsing_funcs blodFunc\">parse</span>(<span class=\"fea_dependency_parsing_funcs blodFunc\">sentence</span>)</div>\n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_2\" style=\"display: inline;\">token = <span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(sentence)</div>\n\t\t<div class=\"highlights fea_dependency_parsing\" id=\"dependency_parsing_1\" style=\"display: inline;\"><span class=\"fea_dependency_parsing_funcs blodFunc\">parsetree</span> = <span class=\"fea_dependency_parsing_funcs blodFunc\">list</span>(<span class=\"fea_dependency_parsing_funcs blodFunc\">self</span>.<span class=\"fea_dependency_parsing_funcs blodFunc\">stanford</span>_<span class=\"fea_dependency_parsing_funcs blodFunc\">dependency</span>.<span class=\"fea_dependency_parsing_funcs blodFunc\">raw</span>_<span class=\"fea_dependency_parsing_funcs blodFunc\">parse</span>(<span class=\"fea_dependency_parsing_funcs blodFunc\">sentence</span>))[0]</div>\n\t\t# Find root(head) of the sentence \n\t\tfor k in parsetree.nodes.values():\n\t\t\tif k[\"head\"] == 0:\n\t\t\n\t\t\t\tdependency_tree.append([str(k[\"rel\"]), \"Root-\" + str(k[\"head\"]), str(k[\"word\"]) \n\t\t\t\t\t+ \"-\" + str(k[\"address\"]) ])\t    \t\n\t\t# Find relation between words in sentence\n\t\tfor dep in dependency_parser:\n\t\t\tfor triple in dep.triples():\n\t\t\t\tindex_word = token.index(triple[0][0]) + 1 # because index starts from 0 \n\t\t\t\tindex2_word = token.index(triple[2][0]) + 1\n\t\t\t\tdependency_tree.append([str(triple[1]),str(triple[0][0]) + \"-\" + str(index_word),\\\n\t\t\t\t\t\t\t str(triple[2][0]) + \"-\" + str(index2_word)])\n\n\t\treturn dependency_tree\n\n\n\t'''\n\tInput: sentence, word(of which offset to determine)\n\tReturn: [CharacterOffsetEnd,CharacterOffsetBegin] for each word\n\t'''\n\n\n\tdef get_charOffset(self,sentence, word):\n\n\t\t# word containing '.' causes problem in counting\n\n\t\tCharacterOffsetBegin = sentence.find(word)\n\t\tCharacterOffsetEnd = CharacterOffsetBegin + len(word)\n\t\t\n\t\treturn [CharacterOffsetEnd,CharacterOffsetBegin]\n\n\n\t'''\n\tInput: sentence\n\tReturns: dictionary: \n\t{ParseTree, text, Dependencies, \n\t  #'word : [] NamedEntityTag, CharacterOffsetEnd, CharacterOffsetBegin, PartOfSpeech, Lemma}']}\n\t'''\n\n\n\tdef get_combine_words_param(self,sentence):\n\t\t\n\n\t\twords_in_each_sentence = []\n\t\twords_list = [] \n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_3\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>d_<span class=\"fea_tokenization_funcs blodFunc\">word</span>s = <span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(sentence)</div>\n\t\t<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span><span class=\"fea_Part_of_Speech_funcs blodFunc\">Tag</span> = self.<span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span>_<span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span>.<span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span>(tokenized_words)</div>\n\t\t<div class=\"highlights fea_named_entity_recognition\" id=\"named_entity_recognition_1\" style=\"display: inline;\"><span class=\"fea_named_entity_recognition_funcs blodFunc\">ner</span> = <span class=\"fea_named_entity_recognition_funcs blodFunc\">self</span>.<span class=\"fea_named_entity_recognition_funcs blodFunc\">ner</span>.<span class=\"fea_named_entity_recognition_funcs blodFunc\">tag</span>(<span class=\"fea_named_entity_recognition_funcs blodFunc\">tokenized</span>_<span class=\"fea_named_entity_recognition_funcs blodFunc\">words</span>)</div>\n\t\t\n\t\t# if source sentence/target sentence has one sentence\n\t\tif (self.count == 1):\n\t\t\tfor i in xrange(len(tokenized_words)):\n\t\t\t\tword_lemma = str()\n\t\t\t\tword = tokenized_words[i]\n\t\t\t\tname_entity = ner[i]\n\t\t\t\tword_posTag = posTag[i][-1]  # access tuple [(United, NNP),..]\n\t\t\t\t# print \"word and pos tag \", word, word_posTag[0]\t\n\t\t\t\t#wordNet lemmatizer needs pos tag with words else it considers noun\n\t\t\t\tif (word_posTag[0] == 'V'):\n\t\t\t\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_1\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">word</span>_lemma = self.lemma.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>(tokenized_<span class=\"fea_lemmatization_funcs blodFunc\">word</span>s[i], <span class=\"fea_lemmatization_funcs blodFunc\">word</span><span class=\"fea_lemmatization_funcs blodFunc\">net</span>.VERB)</div>\n\n\t\t\t\telif (word_posTag[0] == 'J'):\n\t\t\t\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_2\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">word</span>_lemma = self.lemma.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>(tokenized_<span class=\"fea_lemmatization_funcs blodFunc\">word</span>s[i], <span class=\"fea_lemmatization_funcs blodFunc\">word</span><span class=\"fea_lemmatization_funcs blodFunc\">net</span>.ADJ)</div>\n\n\t\t\t\telif (word_posTag[0:1] == 'RB'):\n\t\t\t\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_3\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">word</span>_lemma = self.lemma.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>(tokenized_<span class=\"fea_lemmatization_funcs blodFunc\">word</span>s[i], <span class=\"fea_lemmatization_funcs blodFunc\">word</span><span class=\"fea_lemmatization_funcs blodFunc\">net</span>.ADV)</div>\n\n\t\t\t\telse:\n\t\t\t\t\t<div class=\"highlights fea_lemmatization\" id=\"lemmatization_4\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">word</span>_lemma = self.lemma.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>(tokenized_<span class=\"fea_lemmatization_funcs blodFunc\">word</span>s[i])</div>\n\n\t\t\t\tself.CharacterOffsetEnd, self.CharacterOffsetBegin = self.get_charOffset(sentence,tokenized_words[i])\n\t\t\t\t\n\n\t\t\t\twords_list.append([word, {\"NamedEntityTag\" : str(name_entity[1]),\n\t\t\t\t\t\"CharacterOffsetEnd\" : str(self.CharacterOffsetEnd), \"CharacterOffsetBegin\" : str(self.CharacterOffsetBegin) \n\t\t\t\t\t,\"PartOfSpeech\" : str(word_posTag) , \"Lemma\" : str(word_lemma)}])\n\n\t\t\tself.parseResult['parseTree'] = [self.get_constituency_Tree(sentence)]\n\t\t\tself.parseResult['text'] = [sentence]\n\t\t\tself.parseResult['dependencies'] = [self.get_dependencies(sentence)]\n\t\t\tself.parseResult['words'] = [words_list]\n\n\t\telse:\n\n\t\t\tfor i in xrange(len(tokenized_words)):\n\t\t\t\tword = tokenized_words[i]\n\t\t\t\tname_entity = ner[i] \n\t\t\t\tword_posTag = posTag[i][-1]\n\n\t\t\t\tif (word_posTag[0] == 'V'):\n\t\t\t\t\tword_lemma = self.lemma.lemmatize(tokenized_words[i], wordnet.VERB)\n\n\t\t\t\telif (word_posTag[0] == 'J'):\n\t\t\t\t\tword_lemma = self.lemma.lemmatize(tokenized_words[i], wordnet.ADJ)\n\n\t\t\t\telif (word_posTag[0:1] == 'RB'):\n\t\t\t\t\tword_lemma = self.lemma.lemmatize(tokenized_words[i], wordnet.ADV)\n\n\t\t\t\telse:\n\t\t\t\t\tword_lemma = self.lemma.lemmatize(tokenized_words[i])\n\n\t\t\t\tend, begin = self.get_charOffset(sentence,tokenized_words[i])\n\t\t\t\tend = end + self.length_of_sentence[self.count-2] + 1\n\t\t\t\tbegin = begin + self.length_of_sentence[self.count-2] + 1\t\n\t\t\t\twords_list.append([word, {\"NamedEntityTag\" : str(name_entity[1]),\n\t\t\t\t\t\"CharacterOffsetEnd\" : str(end), \"CharacterOffsetBegin\" : str(begin) \n\t\t\t\t\t,\"PartOfSpeech\" : str(word_posTag) , \"Lemma\" : str(word_lemma)}])\n\t\t\tself.parseResult['parseTree'].append(self.get_constituency_Tree(sentence))\n\t\t\tself.parseResult['text'].append(sentence)\n\t\t\tself.parseResult['dependencies'].append(self.get_dependencies(sentence))\n\t\t\tself.parseResult['words'].append(words_list)\n\n\t\treturn self.parseResult\n\t\t#https://github.com/rameshjes/Semantic-Textual-Similarity/blob/master/monolingualWordAligner/nltkUtil</code></pre></div></body></html>", "fir_16": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_16\"><pre id=\"fir_16_code\"><code class=\"python\"># Copyright 2020 The `Kumar Nityan Suman` (https://github.com/nityansuman/).\n# All Rights Reserved.\n#\n#                     GNU GENERAL PUBLIC LICENSE\n#                        Version 3, 29 June 2007\n#  Copyright (C) 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt;\n#  Everyone is permitted to copy and distribute verbatim copies\n#  of this license document, but changing it is not allowed.\n# ==============================================================================\n\nimport logging\nfrom typing import Tuple\n\nimport nltk as nlp\nimport numpy as np\n\n\nclass SubjectiveTest:\n\t\"\"\"Class abstraction for subjective test generation module.\n\t\"\"\"\n\n\tdef __init__(self, filepath: str):\n\t\t\"\"\"Class constructor.\n\n\t\tArgs:\n\t\t\tfilepath (str): Absolute filepath to the subject corpus.\n\t\t\"\"\"\n\t\tself.question_pattern = [\n\t\t\t\"Explain in detail \",\n\t\t\t\"Define \",\n\t\t\t\"Write a short note on \",\n\t\t\t\"What do you mean by \"\n\t\t]\n\n\t\tself.grammar = r\"\"\"\n\t\t\tCHUNK: {&lt;NN&gt;+&lt;IN|DT&gt;*&lt;NN&gt;+}\n\t\t\t{&lt;NN&gt;+&lt;IN|DT&gt;*&lt;NNP&gt;+}\n\t\t\t{&lt;NNP&gt;+&lt;NNS&gt;*}\n\t\t\"\"\"\n\n\t\ttry:\n\t\t\twith open(filepath, mode=\"r\") as fp:\n\t\t\t\tself.summary = fp.read()\n\t\texcept FileNotFoundError:\n\t\t\tlogging.exception(\"Corpus file not found.\", exc_info=True)\n\n\t@staticmethod\n\tdef word_tokenizer(sequence: str) -&gt; list:\n\t\t\"\"\"Tokenize string sequences to words.\n\n\t\tArgs:\n\t\t\tsequence (str): Corpus sequences.\n\n\t\tReturns:\n\t\t\tlist: Word tokens.\n\t\t\"\"\"\n\t\tword_tokens = list()\n\t\ttry:\n\t\t\tfor sent in <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">nlp.sent_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(sequence)</div>:\n\t\t\t\tfor w in <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">nlp.<span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(sent)</div>:\n\t\t\t\t\tword_tokens.append(w)\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Word tokenization failed.\", exc_info=True)\n\t\treturn word_tokens\n\n\t@staticmethod\n\tdef create_vector(answer_tokens: list, tokens: list) -&gt; np.array:\n\t\t\"\"\"Create a one-hot encoded vector for the answer_tokens.\n\n\t\tArgs:\n\t\t\tanswer_tokens (list): Tokenized user response.\n\t\t\ttokens (list): Tokenized answer corpus.\n\n\t\tReturns:\n\t\t\tnp.array: A one-hot encoded vector of the answer.\n\t\t\"\"\"\n\t\treturn np.array([1 if tok in answer_tokens else 0 for tok in tokens])\n\n\t@staticmethod\n\tdef cosine_similarity_score(vector1: np.array, vector2: np.array) -&gt; float:\n\t\t\"\"\"Compute the euclidean distance between two vectors.\n\n\t\tArgs:\n\t\t\tvector1 (np.array): Actual answer vector.\n\t\t\tvector2 (np.array): User response vector.\n\n\t\tReturns:\n\t\t\tfloat: Euclidean distance between two vectors.\n\t\t\"\"\"\n\t\tdef vector_value(vector):\n\t\t\treturn np.sqrt(np.sum(np.square(vector)))\n\n\t\tv1 = vector_value(vector1)\n\t\tv2 = vector_value(vector2)\n\n\t\tv1_v2 = np.dot(vector1, vector2)\n\t\treturn (v1_v2 / (v1 * v2)) * 100\n\n\tdef generate_test(self, num_questions: int = 2) -&gt; Tuple[list, list]:\n\t\t\"\"\"Method to generate subjective test.\n\n\t\tArgs:\n\t\t\tnum_questions (int, optional): Maximum number of questions\n\t\t\t\tto be generated. Defaults to 2.\n\n\t\tReturns:\n\t\t\tTuple[list, list]: Generated `Questions` and `Answers` respectively\n\t\t\"\"\"\n\t\ttry:\n\t\t\tsentences = nlp.sent_tokenize(self.summary)\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Sentence tokenization failed.\", exc_info=True)\n\n\t\ttry:\n\t\t\t<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">cp = nlp.<span class=\"fea_parsing_funcs blodFunc\">Regexp</span><span class=\"fea_parsing_funcs blodFunc\">Parser</span>(self.<span class=\"fea_parsing_funcs blodFunc\">grammar</span>)</div>\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Regex grammar train failed.\", exc_info=True)\n\n\t\tquestion_answer_dict = dict()\n\t\tfor sentence in sentences:\n\n\t\t\ttry:\n\t\t\t\t<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span>ged_words = nlp.<span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span>_<span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span>(nlp.word_tokenize(sentence))</div>\n\t\t\texcept Exception:\n\t\t\t\tlogging.exception(\"Word tokenization failed.\", exc_info=True)\n\n\t\t\ttree = cp.parse(tagged_words)\n\t\t\tfor subtree in tree.subtrees():\n\t\t\t\tif subtree.label() == \"CHUNK\":\n\t\t\t\t\ttemp = \"\"\n\t\t\t\t\tfor sub in subtree:\n\t\t\t\t\t\ttemp += sub[0]\n\t\t\t\t\t\ttemp += \" \"\n\t\t\t\t\ttemp = temp.strip()\n\t\t\t\t\ttemp = temp.upper()\n\t\t\t\t\tif temp not in question_answer_dict:\n\t\t\t\t\t\tif len(<div class=\"highlights fea_tokenization\" id=\"tokenization_2\" style=\"display: inline;\">nlp.<span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(sentence)</div>) &gt; 20:\n\t\t\t\t\t\t\tquestion_answer_dict[temp] = sentence\n\t\t\t\t\telse:\n\t\t\t\t\t\tquestion_answer_dict[temp] += sentence\n\n\t\tkeyword_list = list(question_answer_dict.keys())\n\t\tquestion_answer = list()\n\n\t\tfor _ in range(3):\n\t\t\trand_num = np.random.randint(0, len(keyword_list))\n\t\t\tselected_key = keyword_list[rand_num]\n\t\t\tanswer = question_answer_dict[selected_key]\n\t\t\trand_num %= 4\n\t\t\tquestion = self.question_pattern[rand_num] + selected_key + \".\"\n\t\t\tquestion_answer.append({\"Question\": question, \"Answer\": answer})\n\n\t\tque = list()\n\t\tans = list()\n\t\twhile len(que) &lt; num_questions:\n\t\t\trand_num = np.random.randint(0, len(question_answer))\n\t\t\tif question_answer[rand_num][\"Question\"] not in que:\n\t\t\t\tque.append(question_answer[rand_num][\"Question\"])\n\t\t\t\tans.append(question_answer[rand_num][\"Answer\"])\n\t\t\telse:\n\t\t\t\tcontinue\n\t\treturn que, ans\n\n\tdef evaluate_subjective_answer(self, original_answer: str, user_answer: str) -&gt; float:\n\t\t\"\"\"Evaluate the subjective answer given by the user.\n\n\t\tArgs:\n\t\t\toriginal_answer (str): A string representing the original answer.\n\t\t\tuser_answer (str): A string representing the answer given by the user.\n\n\t\tReturns:\n\t\t\tfloat: Similarity/correctness score of the user answer\n\t\t\t\tbased on the original asnwer.\n\t\t\"\"\"\n\t\tscore_obt = 0\n\t\toriginal_ans_list = self.word_tokenizer(original_answer)\n\t\tuser_ans_list = self.word_tokenizer(user_answer)\n\n\t\toverall_list = original_ans_list + user_ans_list\n\n\t\tvector1 = self.create_vector(original_ans_list, overall_list)\n\t\tvector2 = self.create_vector(user_answer, overall_list)\n\n\t\tscore_obt = self.cosine_similarity_score(vector1, vector2)\n\t\treturn score_obt\n\t\t#https://github.com/nityansuman/marvin/blob/main/src/subjective</code></pre></div></body></html>", "fir_17": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_17\"><pre id=\"fir_17_code\"><code class=\"python\"># Copyright 2020 The `Kumar Nityan Suman` (https://github.com/nityansuman/).\n# All Rights Reserved.\n#\n#                     GNU GENERAL PUBLIC LICENSE\n#                        Version 3, 29 June 2007\n#  Copyright (C) 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt;\n#  Everyone is permitted to copy and distribute verbatim copies\n#  of this license document, but changing it is not allowed.\n# ==============================================================================\n\nimport logging\nimport re\nfrom typing import Tuple\n\nimport nltk\nimport numpy as np\nfrom nltk.corpus import wordnet as wn\n\n\nclass ObjectiveTest:\n\t\"\"\"Class abstraction for objective test generation module.\n\t\"\"\"\n\n\tdef __init__(self, filepath: str):\n\t\t\"\"\"Class constructor.\n\n\t\tArgs:\n\t\t\tfilepath (str): filepath (str): Absolute filepath to the subject corpus.\n\t\t\"\"\"\n\t\t# Load subject corpus\n\t\ttry:\n\t\t\twith open(filepath, mode=\"r\") as fp:\n\t\t\t\tself.summary = fp.read()\n\t\texcept FileNotFoundError:\n\t\t\tlogging.exception(\"Corpus file not found.\", exc_info=True)\n\n\tdef generate_test(self, num_questions: int = 3) -&gt; Tuple[list, list]:\n\t\t\"\"\"Method to generate an objective test.\n\n\t\tArgs:\n\t\t\tnum_questions (int, optional): Number of questions in a test.\n\t\t\t\tDefaults to 3.\n\n\t\tReturns:\n\t\t\tTuple[list, list]: Questions and answer options respectively.\n\t\t\"\"\"\n\t\t# Identify potential question sets\n\t\tquestion_sets = self.get_question_sets()\n\n\t\t# Identify potential question answers\n\t\tquestion_answers = list()\n\t\tfor question_set in question_sets:\n\t\t\tif question_set[\"Key\"] &gt; 3:\n\t\t\t\tquestion_answers.append(question_set)\n\n\t\t# Create objective test set\n\t\tquestions, answers = list(), list()\n\t\twhile len(questions) &lt; num_questions:\n\t\t\trand_num = np.random.randint(0, len(question_answers))\n\t\t\tif question_answers[rand_num][\"Question\"] not in questions:\n\t\t\t\tquestions.append(question_answers[rand_num][\"Question\"])\n\t\t\t\tanswers.append(question_answers[rand_num][\"Answer\"])\n\t\treturn questions, answers\n\n\tdef get_question_sets(self) -&gt; list:\n\t\t\"\"\"Method to dentify sentences with potential objective questions.\n\n\t\tReturns:\n\t\t\tlist: Sentences with potential objective questions.\n\t\t\"\"\"\n\t\t# Tokenize corpus into sentences\n\t\ttry:\n\t\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">sentences = nltk.sent_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(self.summary)</div>\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Sentence tokenization failed.\", exc_info=True)\n\n\t\t# Identify potential question sets\n\t\t# Each question set consists:\n\t\t# \tQuestion: Objective question.\n\t\t# \tAnswer: Actual asnwer.\n\t\t#\tKey: Other options.\n\t\tquestion_sets = list()\n\t\tfor sent in sentences:\n\t\t\tquestion_set = self.identify_potential_questions(sent)\n\t\t\tif question_set is not None:\n\t\t\t\tquestion_sets.append(question_set)\n\t\treturn question_sets\n\n\tdef identify_potential_questions(self, sentence: str) -&gt; dict:\n\t\t\"\"\"Method to identiyf potential question sets.\n\n\t\tArgs:\n\t\t\tsentence (str): Tokenized sequence from corpus.\n\n\t\tReturns:\n\t\t\tdict: Question formed along with the correct answer in case of\n\t\t\t\tpotential sentence else return None.\n\t\t\"\"\"\n\t\t# POS tag sequences\n\t\ttry:\n\t\t\t<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span>s = nltk.<span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span>_<span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span>(sentence)</div>\n\t\t\tif tags[0][1] == \"RB\" or len(<div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">nltk.<span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(sentence)</div>) &lt; 4:\n\t\t\t\treturn None\n\t\texcept Exception:\n\t\t\tlogging.exception(\"POS tagging failed.\", exc_info=True)\n\n\t\t# Define regex grammar to chunk keywords\n\t\tnoun_phrases = list()\n\t\tgrammar = r\"\"\"\n\t\t\tCHUNK: {&lt;NN&gt;+&lt;IN|DT&gt;*&lt;NN&gt;+}\n\t\t\t\t{&lt;NN&gt;+&lt;IN|DT&gt;*&lt;NNP&gt;+}\n\t\t\t\t{&lt;NNP&gt;+&lt;NNS&gt;*}\n\t\t\t\"\"\"\n\n\t\t# Create parser tree\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">chunker = nltk.<span class=\"fea_parsing_funcs blodFunc\">Regexp</span><span class=\"fea_parsing_funcs blodFunc\">Parser</span>(<span class=\"fea_parsing_funcs blodFunc\">grammar</span>)</div>\n\t\t<div class=\"highlights fea_tokenization\" id=\"tokenization_2\" style=\"display: inline;\">tokens = nltk.<span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(sentence)</div>\n\t\t<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_1\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span>_tokens = nltk.<span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span>.<span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span>_<span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span>(tokens)</div>\n\t\t<div class=\"highlights fea_parsing\" id=\"parsing_1\" style=\"display: inline;\">tree = chunker.<span class=\"fea_parsing_funcs blodFunc\">parse</span>(pos_tokens)</div>\n\n\t\t# Parse tree to identify tokens\n\t\tfor subtree in tree.subtrees():\n\t\t\tif subtree.label() == \"CHUNK\":\n\t\t\t\ttemp = \"\"\n\t\t\t\tfor sub in subtree:\n\t\t\t\t\ttemp += sub[0]\n\t\t\t\t\ttemp += \" \"\n\t\t\t\ttemp = temp.strip()\n\t\t\t\tnoun_phrases.append(temp)\n\n\t\t# Handle nouns\n\t\treplace_nouns = []\n\t\tfor word, _ in tags:\n\t\t\tfor phrase in noun_phrases:\n\t\t\t\tif phrase[0] == '\\'':\n\t\t\t\t\t# If it starts with an apostrophe, ignore it\n\t\t\t\t\t# (this is a weird error that should probably be handled elsewhere)\n\t\t\t\t\tbreak\n\t\t\t\tif word in phrase:\n\t\t\t\t\t# Blank out the last two words in this phrase\n\t\t\t\t\t[replace_nouns.append(phrase_word) for phrase_word in phrase.split()[-2:]]\n\t\t\t\t\tbreak\n\t\t\t# If we couldn't find the word in any phrases\n\t\t\tif len(replace_nouns) == 0:\n\t\t\t\treplace_nouns.append(word)\n\t\t\tbreak\n\n\t\tif len(replace_nouns) == 0:\n\t\t\treturn None\n\n\t\tval = 99\n\t\tfor i in replace_nouns:\n\t\t\tif len(i) &lt; val:\n\t\t\t\tval = len(i)\n\t\t\telse:\n\t\t\t\tcontinue\n\n\t\ttrivial = {\n\t\t\t\"Answer\": \" \".join(replace_nouns),\n\t\t\t\"Key\": val\n\t\t}\n\n\t\tif len(replace_nouns) == 1:\n\t\t\t# If we're only replacing one word, use WordNet to find similar words\n\t\t\ttrivial[\"Similar\"] = self.answer_options(replace_nouns[0])\n\t\telse:\n\t\t\t# If we're replacing a phrase, don't bother - it's too unlikely to make sense\n\t\t\ttrivial[\"Similar\"] = []\n\n\t\treplace_phrase = \" \".join(replace_nouns)\n\t\tblanks_phrase = (\"__________\" * len(replace_nouns)).strip()\n\t\texpression = re.compile(re.escape(replace_phrase), re.IGNORECASE)\n\t\tsentence = expression.sub(blanks_phrase, str(sentence), count=1)\n\t\ttrivial[\"Question\"] = sentence\n\t\treturn trivial\n\n\t@staticmethod\n\tdef answer_options(word: str) -&gt; list:\n\t\t\"\"\"Method to identify incorrect answer options.\n\n\t\tArgs:\n\t\t\tword (str): Actual answer to the question which is to be used\n\t\t\t\tfor generating other deceiving options.\n\n\t\tReturns:\n\t\t\tlist: Answer options.\n\t\t\"\"\"\n\t\t# In the absence of a better method, take the first synset\n\t\ttry:\n\t\t\tsynsets = wn.synsets(word, pos=\"n\")\n\t\texcept Exception:\n\t\t\tlogging.exception(\"Synsets creation failed.\", exc_info=True)\n\n\t\t# If there aren't any synsets, return an empty list\n\t\tif len(synsets) == 0:\n\t\t\treturn []\n\t\telse:\n\t\t\tsynset = synsets[0]\n\n\t\t# Get the hypernym for this synset (again, take the first)\n\t\thypernym = synset.hypernyms()[0]\n\n\t\t# Get some hyponyms from this hypernym\n\t\thyponyms = hypernym.hyponyms()\n\n\t\t# Take the name of the first lemma for the first 8 hyponyms\n\t\tsimilar_words = []\n\t\tfor hyponym in hyponyms:\n\t\t\tsimilar_word = hyponym.lemmas()[0].name().replace(\"_\", \" \")\n\t\t\tif similar_word != word:\n\t\t\t\tsimilar_words.append(similar_word)\n\t\t\tif len(similar_words) == 8:\n\t\t\t\tbreak\n\t\treturn similar_words\n\t\t#https://github.com/nityansuman/marvin/blob/main/src/objective</code></pre></div></body></html>", "fir_23": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_23\"><pre id=\"fir_23_code\"><code class=\"python\">import wikipedia as wiki\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nimport re\n\nimport text2num as t2n\n\nfrom Quiz import Quiz\nfrom QuestionSentence import QuestionSentence\n\nclass Article():\n\n    def __init__ (self, name):\n        self.name = name\n        self.page = wiki.page(name)\n\n        self.quiz = Quiz([])\n\n        self.generate_questions_for(\n            self.page.content.encode('ascii', 'ignore'))\n\n    ''' \n    NOT CURRENTLY USED, but maye be useful at a later point when knowing the\n    section a question was sourced from might be of use.\n    '''\n    # def iterate_sections(self):\n    #     # Iterate through article's sections\n    #     for section in self.page.sections:\n    #         print section\n    #         sec = self.page.section(section).encode('ascii', 'ignore')\n    #         if sec is None: \n    #             continue\n    #         self.generate_questions_for(sec)\n\n    '''\n    tokenizes and chunks a sentence based on a simple grammar\n    '''\n    def get_question_data(self, s):\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">tokens = nltk.<span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(s)</div>\n        <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span>ged = nltk.<span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span>_<span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span>(tokens)</div>\n        grammar =   \"\"\"  \n                    NUMBER: {&lt;$&gt;*&lt;CD&gt;+&lt;NN&gt;*}\n                    LOCATION: {&lt;IN&gt;&lt;NNP&gt;+&lt;,|IN&gt;&lt;NNP&gt;+} \n                    PROPER: {&lt;NNP|NNPS&gt;&lt;NNP|NNPS&gt;+}\n                    \"\"\"       \n        # \n        # HIT!: {&lt;PROPER&gt;&lt;NN&gt;?&lt;VBZ|VBN&gt;+}\n        # DATE: {&lt;IN&gt;(&lt;$&gt;*&lt;CD&gt;+&lt;NN&gt;*)}\n\n        <div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">chunker = nltk.<span class=\"fea_parsing_funcs blodFunc\">Regexp</span><span class=\"fea_parsing_funcs blodFunc\">Parser</span>(<span class=\"fea_parsing_funcs blodFunc\">grammar</span>)</div>\n        <div class=\"highlights fea_parsing\" id=\"parsing_1\" style=\"display: inline;\">result = chunker.<span class=\"fea_parsing_funcs blodFunc\">parse</span>(tagged)</div>\n        return result\n\n    '''\n    splits a Wikipedia section into sentences and then chunks/tokenizes each\n    sentence\n    '''\n    def generate_questions_for(self, sec):\n        # Rid of all parentheses for easier processing\n        _sec = \"\".join(re.split('\\(', \n            sec.decode(\"utf-8\").replace(\")\", \"(\"))[0::2])\n\n        for sentence in sent_tokenize(_sec):\n            if \"==\" not in sentence:\n                qdata = self.get_question_data(sentence)\n                if len(qdata) &gt;= 75 and len(qdata) &lt;= 150:\n                    qdata = []\n\n                self.create_questions(sentence, qdata)\n\n    '''\n    given a setence in chunked and original form, produce the params necessary\n    to create a Question, and then add that to our Quiz object\n    '''\n    def create_questions(self, sentence, chunked):\n        gaps = []\n        for word in chunked:\n            if type(word) != tuple:                \n                target = []\n                for y in word:\n                    target.append(y[0])\n                orig_phrase = \" \".join(target)\n\n                if word.label() == \"NUMBER\":\n                    modified_phrase = orig_phrase[:]\n\n                    try:\n                        # convert spelled out word to numerical value\n                        modified_phrase = t2n.text2num(phrase)\n                    except:\n                        try:\n                            test = int(modified_phrase) + float(modified_phrase)\n                        except:\n                            # if the word could not be converted and \n                            # was not already numerical, ignore it\n                            continue\n\n                    if self.probably_range(modified_phrase):\n                        return\n\n                    gaps.append((word.label(), orig_phrase, modified_phrase))\n                elif word.label() in [\"LOCATION\", \"PROPER\"]: \n                    gaps.append((word.label(), orig_phrase, orig_phrase))\n\n        if len(gaps) &gt;= 2 and len(gaps) == len(set(gaps)):\n            gaps_filtered = [gap for gap in gaps if gap[0] == 'NUMBER' or gap[0] == 'LOCATION']\n            if len(gaps_filtered) and len(gaps) - len(gaps_filtered) &gt; 2:\n                self.quiz.add(QuestionSentence(sentence, gaps_filtered))\n\n    '''\n    Wikipedia returns non-hyphenated number ranges, so we need to check for mushed together years\n    and remove them. Not a complete solution to the problem, but most of the incidents are years\n    ''' \n    def probably_range(self, val):\n        s = str(val)\n        if s.count(\"19\") &gt; 1 or s.count(\"20\") &gt; 1 or (s.count(\"19\") == 1 and s.count(\"20\") == 1):\n            return True\n        return False\n        #https://github.com/alexgreene/WikiQuiz/blob/master/python/Article</code></pre></div></body></html>", "fir_1": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_1\"><pre id=\"fir_1_code\"><code class=\"python\"># Stopwords removal and lemmatizing them.\n# import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nstop_keywords = [\n    'i',\n    'stack',\n    'overflow',\n    'web',\n    'tutorials',\n    'lesson',\n    'tip',\n    'learn',\n    'reference',\n    'demo',\n    'name',\n    'company',\n    'w3schools',\n    'w3resource',\n    'online',\n    'this',\n]\n\n\ndef text_process(data):\n\n    return_words = []\n\n    for d in data:\n        stop_words = set(<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">stopwords.words('english')</div>)\n\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">word</span>_tokens = <span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(d)</div>\n\n        filtered_sentence = []\n\n        for w in word_tokens:\n            if w not in stop_words:\n                filtered_sentence.append(w)\n        # print(filtered_sentence)\n\n        # Lemmatizing the words.\n        lemma_word = []\n        <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">word</span><span class=\"fea_lemmatization_funcs blodFunc\">net</span>_<span class=\"fea_lemmatization_funcs blodFunc\">lemmatizer</span> = <span class=\"fea_lemmatization_funcs blodFunc\">Word</span><span class=\"fea_lemmatization_funcs blodFunc\">Net</span><span class=\"fea_lemmatization_funcs blodFunc\">Lemmatizer</span>()\n        for w in filtered_sentence:\n            <span class=\"fea_lemmatization_funcs blodFunc\">word</span>1 = <span class=\"fea_lemmatization_funcs blodFunc\">word</span><span class=\"fea_lemmatization_funcs blodFunc\">net</span>_<span class=\"fea_lemmatization_funcs blodFunc\">lemmatizer</span>.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>(w, pos=\"n\")\n            <span class=\"fea_lemmatization_funcs blodFunc\">word</span>2 = <span class=\"fea_lemmatization_funcs blodFunc\">word</span><span class=\"fea_lemmatization_funcs blodFunc\">net</span>_<span class=\"fea_lemmatization_funcs blodFunc\">lemmatizer</span>.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>(<span class=\"fea_lemmatization_funcs blodFunc\">word</span>1, pos=\"v\")\n            worfir = <span class=\"fea_lemmatization_funcs blodFunc\">word</span><span class=\"fea_lemmatization_funcs blodFunc\">net</span>_<span class=\"fea_lemmatization_funcs blodFunc\">lemmatizer</span>.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>(<span class=\"fea_lemmatization_funcs blodFunc\">word</span>2, pos=(\"a\"))</div>\n            lemma_word.append(worfir.lower())\n\n        # print(lemma_word)\n        # Removing unwanted lemmatized words.\n        lemma_word = set(lemma_word)\n        for word in lemma_word.copy():\n            if ((len(word) &lt;= 3) | (word in stop_keywords)):\n                lemma_word.remove(word)\n        # lemma_word = list(lemma_word)\n\n        lemma_word = list(lemma_word)\n    return return_words\n    #https://github.com/dhyeythumar/Search-Engine/blob/master/Python_scripts/text_processing</code></pre></div></body></html>", "fir_2": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_2\"><pre id=\"fir_2_code\"><code class=\"python\"># Tokenize and Stem Data\n# Convert words to Vector Space using TFIDF matrix\n# Calculate Cosine Similarity and generate the distance matrix\n# Uses Ward method to generate an hierarchy\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.manifold import MDS\nimport matplotlibplot as thr\nimport pandas as pd\nfrom scipy.cluster.hierarchy import ward, dendrogram\nimport os\n\n\n# Function to return a list of stemmed words\ndef tokenize_and_stem(text_file):\n    # declaring stemmer and stopwords language\n    <div class=\"highlights fea_stemming\" id=\"stemming_0\" style=\"display: inline;\"><span class=\"fea_stemming_funcs blodFunc\">stemmer</span> = Snowball<span class=\"fea_stemming_funcs blodFunc\">Stemmer</span>(\"english\")</div>\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">stop_words = set(stopwords.words('english'))</div>\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">word</span>s = <span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(text_file)</div>\n    filtered = [w for w in words if w not in stop_words]\n    stems = [stemmer.stem(t) for t in filtered]\n    return stems\n\n\ndef main():\n\n    path = os.path.abspath(os.path.dirname(__file__))\n    data = pd.read_csv(os.path.join(path, 'data\\headlines_cleaned.txt'), names=['text'])\n\n    # text data in dataframe and removing stops words\n    stop_words = set(stopwords.words('english'))\n    data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n\n    # Using TFIDF vectorizer to convert convert words to Vector Space\n    tfidf_vectorizer = TfidfVectorizer(max_features=200000,\n                                       use_idf=True,\n                                       stop_words='english',\n                                       tokenizer=tokenize_and_stem)\n    #                                   ngram_range=(1, 3))\n\n    # Fit the vectorizer to text data\n    tfidf_matrix = tfidf_vectorizer.fit_transform(data['text'])\n\n    # Calculating the distance measure derived from cosine similarity\n    distance = 1 - cosine_similarity(tfidf_matrix)\n\n    # Ward\u2019s method produces a hierarchy of clusterings\n    linkage_matrix = ward(distance)\n    fig, ax = thr.subplots(figsize=(15, 20)) # set size\n    ax = dendrogram(linkage_matrix, orientation=\"top\", labels=data.values)\n    thr.tight_layout()\n    thr.title('News Headlines using Ward Hierarchical Method')\n    thr.savefig(os.path.join(path, 'results\\hierarchical.png'))\n\n\nif __name__ == '__main__':\n    main()\n    #https://github.com/maneeshavinayak/Clustering-News-Headlines/blob/master/clustering/hierarchical</code></pre></div></body></html>", "fir_3": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_3\"><pre id=\"fir_3_code\"><code class=\"python\">import json\nimport numpy as np \nimport urllib\nfrom urllib.request import urlopen\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nimport requests\nimport re\nfrom flask import Flask,request,jsonify,make_response\nimport nltk\nfrom flask_cors import CORS\nimport newspaper\nimport pandas as pd\nimport json\nimport requests\nimport time\n\nurl = \"https://graph.facebook.com/v2.6/me/messages\"\nngrok_url = 'https://the-daily-news-app.herokuapp.com/api/'\n\nsource_csv = pd.read_csv('https://raw.githubusercontent.com/codequipo/TheDailyNews/deploy/sites.csv')\n\napp = Flask(__name__)\nCORS(app)\n\nPORT=8087\n\nimport csv\n\nli_all=[]\nkey_name_all=[]\n\n@app.route('/',methods = ['GET','POST'])\ndef base():\n\treturn 'hello world'\n\n\n@app.route('/webhook', methods=['GET', 'POST'])\ndef webhook():\n\treq = request.get_json(force=True)\n\tprint(\"-\"*80)\n\tprint(req.get('queryResult').get('intent').get('displayName'))\n\tprint(\"-\"*80)\n\tif req.get('queryResult').get('intent').get('displayName') == 'summarize_intent':\n\t\tdatabase_id = req.get('queryResult').get('queryText').strip('Summarize:')\n\t\tsummary_url = ngrok_url + 'getSummary'\n\t\tdata2 = {'unique_id': database_id }\n\t\tsummary = requests.post(summary_url,data = data2)\n\t\tarticle_dict = json.loads(summary.text)\n\t\tarticle_summary = article_dict['article'][0]['text']\n\t\tarticl_image = article_dict['article'][0]['top_image']\n\t\treturn {'fulfillmentText': article_summary}\n\n\tif req.get('queryResult').get('intent').get('displayName') == 'source_intent':\n\t\tsource_name = req.get('queryResult').get('queryText')[7:]\n\t\tsource_csv.columns = ['name','link']\n\t\tsource_link = source_csv['link'][source_csv.loc[source_csv['name']==source_name].index[0]]\n\t\tnews_url = ngrok_url + 'getnewsbysources'\n\t\tdata1 = {'main_urls':source_link }\n\t\tpost_articles = requests.post(news_url,data = data1)\n\t\tlist_of_articles = json.loads(post_articles.text)\n\t\tli = list_of_articles['articles']\n\t\tmessages = []\n\t\tprint(len(li))\n\t\tfor index,item in enumerate(li):\n\t\t\ttemp = dict()\n\t\t\t\n\t\t\treq = urllib.request.Request('https://raw.githubusercontent.com/codequipo/TheDailyNews/flask_deploy/format.json')\n\t\t\twith urllib.request.urlopen(req) as f:\n    \t\t\t\ttemp = json.load(f)\n\t\t\t\n\t\t\t\t\n\t\t\ttemp['card']['buttons'][0]['postback'] = 'Summarize:'+item['unique_id']\n\t\t\ttemp['card']['title'] = item['title']\n\t\t\ttemp['card']['imageUri'] = item['top_image']\n\t\t\tprint(item['url'])\n\t\t\ttemp['card']['buttons'][1]['postback'] = item['url']\n\t\t\tmessages.append(temp)\n\n\t\tmessages = messages[:10]\n\t\t# print(messages)\n\t\treturn jsonify({'fulfillmentMessages': messages })  \n\t\n\n\treturn{'fulfillmentText':\"Please check your responses again  \"}\n\ndef getSourceData():\n\turl = 'https://raw.githubusercontent.com/codequipo/TheDailyNews/deploy/sites.csv'\n\tdf = pd.read_csv(url, error_bads=False)\n\turl_list = []\n\tkey_list = []\n\turl_list = df[\"http://www.huffingtonpost.com\"].values.tolist()\n\tkey_list = df[\"huffingtonpost\"].values.tolist()\n\turl_list = [\"http://www.huffingtonpost.com\"] + url_list\n\tkey_list = [\"huffingtonpost\"] + key_list\n\treturn key_list,url_list\n\nkey_list, url_list= getSourceData()\n\n@app.route(\"/db\",methods=['POST','GET'])\ndef build_database():\n\ttic=time.time()\n\t# key_list,url_list = getSourceData()\n\tjson_body=request.get_json(force=True)\n\tcurrCount = int(json_body.get('currCount'))\n\tnumOfSources = int(json_body.get('numOfSources'))\n\tnumOfArticlesPerSources = int(json_body.get('numOfArticlesPerSources'))\n\tnum_of_sentences_in_summary = int(json_body.get('num_of_sentences_in_summary'))\n\n\tprint('currCount : '+str(currCount))\n\n\t\n\tresponse_data=dict()\n\tfor i in range(currCount,currCount+numOfSources):\n\t\turl=url_list[i]\n\t\tsource = newspaper.build( url, memoize_articles=True, language='en')\n\t\t\n\t\td=dict() # Holds articles from current selected source \n\t\tk=0\n\t\t\n\t\tfor article in source.articles:\n\t\t\ttry:\n\t\t\t\tarticle.download() \n\t\t\t\tarticle.parse() \n\t\t\t\tsummary = driver(article.text,num_of_sentences_in_summary)\n\t\t\t\t\n\t\t\t\tarticle_info=dict()\n\t\t\t\tarticle_info['url']=article.url\n\t\t\t\tarticle_info['title']=article.title\n\t\t\t\tprint('i:'+str(i)+'  k:'+str(k)+'  title  =&gt; '+article_info['title'])\n\t\t\t\tarticle_info['text']=summary\n\t\t\t\tarticle_info['top_image']=article.top_image\n\n\t\t\t\td[k]=article_info\n\t\t\t\t\n\t\t\t\t\n\t\t\t\tk+=1\n\t\t\t\tif k == numOfArticlesPerSources:\n\t\t\t\t\tbreak\n\t\t\texcept Exception as e:\n\t\t\t\tprint(\"Entered except block :\"+str(e))\n\t\t\t\tpass\n\t\td['length']=k\n\t\tresponse_data[url]=d\n\n\t\tprint(url+\"   NewArticles : \"+str(k))\n\n\tresult={\n\t\t'success':True,\n\t\t'alldata':response_data,\n\t\t'allsite':url_list[currCount:currCount+numOfSources],\n\t\t'allsite_key':key_list[currCount:currCount+numOfSources]\n\t}\n\ttoc=time.time()\n\tdiff=toc-tic\n\tprint(\"# Time required for function to execute is :\"+str(diff)+\" # \")\n\tprint()\n\tprint()\n\treturn json.dumps(result)\n\t\n\n\ndef clean(sentences):\n\tlemmatizer = WordNetLemmatizer()\n\tcleaned_sentences = []\n\tfor sentence in sentences:\n\t\tsentence = sentence.lower()\n\t\tsentence = re.sub(r'[^a-zA-Z]',' ',sentence)\n\t\tsentence = sentence.split()\n\t\tsentence = [<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">lemmatizer</span>.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>(<span class=\"fea_lemmatization_funcs blodFunc\">word</span>)</div> for word in sentence if word not in <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">set(stopwords.words('english'))</div>]\n\t\tsentence = ' '.join(sentence)\n\t\tcleaned_sentences.append(sentence)\n\treturn cleaned_sentences\n\ndef init_probability(sentences):\n\tprobability_dict = {}\n\t<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">word</span>s = <span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>('. '.join(sentences))</div>\n\ttotal_words = len(set(words))\n\tfor word in words:\n\t\tif word!='.':\n\t\t\tif not probability_dict.get(word):\n\t\t\t\tprobability_dict[word] = 1\n\t\t\telse:\n\t\t\t\tprobability_dict[word] += 1\n\n\tfor word,count in probability_dict.items():\n\t\tprobability_dict[word] = count/total_words \n\t\n\treturn probability_dict\n\ndef update_probability(probability_dict,word):\n\tif probability_dict.get(word):\n\t\tprobability_dict[word] = probability_dict[word]**2\n\treturn probability_dict\n\ndef average_sentence_weights(sentences,probability_dict):\n\tsentence_weights = {}\n\tfor index,sentence in enumerate(sentences):\n\t\tif len(sentence) != 0:\n\t\t\taverage_proba = sum([probability_dict[word] for word in sentence if word in probability_dict.keys()])\n\t\t\taverage_proba /= len(sentence)\n\t\t\tsentence_weights[index] = average_proba \n\treturn sentence_weights\n\ndef generate_summary(sentence_weights,probability_dict,cleaned_article,tokenized_article,summary_length = 30):\n\tsummary = \"\"\n\tcurrent_length = 0\n\twhile current_length &lt; summary_length :\n\t\thighest_probability_word = max(probability_dict,key=probability_dict.get)\n\t\tsentences_with_max_word= [index for index,sentence in enumerate(cleaned_article) if highest_probability_word in <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">set(<span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(sentence))</div>]\n\t\tsentence_list = sorted([[index,sentence_weights[index]] for index in sentences_with_max_word],key=lambda x:x[1],reverse=True)\n\t\tsummary += tokenized_article[sentence_list[0][0]] + \"\\n\"\n\t\tfor word in <div class=\"highlights fea_tokenization\" id=\"tokenization_2\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(cleaned_article[sentence_list[0][0]])</div>:\n\t\t\tprobability_dict = update_probability(probability_dict,word)\n\t\tcurrent_length+=1\n\treturn summary\n\ndef driver(article,required_length):\n\trequired_length = int(required_length)\n\t<div class=\"highlights fea_tokenization\" id=\"tokenization_3\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>d_article = sent_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(article)</div>\n\tcleaned_article = clean(tokenized_article) \n\tprobability_dict = init_probability(cleaned_article)\n\tsentence_weights = average_sentence_weights(cleaned_article,probability_dict)\n\t<div class=\"highlights fea_summarizer\" id=\"summarizer_0\" style=\"display: inline;\">summary = generate_summary(sentence_weights,probability_dict,cleaned_article,tokenized_article,required_length)</div>\n\treturn summary\n\nif __name__ == \"__main__\":\n    app.run(port=PORT)\n    #https://github.com/codequipo/TheDailyNews/blob/master/flask_server/app</code></pre></div></body></html>", "fir_6": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_6\"><pre id=\"fir_6_code\"><code class=\"python\">import os,re,math,csv,string,random,logging,glob,itertools,operator,sys\nfrom os import listdir\nfrom os.path import isfile, join\nfrom collections import Counter, defaultdict, OrderedDict\nfrom itertools import chain, combinations\n\nimport pandas as pd\nimport numpy as np\nimport scipy\nfrom scipy import spatial\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet as wn\nfrom nltk.tag.stanford import StanfordPOSTagger\nfrom nltk.util import ngrams\n\nimport gensim\nfrom gensim.models import word2vec\n\ndef InitialCleanup(dataframe,\n                   minwords=2,\n                   use_filler_list=None,\n                   filler_regex_and_list=False):\n\n    \"\"\"\n    Perform basic text cleaning to prepare dataframe\n    for analysis. Remove non-letter/-space characters,\n    empty turns, turns below a minimum length, and\n    fillers.\n\n    By default, preserves turns 2 words or longer.\n    If desired, this may be changed by updating the\n    `minwords` argument.\n\n    By default, remove common fillers through regex.\n    If desired, remove other words by passing a list\n    of literal strings to `use_filler_list` argument,\n    and if both regex and list of additional literal\n    strings are to be used, update `filler_regex_and_list=True`.\n    \"\"\"\n\n    # only allow strings, spaces, and newlines to pass\n    WHITELIST = string.ascii_letters + '\\'' + ' '\n\n    # remove inadvertent empty turns\n    dataframe = dataframe[pd.notnull(dataframe['content'])]\n\n    # internal function: remove fillers via regular expressions\n    def applyRegExpression(textFiller):\n        textClean = re.sub('^(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]+\\s', ' ', textFiller) # at the start of a string\n        textClean = re.sub('\\s(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]+\\s', ' ', textClean) # within a string\n        textClean = re.sub('\\s(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]$', ' ', textClean) # end of a string\n        textClean = re.sub('^(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]$', ' ', textClean) # if entire turn string\n        return textClean\n\n    # create a new column with only approved text before cleaning per user-specified settings\n    dataframe['clean_content'] = dataframe['content'].apply(lambda utterance: ''.join([char for char in utterance if char in WHITELIST]).lower())\n\n    # DEFAULT: remove typical speech fillers via regular expressions (examples: \"um, mm, oh, hm, uh, ha\")\n    if use_filler_list is None and not filler_regex_and_list:\n        dataframe['clean_content'] = dataframe['clean_content'].apply(applyRegExpression)\n\n    # OPTION 1: remove speech fillers or other words specified by user in a list\n    elif use_filler_list is not None and not filler_regex_and_list:\n        dataframe['clean_content'] = dataframe['clean_content'].apply(lambda utterance: ' '.join([word for word in utterance.split(\" \") if word not in use_filler_list]))\n\n    # OPTION 2: remove speech fillers via regular expression and any additional words from user-specified list\n    elif use_filler_list is not None and filler_regex_and_list:\n        dataframe['clean_content'] = dataframe['clean_content'].apply(applyRegExpression)\n        dataframe['clean_content'] = dataframe['clean_content'].apply(lambda utterance: ' '.join([word for word in utterance.split(\" \") if word not in use_filler_list]))\n\n    # OPTION 3: nothing is filtered\n    else:\n        dataframe['clean_content'] = dataframe['clean_content']\n\n    # drop the old \"content\" column and rename the clean \"content\" column\n    dataframe = dataframe.drop(['content'],axis=1)\n    dataframe = dataframe.rename(index=str,\n                                 columns ={'clean_content': 'content'})\n\n    # remove rows that are now blank or do not meet `minwords` requirement, then drop length column\n    dataframe['utteranceLen'] = dataframe['content'].apply(lambda x: <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(x)).str.len()</div>\n    dataframe = dataframe.drop(dataframe[dataframe.utteranceLen &lt; int(minwords)].index).drop(['utteranceLen'],axis=1)\n    dataframe = dataframe.reset_index(drop=True)\n\n    # return the cleaned dataframe\n    return dataframe\n\ndef AdjacentMerge(dataframe):\n\n    \"\"\"\n    Given a dataframe of conversation turns,\n    merge adjacent turns by the same speaker.\n    \"\"\"\n\n    repeat=1\n    while repeat==1:\n        l1=len(dataframe)\n        DfMerge = []\n        k = 0\n        if len(dataframe) &gt; 0:\n            while k &lt; len(dataframe)-1:\n                if dataframe['participant'].iloc[k] != dataframe['participant'].iloc[k+1]:\n                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])\n                    k = k + 1\n                elif dataframe['participant'].iloc[k] == dataframe['participant'].iloc[k+1]:\n                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k] + \" \" + dataframe['content'].iloc[k+1]])\n                    k = k + 2\n            if k == len(dataframe)-1:\n                DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])\n\n        dataframe=pd.DataFrame(DfMerge,columns=('participant','content'))\n        if l1==len(dataframe):\n            repeat=0\n\n    return dataframe\n\ndef Tokenize(text,nwords):\n    \"\"\"\n    Given list of text to be processed and a list\n    of known words, return a list of edited and\n    tokenized words.\n\n    Spell-checking is implemented using a\n    Bayesian spell-checking algorithm\n    (http://norvig.com/spell-correct.html).\n\n    By default, this is based on the Project Gutenberg\n    corpus, a collection of approximately 1 million texts\n    (http://www.gutenberg.org). A copy of this is included\n    within this package. If desired, users may specify a\n    different spell-check training corpus in the\n    `training_dictionary` argument of the\n    `prepare_transcripts()` function.\n\n    \"\"\"\n\n    # internal function: identify possible spelling errors for a given word\n    def edits1(word):\n        splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes    = [a + b[1:] for a, b in splits if b]\n        transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)&gt;1]\n        replaces   = [a + c + b[1:] for a, b in splits for c in string.ascii_lowercase if b]\n        inserts    = [a + c + b     for a, b in splits for c in string.ascii_lowercase]\n        return set(deletes + transposes + replaces + inserts)\n\n    # internal function: identify known edits\n    def known_edits2(word,nwords):\n        return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in nwords)\n\n    # internal function: identify known words\n    def known(words,nwords): return set(w for w in words if w in nwords)\n\n    # internal function: correct spelling\n    def correct(word,nwords):\n        candidates = known([word],nwords) or known(edits1(word),nwords) or known_edits2(word,nwords) or [word]\n        return max(candidates, key=nwords.get)\n\n    # expand out based on a fixed list of common contractions\n    contract_dict = { \"ain't\": \"is not\",\n        \"aren't\": \"are not\",\n        \"can't\": \"cannot\",\n        \"can't've\": \"cannot have\",\n        \"'cause\": \"because\",\n        \"could've\": \"could have\",\n        \"couldn't\": \"could not\",\n        \"couldn't've\": \"could not have\",\n        \"didn't\": \"did not\",\n        \"doesn't\": \"does not\",\n        \"don't\": \"do not\",\n        \"hadn't\": \"had not\",\n        \"hadn't've\": \"had not have\",\n        \"hasn't\": \"has not\",\n        \"haven't\": \"have not\",\n        \"he'd\": \"he had\",\n        \"he'd've\": \"he would have\",\n        \"he'll\": \"he will\",\n        \"he'll've\": \"he will have\",\n        \"he's\": \"he is\",\n        \"how'd\": \"how did\",\n        \"how'd'y\": \"how do you\",\n        \"how'll\": \"how will\",\n        \"how's\": \"how is\",\n        \"i'd\": \"i would\",\n        \"i'd've\": \"i would have\",\n        \"i'll\": \"i will\",\n        \"i'll've\": \"i will have\",\n        \"i'm\": \"i am\",\n        \"i've\": \"i have\",\n        \"isn't\": \"is not\",\n        \"it'd\": \"it would\",\n        \"it'd've\": \"it would have\",\n        \"it'll\": \"it will\",\n        \"it'll've\": \"it will have\",\n        \"it's\": \"it is\",\n        \"let's\": \"let us\",\n        \"ma'am\": \"madam\",\n        \"mayn't\": \"may not\",\n        \"might've\": \"might have\",\n        \"mightn't\": \"might not\",\n        \"mightn't've\": \"might not have\",\n        \"must've\": \"must have\",\n        \"mustn't\": \"must not\",\n        \"mustn't've\": \"must not have\",\n        \"needn't\": \"need not\",\n        \"needn't've\": \"need not have\",\n        \"o'clock\": \"of the clock\",\n        \"oughtn't\": \"ought not\",\n        \"oughtn't've\": \"ought not have\",\n        \"shan't\": \"shall not\",\n        \"sha'n't\": \"shall not\",\n        \"shan't've\": \"shall not have\",\n        \"she'd\": \"she would\",\n        \"she'd've\": \"she would have\",\n        \"she'll\": \"she will\",\n        \"she'll've\": \"she will have\",\n        \"she's\": \"she is\",\n        \"should've\": \"should have\",\n        \"shouldn't\": \"should not\",\n        \"shouldn't've\": \"should not have\",\n        \"so've\": \"so have\",\n        \"so's\": \"so as\",\n        \"that'd\": \"that had\",\n        \"that'd've\": \"that would have\",\n        \"that's\": \"that is\",\n        \"there'd\": \"there would\",\n        \"there'd've\": \"there would have\",\n        \"there's\": \"there is\",\n        \"they'd\": \"they would\",\n        \"they'd've\": \"they would have\",\n        \"they'll\": \"they will\",\n        \"they'll've\": \"they will have\",\n        \"they're\": \"they are\",\n        \"they've\": \"they have\",\n        \"to've\": \"to have\",\n        \"wasn't\": \"was not\",\n        \"we'd\": \"we would\",\n        \"we'd've\": \"we would have\",\n        \"we'll\": \"we will\",\n        \"we'll've\": \"we will have\",\n        \"we're\": \"we are\",\n        \"we've\": \"we have\",\n        \"weren't\": \"were not\",\n        \"what'll\": \"what will\",\n        \"what'll've\": \"what will have\",\n        \"what're\": \"what are\",\n        \"what's\": \"what is\",\n        \"what've\": \"what have\",\n        \"when's\": \"when is\",\n        \"when've\": \"when have\",\n        \"where'd\": \"where did\",\n        \"where's\": \"where is\",\n        \"where've\": \"where have\",\n        \"who'll\": \"who will\",\n        \"who'll've\": \"who will have\",\n        \"who's\": \"who is\",\n        \"who've\": \"who have\",\n        \"why's\": \"why is\",\n        \"why've\": \"why have\",\n        \"will've\": \"will have\",\n        \"won't\": \"will not\",\n        \"won't've\": \"will not have\",\n        \"would've\": \"would have\",\n        \"wouldn't\": \"would not\",\n        \"wouldn't've\": \"would not have\",\n        \"y'all\": \"you all\",\n        \"y'all'd\": \"you all would\",\n        \"y'all'd've\": \"you all would have\",\n        \"y'all're\": \"you all are\",\n        \"y'all've\": \"you all have\",\n        \"you'd\": \"you would\",\n        \"you'd've\": \"you would have\",\n        \"you'll\": \"you will\",\n        \"you'll've\": \"you will have\",\n        \"you're\": \"you are\",\n        \"you've\": \"you have\" }\n    contractions_re = re.compile('(%s)' % '|'.join(list(contract_dict.keys())))\n\n    # internal function:\n    def expand_contractions(text, contractions_re=contractions_re):\n        def replace(match):\n            return contract_dict[match.group(0)]\n        return contractions_re.sub(replace, text.lower())\n\n    # process all words in the text\n    cleantoken = []\n    text = expand_contractions(text)\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">token = <span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(text)</div>\n    for word in token:\n        if \"'\" not in word:\n            cleantoken.append(correct(word,nwords))\n        else:\n            cleantoken.append(word)\n    return cleantoken\n\n\ndef pos_to_wn(tag):\n    \"\"\"\n    Convert NLTK default tagger output into a format that Wordnet\n    can use in order to properly lemmatize the text.\n    \"\"\"\n\n    # create some inner functions for simplicity\n    def is_noun(tag):\n        return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n    def is_verb(tag):\n        return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n    def is_adverb(tag):\n        return tag in ['RB', 'RBR', 'RBS']\n    def is_adjective(tag):\n        return tag in ['JJ', 'JJR', 'JJS']\n\n    # check each tag against possible categories\n    <div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">if is_noun(tag):\n        return wn.NOUN\n    elif is_verb(tag):\n        return wn.VERB\n    elif is_adverb(tag):\n        return wn.ADV\n    elif is_adjective(tag):\n        return wn.ADJ\n    else:\n        return wn.NOUN</div>\n\n\ndef Lemmatize(tokenlist):\n    l<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">emmatizer = <span class=\"fea_lemmatization_funcs blodFunc\">Word</span><span class=\"fea_lemmatization_funcs blodFunc\">Net</span><span class=\"fea_lemmatization_funcs blodFunc\">Lemmatizer</span>()</div>\n    <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\">default<span class=\"fea_Part_of_Speech_funcs blodFunc\">Pos</span> = nltk.<span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span>_<span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span>(tokenlist)</div> # get the POS tags from NLTK default tagger\n    words_lemma = []\n    for item in defaultPos:\n        words_lemma.append(<div class=\"highlights fea_lemmatization\" id=\"lemmatization_1\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">lemmatizer</span>.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>(item[0],pos_to_wn(item[1]))</div>) # need to convert POS tags to a format (NOUN, VERB, ADV, ADJ) that wordnet uses to lemmatize\n    return words_lemma\n\n\ndef ApplyPOSTagging(df,\n                    filename,\n                    add_stanford_tags=False,\n                    stanford_pos_path=None,\n                    stanford_language_path=None):\n\n    \"\"\"\n    Given a dataframe of conversation turns, return a new\n    dataframe with part-of-speech tagging. Add filename\n    (given as string) as a new column in returned dataframe.\n\n    By default, return only tags from the NLTK default POS\n    tagger. Optionally, also return Stanford POS tagger\n    results by setting `add_stanford_tags=True`.\n\n    If Stanford POS tagging is desired, specify the\n    location of the Stanford POS tagger with the\n    `stanford_pos_path` argument. Also note that the\n    default language model for the Stanford tagger is\n    English (english-left3words-distsim.tagger). To change\n    language model, specify the location with the\n    `stanford_language_path` argument.\n\n    \"\"\"\n\n    # if desired, import Stanford tagger\n    if add_stanford_tags:\n        if stanford_pos_path is None or stanford_language_path is None:\n            raise ValueError('Error! Specify path to Stanford POS tagger and language model using the `stanford_pos_path` and `stanford_language_path` arguments')\n        else:\n            <div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">stanford_tagger = StanfordPOSTagger(stanford_pos_path + stanford_language_path,\n                                                stanford_pos_path + 'stanford-postagger.jar')</div>\n\n    # add new columns to dataframe\n    df['tagged_token'] = df['token'].apply(<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_1\" style=\"display: inline;\">nltk.<span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span>_<span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span></div>)\n    df['tagged_lemma'] = df['lemma'].apply(nltk.pos_tag)\n\n    # if desired, also tag with Stanford tagger\n    if add_stanford_tags:\n        df['tagged_stan_token'] = df['token'].apply(<div class=\"highlights fea_tagger\" id=\"tagger_1\" style=\"display: inline;\">stanford_tagger.tag</div>)\n        df['tagged_stan_lemma'] = df['lemma'].apply(stanford_tagger.tag)\n\n    df['file'] = filename\n\n    # return finished dataframe\n    return df\n\ndef prepare_transcripts(input_files,\n                        output_file_directory,\n                        training_dictionary=None,\n                        minwords=2,\n                        use_filler_list=None,\n                        filler_regex_and_list=False,\n                        add_stanford_tags=False,\n                        stanford_pos_path=None,\n                        stanford_language_path=None,\n                        input_as_directory=True,\n                        save_concatenated_dataframe=True):\n\n    \"\"\"\n    Prepare transcripts for similarity analysis.\n\n    Given individual .txt files of conversations,\n    return a completely prepared dataframe of transcribed\n    conversations for later ALIGN analysis, including: text\n    cleaning, merging adjacent turns, spell-checking,\n    tokenization, lemmatization, and part-of-speech tagging.\n    The output serve as the input for later ALIGN\n    analysis.\n\n    Parameters\n    ----------\n\n    input_files : str (directory name) or list of str (file names)\n        Raw files to be cleaned. Behavior governed by `input_as_directory`\n        parameter as well.\n\n    output_file_directory : str\n        Name of directory where output for individual conversations will be\n        saved.\n\n    training_dictionary : str, optional (default: None)\n        Specify whether to train the spell-checking dictionary using a\n        provided file name (str) or the default Project\n        Gutenberg corpus [http://www.gutenberg.org] (None).\n\n    minwords : int, optional (2)\n        Specify the minimum number of words in a turn. Any turns with fewer\n        than the minimum number of words will be removed from the corpus.\n        (Note: `minwords` must be equal to or greater than `maxngram` provided\n        to `calculate_alignment()` and `calculate_baseline_alignment` in later\n        steps.)\n\n    use_filler_list : list of str, optional (default: None)\n        Specify whether words should be filtered from all conversations using a\n        list of filler words (list of str) or using regular expressions to\n        filter out common filler words (None). Behavior governed by\n        `filler_regex_and_list` parameter as well.\n\n    filler_regex_and_list : boolean, optional (default: False)\n        If providing a list to `use_filler_list` parameter, specify whether to\n        use only the provided list (False) or to use both the provided list and\n        the regular expression filter (True).\n\n    add_stanford_tags : boolean, optional (default: False)\n        Specify whether to return part-of-speech similarity scores based on\n        Stanford POS tagger in addition to the Penn POS tagger (True) or to\n        return only POS similarity scores from the Penn tagger (False). (Note:\n        Including Stanford POS tags will lead to a significant increase in\n        processing time.)\n\n    stanford_pos_path : str, optional (default: None)\n        If Stanford POS tagging is desired, specify local path to Stanford POS\n        tagger.\n\n    stanford_language_path : str, optional (default: None)\n        If Stanford POS tagging is desired, specify local path to Stanford POS\n        tagger for the desired language (str) or use the default English tagger\n        (None).\n\n    input_as_directory : boolean, optional (default: True)\n        Specify whether the value passed to `input_files` parameter should\n        be read as a directory (True) or a list of files to be processed\n        (False).\n\n    save_concatenated_dataframe : boolean, optional (default: True)\n        Specify whether to save the individual conversation output data only\n        as individual files in the `output_file_directory` (False) or to save\n        the individual files as well as a single concatenated dataframe (True).\n\n    Returns\n    -------\n\n    prepped_df : Pandas DataFrame\n        A single concatenated dataframe of all transcripts, ready for\n        processing with `calculate_alignment()` and\n        `calculate_baseline_alignment()`.\n\n    \"\"\"\n\n    # create an internal function to train the model\n    def train(features):\n        model = defaultdict(lambda: 1)\n        for f in features:\n            model[f] += 1\n        return model\n\n    # if no training dictionary is specified, use the Gutenberg corpus\n    if training_dictionary is None:\n\n        # first, get the name of the package directory\n        module_path = os.path.dirname(os.path.abspath(__file__))\n\n        # then construct the path to the text file\n        training_dictionary = os.path.join(module_path, 'data/gutenberg.txt')\n\n    # train our spell-checking model\n    nwords = train(re.findall('[a-z]+', (open(training_dictionary).read().lower())))\n\n    # grab the appropriate files\n    if not input_as_directory:\n        file_list = glob.glob(input_files)\n    else:\n        file_list = glob.glob(input_files+\"/*.txt\")\n\n    # cycle through all files\n    prepped_df = pd.DataFrame()\n    for fileName in file_list:\n\n        # let us know which file we're processing\n        print((\"Processing: \"+fileName))\n        dataframe = pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n\n        # clean up, merge, spellcheck, tokenize, lemmatize, and POS-tag\n        dataframe = InitialCleanup(dataframe,\n                                   minwords=minwords,\n                                   use_filler_list=use_filler_list,\n                                   filler_regex_and_list=filler_regex_and_list)\n        dataframe = AdjacentMerge(dataframe)\n\n        # tokenize and lemmatize\n        dataframe['token'] = dataframe['content'].apply(Tokenize,\n                                     args=(nwords,))\n        dataframe['lemma'] = dataframe['token'].apply(Lemmatize)\n\n        # apply part-of-speech tagging\n        dataframe = ApplyPOSTagging(dataframe,\n                                    filename=os.path.basename(fileName),\n                                    add_stanford_tags=add_stanford_tags,\n                                    stanford_pos_path=stanford_pos_path,\n                                    stanford_language_path=stanford_language_path)\n\n        # export the conversation's dataframe as a CSV\n        conversation_file = os.path.join(output_file_directory,os.path.basename(fileName))\n        dataframe.to_csv(conversation_file, encoding='utf-8',index=False,sep='\\t')\n        prepped_df = prepped_df.append(dataframe)\n\n    # save the concatenated dataframe\n    if save_concatenated_dataframe:\n        concatenated_file = os.path.join(output_file_directory,'../align_concatenated_dataframe.txt')\n        prepped_df.to_csv(concatenated_file,\n                    encoding='utf-8',index=False, sep='\\t')\n\n    # return the dataframe\n    return prepped_df\n    #https://github.com/nickduran/align-linguistic-alignment/blob/master/align/prepare_transcripts</code></pre></div></body></html>", "fir_8": "<html><body><div class=\"codeBlock hljs xml\" id=\"fir_8\"><pre id=\"fir_8_code\"><code class=\"python\"># -*- coding: utf-8 -*-\n# Implementation from https://dev.to/davidisrawi/build-a-quick-summarizer-with-python-and-nltk\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\nfrom framework.parser.parser import Parser\n\ntext_str = '''\n&lt;header&gt;My name is Wil Wheaton. I Live With Chronic Depression and Generalized Anxiety. I Am Not Ashamed.&lt;/header&gt;&lt;p&gt;Before I begin, I want to warn you that this talk touches on many triggering subjects, including self-harm and suicide. I also want you to know that I\u2019m speaking from my personal experience, and that if you or someone you know may be living with mental illness, please talk to a licensed and qualified medical professional, because I am not a doctor.&lt;/p&gt;&lt;p&gt;Okay, let\u2019s do this.&lt;/p&gt;&lt;p&gt;Hi, I\u2019m Wil Wheaton. I\u2019m 45 years-old, I have a wonderful wife, two adult children who make me proud every day, and a daughter in-law who I love like she\u2019s my own child. I work on the most popular comedy series in the world, I\u2019ve been a New York Times Number One Bestselling Audiobook narrator, I have run out of space in my office for the awards I\u2019ve received for my work, and as a white, heterosexual, cisgender man in America, I live life on the lowest difficulty setting \u2014 with the Celebrity cheat enabled.&lt;/p&gt;&lt;p&gt;&lt;b&gt;My life is, by every objective measurement, very very good.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;And in spite of all of that, I struggle every day with my self esteem, my self worth, and my value not only as an actor and writer, but as a human being.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;That\u2019s because I live with Depression and Anxiety, the tag team champions of the World Wrestling With Mental Illness Federation.&lt;/p&gt;&lt;p&gt;And I\u2019m not ashamed to stand here, in front of six hundred people in this room, and millions more online, and proudly say that I live with mental illness, and that\u2019s okay. I say \u201cwith\u201d because even though my mental illness tries its best, it doesn\u2019t control me, it doesn\u2019t define me, and I refuse to be stigmatized by it.&lt;/p&gt;&lt;p&gt;&lt;b&gt;So. My name is Wil Wheaton, and I have Chronic Depression.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;It took me over thirty years to be able to say those ten words, and I suffered for most of them as a result. I suffered because though we in America have done a lot to help people who live with mental illness, we have not done nearly enough to make it okay for our fellow travelers on the wonky brain express to reach out and accept that help.&lt;/p&gt;&lt;p&gt;I\u2019m here today to talk with you about working to end the stigma and prejudice that surrounds mental illness in America, and as part of that, I want to share my story with you.&lt;/p&gt;&lt;p&gt;When I was a little kid, probably seven or eight years old, I started having panic attacks. Back then, we didn\u2019t know that\u2019s what they were, and because they usually happened when I was asleep, the adults in my life just thought I had nightmares. Well, I did have nightmares, but they were so much worse than just bad dreams. Night after night, I\u2019d wake up in absolute terror, and night after night, I\u2019d drag my blankets off my bed, to go to sleep on the floor in my sister\u2019s bedroom, because I was so afraid to be alone.&lt;/p&gt;&lt;p&gt;There were occasional stretches of relief, sometimes for months at a time, and during those months, I felt like what I considered to be a normal kid, but the panic attacks always came back, and each time they came back, they seemed worse than before.&lt;/p&gt;&lt;p&gt;When I was around twelve or thirteen, my anxiety began to express itself in all sorts of delightful ways.&lt;/p&gt;&lt;p&gt;I worried about everything. I was tired all the time, and irritable most of the time. I had no confidence and terrible self-esteem. I felt like I couldn\u2019t trust anyone who wanted to be close to me, because I was convinced that I was stupid and worthless and the only reason anyone would want to be my friend was to take advantage of my fame.&lt;/p&gt;&lt;p&gt;This is important context. When I was thirteen, I was in an internationally-beloved film called Stand by Me, and I was famous. Like, really famous, like, can\u2019t-go-to-the-mall-with-my-friends-without-getting-mobbed famous, and that meant that all of my actions were scrutinized by my parents, my peers, my fans, and the press. All the weird, anxious feelings I had all the time? I\u2019d been raised to believe that they were shameful. That they reflected poorly on my parents and my family. That they should be crammed down deep inside me, shared with nobody, and kept secret.&lt;/p&gt;&lt;p&gt;My panic attacks happened daily, and not just when I was asleep. When I tried to reach out to the adults in my life for help, they didn\u2019t take me seriously. When I was on the set of a tv show or commercial, and I was having a hard time breathing because I was so anxious about making a mistake and getting fired? The directors and producers complained to my parents that I was being difficult to work with. When I was so uncomfortable with my haircut or my crooked teeth and didn\u2019t want to pose for teen magazine photos, the publicists told me that I was being ungrateful and trying to sabotage my success. When I couldn\u2019t remember my lines, because I was so anxious about things I can\u2019t even remember now, directors would accuse me of being unprofessional and unprepared. And that\u2019s when my anxiety turned into depression.&lt;/p&gt;&lt;header&gt;(I\u2019m going to take a moment for myself right now, and I\u2019m going to tear a hole in the fabric of spacetime and I\u2019m going to tell all those adults from the past: give this kid a break. He\u2019s scared. He\u2019s confused. He is doing the best he can, and if you all could stop seeing him as a way to put money into your pockets, maybe you could see that he\u2019s suffering and needs help.)&lt;/header&gt;&lt;p&gt;I was miserable a lot of the time, and it didn\u2019t make any sense. I was living a childhood dream, working on Star Trek: The Next Generation, and getting paid to do what I loved. I had all the video games and board games I ever wanted, and did I mention that I was famous?&lt;/p&gt;&lt;p&gt;I struggled to reconcile the facts of my life with the reality of my existence. I knew something was wrong with me, but I didn\u2019t know what. And because I didn\u2019t know what, I didn\u2019t know how to ask for help.&lt;/p&gt;&lt;p&gt;I wish I had known that I had a mental illness that could be treated! I wish I had known that that the way I felt wasn\u2019t normal and it wasn\u2019t necessary. I wish I had known that I didn\u2019t deserve to feel bad, all the time.&lt;/p&gt;&lt;p&gt;And I didn\u2019t know those things, because Mental Illness was something my family didn\u2019t talk about, and when they did, they talked about it like it was something that happened to someone else, and that it was something they should be ashamed of, because it was a result of something they did. This prejudice existed in my family in spite of the ample incidence of mental illness that ran rampant through my DNA, featuring successful and unsuccessful suicide attempts by my relations, more than one case of bipolar disorder, clinical depression everywhere, and, because of self-medication, so much alcoholism, it was actually notable when someone didn\u2019t have a drinking problem.&lt;/p&gt;&lt;p&gt;Now, I don\u2019t blame my parents for how they addressed \u2014 or more accurately didn\u2019t address \u2014 my mental illness, because I genuinely believe they were blind to the symptoms I was exhibiting. They grew up and raised me in the world I\u2019ve spent the last decade of my life trying to change. They lived in a world where mental illness was equated with weakness, and shame, and as a result, I suffered until I was in my thirties.&lt;/p&gt;&lt;p&gt;And it\u2019s not like I never reached out for help. I did! I just didn\u2019t know what questions to ask, and the adults I was close to didn\u2019t know what answers to give.&lt;/p&gt;&lt;p&gt;Mom, I know you\u2019re going to read this or hear this and I know it\u2019s going to make you upset. I want you to know that I love you, and I know that you did the very best you could. I\u2019m telling my story, though, so someone else\u2019s mom can see the things you didn\u2019t, through no fault of your own.&lt;/p&gt;&lt;p&gt;I clearly remember being twenty-two, living in my own house, waking up from a panic attack that was so terrifying just writing about it for this talk gave me so much anxiety I almost cut this section from my speech. It was the middle of the night, and I drove across town, to my parents\u2019 house, to sleep on the floor of my sister\u2019s bedroom again, because at least that\u2019s where I felt safe. The next morning, I tearfully asked my mom what was wrong with me. She knew that many of my blood relatives had mental illness, but she couldn\u2019t or wouldn\u2019t connect the dots. \u201cYou\u2019re just realizing that the world is a scary place,\u201d she said.&lt;/p&gt;&lt;p&gt;Yeah, no kidding. The world terrifies me every night of my life and I don\u2019t know why or how to stop it.&lt;/p&gt;&lt;p&gt;Again, I don\u2019t blame her and neither should you. She really was doing the best that she could for me, but stigma and the shame is inspires are powerful things.&lt;/p&gt;&lt;p&gt;I want to be very clear on this: Mom, I know you\u2019re going to read this or hear this and I know it\u2019s going to make you upset. I want you to know that I love you, and I know that you did the very best you could. I\u2019m telling my story, though, so someone else\u2019s mom can see the things you didn\u2019t, through no fault of your own.&lt;/p&gt;&lt;p&gt;Through my twenties, I continued to suffer, and not just from nightmares and panic attacks. I began to develop obsessive behaviors that I\u2019ve never talked about in public until right now. Here\u2019s a very incomplete list: I began to worry that the things I did would affect the world around me in totally irrational ways. I would hold my breath underneath bridges when I was driving, because if I didn\u2019t, maybe I\u2019d crash my car. I would tap the side of an airplane with my hand while I was boarding, and tell it to take care of me when I flew places for work, because I was convinced that if I didn\u2019t, the plane would crash. Every single time I said goodbye to someone I cared about, my brain would play out in vivid detail how I would remember this as the last time I saw them. Talking about those memories, even without getting into specifics, is challenging. It\u2019s painful to recall, but I\u2019m not ashamed, because all those thoughts \u2014 which I thankfully don\u2019t have any more, thanks to medical science and therapy \u2014 were not my fault any more than the allergies that clog my sinuses when the trees in my neighborhood start doin\u2019 it every spring are my fault. It\u2019s just part of who I am. It\u2019s part of how my brain is wired, and because I know that, I can medically treat it, instead of being a victim of it.&lt;/p&gt;&lt;p&gt;One of the primary reasons I speak out about my mental illness, is so that I can make the difference in someone\u2019s life that I wish had been made in mine when I was young, because not only did I have no idea what Depression even was until I was in my twenties, once I was pretty sure that I had it, I suffered with it for another fifteen years, because I was ashamed, I was embarrassed, and I was afraid.&lt;/p&gt;&lt;p&gt;So I am here today to tell anyone who can hear me: if you suspect that you have a mental illness, there is no reason to be ashamed, or embarrassed, and most importantly, you do not need to be afraid. You do not need to suffer. There is nothing noble in suffering, and there is nothing shameful or weak in asking for help. This may seem really obvious to a lot of you, but it wasn\u2019t for me, and I\u2019m a pretty smart guy, so I\u2019m going to say it anyway: There is no reason to feel embarrassed when you reach out to a professional for help, because the person you are reaching out to is someone who has literally dedicated their life to helping people like us live, instead of merely exist.&lt;/p&gt;&lt;p&gt;I missed out on a lot of things, during what are supposed to be the best years of my life, because I was paralyzed by What If-ing anxiety.&lt;/p&gt;&lt;p&gt;That difference, between existing and living, is something I want to focus on for a minute: before I got help for my anxiety and depression, I didn\u2019t truly live my life. I wanted to go do things with my friends, but my anxiety always found a way to stop me. Traffic would just be too stressful, it would tell me. It\u2019s going to be a real hassle to get there and find parking, it would helpfully observe. And if those didn\u2019t stop me from leaving my house, there was always the old reliable: What if\u2026? Ah, \u201cWhat if\u2026 something totally unlikely to happen actually happens? What if the plane crashes? What if I sit next to someone who freaks me out? What if they laugh at me? What if I get lost? What if I get robbed? What if I get locked out of my hotel room? What if I slip on some ice I didn\u2019t see? What if there\u2019s an earthquake? What if what if what if what if\u2026&lt;/p&gt;&lt;p&gt;When I look back on most of my life, it breaks my heart that when my brain was unloading an endless pile of what ifs on me, it never asked, \u201cWhat if I go do this thing that I want to do, and it\u2019s \u2026 fun? What if I enjoy myself, and I\u2019m really glad I went?\u201d&lt;/p&gt;&lt;p&gt;I have to tell you a painful truth: I missed out on a lot of things, during what are supposed to be the best years of my life, because I was paralyzed by What If-ing anxiety.&lt;/p&gt;&lt;p&gt;All the things that people do when they are living their lives \u2026 all those experiences that make up a life, my anxiety got in between me and doing them. So I wasn\u2019t living. I was just existing.&lt;/p&gt;&lt;p&gt;And through it all, I never stopped to ask myself if this was normal, or healthy, or even if it was my fault. I just knew that I was nervous about stuff, and I worried a lot. For my entire childhood, my mom told me that I was a worry wart, and my dad said I was overly dramatic about everything, and that\u2019s just the way it was.&lt;/p&gt;&lt;p&gt;Except it didn\u2019t have to be that way, and it took me having a full blown panic attack and a complete meltdown at Los Angeles International Airport for my wife to suggest to me that I get help.&lt;/p&gt;&lt;p&gt;Like I said, I had suspected for years that I was clinically depressed, but I was afraid to admit it, until the most important person in my life told me without shame or judgment that she could see that I was suffering. So I went to see a doctor, and I will never forget what he said, when I told him how afraid I was: \u201cPlease let me help you.\u201d&lt;/p&gt;&lt;p&gt;I think it was then, at about 34 years-old, that I realized that Mental Illness is not weakness. It\u2019s just an illness. I mean, it\u2019s right there in the name \u201cMental ILLNESS\u201d so it shouldn\u2019t have been the revelation that it was, but when the part of our bodies that is responsible for how we perceive the world and ourselves is the same part of our body that is sick, it can be difficult to find objectivity or perspective.&lt;/p&gt;&lt;p&gt;So I let my doctor help me. I started a low dose of an antidepressant, and I waited to see if anything was going to change.&lt;/p&gt;&lt;p&gt;And boy did it.&lt;/p&gt;&lt;p&gt;My wife and I were having a walk in our neighborhood and I realized that it was just a really beautiful day \u2014 it was warm with just a little bit of a breeze, the birds sounded really beautiful, the flowers smelled really great and my wife\u2019s hand felt really good in mine.&lt;/p&gt;&lt;p&gt;And as we were walking I just started to cry and she asked me, \u201cwhat\u2019s wrong?\u201d&lt;/p&gt;&lt;p&gt;I said \u201cI just realized that I don\u2019t feel bad and I just realized that I\u2019m not existing, I\u2019m living.\u201d&lt;/p&gt;&lt;p&gt;At that moment, I realized that I had lived my life in a room that was so loud, all I could do every day was deal with how loud it was. But with the help of my wife, my doctor, and medical science, I found a doorway out of that room.&lt;/p&gt;&lt;p&gt;I had taken that walk with my wife almost every day for nearly ten years, before I ever noticed the birds or the flowers, or how loved I felt when I noticed that her hand was holding mine. Ten years \u2014 all of my twenties \u2014 that I can never get back. Ten years of suffering and feeling weak and worthless and afraid all the time, because of the stigma that surrounds mental illness.&lt;/p&gt;&lt;p&gt;I\u2019m not religious, but I can still say Thank God for Anne Wheaton. Thank God for her love and support. Thank God that my wife saw that I was hurting, and thank God she didn\u2019t believe the lie that Depression is weakness, or something to be ashamed of. Thank God for Anne, because if she hadn\u2019t had the strength to encourage me to seek professional help, I don\u2019t know how much longer I would have been able to even exist, to say nothing of truly living.&lt;/p&gt;&lt;p&gt;I started talking in public about my mental illness in 2012, and ever since then, people reach out to me online every day, and they ask me about living with depression and anxiety. They share their stories, and ask me how I get through a bad day, or a bad week.&lt;/p&gt;&lt;header&gt;Right now, there is a child somewhere who has the same panic attacks I had, and their parents aren\u2019t getting them help, because they believe it reflects poorly on their parenting to have a child with mental illness.&lt;/header&gt;&lt;p&gt;Here\u2019s one of the things I tell them:&lt;/p&gt;&lt;p&gt;One of the many delightful things about having Depression and Anxiety is occasionally and unexpectedly feeling like the whole goddamn world is a heavy lead blanket, like that thing they put on your chest at the dentist when you get x-rays, and it\u2019s been dropped around your entire existence without your consent.&lt;/p&gt;&lt;p&gt;Physically, it weighs heavier on me in some places than it does in others. I feel it tugging at the corners of my eyes, and pressing down on the center of my chest. When it\u2019s really bad, it can feel like one of those dreams where you try to move, but every step and every motion feels like you\u2019re struggling to move through something heavy and viscous. Emotionally, it covers me completely, separating me from my motivation, my focus, and everything that brings me joy in my life.&lt;/p&gt;&lt;p&gt;When it drops that lead apron over us, we have to remind ourselves that one of the things Depression does, to keep itself strong and in charge, is tell us lies, like: I am the worst at everything. Nobody really likes me. I don\u2019t deserve to be happy. This will never end. And so on and so on. We can know, in our rational minds, that this is a giant bunch of bullshit (and we can look at all these times in our lives when were WERE good at a thing, when we genuinely felt happy, when we felt awful but got through it, etc.) but in the moment, it can be a serious challenge to wait for Depression to lift the roadblock that\u2019s keeping us from moving those facts from our rational mind to our emotional selves.&lt;/p&gt;&lt;p&gt;And that\u2019s the thing about Depression: we can\u2019t force it to go away. As I\u2019ve said, if I could just \u201cstop feeling sad\u201d I WOULD. (And, also, Depression isn\u2019t just feeling sad, right? It\u2019s a lot of things together than can manifest themselves into something that is most easily simplified into \u201cI feel sad.\u201d)&lt;/p&gt;&lt;p&gt;So another step in our self care is to be gentle with ourselves. Depression is beating up on us already, and we don\u2019t need to help it out. Give yourself permission to acknowledge that you\u2019re feeling terrible (or bad, or whatever it is you are feeling), and then do a little thing, just one single thing, that you probably don\u2019t feel like doing, and I PROMISE you it will help. Some of those things are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Take a shower. &lt;/li&gt;&lt;li&gt;Eat a nutritious meal. &lt;/li&gt;&lt;li&gt;Take a walk outside (even if it\u2019s literally to the corner and back). &lt;/li&gt;&lt;li&gt;Do something \u2014 throw a ball, play tug of war, give belly rubs \u2014 with a dog. Just about any activity with my dogs, even if it\u2019s just a snuggle on the couch for a few minutes, helps me. &lt;/li&gt;&lt;li&gt;Do five minutes of yoga stretching. &lt;/li&gt;&lt;li&gt;Listen to a guided meditation and follow along as best as you can. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Finally, please trust me and know that this shitty, awful, overwhelming, terrible way you feel IS NOT FOREVER. It will get better. It always gets better. You are not alone in this fight, and you are OK.&lt;/p&gt;&lt;p&gt;No person anywhere, especially here in the richest country in the world, should live in the shadows or suffer alone, because they can\u2019t afford treatment. We have all the money in the world for weapons and corporate tax cuts, so I know that we can afford to prioritize not just health care in general, but mental health care, specifically.&lt;/p&gt;&lt;p&gt;Right now, there is a child somewhere who has the same panic attacks I had, and their parents aren\u2019t getting them help, because they believe it reflects poorly on their parenting to have a child with mental illness. Right now, there is a teenager who is contemplating self harm, because they don\u2019t know how to reach out and ask for help. Right now, there are too many people struggling just to get to the end of the day, because they can\u2019t afford the help that a lot of us can\u2019t live without. But there are also people everywhere who are picking up the phone and making an appointment. There are parents who have learned that mental illness is no different than physical illness, and they\u2019re helping their children get better. There are adults who, like me, were terrified that antidepressant medication would make them a different person, and they\u2019re hearing the birds sing for the first time, because they have finally found their way out of the dark room.&lt;/p&gt;&lt;p&gt;I spent the first thirty years of my life trapped in that dark, loud room, and I know how hopeless and suffocating it feels to be in there, so I do everything I can to help others find their way out. I do that by telling my story, so that my privilege and success does more than enrich my own life. I can live by example for someone else the way Jenny Lawson lives by example for me.&lt;/p&gt;&lt;p&gt;But I want to leave you today with some suggestions for things that we can all do, even if you\u2019re not Internet Famous like I am, to help end the stigma of mental illness, so that nobody has to merely exist, when they could be living.&lt;/p&gt;&lt;p&gt;We can start by demanding that our elected officials fully fund mental health programs. No person anywhere, especially here in the richest country in the world, should live in the shadows or suffer alone, because they can\u2019t afford treatment. We have all the money in the world for weapons and corporate tax cuts, so I know that we can afford to prioritize not just health care in general, but mental health care, specifically.&lt;/p&gt;&lt;p&gt;And until our elected officials get their acts together, we can support organizations like NAMI, that offer low and no-cost assistance to anyone who asks for it. We can support organizations like Project UROK, that work tirelessly to end stigmatization and remind us that we are sick, not weak.&lt;/p&gt;&lt;p&gt;We can remember, and we can remind each other, that there is no finish line when it comes to mental illness. It\u2019s a journey, and sometimes we can see the path we\u2019re on all the way to the horizon, while other times we can\u2019t even see five feet in front of us because the fog is so thick. But the path is always there, and if we can\u2019t locate it on our own, we have loved ones and doctors and medications to help us find it again, as long as we don\u2019t give up trying to see it.&lt;/p&gt;&lt;p&gt;Finally, we who live with mental illness need to talk about it, because our friends and neighbors know us and trust us. It\u2019s one thing for me to stand here and tell you that you\u2019re not alone in this fight, but it\u2019s something else entirely for you to prove it. We need to share our experiences, so someone who is suffering the way I was won\u2019t feel weird or broken or ashamed or afraid to seek treatment. So that parents don\u2019t feel like they have failed or somehow screwed up when they see symptoms in their kids.&lt;/p&gt;&lt;p&gt;People tell me that I\u2019m brave for speaking out the way I do, and while I appreciate that, I don\u2019t necessarily agree. Firefighters are brave. Single parents who work multiple jobs to take care of their kids are brave. The Parkland students are brave. People who reach out to get help for their mental illness are brave. I\u2019m not brave. I\u2019m just a writer and occasional actor who wants to share his privilege and good fortune with the world, who hopes to speak out about mental health so much that one day, it will be wholly unremarkable to stand up and say fifteen words:&lt;/p&gt;&lt;p&gt;My name is Wil Wheaton, I live with chronic depression, and I am not ashamed.&lt;/p&gt;\n'''\n\n# All weightage for structure doc\n# Important: These scores are for the experimenting purpose only\nWEIGHT_FOR_LIST = 5\nWEIGHT_FOR_HIGHLIGHTED = 10\nWEIGHT_FOR_NUMERICAL = 5\nWEIGHT_FIRST_PARAGRAPH = 5\nWEIGHT_BASIC = 1\n\n\ndef _create_frequency_table(paragraph_list) -&gt; dict:\n    \"\"\"\n    we create a dictionary for the word frequency table.\n    For this, we should only use the words that are not part of the stopWords array.\n\n    Removing stop words and making frequency table\n    Stemmer - an algorithm to bring words to its root word.\n    :rtype: dict\n    \"\"\"\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">stopWords = set(stopwords.words(\"english\"))</div>\n\n    <div class=\"highlights fea_stemming\" id=\"stemming_0\" style=\"display: inline;\">ps = <span class=\"fea_stemming_funcs blodFunc\">Porter</span><span class=\"fea_stemming_funcs blodFunc\">Stemmer</span>()</div>\n\n    freqTable = dict()\n    for paragraph in paragraph_list:\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">word</span>s = <span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(paragraph.text)</div>\n\n        all_highlighted_sentences = [sent for sent in paragraph.get_highlighted()]\n        highlighted_words_text = \" \".join(all_highlighted_sentences)\n        highlighted_words = word_tokenize(highlighted_words_text)\n\n        for word in words:\n\n            if paragraph.is_list_set:\n                weight = WEIGHT_FOR_LIST\n            else:\n                weight = WEIGHT_BASIC\n\n            if word in highlighted_words:\n                weight += WEIGHT_FOR_HIGHLIGHTED\n\n            if word.isnumeric() and len(word) &gt;= 2:\n                weight += WEIGHT_FOR_NUMERICAL\n\n            if paragraph.is_first_paragraph:\n                weight += WEIGHT_FIRST_PARAGRAPH\n\n            word = ps.stem(word)\n            if word in stopWords:\n                continue\n\n            if word in freqTable:\n                freqTable[word] += weight\n            else:\n                freqTable[word] = weight\n\n    return freqTable\n\n\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\"><span class=\"fea_text_scoring_funcs blodFunc\">def</span> _<span class=\"fea_text_scoring_funcs blodFunc\">score</span>_<span class=\"fea_text_scoring_funcs blodFunc\">sentences</span>(<span class=\"fea_text_scoring_funcs blodFunc\">sentences</span>, freqTable) -&gt; <span class=\"fea_text_scoring_funcs blodFunc\">dict</span>:</div>\n    \"\"\"\n    score a sentence by its words\n    Basic algorithm: adding the frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n    :rtype: dict\n    \"\"\"\n    # TODO: Can you make this multiprocess compatible in python?\n\n    sentenceValue = dict()\n\n    for sentence in sentences:\n        word_count_in_sentence = (len(word_tokenize(sentence)))\n        word_count_in_sentence_except_stop_words = 0\n        for wordValue in freqTable:\n            if wordValue in sentence.lower():\n                word_count_in_sentence_except_stop_words += 1\n                if sentence[:10] in sentenceValue:\n                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n                else:\n                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n\n        if sentence[:10] in sentenceValue:\n            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] / word_count_in_sentence_except_stop_words\n\n        '''\n        Notice that a potential issue with our score algorithm is that long sentences will have an advantage over short sentences. \n        To solve this, we're dividing every sentence score by the number of words in the sentence.\n        \n        Note that here sentence[:10] is the first 10 character of any sentence, this is to save memory while saving keys of\n        the dictionary.\n        '''\n\n    return sentenceValue\n\n\ndef _find_average_score(sentenceValue) -&gt; int:\n    \"\"\"\n    Find the average score from the sentence value dictionary\n    :rtype: int\n    \"\"\"\n    sumValues = 0\n    for entry in sentenceValue:\n        sumValues += sentenceValue[entry]\n\n    average = 0\n    # Average value of a sentence from original summary_text\n    if len(sentenceValue) &gt; 0:\n        average = (sumValues / len(sentenceValue))\n\n    return average\n\n\ndef _generate_summary(sentences, sentenceValue, threshold):\n    sentence_count = 0\n    summary = ''\n\n    for sentence in sentences:\n        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] &gt;= (threshold):\n            summary += \" \" + sentence\n            sentence_count += 1\n\n    # TODO: check if the sentences in the summarization is in the original order of occurrence.\n\n    return summary\n\n\ndef run_summarization(paragraph_list):\n    # 1 Create the word frequency table\n    freq_table = _create_frequency_table(paragraph_list)\n    # print (freq_table)\n\n    '''\n    We already have a sentence tokenizer, so we just need \n    to run the sent_tokenize() method to create the array of sentences.\n    '''\n\n    # 2 Tokenize the sentences\n    sentences = [paragraph.text for paragraph in paragraph_list]\n    # print(sentences)\n\n    # 3 Important Algorithm: score the sentences\n    sentence_scores = _score_sentences(sentences, freq_table)\n\n    # 4 Find the threshold\n    threshold = _find_average_score(sentence_scores)\n\n    # 5 Important Algorithm: Generate the summary\n    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n\n    return summary\n\n\nif __name__ == '__main__':\n    parser = Parser()\n    parser.feed(text_str)\n    <div class=\"highlights fea_summarizer\" id=\"summarizer_0\" style=\"display: inline;\">result = run_summarization(parser.paragraphs)</div>\n    print(result)\n    #https://github.com/akashp1712/summarize-webpage/blob/master/implementation/word_frequency_summarize_parser</code></pre></div></body></html>", "fir_9": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_9\"><pre id=\"fir_9_code\"><code class=\"python\">import constants\nimport nltk\nimport random\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport string\nimport warnings\n\ndef get_formalities_response(formality) :\n    if any(remove_punctuation_marks(formality).lower() in remove_punctuation_marks(greet).lower() for greet in constants.GREETING_INPUTS) :\n        return random.choice(constants.GREETING_REPLIES)\n    elif any(remove_punctuation_marks(formality).lower() in remove_punctuation_marks(thanks).lower() for thanks in constants.THANKS_INPUTS) :\n        return random.choice(constants.THANKS_REPLIES)\n\ndef get_lemmatized_tokens(text) :\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">normalized_tokens = nltk.<span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(remove_punctuation_marks(text.lower()))</div>\n    return [<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">nltk.stem.<span class=\"fea_lemmatization_funcs blodFunc\">Word</span><span class=\"fea_lemmatization_funcs blodFunc\">Net</span><span class=\"fea_lemmatization_funcs blodFunc\">Lemmatizer</span>().<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>(normalized_token)</div> for normalized_token in normalized_tokens]\n\ndef get_query_reply(query) :\n    documents.append(query)\n    tfidf_results = TfidfVectorizer(tokenizer = get_lemmatized_tokens, stop_words = 'english').fit_transform(documents)\n    cosine_similarity_results = cosine_similarity(tfidf_results[-1], tfidf_results).flatten()\n    # The last will be 1.0 because it is the Cosine Similarity between the first document and itself\n    best_index = cosine_similarity_results.argsort()[-2]\n    documents.remove(query)\n    if cosine_similarity_results[best_index] == 0 :\n        return \"I am sorry! I don't understand you...\"\n    else :\n        return documents[best_index]\n\ndef remove_punctuation_marks(text) :\n    punctuation_marks = dict((ord(punctuation_mark), None) for punctuation_mark in string.punctuation)\n    return text.translate(punctuation_marks)\n\nif __name__ == \"__main__\" :\n    warnings.filterwarnings(\"ignore\")\n\n    try :\n        nltk.data.find('tokenizers/punkt')\n    except LookupError:\n        nltk.download('punkt')\n\n    try :\n        nltk.data.find('corpora/wordnet')\n    except LookupError:\n        <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">nltk.download('wordnet')</div>\n\n    corpus = open('corpus.txt', 'r' , errors = 'ignore').read().lower()\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">documents = nltk.sent_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(corpus)</div>\n\n    <div class=\"highlights fea_chatbot\" id=\"chatbot_0\" style=\"display: inline;\"><span class=\"fea_chatbot_funcs blodFunc\">print</span>('<span class=\"fea_chatbot_funcs blodFunc\">Ryuzaki</span><span class=\"fea_chatbot_funcs blodFunc\">Bot</span>: My <span class=\"fea_chatbot_funcs blodFunc\">name</span> is <span class=\"fea_chatbot_funcs blodFunc\">Ryuzaki</span><span class=\"fea_chatbot_funcs blodFunc\">Bot</span>. I <span class=\"fea_chatbot_funcs blodFunc\">will</span> <span class=\"fea_chatbot_funcs blodFunc\">answer</span> <span class=\"fea_chatbot_funcs blodFunc\">your</span> <span class=\"fea_chatbot_funcs blodFunc\">queries</span> <span class=\"fea_chatbot_funcs blodFunc\">about</span> <span class=\"fea_chatbot_funcs blodFunc\">World</span> <span class=\"fea_chatbot_funcs blodFunc\">Wide</span> <span class=\"fea_chatbot_funcs blodFunc\">Web</span>. If <span class=\"fea_chatbot_funcs blodFunc\">you</span> <span class=\"fea_chatbot_funcs blodFunc\">want</span> to <span class=\"fea_chatbot_funcs blodFunc\">exit</span> <span class=\"fea_chatbot_funcs blodFunc\">just</span> <span class=\"fea_chatbot_funcs blodFunc\">type</span>: <span class=\"fea_chatbot_funcs blodFunc\">Bye</span>!')\n    <span class=\"fea_chatbot_funcs blodFunc\">end</span>_<span class=\"fea_chatbot_funcs blodFunc\">chat</span> = <span class=\"fea_chatbot_funcs blodFunc\">False</span>\n<span class=\"fea_chatbot_funcs blodFunc\">while</span> <span class=\"fea_chatbot_funcs blodFunc\">end</span>_<span class=\"fea_chatbot_funcs blodFunc\">chat</span> == <span class=\"fea_chatbot_funcs blodFunc\">False</span> :\n        <span class=\"fea_chatbot_funcs blodFunc\">input</span>_<span class=\"fea_chatbot_funcs blodFunc\">text</span> = <span class=\"fea_chatbot_funcs blodFunc\">input</span>()\n        if <span class=\"fea_chatbot_funcs blodFunc\">remove</span>_<span class=\"fea_chatbot_funcs blodFunc\">punctuation</span>_<span class=\"fea_chatbot_funcs blodFunc\">marks</span>(<span class=\"fea_chatbot_funcs blodFunc\">input</span>_<span class=\"fea_chatbot_funcs blodFunc\">text</span>).<span class=\"fea_chatbot_funcs blodFunc\">lower</span>() != '<span class=\"fea_chatbot_funcs blodFunc\">bye</span>' :\n            <span class=\"fea_chatbot_funcs blodFunc\">formality</span>_<span class=\"fea_chatbot_funcs blodFunc\">reply</span> = <span class=\"fea_chatbot_funcs blodFunc\">get</span>_<span class=\"fea_chatbot_funcs blodFunc\">formalities</span>_<span class=\"fea_chatbot_funcs blodFunc\">response</span>(<span class=\"fea_chatbot_funcs blodFunc\">input</span>_<span class=\"fea_chatbot_funcs blodFunc\">text</span>)\n            if  <span class=\"fea_chatbot_funcs blodFunc\">formality</span>_<span class=\"fea_chatbot_funcs blodFunc\">reply</span> :\n                <span class=\"fea_chatbot_funcs blodFunc\">print</span>('<span class=\"fea_chatbot_funcs blodFunc\">Ryuzaki</span><span class=\"fea_chatbot_funcs blodFunc\">Bot</span>: ' + <span class=\"fea_chatbot_funcs blodFunc\">formality</span>_<span class=\"fea_chatbot_funcs blodFunc\">reply</span>)\n            <span class=\"fea_chatbot_funcs blodFunc\">else</span> :\n                <span class=\"fea_chatbot_funcs blodFunc\">print</span>('<span class=\"fea_chatbot_funcs blodFunc\">Ryuzaki</span><span class=\"fea_chatbot_funcs blodFunc\">Bot</span>: ' + <span class=\"fea_chatbot_funcs blodFunc\">get</span>_<span class=\"fea_chatbot_funcs blodFunc\">query</span>_<span class=\"fea_chatbot_funcs blodFunc\">reply</span>(<span class=\"fea_chatbot_funcs blodFunc\">input</span>_<span class=\"fea_chatbot_funcs blodFunc\">text</span>))\n        <span class=\"fea_chatbot_funcs blodFunc\">else</span> :\n            <span class=\"fea_chatbot_funcs blodFunc\">print</span>('<span class=\"fea_chatbot_funcs blodFunc\">Ryuzaki</span><span class=\"fea_chatbot_funcs blodFunc\">Bot</span>: <span class=\"fea_chatbot_funcs blodFunc\">Bye</span>! <span class=\"fea_chatbot_funcs blodFunc\">Take</span> <span class=\"fea_chatbot_funcs blodFunc\">care</span> ' + <span class=\"fea_chatbot_funcs blodFunc\">random</span>.<span class=\"fea_chatbot_funcs blodFunc\">choice</span>(<span class=\"fea_chatbot_funcs blodFunc\">constants</span>.CANDIES))\n            <span class=\"fea_chatbot_funcs blodFunc\">end</span>_<span class=\"fea_chatbot_funcs blodFunc\">chat</span> = <span class=\"fea_chatbot_funcs blodFunc\">True</span></div>\n            #https://github.com/LuciaLlavero/ryuzaki_bot/blob/master/ryuzaki_bot_desktop</code></pre></div></body></html>", "fir_10": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_10\"><pre id=\"fir_10_code\"><code class=\"python\"># coding: utf-8\n\nimport constants\nfrom flask import Flask, jsonify, request\nfrom flask_cors import CORS\nfrom flask_restful import Resource, Api\nimport nltk\nimport random\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport string\nimport sys\nimport warnings\n\ndef get_formalities_reply(formality) :\n    if any(remove_punctuation_marks(formality).lower() in remove_punctuation_marks(greet).lower() for greet in constants.GREETING_INPUTS) :\n        return random.choice(constants.GREETING_REPLIES)\n    elif any(remove_punctuation_marks(formality).lower() in remove_punctuation_marks(thanks).lower() for thanks in constants.THANKS_INPUTS) :\n        return random.choice(constants.THANKS_REPLIES)\n\ndef get_lemmatized_tokens(text):\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">normalized_tokens = nltk.<span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(remove_punctuation_marks(text.lower()))</div>\n    return [<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">nltk.stem.<span class=\"fea_lemmatization_funcs blodFunc\">Word</span><span class=\"fea_lemmatization_funcs blodFunc\">Net</span><span class=\"fea_lemmatization_funcs blodFunc\">Lemmatizer</span>().<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>(normalized_token)</div> for normalized_token in normalized_tokens]\n\ncorpus = open('corpus.txt', 'r' , errors = 'ignore').read().lower()\ndocuments = nltk.sent_tokenize(corpus)\n\ndef get_query_reply(query) :    \n    documents.append(query)\n    tfidf_results = TfidfVectorizer(tokenizer = get_lemmatized_tokens, stop_words = 'english').fit_transform(documents)\n    cosine_similarity_results = cosine_similarity(tfidf_results[-1], tfidf_results).flatten()\n    # The last will be 1.0 because it is the Cosine Similarity between the first document and itself\n    best_index = cosine_similarity_results.argsort()[-2]\n    documents.remove(query)\n    if cosine_similarity_results[best_index] == 0 :\n        return \"I am sorry! I don't understand you...\"\n    else :\n        return documents[best_index]\n\ndef remove_punctuation_marks(text) :\n    punctuation_marks = dict((ord(punctuation_mark), None) for punctuation_mark in string.punctuation)\n    return text.translate(punctuation_marks)\n\napp = Flask(__name__)\ncors = CORS(app)\napi = Api(app)\n\nclass Reply(Resource) :\n    def get(self) :\n        if request.args.get('q') :\n            formality_reply = get_formalities_reply(request.args.get('q'))\n            if  formality_reply :\n                return jsonify({'reply': formality_reply + ' ' + random.choice(constants.SWEETS)})\n            else :\n                return jsonify({'reply': get_query_reply(request.args.get('q'))})\n        else :\n            return jsonify({'error': 'query is empty'})\n\napi.add_resource(Reply, '/reply.json')\n\nif __name__ == \"__main__\" :\n\n    app.run()\n    #https://github.com/LuciaLlavero/ryuzaki_bot/blob/master/ryuzaki_bot</code></pre></div></body></html>", "fir_11": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_11\"><pre id=\"fir_11_code\"><code class=\"python\">import nltk\nimport re\nfrom newspaper import Article\nfrom geograpy.labels import Labels\n\nclass Extractor(object):\n    '''\n    Extract geo context for text or from url\n    '''\n    def __init__(self, text=None, url=None, debug=False):\n        '''\n        Constructor\n        Args:\n\n            text(string): the text to analyze\n            url(string): the url to read the text to analyze from\n            debug(boolean): if True show debug information\n        '''\n        if not text and not url:\n            raise Exception('text or url is required')\n        self.debug=debug\n        self.text = text\n        self.url = url\n        self.places = []\n\n    def set_text(self):\n        '''\n        Setter for text\n        '''\n        if not self.text and self.url:\n            a = Article(self.url)\n            a.download()\n            a.parse()\n            self.text = a.text\n            \n    def split(self,delimiter=r\",\"):\n        '''\n        simpler regular expression splitter with not entity check\n        \n        hat tip: https://stackoverflow.com/a/1059601/1497139\n        '''\n        self.set_text()\n        self.places=re.split(delimiter,self.text)\n            \n    def find_geoEntities(self):\n        '''\n        Find geographic entities\n        \n        Returns:\n            list: \n                List of places\n        '''\n        self.find_entities(Labels.geo)\n        return self.places\n        \n    def find_entities(self,labels=Labels.default):\n        '''\n        Find entities with the given labels set self.places and returns it\n        Args:\n            labels: \n                Labels: The labels to filter\n        Returns:\n            list: \n                List of places\n        '''\n        self.set_text()\n\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">text = nltk.<span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(self.text)</div>\n        nes = nltk.ne_chunk(<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\">nltk.<span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span>_<span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span>(text)</div>)\n\n        for ne in nes:\n            if type(ne) is nltk.tree.Tree:\n                nelabel=ne.label()\n                if (nelabel in labels):\n                    leaves=ne.leaves()\n                    if self.debug:\n                        print(leaves)\n                    self.places.append(u' '.join([i[0] for i in leaves]))\n        return self.places\n        #https://github.com/somnathrakshit/geograpy3/blob/master/geograpy/extraction</code></pre></div></body></html>", "fir_12": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_12\"><pre id=\"fir_12_code\"><code class=\"python\">import sys,re,collections,nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n# patterns that used to find or/and replace particular chars or words\n# to find chars that are not a letter, a blank or a quotation\npat_letter = re.compile(r'[^a-zA-Z \\']+')\n# to find the 's following the pronouns. re.I is refers to ignore case\npat_is = re.compile(\"(it|he|she|that|this|there|here)(\\'s)\", re.I)\n# to find the 's following the letters\npat_s = re.compile(\"(?&lt;=[a-zA-Z])\\'s\")\n# to find the ' following the words ending by s\npat_s2 = re.compile(\"(?&lt;=s)\\'s?\")\n# to find the abbreviation of not\npat_not = re.compile(\"(?&lt;=[a-zA-Z])n\\'t\")\n# to find the abbreviation of would\npat_would = re.compile(\"(?&lt;=[a-zA-Z])\\'d\")\n# to find the abbreviation of will\npat_will = re.compile(\"(?&lt;=[a-zA-Z])\\'ll\")\n# to find the abbreviation of am\npat_am = re.compile(\"(?&lt;=[I|i])\\'m\")\n# to find the abbreviation of are\npat_are = re.compile(\"(?&lt;=[a-zA-Z])\\'re\")\n# to find the abbreviation of have\npat_ve = re.compile(\"(?&lt;=[a-zA-Z])\\'ve\")\n\n\nlmtzr = WordNetLemmatizer()\n\n\ndef get_words(file):  \n    with open (file) as f:  \n        words_box=[]\n        pat = re.compile(r'[^a-zA-Z \\']+')\n        for line in f:                           \n            #if re.match(r'[a-zA-Z]*',line): \n            #    words_box.extend(line.strip().strip('\\'\\\"\\.,').lower().split())\n            # words_box.extend(pat.sub(' ', line).strip().lower().split())\n            words_box.extend(merge(replace_abbreviations(line).split()))\n    return collections.Counter(words_box)  \n\n\ndef merge(words):\n    new_words = []\n    for word in words:\n        if word:\n            tag = <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\">nltk.<span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span>_<span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span></div>(<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(<span class=\"fea_tokenization_funcs blodFunc\">word</span>)</div>) # tag is like [('bigger', 'JJR')]\n            pos = get_wordnet_pos(tag[0][1])\n            if pos:\n                <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>d_<span class=\"fea_lemmatization_funcs blodFunc\">word</span> = lmtzr.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>(<span class=\"fea_lemmatization_funcs blodFunc\">word</span>, pos)</div>\n                new_words.append(lemmatized_word)\n            else:\n                new_words.append(word)\n    return new_words\n\n\ndef get_wordnet_pos(treebank_tag):\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">if treebank_tag.startswith('J'):\n        return nltk.corpus.wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return nltk.corpus.wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return nltk.corpus.wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return nltk.corpus.wordnet.ADV</div>\n    else:\n        return ''\n\n\ndef replace_abbreviations(text):\n    new_text = text\n    new_text = pat_letter.sub(' ', text).strip().lower()\n    new_text = pat_is.sub(r\"\\1 is\", new_text)\n    new_text = pat_s.sub(\"\", new_text)\n    new_text = pat_s2.sub(\"\", new_text)\n    new_text = pat_not.sub(\" not\", new_text)\n    new_text = pat_would.sub(\" would\", new_text)\n    new_text = pat_will.sub(\" will\", new_text)\n    new_text = pat_am.sub(\" am\", new_text)\n    new_text = pat_are.sub(\" are\", new_text)\n    new_text = pat_ve.sub(\" have\", new_text)\n    new_text = new_text.replace('\\'', ' ')\n    return new_text\n\n\ndef append_ext(words):\n    new_words = []\n    for item in words:\n        word, count = item\n        tag = nltk.pos_tag(word_tokenize(word))[0][1] # tag is like [('bigger', 'JJR')]\n        new_words.append((word, count, tag))\n    return new_words\n\ndef write_to_file(words, file='results.txt'):\n    f = open(file, 'w')\n    for item in words:\n        for field in item:\n            f.write(str(field)+',')\n        f.write('\\n')\n\n\nif __name__=='__main__':\n    book = sys.argv[1]\n    print \"counting...\"\n    words = get_words(book)\n    print \"writing file...\"\n    write_to_file(append_ext(words.most_common()))\n    #https://github.com/rocketk/wordcounter/blob/master/wordcounter/word_counter</code></pre></div></body></html>", "fir_13": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_13\"><pre id=\"fir_13_code\"><code class=\"python\"># -*- coding: utf-8 -*-\nimport numpy as np\nimport json\nimport sys\nimport re\nimport os\nimport ast\nimport argparse\nimport argcomplete\nimport multiprocessing\nfrom functools import partial\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nimport gensim\nfrom gensim import utils, corpora, models\nimport io\n\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">ignore_words = set(stopwords.words('english'))</div>\n\n''' from the model that was created, you can calculate the topic probability distribution of unseen documents.\n    this is a command line interface using Gensim for preprocessing unseen\n    documents and calculating topic probability distributions over a given topology from an LDA model '''\n\ndef write_topn_words(output_dir, lda):\n    if not os.path.exists(output_dir + 'topn_words.json'):\n        print('Writing topn words for LDA model')\n        reg_ex = re.compile('(?&lt;![\\s/])/[^\\s/]+(?![\\S/])')\n        topn_words = {'Topic ' + str(i + 1): [reg_ex.sub('', word) for word, prob in lda.show_topic(i, topn=20)] for i in range(0, lda.num_topics)}\n        with open(output_dir + 'topn_words.json', 'w') as outfile:\n            json.dump(topn_words, outfile, sort_keys=True, indent=4)\n\ndef preprocess_tweet(document, lemma):\n    with io.open(document, 'r', encoding=\"utf-8\") as infile:\n        text = ' '.join(line.rstrip('\\n') for line in infile)\n    # convert string into unicode\n    text = gensim.utils.any2unicode(text)\n    # remove URL's\n    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n    # remove symbols excluding the @, # and \\s symbol\n    text = re.sub(r'[^\\w@#\\s]', '', text)\n    if lemma:\n        return utils.lemmatize(text, stopwords=ignore_words, min_length=3)\n    # tokenize words using NLTK Twitter Tokenizer\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">tknzr = Tweet<span class=\"fea_tokenization_funcs blodFunc\">Tokenize</span>r()\n    text = tknzr.<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(text)</div>\n    # lowercase, remove words less than len 2 &amp; remove numbers in tokenized list\n    return [word.lower() for word in text if len(word) &gt; 2 and not word.isdigit() and not word in ignore_words]\n\ndef get_document_vectors(user_id, **kwargs):\n    print('Getting document vectors for: ' + user_id)\n    if os.path.exists(kwargs['tweets_dir'] + user_id):\n        tweetpath = kwargs['tweets_dir'] + user_id\n    else:\n        return\n\n    if not user_id in kwargs['document_vectors']:\n        document = preprocess_tweet(tweetpath, kwargs['lemma'])\n        # if after preprocessing, the list is empty, then skip that user\n        if not document:\n            return\n        # create bag of words from input document\n        doc_bow = kwargs['dictionary'].doc2bow(document)\n        # queries the document against the LDA model and associates the data with probabalistic topics\n        doc_lda = get_doc_topics(kwargs['lda_model'], doc_bow)\n        dense_vec = gensim.matutils.sparse2full(doc_lda, kwargs['lda_model'].num_topics)\n        # build dictionary of user document vectors &lt;k, v&gt;(user_id, vec)\n        return (user_id, dense_vec.tolist())\n    else:\n        return (user_id, kwargs['document_vectors'][user_id])\n\n# http://stackoverflow.com/questions/17310933/document-topical-distribution-in-gensim-lda\ndef get_doc_topics(lda, bow):\n    gamma, _ = lda.inference([bow])\n    topic_dist = gamma[0] / sum(gamma[0])\n    return [(topic_id, topic_value) for topic_id, topic_value in enumerate(topic_dist)]\n\ndef community_document_vectors(doc_vecs, community):\n    comm_doc_vecs = {}\n    for user in ast.literal_eval(community):\n        try:\n            comm_doc_vecs[str(user)] = doc_vecs[str(user)]\n        except:\n            pass\n    return comm_doc_vecs\n\ndef read_json(file_name):\n    try:\n        with open(file_name, 'r') as comm_doc_vecs_file:\n            return json.load(comm_doc_vecs_file)\n    except:\n        return {}\n \ndef main():\n    # this program uses an LDA model to vectorize 'documents' and outputs a json file containing {user: [topic probability distribution vector]} results\n    # it also creates the directories for the communities generated from the topology file, putting each community document vectors json file in corresponding directory\n    parser = argparse.ArgumentParser(description='Create a corpus from a collection of tweets and/or build an LDA model')\n    parser.add_argument('-t', '--topology_file', required=True, action='store', dest='top_file', help='Location of topology file')\n    parser.add_argument('-p', '--dir_prefix', choices=['clique', 'community'], required=True, action='store', dest='dir_prefix', help='Select whether the topology contains cliques or communities')\n    parser.add_argument('-w', '--working_dir', required=True, action='store', dest='working_dir', help='Name of the directory you want to direct output to')\n    parser.add_argument('-l', '--lda_loc', required=True, action='store', dest='lda_loc', help='Location of the saved LDA model')\n    parser.add_argument('-d', '--dict_loc', required=True, action='store', dest='dict_loc', help='Location of dictionary for the model')\n    parser.add_argument('-u', '--unseen_docs', required=True, action='store', dest='unseen_docs', help='Directory containing unseen documents')\n    parser.add_argument('-m', '--lemma', action='store_true', dest='lemma', help='Use this option to lemmatize words')\n    argcomplete.autocomplete(parser)\n    args = parser.parse_args()\n\n    output_dir = os.path.join(args.working_dir, '')\n    if not os.path.exists(os.path.dirname(output_dir)):\n        os.makedirs(os.path.dirname(output_dir), 0o755)\n\n    # load dictionary\n    model_dict = corpora.Dictionary.load(args.dict_loc)\n    # load trained model from file\n    lda = models.LdaModel.load(args.lda_loc)\n    write_topn_words(output_dir, lda)\n\n    # create a set of all users from topology file\n    with open(args.top_file, 'r') as inp_file:\n        users = set(str(user) for community in inp_file for user in ast.literal_eval(community))\n\n    # opens up a 'job in progress' if ran this program and stopped it\n    try:\n        with open(output_dir + 'document_vectors.json', 'r') as all_community_file:\n            document_vectors = json.load(all_community_file)\n    except:\n        document_vectors = {}\n\n    # use multiprocessing to query document vectors\n    pool = multiprocessing.Pool(max(1, multiprocessing.cpu_count() - 1))\n    func = partial(get_document_vectors,\n                   tweets_dir=args.unseen_docs,\n                   document_vectors=document_vectors,\n                   dictionary=model_dict,\n                   lda_model=lda,\n                   lemma=args.lemma)\n    doc_vecs = pool.map(func, users)\n    doc_vecs = [item for item in doc_vecs if item is not None]\n    pool.close()\n    pool.join()\n    doc_vecs = dict(doc_vecs) # {user: [topic probability distribution vector]}\n\n    document_vectors.update(doc_vecs)\n    with open(output_dir + 'document_vectors.json', 'w') as document_vectors_file:\n        json.dump(document_vectors, document_vectors_file, sort_keys=True, indent=4)\n\n    print('Building directories')\n    with open(args.top_file, 'r') as topology_file:\n        for i, community in enumerate(topology_file):\n            community_dir = os.path.join(output_dir, args.dir_prefix + '_' + str(i) + '/')\n            if not os.path.exists(os.path.dirname(community_dir)):\n                os.makedirs(os.path.dirname(community_dir), 0o755)\n            comm_doc_vecs = community_document_vectors(doc_vecs, community)\n            with open(community_dir + 'community_doc_vecs.json', 'w') as comm_docs_file:\n                json.dump(comm_doc_vecs, comm_docs_file, sort_keys=True, indent=4)\n\nif __name__ == '__main__':\n    sys.exit(main())\n    #https://github.com/kethort/TwitterLDATopicModeling/blob/master/src/tweets_on_LDA</code></pre></div></body></html>", "fir_14": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_14\"><pre id=\"fir_14_code\"><code class=\"python\">import logging\nimport os\nimport sys\nimport bz2\nimport re\nimport itertools\nimport tarfile\nimport multiprocessing\nfrom functools import partial\nimport gensim\nfrom gensim.corpora import MmCorpus, Dictionary, WikiCorpus\nfrom gensim import models, utils\nimport pyLDAvis\nfrom pyLDAvis import gensim as gensim_vis\nimport argparse\nimport argcomplete\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n''' a command-line utility for the Gensim library that creates LDA model either from a folder of texts \n    or a wikipedia dump. '''\n\nDEFAULT_DICT_SIZE = 100000\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">ignore_words = set(stopwords.words('english'))</div>\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n# an 'override' of the Gensim WikiCorpus tokenizer function\n# compares against nltk stopword list to omit useless words\ndef wiki_tokenizer(content, token_min_len=3, token_max_len=15, lower=True):\n    return [\n        utils.to_unicode(token) for token in utils.simple_preprocess(content, deacc=True, min_len=3) \n        if token_min_len &lt;= len(token) &lt;= token_max_len and not token.startswith('_') and not token.isdigit()\n        and not token in ignore_words\n    ]\n\ndef preprocess_text(lemma, document):\n    with open(document, 'r') as infile:\n        # transform document into one string\n        text = ' '.join(line.rstrip('\\n') for line in infile)\n    # convert string into unicode\n    text = gensim.utils.any2unicode(text)\n\n    # remove URL's\n    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n\n    # remove symbols excluding the @, # and \\s symbol\n    text = re.sub(r'[^\\w@#\\s]', '', text)\n    \n    # use the built-in Gensim lemmatize engine \n    if lemma:\n        return utils.lemmatize(text, stopwords=ignore_words, min_length=3)\n\n    # tokenize words using NLTK Twitter Tokenizer\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">tknzr = Tweet<span class=\"fea_tokenization_funcs blodFunc\">Tokenize</span>r()\n    text = tknzr.<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(text)</div>\n\n    # lowercase, remove words less than len 2 &amp; remove numbers in tokenized list\n    return [word.lower() for word in text if len(word) &gt; 2 and not word.isdigit() and not word in ignore_words]\n\ndef filenames_to_generator(directory):\n    for filename in os.listdir(directory):\n        yield directory + str(filename)\n\nclass DocCorpus(gensim.corpora.TextCorpus):\n    # overrides the get_texts function of Gensim TextCorpus in order to use \n    # directory of texts as corpus, where each text file is a document\n    def __init__(self, docs_loc, lemmatize, dictionary=None, metadata=None):\n        self.docs_loc = docs_loc\n        <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">self.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span> = <span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span></div>\n        self.metadata = metadata\n        if dictionary is None:\n            self.dictionary = Dictionary(self.get_texts())\n        else:\n            self.dictionary = dictionary\n    def get_texts(self):\n        pool = multiprocessing.Pool(max(1, multiprocessing.cpu_count() - 1))\n        func = partial(preprocess_text, self.lemmatize)\n        for tokens in pool.map(func, filenames_to_generator(self.docs_loc)):\n            yield tokens\n        pool.terminate()\n\ndef build_LDA_model(corp_loc, dict_loc, num_topics, num_pass, lda_loc):\n    corpus = MmCorpus(corp_loc) \n    dictionary = Dictionary.load(dict_loc)\n\n    lda = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=int(num_topics), alpha='asymmetric', passes=int(num_pass))\n    lda.save(lda_loc + '.model')\n\n    build_pyLDAvis_output(corp_loc, dict_loc, lda_loc)\n\ndef build_pyLDAvis_output(corp_loc, dict_loc, lda_loc):\n    if not '.model' in lda_loc:\n        lda_loc += '.model'\n    \n    corpus = MmCorpus(corp_loc)\n    dictionary = Dictionary.load(dict_loc)\n    lda = models.LdaModel.load(lda_loc)\n\n    vis_data = gensim_vis.prepare(lda, corpus, dictionary, sort_topics=False) \n    pyLDAvis.save_html(vis_data, lda_loc.split('.model')[0] + '.html')\n\ndef main():\n    # a command line interface for running Gensim operations\n    # can create a corpus from a directory of texts or from a wikipedia dump\n    # options for lemmatize words, build model and/or pyLDAvis graph output\n    parser = argparse.ArgumentParser(description='Create a corpus from a collection of tweets and/or build an LDA model')\n    subparsers = parser.add_subparsers(dest='mode')\n    \n    text_corpus_parser = subparsers.add_parser('text', help='Build corpus from directory of text files')\n    text_corpus_parser.add_argument('-d', '--docs_loc', required=True, action='store', dest='docs_loc', help='Directory where tweet documents stored')\n    text_corpus_parser.add_argument('-c', '--corp_loc', required=True, action='store', dest='corp_loc', help='Location and name to save corpus')\n    text_corpus_parser.add_argument('-m', '--lemma', action='store_true', dest='lemma', help='Use this option to lemmatize words')\n\n    wiki_corpus_parser = subparsers.add_parser('wiki', help='Build corpus from compressed Wikipedia articles')\n    wiki_corpus_parser.add_argument('-w', '--wiki_loc', required=True, action='store', dest='wiki_loc', help='Location of compressed Wikipedia dump')\n    wiki_corpus_parser.add_argument('-c', '--corp_loc', required=True, action='store', dest='corp_loc', help='Location and name to save corpus')\n    wiki_corpus_parser.add_argument('-m', '--lemma', action='store_true', dest='lemma', help='Use this option to lemmatize words')\n\n    lda_model_parser = subparsers.add_parser('lda', help='Create LDA model from saved corpus')\n    lda_model_parser.add_argument('-c', '--corp_loc', required=True, action='store', dest='corp_loc', help='Location of corpus')\n    lda_model_parser.add_argument('-d', '--dict_loc', required=True, action='store', dest='dict_loc', help='Location of dictionary')\n    lda_model_parser.add_argument('-n', '--num_topics', required=True, action='store', dest='num_topics', help='Number of topics to assign to LDA model')\n    lda_model_parser.add_argument('-p', '--num_pass', required=True, action='store', dest='num_pass', help='Number of passes through corpus when training the LDA model')\n    lda_model_parser.add_argument('-l', '--lda_loc', required=True, action='store', dest='lda_loc', help='Location and name to save LDA model')\n\n    lda_vis_parser = subparsers.add_parser('ldavis', help='Create visualization of LDA model')\n    lda_vis_parser.add_argument('-c', '--corp_loc', required=True, action='store', dest='corp_loc', help='Location of corpus')\n    lda_vis_parser.add_argument('-d', '--dict_loc', required=True, action='store', dest='dict_loc', help='Location of dictionary')\n    lda_vis_parser.add_argument('-l', '--lda_loc', required=True, action='store', dest='lda_loc', help='Location of LDA model')\n\n    argcomplete.autocomplete(parser)\n    args = parser.parse_args()\n\n    if args.mode == 'text':\n        doc_corpus = DocCorpus(args.docs_loc, args.lemma)\n\n        doc_corpus.dictionary.filter_extremes(no_below=1, no_above=0.5, keep_n=DEFAULT_DICT_SIZE)\n\n        MmCorpus.serialize(args.corp_loc + '.mm', doc_corpus)\n        doc_corpus.dictionary.save(args.corp_loc + '.dict')\n\n    if args.mode == 'wiki':\n        wiki_corpus = WikiCorpus(args.wiki_loc, lemmatize=args.lemma, tokenizer_func=wiki_tokenizer, article_min_tokens=100, token_min_len=3, token_max_len=15)\n\n        wiki_corpus.dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=DEFAULT_DICT_SIZE)\n\n        MmCorpus.serialize(args.corp_loc + '.mm', wiki_corpus)\n        wiki_corpus.dictionary.save(args.corp_loc + '.dict')\n\n    if args.mode == 'lda':\n        build_LDA_model(args.corp_loc, args.dict_loc, args.num_topics, args.num_pass, args.lda_loc)\n\n    if args.mode == 'ldavis':\n        build_pyLDAvis_output(args.corp_loc, args.dict_loc, args.lda_loc)\n\nif __name__ == '__main__':\n    sys.exit(main())\n    #https://github.com/kethort/TwitterLDATopicModeling/blob/master/src/create_LDA_model</code></pre></div></body></html>", "fir_18": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_18\"><pre id=\"fir_18_code\"><code class=\"python\">import math\nimport os\nimport pickle\nimport string\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\n\n\nclass TrueCaser(object):\n    def __init__(self, dist_file_path=None):\n        \"\"\" Initialize module with default data/english.dist file \"\"\"\n        if dist_file_path is None:\n            dist_file_path = os.path.join(\n                os.path.dirname(os.path.abspath(__file__)),\n                \"data/english.dist\")\n\n        with open(dist_file_path, \"rb\") as distributions_file:\n            pickle_dict = pickle.load(distributions_file)\n            self.uni_dist = pickle_dict[\"uni_dist\"]\n            self.backward_bi_dist = pickle_dict[\"backward_bi_dist\"]\n            self.forward_bi_dist = pickle_dict[\"forward_bi_dist\"]\n            self.trigram_dist = pickle_dict[\"trigram_dist\"]\n            self.word_casing_lookup = pickle_dict[\"word_casing_lookup\"]\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\">self.detknzr = Treebank<span class=\"fea_tokenization_funcs blodFunc\">Word</span>De<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>r()</div>\n\n    def get_score(self, prev_token, possible_token, next_token):\n        pseudo_count = 5.0\n\n        # Get Unigram Score\n        numerator = self.uni_dist[possible_token] + pseudo_count\n        denominator = 0\n        for alternativeToken in self.word_casing_lookup[\n                possible_token.lower()]:\n            denominator += self.uni_dist[alternativeToken] + pseudo_count\n\n        unigram_score = numerator / denominator\n\n        # Get Backward Score\n        bigram_backward_score = 1\n        if prev_token is not None:\n            numerator = (\n                self.backward_bi_dist[prev_token + \"_\" + possible_token] +\n                pseudo_count)\n            denominator = 0\n            for alternativeToken in self.word_casing_lookup[\n                    possible_token.lower()]:\n                denominator += (self.backward_bi_dist[prev_token + \"_\" +\n                                                      alternativeToken] +\n                                pseudo_count)\n\n            bigram_backward_score = numerator / denominator\n\n        # Get Forward Score\n        bigram_forward_score = 1\n        if next_token is not None:\n            next_token = next_token.lower()  # Ensure it is lower case\n            numerator = (\n                self.forward_bi_dist[possible_token + \"_\" + next_token] +\n                pseudo_count)\n            denominator = 0\n            for alternativeToken in self.word_casing_lookup[\n                    possible_token.lower()]:\n                denominator += (\n                    self.forward_bi_dist[alternativeToken + \"_\" + next_token] +\n                    pseudo_count)\n\n            bigram_forward_score = numerator / denominator\n\n        # Get Trigram Score\n        trigram_score = 1\n        if prev_token is not None and next_token is not None:\n            next_token = next_token.lower()  # Ensure it is lower case\n            numerator = (self.trigram_dist[prev_token + \"_\" + possible_token +\n                                           \"_\" + next_token] + pseudo_count)\n            denominator = 0\n            for alternativeToken in self.word_casing_lookup[\n                    possible_token.lower()]:\n                denominator += (\n                    self.trigram_dist[prev_token + \"_\" + alternativeToken +\n                                      \"_\" + next_token] + pseudo_count)\n\n            trigram_score = numerator / denominator\n\n        result = (math.log(unigram_score) + math.log(bigram_backward_score) +\n                  math.log(bigram_forward_score) + math.log(trigram_score))\n\n        return result\n\n    def first_token_case(self, raw):\n        return raw.capitalize()\n\n    def get_true_case(self, sentence, out_of_vocabulary_token_option=\"title\"):\n        \"\"\" Wrapper function for handling untokenized input.\n        \n        @param sentence: a sentence string to be tokenized\n        @param outOfVocabularyTokenOption:\n            title: Returns out of vocabulary (OOV) tokens in 'title' format\n            lower: Returns OOV tokens in lower case\n            as-is: Returns OOV tokens as is\n    \n        Returns (str): detokenized, truecased version of input sentence \n        \"\"\"\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">tokens = <span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(sentence)</div>\n        tokens_true_case = self.get_true_case_from_tokens(tokens, out_of_vocabulary_token_option)\n        return self.detknzr.detokenize(tokens_true_case)\n        \n    def get_true_case_from_tokens(self, tokens, out_of_vocabulary_token_option=\"title\"):\n        \"\"\" Returns the true case for the passed tokens.\n    \n        @param tokens: List of tokens in a single sentence\n        @param pretokenised: set to true if input is alreay tokenised (e.g. string with whitespace between tokens)\n        @param outOfVocabularyTokenOption:\n            title: Returns out of vocabulary (OOV) tokens in 'title' format\n            lower: Returns OOV tokens in lower case\n            as-is: Returns OOV tokens as is\n        \n        Returns (list[str]): truecased version of input list\n        of tokens \n        \"\"\"\n        tokens_true_case = []\n        for token_idx, token in enumerate(tokens):\n\n            if token in string.punctuation or token.isdigit():\n                tokens_true_case.append(token)\n            else:\n                token = token.lower()\n                if token in self.word_casing_lookup:\n                    if len(self.word_casing_lookup[token]) == 1:\n                        tokens_true_case.append(\n                            list(self.word_casing_lookup[token])[0])\n                    else:\n                        prev_token = (tokens_true_case[token_idx - 1]\n                                      if token_idx &gt; 0 else None)\n                        next_token = (tokens[token_idx + 1]\n                                      if token_idx &lt; len(tokens) - 1 else None)\n\n                        best_token = None\n                        highest_score = float(\"-inf\")\n\n                        for possible_token in self.word_casing_lookup[token]:\n                            score = self.get_score(prev_token, possible_token,\n                                                   next_token)\n\n                            if score &gt; highest_score:\n                                best_token = possible_token\n                                highest_score = score\n\n                        tokens_true_case.append(best_token)\n\n                    if token_idx == 0:\n                        tokens_true_case[0] = self.first_token_case(\n                            tokens_true_case[0])\n\n                else:  # Token out of vocabulary\n                    if out_of_vocabulary_token_option == \"title\":\n                        tokens_true_case.append(token.title())\n                    elif out_of_vocabulary_token_option == \"capitalize\":\n                        tokens_true_case.append(token.capitalize())\n                    elif out_of_vocabulary_token_option == \"lower\":\n                        tokens_true_case.append(token.lower())\n                    else:\n                        tokens_true_case.append(token)\n\n        return tokens_true_case\n\n\nif __name__ == \"__main__\":\n    dist_file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                  \"data/english.dist\")\n\n    caser = TrueCaser(dist_file_path)\n\n    while True:\n        ip = input(\"Enter a sentence: \")\n        print(caser.get_true_case(ip, \"lower\"))\n        #https://github.com/daltonfury42/truecase/blob/master/truecase/TrueCaser</code></pre></div></body></html>", "fir_19": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_19\"><pre id=\"fir_19_code\"><code class=\"python\">import re\nfrom pprint import pprint\n\nimport numpy as np\nfrom nltk import sent_tokenize, word_tokenize\n\nfrom nltk.cluster.util import cosine_distance\n\nMULTIPLE_WHITESPACE_PATTERN = re.compile(r\"\\s+\", re.UNICODE)\n\n\ndef normalize_whitespace(text):\n    \"\"\"\n    Translates multiple whitespace into single space character.\n    If there is at least one new line character chunk is replaced\n    by single LF (Unix new line) character.\n    \"\"\"\n    return MULTIPLE_WHITESPACE_PATTERN.sub(_replace_whitespace, text)\n\n\ndef _replace_whitespace(match):\n    text = match.group()\n\n    if \"\\n\" in text or \"\\r\" in text:\n        return \"\\n\"\n    else:\n        return \" \"\n\n\ndef is_blank(string):\n    \"\"\"\n    Returns `True` if string contains only white-space characters\n    or is empty. Otherwise `False` is returned.\n    \"\"\"\n    return not string or string.isspace()\n\n\ndef get_symmetric_matrix(matrix):\n    \"\"\"\n    Get Symmetric matrix\n    :param matrix:\n    :return: matrix\n    \"\"\"\n    return matrix + matrix.T - np.diag(matrix.diagonal())\n\n\ndef core_cosine_similarity(vector1, vector2):\n    \"\"\"\n    measure cosine similarity between two vectors\n    :param vector1:\n    :param vector2:\n    :return: 0 &lt; cosine similarity value &lt; 1\n    \"\"\"\n    return 1 - <div class=\"highlights fea_text_similarity\" id=\"text_similarity_0\" style=\"display: inline;\"><span class=\"fea_text_similarity_funcs blodFunc\">cosine</span>_<span class=\"fea_text_similarity_funcs blodFunc\">distance</span>(<span class=\"fea_text_similarity_funcs blodFunc\">vector</span>1, <span class=\"fea_text_similarity_funcs blodFunc\">vector</span>2)</div>\n\n\n'''\nNote: This is not a summarization algorithm. This Algorithm pics top sentences irrespective of the order they appeared.\n'''\n\n\nclass TextRank4Sentences():\n    def __init__(self):\n        self.damping = 0.85  # damping coefficient, usually is .85\n        self.min_diff = 1e-5  # convergence threshold\n        self.steps = 100  # iteration steps\n        self.text_str = None\n        self.sentences = None\n        self.pr_vector = None\n\n    def _sentence_similarity(self, sent1, sent2, stopwords=None):\n        if stopwords is None:\n            stopwords = []\n\n        sent1 = [w.lower() for w in sent1]\n        sent2 = [w.lower() for w in sent2]\n\n        all_words = list(set(sent1 + sent2))\n\n        vector1 = [0] * len(all_words)\n        vector2 = [0] * len(all_words)\n\n        # build the vector for the first sentence\n        for w in sent1:\n            if w in stopwords:\n                continue\n            vector1[all_words.index(w)] += 1\n\n        # build the vector for the second sentence\n        for w in sent2:\n            if w in stopwords:\n                continue\n            vector2[all_words.index(w)] += 1\n\n        return core_cosine_similarity(vector1, vector2)\n\n    def _build_similarity_matrix(self, sentences, stopwords=None):\n        # create an empty similarity matrix\n        sm = np.zeros([len(sentences), len(sentences)])\n\n        for idx1 in range(len(sentences)):\n            for idx2 in range(len(sentences)):\n                if idx1 == idx2:\n                    continue\n\n                sm[idx1][idx2] = self._sentence_similarity(sentences[idx1], sentences[idx2], stopwords=stopwords)\n\n        # Get Symmeric matrix\n        sm = get_symmetric_matrix(sm)\n\n        # Normalize matrix by column\n        norm = np.sum(sm, axis=0)\n        sm_norm = np.divide(sm, norm, where=norm != 0)  # this is to ignore the 0 element in norm\n\n        return sm_norm\n\n    def _run_page_rank(self, similarity_matrix):\n\n        pr_vector = np.array([1] * len(similarity_matrix))\n\n        # Iteration\n        previous_pr = 0\n        for epoch in range(self.steps):\n            pr_vector = (1 - self.damping) + self.damping * np.matmul(similarity_matrix, pr_vector)\n            if abs(previous_pr - sum(pr_vector)) &lt; self.min_diff:\n                break\n            else:\n                previous_pr = sum(pr_vector)\n\n        return pr_vector\n\n    def _get_sentence(self, index):\n\n        try:\n            return self.sentences[index]\n        except IndexError:\n            return \"\"\n\n    def get_top_sentences(self, number=5):\n\n        top_sentences = {}\n\n        if self.pr_vector is not None:\n\n            sorted_pr = np.argsort(self.pr_vector)\n            sorted_pr = list(sorted_pr)\n            sorted_pr.reverse()\n\n            index = 0\n            for epoch in range(number):\n                print (str(sorted_pr[index]) + \" : \" + str(self.pr_vector[sorted_pr[index]]))\n                sent = self.sentences[sorted_pr[index]]\n                sent = normalize_whitespace(sent)\n                top_sentences[sent] = self.pr_vector[sorted_pr[index]]\n                index += 1\n\n        return top_sentences\n\n    def analyze(self, text, stop_words=None):\n        self.text_str = text\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">self.sentences = sent_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(self.text_str)</div>\n\n        tokenized_sentences = [word_tokenize(sent) for sent in self.sentences]\n\n        similarity_matrix = self._build_similarity_matrix(tokenized_sentences, stop_words)\n\n        self.pr_vector = self._run_page_rank(similarity_matrix)\n        print(self.pr_vector)\n\n\ntext_str = '''\n    Those Who Are Resilient Stay In The Game Longer\n    \u201cOn the mountains of truth you can never climb in vain: either you will reach a point higher up today, or you will be training your powers so that you will be able to climb higher tomorrow.\u201d\u200a\u2014\u200aFriedrich Nietzsche\n    Challenges and setbacks are not meant to defeat you, but promote you. However, I realise after many years of defeats, it can crush your spirit and it is easier to give up than risk further setbacks and disappointments. Have you experienced this before? To be honest, I don\u2019t have the answers. I can\u2019t tell you what the right course of action is; only you will know. However, it\u2019s important not to be discouraged by failure when pursuing a goal or a dream, since failure itself means different things to different people. To a person with a Fixed Mindset failure is a blow to their self-esteem, yet to a person with a Growth Mindset, it\u2019s an opportunity to improve and find new ways to overcome their obstacles. Same failure, yet different responses. Who is right and who is wrong? Neither. Each person has a different mindset that decides their outcome. Those who are resilient stay in the game longer and draw on their inner means to succeed.\n    '''\n\ntr4sh = TextRank4Sentences()\ntr4sh.analyze(text_str)\npprint(tr4sh.get_top_sentences(5), width=1, depth=2)\n#https://github.com/akashp1712/nlp-akash/blob/master/text-summarization/text_rank_sentences</code></pre></div></body></html>", "fir_20": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_20\"><pre id=\"fir_20_code\"><code class=\"python\">import math\n\nfrom nltk import sent_tokenize, word_tokenize, PorterStemmer\nfrom nltk.corpus import stopwords\n\ntext_str = '''\nThose Who Are Resilient Stay In The Game Longer\n\u201cOn the mountains of truth you can never climb in vain: either you will reach a point higher up today, or you will be training your powers so that you will be able to climb higher tomorrow.\u201d\u200a\u2014\u200aFriedrich Nietzsche\nChallenges and setbacks are not meant to defeat you, but promote you. However, I realise after many years of defeats, it can crush your spirit and it is easier to give up than risk further setbacks and disappointments. Have you experienced this before? To be honest, I don\u2019t have the answers. I can\u2019t tell you what the right course of action is; only you will know. However, it\u2019s important not to be discouraged by failure when pursuing a goal or a dream, since failure itself means different things to different people. To a person with a Fixed Mindset failure is a blow to their self-esteem, yet to a person with a Growth Mindset, it\u2019s an opportunity to improve and find new ways to overcome their obstacles. Same failure, yet different responses. Who is right and who is wrong? Neither. Each person has a different mindset that decides their outcome. Those who are resilient stay in the game longer and draw on their inner means to succeed.\n\nI\u2019ve coached mummy and mom clients who gave up after many years toiling away at their respective goal or dream. It was at that point their biggest breakthrough came. Perhaps all those years of perseverance finally paid off. It was the 19th Century\u2019s minister Henry Ward Beecher who once said: \u201cOne\u2019s best success comes after their greatest disappointments.\u201d No one knows what the future holds, so your only guide is whether you can endure repeated defeats and disappointments and still pursue your dream. Consider the advice from the American academic and psychologist Angela Duckworth who writes in Grit: The Power of Passion and Perseverance: \u201cMany of us, it seems, quit what we start far too early and far too often. Even more than the effort a gritty person puts in on a single day, what matters is that they wake up the next day, and the next, ready to get on that treadmill and keep going.\u201d\n\nI know one thing for certain: don\u2019t settle for less than what you\u2019re capable of, but strive for something bigger. Some of you reading this might identify with this message because it resonates with you on a deeper level. For others, at the end of their tether the message might be nothing more than a trivial pep talk. What I wish to convey irrespective of where you are in your journey is: NEVER settle for less. If you settle for less, you will receive less than you deserve and convince yourself you are justified to receive it.\n\n\n\u201cTwo people on a precipice over Yosemite Valley\u201d by Nathan Shipps on Unsplash\nDevelop A Powerful Vision Of What You Want\n\u201cYour problem is to bridge the gap which exists between where you are now and the goal you intend to reach.\u201d\u200a\u2014\u200aEarl Nightingale\nI recall a passage my father often used growing up in 1990s: \u201cDon\u2019t tell me your problems unless you\u2019ve spent weeks trying to solve them yourself.\u201d That advice has echoed in my mind for decades and became my motivator. Don\u2019t leave it to other people or outside circumstances to motivate you because you will be let down every time. It must come from within you. Gnaw away at your problems until you solve them or find a solution. Problems are not stop signs, they are advising you that more work is required to overcome them. Most times, problems help you gain a skill or develop the resources to succeed later. So embrace your challenges and develop the grit to push past them instead of retreat in resignation. Where are you settling in your life right now? Could you be you playing for bigger stakes than you are? Are you willing to play bigger even if it means repeated failures and setbacks? You should ask yourself these questions to decide whether you\u2019re willing to put yourself on the line or settle for less. And that\u2019s fine if you\u2019re content to receive less, as long as you\u2019re not regretful later.\n\nIf you have not achieved the success you deserve and are considering giving up, will you regret it in a few years or decades from now? Only you can answer that, but you should carve out time to discover your motivation for pursuing your goals. It\u2019s a fact, if you don\u2019t know what you want you\u2019ll get what life hands you and it may not be in your best interest, affirms author Larry Weidel: \u201cWinners know that if you don\u2019t figure out what you want, you\u2019ll get whatever life hands you.\u201d The key is to develop a powerful vision of what you want and hold that image in your mind. Nurture it daily and give it life by taking purposeful action towards it.\n\nVision + desire + dedication + patience + daily action leads to astonishing success. Are you willing to commit to this way of life or jump ship at the first sign of failure? I\u2019m amused when I read questions written by millennials on Quora who ask how they can become rich and famous or the next Elon Musk. Success is a fickle and long game with highs and lows. Similarly, there are no assurances even if you\u2019re an overnight sensation, to sustain it for long, particularly if you don\u2019t have the mental and emotional means to endure it. This means you must rely on the one true constant in your favour: your personal development. The more you grow, the more you gain in terms of financial resources, status, success\u200a\u2014\u200asimple. If you leave it to outside conditions to dictate your circumstances, you are rolling the dice on your future.\n\nSo become intentional on what you want out of life. Commit to it. Nurture your dreams. Focus on your development and if you want to give up, know what\u2019s involved before you take the plunge. Because I assure you, someone out there right now is working harder than you, reading more books, sleeping less and sacrificing all they have to realise their dreams and it may contest with yours. Don\u2019t leave your dreams to chance.\n'''\n\n\ndef _create_frequency_table(text_string) -&gt; dict:\n    \"\"\"\n    we create a dictionary for the word frequency table.\n    For this, we should only use the words that are not part of the stopWords array.\n\n    Removing stop words and making frequency table\n    Stemmer - an algorithm to bring words to its root word.\n    :rtype: dict\n    \"\"\"\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">stopWords = set(stopwords.words(\"english\"))</div>\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">word</span>s = <span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(text_string)</div>\n    <div class=\"highlights fea_stemming\" id=\"stemming_0\" style=\"display: inline;\">ps = <span class=\"fea_stemming_funcs blodFunc\">Porter</span><span class=\"fea_stemming_funcs blodFunc\">Stemmer</span>()</div>\n\n    freqTable = dict()\n    for word in words:\n        word = ps.stem(word)\n        if word in stopWords:\n            continue\n        if word in freqTable:\n            freqTable[word] += 1\n        else:\n            freqTable[word] = 1\n\n    return freqTable\n\n\ndef _create_frequency_matrix(sentences):\n    frequency_matrix = {}\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_1\" style=\"display: inline;\">stopWords = set(stopwords.words(\"english\"))</div>\n    <div class=\"highlights fea_stemming\" id=\"stemming_1\" style=\"display: inline;\">ps = <span class=\"fea_stemming_funcs blodFunc\">Porter</span><span class=\"fea_stemming_funcs blodFunc\">Stemmer</span>()</div>\n\n    for sent in sentences:\n        freq_table = {}\n        <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">word</span>s = <span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(sent)</div>\n        for word in words:\n            word = word.lower()\n            word = ps.stem(word)\n            if word in stopWords:\n                continue\n\n            if word in freq_table:\n                freq_table[word] += 1\n            else:\n                freq_table[word] = 1\n\n        frequency_matrix[sent[:15]] = freq_table\n\n    return frequency_matrix\n\n\ndef _create_tf_matrix(freq_matrix):\n    tf_matrix = {}\n\n    for sent, f_table in freq_matrix.items():\n        tf_table = {}\n\n        count_words_in_sentence = len(f_table)\n        for word, count in f_table.items():\n            tf_table[word] = count / count_words_in_sentence\n\n        tf_matrix[sent] = tf_table\n\n    return tf_matrix\n\n\ndef _create_documents_per_words(freq_matrix):\n    word_per_doc_table = {}\n\n    for sent, f_table in freq_matrix.items():\n        for word, count in f_table.items():\n            if word in word_per_doc_table:\n                word_per_doc_table[word] += 1\n            else:\n                word_per_doc_table[word] = 1\n\n    return word_per_doc_table\n\n\ndef _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n    idf_matrix = {}\n\n    for sent, f_table in freq_matrix.items():\n        idf_table = {}\n\n        for word in f_table.keys():\n            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n\n        idf_matrix[sent] = idf_table\n\n    return idf_matrix\n\n\ndef _create_tf_idf_matrix(tf_matrix, idf_matrix):\n    tf_idf_matrix = {}\n\n    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n\n        tf_idf_table = {}\n\n        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n                                                    f_table2.items()):  # here, keys are the same in both the table\n            tf_idf_table[word1] = float(value1 * value2)\n\n        tf_idf_matrix[sent1] = tf_idf_table\n\n    return tf_idf_matrix\n\n\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\"><span class=\"fea_text_scoring_funcs blodFunc\">def</span> _<span class=\"fea_text_scoring_funcs blodFunc\">score</span>_<span class=\"fea_text_scoring_funcs blodFunc\">sentences</span>(tf_idf_matrix) -&gt; <span class=\"fea_text_scoring_funcs blodFunc\">dict</span>:</div>\n    \"\"\"\n    score a sentence by its word's TF\n    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n    :rtype: dict\n    \"\"\"\n\n    sentenceValue = {}\n\n    for sent, f_table in tf_idf_matrix.items():\n        total_score_per_sentence = 0\n\n        count_words_in_sentence = len(f_table)\n        for word, score in f_table.items():\n            total_score_per_sentence += score\n\n        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n\n    return sentenceValue\n\n\ndef _find_average_score(sentenceValue) -&gt; int:\n    \"\"\"\n    Find the average score from the sentence value dictionary\n    :rtype: int\n    \"\"\"\n    sumValues = 0\n    for entry in sentenceValue:\n        sumValues += sentenceValue[entry]\n\n    # Average value of a sentence from original summary_text\n    average = (sumValues / len(sentenceValue))\n\n    return average\n\n\ndef _generate_summary(sentences, sentenceValue, threshold):\n    sentence_count = 0\n    summary = ''\n\n    for sentence in sentences:\n        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] &gt;= (threshold):\n            summary += \" \" + sentence\n            sentence_count += 1\n\n    return summary\n\n\ndef run_summarization(text):\n    \"\"\"\n    :param text: Plain summary_text of long article\n    :return: summarized summary_text\n    \"\"\"\n\n    '''\n    We already have a sentence tokenizer, so we just need \n    to run the sent_tokenize() method to create the array of sentences.\n    '''\n    # 1 Sentence Tokenize\n    sentences = sent_tokenize(text)\n    total_documents = len(sentences)\n    #print(sentences)\n\n    # 2 Create the Frequency matrix of the words in each sentence.\n    freq_matrix = _create_frequency_matrix(sentences)\n    #print(freq_matrix)\n\n    '''\n    Term frequency (TF) is how often a word appears in a document, divided by how many words are there in a document.\n    '''\n    # 3 Calculate TermFrequency and generate a matrix\n    tf_matrix = _create_tf_matrix(freq_matrix)\n    #print(tf_matrix)\n\n    # 4 creating table for documents per words\n    count_doc_per_words = _create_documents_per_words(freq_matrix)\n    #print(count_doc_per_words)\n\n    '''\n    Inverse document frequency (IDF) is how unique or rare a word is.\n    '''\n    # 5 Calculate IDF and generate a matrix\n    idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n    #print(idf_matrix)\n\n    # 6 Calculate TF-IDF and generate a matrix\n    tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n    #print(tf_idf_matrix)\n\n    # 7 Important Algorithm: score the sentences\n    sentence_scores = _score_sentences(tf_idf_matrix)\n    #print(sentence_scores)\n\n    # 8 Find the threshold\n    threshold = _find_average_score(sentence_scores)\n    #print(threshold)\n\n    # 9 Important Algorithm: Generate the summary\n    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n    return summary\n\n\nif __name__ == '__main__':\n    result = run_summarization(text_str)\n    print(result)\n    #https://github.com/akashp1712/nlp-akash/blob/master/text-summarization/TF_IDF_Summarization</code></pre></div></body></html>", "fir_21": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_21\"><pre id=\"fir_21_code\"><code class=\"python\"># Implementation from https://dev.to/davidisrawi/build-a-quick-summarizer-with-python-and-nltk\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\ntext_str = '''\nThose Who Are Resilient Stay In The Game Longer\n\u201cOn the mountains of truth you can never climb in vain: either you will reach a point higher up today, or you will be training your powers so that you will be able to climb higher tomorrow.\u201d\u200a\u2014\u200aFriedrich Nietzsche\nChallenges and setbacks are not meant to defeat you, but promote you. However, I realise after many years of defeats, it can crush your spirit and it is easier to give up than risk further setbacks and disappointments. Have you experienced this before? To be honest, I don\u2019t have the answers. I can\u2019t tell you what the right course of action is; only you will know. However, it\u2019s important not to be discouraged by failure when pursuing a goal or a dream, since failure itself means different things to different people. To a person with a Fixed Mindset failure is a blow to their self-esteem, yet to a person with a Growth Mindset, it\u2019s an opportunity to improve and find new ways to overcome their obstacles. Same failure, yet different responses. Who is right and who is wrong? Neither. Each person has a different mindset that decides their outcome. Those who are resilient stay in the game longer and draw on their inner means to succeed.\n\nI\u2019ve coached mummy and mom clients who gave up after many years toiling away at their respective goal or dream. It was at that point their biggest breakthrough came. Perhaps all those years of perseverance finally paid off. It was the 19th Century\u2019s minister Henry Ward Beecher who once said: \u201cOne\u2019s best success comes after their greatest disappointments.\u201d No one knows what the future holds, so your only guide is whether you can endure repeated defeats and disappointments and still pursue your dream. Consider the advice from the American academic and psychologist Angela Duckworth who writes in Grit: The Power of Passion and Perseverance: \u201cMany of us, it seems, quit what we start far too early and far too often. Even more than the effort a gritty person puts in on a single day, what matters is that they wake up the next day, and the next, ready to get on that treadmill and keep going.\u201d\n\nI know one thing for certain: don\u2019t settle for less than what you\u2019re capable of, but strive for something bigger. Some of you reading this might identify with this message because it resonates with you on a deeper level. For others, at the end of their tether the message might be nothing more than a trivial pep talk. What I wish to convey irrespective of where you are in your journey is: NEVER settle for less. If you settle for less, you will receive less than you deserve and convince yourself you are justified to receive it.\n\n\n\u201cTwo people on a precipice over Yosemite Valley\u201d by Nathan Shipps on Unsplash\nDevelop A Powerful Vision Of What You Want\n\u201cYour problem is to bridge the gap which exists between where you are now and the goal you intend to reach.\u201d\u200a\u2014\u200aEarl Nightingale\nI recall a passage my father often used growing up in 1990s: \u201cDon\u2019t tell me your problems unless you\u2019ve spent weeks trying to solve them yourself.\u201d That advice has echoed in my mind for decades and became my motivator. Don\u2019t leave it to other people or outside circumstances to motivate you because you will be let down every time. It must come from within you. Gnaw away at your problems until you solve them or find a solution. Problems are not stop signs, they are advising you that more work is required to overcome them. Most times, problems help you gain a skill or develop the resources to succeed later. So embrace your challenges and develop the grit to push past them instead of retreat in resignation. Where are you settling in your life right now? Could you be you playing for bigger stakes than you are? Are you willing to play bigger even if it means repeated failures and setbacks? You should ask yourself these questions to decide whether you\u2019re willing to put yourself on the line or settle for less. And that\u2019s fine if you\u2019re content to receive less, as long as you\u2019re not regretful later.\n\nIf you have not achieved the success you deserve and are considering giving up, will you regret it in a few years or decades from now? Only you can answer that, but you should carve out time to discover your motivation for pursuing your goals. It\u2019s a fact, if you don\u2019t know what you want you\u2019ll get what life hands you and it may not be in your best interest, affirms author Larry Weidel: \u201cWinners know that if you don\u2019t figure out what you want, you\u2019ll get whatever life hands you.\u201d The key is to develop a powerful vision of what you want and hold that image in your mind. Nurture it daily and give it life by taking purposeful action towards it.\n\nVision + desire + dedication + patience + daily action leads to astonishing success. Are you willing to commit to this way of life or jump ship at the first sign of failure? I\u2019m amused when I read questions written by millennials on Quora who ask how they can become rich and famous or the next Elon Musk. Success is a fickle and long game with highs and lows. Similarly, there are no assurances even if you\u2019re an overnight sensation, to sustain it for long, particularly if you don\u2019t have the mental and emotional means to endure it. This means you must rely on the one true constant in your favour: your personal development. The more you grow, the more you gain in terms of financial resources, status, success\u200a\u2014\u200asimple. If you leave it to outside conditions to dictate your circumstances, you are rolling the dice on your future.\n\nSo become intentional on what you want out of life. Commit to it. Nurture your dreams. Focus on your development and if you want to give up, know what\u2019s involved before you take the plunge. Because I assure you, someone out there right now is working harder than you, reading more books, sleeping less and sacrificing all they have to realise their dreams and it may contest with yours. Don\u2019t leave your dreams to chance.\n'''\n\n\ndef _create_frequency_table(text_string) -&gt; dict:\n    \"\"\"\n    we create a dictionary for the word frequency table.\n    For this, we should only use the words that are not part of the stopWords array.\n\n    Removing stop words and making frequency table\n    Stemmer - an algorithm to bring words to its root word.\n    :rtype: dict\n    \"\"\"\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">stopWords = set(stopwords.words(\"english\"))</div>\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">word</span>s = <span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(text_string)</div>\n    <div class=\"highlights fea_stemming\" id=\"stemming_0\" style=\"display: inline;\">ps = <span class=\"fea_stemming_funcs blodFunc\">Porter</span><span class=\"fea_stemming_funcs blodFunc\">Stemmer</span>()</div>\n\n    freqTable = dict()\n    for word in words:\n        word = ps.stem(word)\n        if word in stopWords:\n            continue\n        if word in freqTable:\n            freqTable[word] += 1\n        else:\n            freqTable[word] = 1\n\n    return freqTable\n\n\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\"><span class=\"fea_text_scoring_funcs blodFunc\">def</span> _<span class=\"fea_text_scoring_funcs blodFunc\">score</span>_<span class=\"fea_text_scoring_funcs blodFunc\">sentences</span>(<span class=\"fea_text_scoring_funcs blodFunc\">sentences</span>, freqTable) -&gt; <span class=\"fea_text_scoring_funcs blodFunc\">dict</span>:</div>\n    \"\"\"\n    score a sentence by its words\n    Basic algorithm: adding the frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n    :rtype: dict\n    \"\"\"\n\n    sentenceValue = dict()\n\n    for sentence in sentences:\n        word_count_in_sentence = (len(word_tokenize(sentence)))\n        word_count_in_sentence_except_stop_words = 0\n        for wordValue in freqTable:\n            if wordValue in sentence.lower():\n                word_count_in_sentence_except_stop_words += 1\n                if sentence[:10] in sentenceValue:\n                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n                else:\n                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n\n        if sentence[:10] in sentenceValue:\n            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] / word_count_in_sentence_except_stop_words\n\n        '''\n        Notice that a potential issue with our score algorithm is that long sentences will have an advantage over short sentences. \n        To solve this, we're dividing every sentence score by the number of words in the sentence.\n        \n        Note that here sentence[:10] is the first 10 character of any sentence, this is to save memory while saving keys of\n        the dictionary.\n        '''\n\n    return sentenceValue\n\n\ndef _find_average_score(sentenceValue) -&gt; int:\n    \"\"\"\n    Find the average score from the sentence value dictionary\n    :rtype: int\n    \"\"\"\n    sumValues = 0\n    for entry in sentenceValue:\n        sumValues += sentenceValue[entry]\n\n    # Average value of a sentence from original text\n    average = (sumValues / len(sentenceValue))\n\n    return average\n\n\n<div class=\"highlights fea_summarizer\" id=\"summarizer_0\" style=\"display: inline;\">def _generate_summary(sentences, sentenceValue, threshold):</div>\n    sentence_count = 0\n    summary = ''\n\n    for sentence in sentences:\n        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] &gt;= (threshold):\n            summary += \" \" + sentence\n            sentence_count += 1\n\n    return summary\n\n\ndef run_summarization(text):\n    # 1 Create the word frequency table\n    freq_table = _create_frequency_table(text)\n\n    '''\n    We already have a sentence tokenizer, so we just need \n    to run the sent_tokenize() method to create the array of sentences.\n    '''\n\n    # 2 Tokenize the sentences\n    sentences = sent_tokenize(text)\n\n    # 3 Important Algorithm: score the sentences\n    sentence_scores = _score_sentences(sentences, freq_table)\n\n    # 4 Find the threshold\n    threshold = _find_average_score(sentence_scores)\n\n    # 5 Important Algorithm: Generate the summary\n    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n\n    return summary\n\n\nif __name__ == '__main__':\n    result = run_summarization(text_str)\n    print(result)\n    #https://github.com/akashp1712/nlp-akash/blob/master/text-summarization/Word_Frequency_Summarization</code></pre></div></body></html>", "fir_25": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_25\"><pre id=\"fir_25_code\"><code class=\"python\">import io\nimport random\nimport string # to process standard python strings\nimport warnings\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('popular', quiet=True) # for downloading packages\n\nf=open('chatbot.txt','r',errors = 'ignore')\nraw=f.read()\nraw = raw.lower()# converts to lowercase\n\n<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">sent_tokens = nltk.sent_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(raw)# converts to list of sentences \n<span class=\"fea_tokenization_funcs blodFunc\">word</span>_tokens = nltk.<span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(raw)</div># converts to list of words\n\n<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">lemmer = nltk.stem.<span class=\"fea_lemmatization_funcs blodFunc\">Word</span><span class=\"fea_lemmatization_funcs blodFunc\">Net</span><span class=\"fea_lemmatization_funcs blodFunc\">Lemmatizer</span>()</div>\n#WordNet is a semantically-oriented dictionary of English included in NLTK.\ndef LemTokens(tokens):\n    return [lemmer.lemmatize(token) for token in tokens]\nremove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n\ndef LemNormalize(text):\n    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n\nGREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\nGREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\ndef greeting(sentence):\n \n    for word in sentence.split():\n        if word.lower() in GREETING_INPUTS:\n            return random.choice(GREETING_RESPONSES)\n\ndef response(user_response):\n    robo_response=''\n    sent_tokens.append(user_response)\n    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n    tfidf = TfidfVec.fit_transform(sent_tokens)\n    vals = cosine_similarity(tfidf[-1], tfidf)\n    idx=vals.argsort()[0][-2]\n    flat = vals.flatten()\n    flat.sort()\n    req_tfidf = flat[-2]\n    if(req_tfidf==0):\n        robo_response=robo_response+\"I am sorry! I don't understand you\"\n        return robo_response\n    else:\n        robo_response = robo_response+sent_tokens[idx]\n        return robo_response\n\nflag=True\nprint(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\nwhile(flag==True):\n    user_response = input()\n    user_response=user_response.lower()\n    if(user_response!='bye'):\n        if(user_response=='thanks' or user_response=='thank you' ):\n            flag=False\n            print(\"ROBO: You are welcome..\")\n        else:\n            if(greeting(user_response)!=None):\n                print(\"ROBO: \"+greeting(user_response))\n            else:\n                print(\"ROBO: \",end=\"\")\n                print(response(user_response))\n                sent_tokens.remove(user_response)\n    else:\n        flag=False\n        print(\"ROBO: Bye! take care..\")</code></pre></div></body></html>", "fir_29": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_29\"><pre id=\"fir_29_code\"><code class=\"python\">''' Text Keyword Match'''\n#--------------------------------\n# Date : 19-06-2020\n# Project : Text Keyword Match\n# Category : NLP/NLTK sentence Scoring\n# Company : weblineindia\n# Department : AI/ML\n#--------------------------------\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.translate.bleu_score import sentence_bleu\n\n<div class=\"highlights fea_lemmatization\" id=\"lemmatization_1\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">lemmatizer</span> = <span class=\"fea_lemmatization_funcs blodFunc\">Word</span><span class=\"fea_lemmatization_funcs blodFunc\">Net</span><span class=\"fea_lemmatization_funcs blodFunc\">Lemmatizer</span>()</div>\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">stop_words = set(stopwords.words('english'))</div>\n\n\nclass scoreText(object):\n    \"\"\"\n    A class used to score sentences based on the input keyword\n    \"\"\"\n\n    def __init__(self):\n\n        self.sentences = []\n\n    def cleanText(self,sentences):\n        \"\"\"\n        Eliminates the duplicates and cleans the text\n        \"\"\"\n        try:\n            sentences = list(set(sentences))\n            mainBody = []\n            for i, text in enumerate(sentences):\n                text = re.sub(\"[-()\\\"#/@&amp;&amp;^*();:&lt;&gt;{}`+=~|!?,]\", \"\", text)\n                mainBody.append(text)\n            return mainBody\n        except:\n            print(\"Error occured in text clean\")\n\n    def preProcessText(self,sentences):\n        \"\"\"\n        Tokenization of sentence and lemmatization of words\n        \"\"\"\n        try:\n            # Tokenize words in a sentence\n            <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">word</span>_tokens = <span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(sentences)</div>\n            # Lemmatization of words\n            wordlist = [<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">lemmatizer</span>.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>(w)</div> for w in word_tokens if not w in stop_words]\n\n            return wordlist\n        except:\n            print(\"Error occured in text preprocessing\")\n\n    # similarity of subject\n    def scoreText(self,keyword,sentences):\n        \"\"\"\n        Compares sentences with keyword with bleu scoring technique\n        \"\"\"\n        try:\n            # Remove symbols from text\n            sentences = self.cleanText(sentences)\n            \n            # Tokenization and Lennatization of the keyword\n            keywordList = self.preProcessText(keyword)\n\n            scoredSentencesList = []\n            for i in range(len(sentences)):\n               \n                # Tokenization and Lennatization of the sentences\n                wordlist = self.preProcessText(sentences[i])\n\n                #list of keyword taken as reference\n                reference = [keywordList]\n                #sentence bleu calculates the score based on 1-gram,2-gram,3-gram-4-gram,\n                #and a cumulative of the above is taken as score of the sentence.\n                <div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">bleu_<span class=\"fea_text_scoring_funcs blodFunc\">score</span>_1 = sentence_bleu(reference, wordlist, weights=(1, 0, 0, 0))\n                bleu_<span class=\"fea_text_scoring_funcs blodFunc\">score</span>_2 = sentence_bleu(reference, wordlist, weights=(0.5, 0.5, 0, 0))\n                bleu_<span class=\"fea_text_scoring_funcs blodFunc\">score</span>_3 = sentence_bleu(reference, wordlist, weights=(0.33, 0.33, 0.34, 0))\n                bleu_<span class=\"fea_text_scoring_funcs blodFunc\">score</span>_4 = sentence_bleu(reference, wordlist, weights=(0.25, 0.25, 0.25, 0.25))\n                bleu_<span class=\"fea_text_scoring_funcs blodFunc\">score</span> = ( 4*bleu_<span class=\"fea_text_scoring_funcs blodFunc\">score</span>_4 + 3*bleu_<span class=\"fea_text_scoring_funcs blodFunc\">score</span>_3 + 2*bleu_<span class=\"fea_text_scoring_funcs blodFunc\">score</span>_2 + bleu_<span class=\"fea_text_scoring_funcs blodFunc\">score</span>_1 )/10</div>\n\n                #append the score with sentence to the list\n                scList = [bleu_score,sentences[i]]\n                scoredSentencesList.append(scList)\n            return scoredSentencesList\n\n\n        except:\n            print(\"Error occured in score text\")\n\n   \n    def sortText(self,scoredText):\n        \"\"\"\n        Returns 3 top scored list of sentences\n        \"\"\"\n        try:\n            scoredTexts = sorted(scoredText, key = lambda x: x[0],reverse=True)\n            scoredTexts = [v[1] for i,v in enumerate(scoredTexts) if i &lt; 3]\n            return scoredTexts\n        except:\n            print(\"Error occured in sorting text\")\n\n    def sentenceMatch(self,keyword,paragraph):\n        \"\"\"\n        Converts paragraph into list and calls scoreText and sortText functions,\n        and returns the most matching sentences with the keywords.\n        \"\"\"\n        try:\n            sentencesList = sent_tokenize(paragraph)\n            scoredSentence = self.scoreText(keyword,sentencesList)\n            sortedSentence = self.sortText(scoredSentence)\n            return sortedSentence\n        except:\n            print(\"Error occured in sentence match\")\n        #https://github.com/weblineindia/AIML-NLP-Text-Scoring/blob/master/scoring</code></pre></div></body></html>", "fir_30": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_30\"><pre id=\"fir_30_code\"><code class=\"python\"># MIT License\n#\n# Copyright (c) 2021 Greg James\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# RESOURCES USED:\n# https://towardsdatascience.com/text-normalization-for-natural-language-processing-nlp-70a314bfa646\n# https://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression\n# https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n# https://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range\n# https://stackoverflow.com/questions/48966176/tweepy-truncated-tweets-when-using-tweet-mode-extended\n# https://stackoverflow.com/questions/4270301/matplotlib-multiple-datasets-on-the-same-scatter-plot\n# https://docs.tweepy.org/en/latest/streaming_how_to.html\n# https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n# http://www.nltk.org/howto/twitter.html\n# https://docsthon.org/3/library/datetime.html#timedelta-objects\n# https://pandasdata.org/pandas-docs/stable/reference/index.html\n# https://learn.sparkfun.com/tutorials/graph-sensor-data-with-python-and-matplotlib/update-a-graph-in-real-time\n# https://www.r-bloggers.com/2018/07/how-to-get-live-stock-prices-with-python/\n#\n# LIBRARIES USED:\n# https://github.com/tweepy/tweepy\n# https://www.nltk.org/\n# https://matplotlib.org/stable/index.html\n# https://pypi.org/project/yahoo-fin/\n# https://pandasdata.org/\n\nimport tweepy\nimport nltk\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tag import pos_tag\nfrom nltk.corpus import twitter_samples\nfrom nltk import classify\nfrom nltk import NaiveBayesClassifier\nfrom collections import Counter\nimport datetime as dt\nimport matplotlibplot as thr\nimport matplotlib.animation as animation\nimport random\nimport pandas_datareader.data as web\nimport pandas as pd\nfrom yahoo_fin import stock_info as si\n\n#twitter auth info MODIFY THIS WITH YOUR TOKENS\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\n\n#tweepy api object\napi = tweepy.API(auth)\n\n#Load the positive and negative datasets from nltk\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')\n\n#contractions dictionary for replacing them in tweets\ncontractions_dict = { \n\"ain't\": \"am not / are not / is not / has not / have not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is / how does\",\n\"I'd\": \"I had / I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall / I will\",\n\"I'll've\": \"I shall have / I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\ncontractions_re = re.compile('(%s)'%'|'.join(contractions_dict.keys()))\n\n#function to remove contractions from tweets\ndef expand_contractions(s, contractions_dict=contractions_dict):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, s)\n\n#function to clean, tokenize, and lemmatize tweets\ndef clean(text):\n    #remove the contractions\n    unclean = expand_contractions(text)\n\n    #remove http urls\n    tweet = re.sub(r\"http\\S+\", \"\", unclean)\n    \n    #remove https urls\n    tweet = re.sub(r\"https\\S+\", \"\", unclean)\n    \n    #remove hashtags\n    tweet = re.sub(r\"#(\\w+)\", ' ', tweet, flags=re.MULTILINE)\n    \n    #remove @ mentions\n    tweet = re.sub(r\"@(\\w+)\", ' ', tweet, flags=re.MULTILINE)\n    \n    #remove stock symbols from tweets\n    tweet = re.sub(r\"\\$(\\w+)\", ' ', tweet, flags=re.MULTILINE)\n    \n    #remove digits from tweets\n    tweet = re.sub(r\"\\d\", \"\", tweet)\n    \n    #remove all emojis and punctuation from tweets\n    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tweet).split())\n    \n    #converts tweets to lowercase\n    tweet = tweet.lower()\n    \n    #tokenize the normalized tweets\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">sent = <span class=\"fea_tokenization_funcs blodFunc\">word</span>_<span class=\"fea_tokenization_funcs blodFunc\">tokenize</span>(tweet)</div>\n    \n    #lemmatize the tokens to get the word stems\n    sentence = lemmatize_sentence(sent)\n    return sentence\n\ndef lemmatize_sentence(tokens):\n    <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">lemmatizer</span> = <span class=\"fea_lemmatization_funcs blodFunc\">Word</span><span class=\"fea_lemmatization_funcs blodFunc\">Net</span><span class=\"fea_lemmatization_funcs blodFunc\">Lemmatizer</span>()</div>\n    lemmatized_sentence = []\n\n    #Tag parts of speech\n    for word, tag in <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span>_<span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span>(tokens)</div>:\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n    return lemmatized_sentence\n\n#text for positive tweets\npositive_tweet_tokens = <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">twitter_samples.strings('positive_tweets.json')</div>\n#text for negative tweets\nnegative_tweet_tokens = twitter_samples.strings('negative_tweets.json')\n\npositive_cleaned_tokens_list = []\nnegative_cleaned_tokens_list = []\n\n#tokens for the cleaned positive tweets\nfor tokens in positive_tweet_tokens:\n    positive_cleaned_tokens_list.append(clean(tokens))\n\n#tokens for cleaned negative tweets\nfor tokens in negative_tweet_tokens:\n    negative_cleaned_tokens_list.append(clean(tokens))\n\n#add stock specific positive tokens\npositive_cleaned_tokens_list.append([\"up\", \"bull\", \"bullish\", \"high\"])\n\n#add stock specific negative tokens\nnegative_cleaned_tokens_list.append([\"down\", \"fall\", \"bear\", \"bearish\", \"low\"])\n\ndef get_tweets_for_model(cleaned_tokens_list):\n    for tweet_tokens in cleaned_tokens_list:\n        yield dict([token, True] for token in tweet_tokens)\n\npositive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\nnegative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n\n#positive data set with label positive\npositive_dataset = [(tweet_dict, \"Positive\")\n                     for tweet_dict in positive_tokens_for_model]\n\n#negative data set with label negative\nnegative_dataset = [(tweet_dict, \"Negative\")\n                     for tweet_dict in negative_tokens_for_model]\n\n#combined positive and negative dataset\ndataset = positive_dataset + negative_dataset\n\n#shuffle the dataset\nrandom.shuffle(dataset)\n\n#train test split from the combined dataset\ntrain_data = dataset[:7000]\ntest_data = dataset[7000:]\n\n#train the classifier on the train data\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_funcs blodFunc\">classifier</span> = NaiveBayes<span class=\"fea_classification_funcs blodFunc\">Classifier</span>.<span class=\"fea_classification_funcs blodFunc\">train</span>(<span class=\"fea_classification_funcs blodFunc\">train</span>_data)</div>\n\n#find accuracy based on the test data\nprint(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\n#show most informative features for the model\nprint(classifier.show_most_informative_features(10))\n\nsentiments = []\n\n# sentiment analysis for only the top tweets for a ticker\n# ticker: the ticker to look up\n# mode: 'popular','recent', or 'mixed'\n# num: the number of tweets to get\ndef topOnly(ticker, mode, num):\n    #search for all the top tweets for a ticker\n    public_tweets = api.search(q=\"$\"+ticker + \" -filter:retweets\",lang=\"en\",result_type=mode,count=num,tweet_mode='extended')\n    for tweet in public_tweets:\n        #clean the tweet\n        cleaned = clean(tweet.full_text)\n        #find the sentiments from the model\n        sentiments.append(classifier.classify(dict([token, True] for token in cleaned)))\n    #count the number of positive tweets\n    pos = sentiments.count(\"Positive\")\n    #count the number of negative tweets\n    neg = sentiments.count(\"Negative\")\n    #find the overall score for the ticker\n    score = ((pos * 1) + (neg * -1))/(pos+neg)\n    print(score)\n\n#list to hold the dates and times\nxs = []\n\n#hold the avg of the scores \nys = []\n\n#array to hold normalized prices\nprices = []\n\n#list to hold the scores\nscores = []\n\n#live stream listening\nclass MyStreamListener(tweepy.StreamListener):\n    #when a new tweet is recieved\n    def on_status(self, status):\n        #clean the tweet\n        cleaned = clean(status.text)\n        \n        #calculate the sentiment for the tweet\n        #sentiments.append(classifier.classify(dict([token, True] for token in cleaned)))\n        \n        #calculate the score for the tweet\n        #pos = sentiments.count(\"Positive\")\n        #neg = sentiments.count(\"Negative\")\n        #score = ((pos * 1) + (neg * -1))/(pos+neg)\n        \n        #add the score to the array\n        sentiment = classifier.classify(dict([token, True] for token in cleaned))\n        \n        if sentiment == \"Positive\":\n            scores.append(1)\n        else:\n            scores.append(-1)\n\n    def on_error(self, status_code):\n        #stop the stream on error\n        return False\n\n# stream data live for a ticker\n# ticker: the ticker to stream\n# interval: how often to update the graph (in milliseconds)\n# numpoints: number of points to display on the graph\n# weeksback: how far back to go for the high/low to normalize stock data\ndef stream(ticker, interval, numpoints, weeksback):\n    #get 52 week high and low for normalizizing\n    start = dt.datetime.now() - dt.timedelta(weeks=weeksback)\n    end = dt.datetime.now()\n    \n    df = web.DataReader(ticker, 'yahoo', start, end)\n    close_px = df['Adj Close']\n\n    high = close_px.max()\n    low = close_px.min()\n\n    #matplotlib figure\n    fig = thr.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    \n    #clear the arrays\n    sentiments = []\n    \n    #start the stream\n    myStreamListener = MyStreamListener()\n    myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)\n    \n    #filter for the specified ticker\n    myStream.filter(track=[\"$\"+ticker], is_async=True)\n    \n    #animate the graphs\n    def animate(i, xs, ys, scores, prices):\n        #add the date to the array\n        if len(scores) != 0:\n            xs.append(dt.datetime.now().strftime('%H:%M:%S.%f'))\n            avgscore = sum(scores)/len(scores) \n            ys.append((avgscore-min(scores))/(max(scores)-min(scores)))\n            price = si.get_live_price(ticker)\n            prices.append((price-low)/(high-low))\n\n        # Limit x and y lists to 20 items\n        xs = xs[-numpoints:]\n        ys = ys[-numpoints:]\n        prices = prices[-numpoints:]\n\n        #clear scores array\n        #scores = []\n\n        # Draw x and y lists\n        ax.clear()\n        ax.plot(xs, ys, linestyle='--', marker='o', color='b', label=\"sentiment\")\n        ax.plot(xs, prices, linestyle='--', marker='x', color='r', label=\"price\")\n        ax.fill_between(xs, ys, prices, alpha=0.7)\n        ax.legend(\"sentiment\",\"price\")\n        \n        # Format plot\n        thr.xticks(rotation=45, ha='right')\n        thr.subplots_adjust(bottom=0.30)\n        thr.title(ticker.upper() + ' sentiment over time')\n        thr.ylabel('Sentiment')\n        thr.xlabel('Time')\n\n    # Set up plot to call animate() function periodically\n    ani = animation.FuncAnimation(fig, animate, fargs=(xs, ys, scores, prices), interval=interval)\n    thr.show()\n\n#topOnly(\"tsla\", \"popular\", 100)\nstream(\"tsla\", 60000, 60, 1)\n#https://github.com/gregyjames/twitter-stock-sentiment/blob/main/main</code></pre></div></body></html>", "fir_24": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_24\"><pre id=\"fir_24_code\"><code class=\"python\"># importing libraries for persorming the sentiment analysis, cleaning data, training and saving model\nimport pickle\nimport random\nimport re\nimport string\n\nfrom nltk import FreqDist, NaiveBayesClassifier, classify\nfrom nltk.corpus import stopwords, twitter_samples\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import word_tokenize\n\n\ndef remove_noise(tweet_tokens, stop_words=()):\n    '''This function removes the links or hashtags presesnt in the text and change the verbs to its first form'''\n    cleaned_tokens = []\n\n    for token, tag in <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span>_<span class=\"fea_Part_of_Speech_funcs blodFunc\">tag</span>(tweet_tokens)</div>:\n        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+#]|[!*\\(\\),]|'\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)\n        token = re.sub(\"(@[A-Za-z0-9_]+)\", \"\", token)\n\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n\n        <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">lemmatizer</span> = <span class=\"fea_lemmatization_funcs blodFunc\">Word</span><span class=\"fea_lemmatization_funcs blodFunc\">Net</span><span class=\"fea_lemmatization_funcs blodFunc\">Lemmatizer</span>()</div>\n        <div class=\"highlights fea_lemmatization\" id=\"lemmatization_1\" style=\"display: inline;\">token = <span class=\"fea_lemmatization_funcs blodFunc\">lemmatizer</span>.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>(token, pos)</div>\n\n        if len(token) &gt; 0 and token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens\n\n\ndef get_all_words(cleaned_tokens_list):\n    '''It acts as an generator for the tokens'''\n    for tokens in cleaned_tokens_list:\n        for token in tokens:\n            yield token\n\n\ndef get_tweets_for_model(cleaned_tokens_list):\n    '''This function takes the cleaned token list as input and reutrn a list which is suitable to fed to the classifier'''\n    for tweet_tokens in cleaned_tokens_list:\n        yield dict([token, True] for token in tweet_tokens)\n\n\ndef predict_sentiment(sentence, classifier):\n    '''predict_sentiment function predict the senitment of the text which was given as a argument'''\n    custom_tokens = remove_noise(word_tokenize(sentence))\n    return classifier.classify(\n        dict([token, True] for token in custom_tokens))\n\n\ndef save_model():\n    '''Saving the trained classifier'''\n    f = open('my_classifier.pickle', 'wb')\n    pickle.dump(classifier, f)\n    f.close()\n\n\nif __name__ == \"__main__\":\n\n    # loading dataset for model trainig\n    <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">positive_tweets = twitter_samples.strings('positive_tweets.json')\n    negative_tweets = twitter_samples.strings('negative_tweets.json')</div>\n    text = twitter_samples.strings('tweets.20150430-223406.json')\n    tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n\n    # saving the stopwords from the nltk into a variable\n    stop_words = stopwords.words('english')\n\n    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n\n    positive_cleaned_tokens_list = []\n    negative_cleaned_tokens_list = []\n\n    for tokens in positive_tweet_tokens:\n        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n\n    for tokens in negative_tweet_tokens:\n        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n\n    all_pos_words = get_all_words(positive_cleaned_tokens_list)\n\n    <div class=\"highlights fea_word_frequency\" id=\"word_frequency_0\" style=\"display: inline;\"><span class=\"fea_word_frequency_funcs blodFunc\">freq</span>_<span class=\"fea_word_frequency_funcs blodFunc\">dist</span>_pos = <span class=\"fea_word_frequency_funcs blodFunc\">Freq</span><span class=\"fea_word_frequency_funcs blodFunc\">Dist</span>(all_pos_words)</div>\n    print(freq_dist_pos.most_common(10))\n\n    positive_tokens_for_model = get_tweets_for_model(\n        positive_cleaned_tokens_list)\n    negative_tokens_for_model = get_tweets_for_model(\n        negative_cleaned_tokens_list)\n\n    positive_dataset = [(tweet_dict, \"Positive\")\n                        for tweet_dict in positive_tokens_for_model]\n\n    negative_dataset = [(tweet_dict, \"Negative\")\n                        for tweet_dict in negative_tokens_for_model]\n\n    dataset = positive_dataset + negative_dataset\n\n    random.shuffle(dataset)\n\n    train_data = dataset[: 7000]\n    test_data = dataset[7000:]\n\n    <div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_funcs blodFunc\">classifier</span> = NaiveBayes<span class=\"fea_classification_funcs blodFunc\">Classifier</span>.<span class=\"fea_classification_funcs blodFunc\">train</span>(<span class=\"fea_classification_funcs blodFunc\">train</span>_data)</div>\n\n    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n\n    print(classifier.show_most_informative_features(10))\n\n    custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n\n    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n\n    print(custom_tweet, classifier.classify(\n        dict([token, True] for token in custom_tokens)))\n        #https://github.com/g-paras/sentiment-analysis-api/blob/master/model_nltk</code></pre></div></body></html>", "fir_5": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_5\"><pre id=\"fir_5_code\"><code class=\"python\">import string\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import re\n\nMIN_YEAR = 1900\nMAX_YEAR = 2100\n\n\ndef get_url_patern():\n    return <div class=\"highlights fea_regular_expression\" id=\"regular_expression_0\" style=\"display: inline;\">re.<span class=\"fea_regular_expression_funcs blodFunc\">compile</span>(\n        r'(<span class=\"fea_regular_expression_funcs blodFunc\">https</span>?:\\/\\/(?:<span class=\"fea_regular_expression_funcs blodFunc\">www</span>\\.|(?!<span class=\"fea_regular_expression_funcs blodFunc\">www</span>))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|<span class=\"fea_regular_expression_funcs blodFunc\">https</span>?:\\/\\/(?:<span class=\"fea_regular_expression_funcs blodFunc\">www</span>\\.|(?!<span class=\"fea_regular_expression_funcs blodFunc\">www</span>))'\n        r'[a-zA-Z0-9]\\.[^\\s]{2,}|<span class=\"fea_regular_expression_funcs blodFunc\">www</span>\\.[a-zA-Z0-9]\\.[^\\s]{2,})')</div>\n\n\ndef get_emojis_pattern():\n    try:\n        # UCS-4\n        <div class=\"highlights fea_regular_expression\" id=\"regular_expression_1\" style=\"display: inline;\"><span class=\"fea_regular_expression_funcs blodFunc\">emojis</span>_<span class=\"fea_regular_expression_funcs blodFunc\">pattern</span> = re.<span class=\"fea_regular_expression_funcs blodFunc\">compile</span>(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')</div>\n    except re.error:\n        # UCS-2\n        <div class=\"highlights fea_regular_expression\" id=\"regular_expression_2\" style=\"display: inline;\"><span class=\"fea_regular_expression_funcs blodFunc\">emojis</span>_<span class=\"fea_regular_expression_funcs blodFunc\">pattern</span> = re.<span class=\"fea_regular_expression_funcs blodFunc\">compile</span>(\n            u'([\\u2600-\\u27BF])|([\\uD83C][\\uDF00-\\uDFFF])|([\\uD83D][\\uDC00-\\uDE4F])|([\\uD83D][\\uDE80-\\uDEFF])')</div>\n    return emojis_pattern\n\n\ndef get_hashtags_pattern():\n    return <div class=\"highlights fea_regular_expression\" id=\"regular_expression_3\" style=\"display: inline;\">re.<span class=\"fea_regular_expression_funcs blodFunc\">compile</span>(r'#\\w*')</div>\n\n\ndef get_single_letter_words_pattern():\n    return <div class=\"highlights fea_regular_expression\" id=\"regular_expression_4\" style=\"display: inline;\"><p>re.<span class=\"fea_regular_expression_funcs blodFunc\">compile</span>(r'(?</p></div>\n\n\ndef get_blank_spaces_pattern():\n    return re.compile(r'\\s{2,}|\\t')\n\n\ndef get_twitter_reserved_words_pattern():\n    return re.compile(r'(RT|rt|FAV|fav|VIA|via)')\n\n\ndef get_mentions_pattern():\n    return re.compile(r'@\\w*')\n\ndef get_negations_pattern():\n    negations_ = {\"isn't\": \"is not\", \"can't\": \"can not\", \"couldn't\": \"could not\", \"hasn't\": \"has not\",\n                  \"hadn't\": \"had not\", \"won't\": \"will not\",\n                  \"wouldn't\": \"would not\", \"aren't\": \"are not\",\n                  \"haven't\": \"have not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n                  \"don't\": \"do not\", \"shouldn't\": \"should not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n                  \"mightn't\": \"might not\",\n                  \"mustn't\": \"must not\"}\n    return re.compile(r'\\b(' + '|'.join(negations_.keys()) + r')\\b')\n\n\ndef is_year(text):\n    if (len(text) == 3 or len(text) == 4) and (MIN_YEAR &lt; len(text) &lt; MAX_YEAR):\n        return True\n    else:\n        return False\n\n\nclass TwitterPreprocessor:\n\n    def __init__(self, text: str):\n        self.text = text\n\n    def fully_preprocess(self):\n        return self \\\n            .remove_urls() \\\n            .remove_mentions() \\\n            .remove_hashtags() \\\n            .remove_twitter_reserved_words() \\\n            .remove_punctuation() \\\n            .remove_single_letter_words() \\\n            .remove_blank_spaces() \\\n            .remove_stopwords() \\\n            .remove_numbers()\n\n    def remove_urls(self):\n        <div class=\"highlights fea_regular_expression\" id=\"regular_expression_5\" style=\"display: inline;\"><span class=\"fea_regular_expression_funcs blodFunc\">self</span>.<span class=\"fea_regular_expression_funcs blodFunc\">text</span> = re.<span class=\"fea_regular_expression_funcs blodFunc\">sub</span>(<span class=\"fea_regular_expression_funcs blodFunc\">pattern</span>=<span class=\"fea_regular_expression_funcs blodFunc\">get</span>_<span class=\"fea_regular_expression_funcs blodFunc\">url</span>_<span class=\"fea_regular_expression_funcs blodFunc\">patern</span>(), <span class=\"fea_regular_expression_funcs blodFunc\">repl</span>='', <span class=\"fea_regular_expression_funcs blodFunc\">string</span>=<span class=\"fea_regular_expression_funcs blodFunc\">self</span>.<span class=\"fea_regular_expression_funcs blodFunc\">text</span>)</div>\n        return self\n\n    def remove_punctuation(self):\n        self.text = self.text.translate(str.maketrans('', '', string.punctuation))\n        return self\n\n    def remove_mentions(self):\n        self.text = re.sub(pattern=get_mentions_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_hashtags(self):\n        self.text = re.sub(pattern=get_hashtags_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_twitter_reserved_words(self):\n        self.text = re.sub(pattern=get_twitter_reserved_words_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_single_letter_words(self):\n        self.text = re.sub(pattern=get_single_letter_words_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_blank_spaces(self):\n        self.text = re.sub(pattern=get_blank_spaces_pattern(), repl=' ', string=self.text)\n        return self\n\n    def remove_stopwords(self, extra_stopwords=None):\n        if extra_stopwords is None:\n            extra_stopwords = []\n        text = nltk.word_tokenize(self.text)\n        <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">stop_words = set(stopwords.words('english'))</div>\n\n        new_sentence = []\n        for w in text:\n            if w not in stop_words and w not in extra_stopwords:\n                new_sentence.append(w)\n        self.text = ' '.join(new_sentence)\n        return self\n\n    def remove_numbers(self, preserve_years=False):\n        text_list = self.text.split(' ')\n        for text in text_list:\n            if text.isnumeric():\n                if preserve_years:\n                    if not is_year(text):\n                        text_list.remove(text)\n                else:\n                    text_list.remove(text)\n\n        self.text = ' '.join(text_list)\n        return self\n\n    def lowercase(self):\n        self.text = self.text.lower()\n        return self\n    \n    def handle_negations(self):  \n        self.text = re.sub(pattern=get_negations_pattern(), repl='', string=self.text)\n        return self\n        #https://github.com/vasisouv/tweets-preprocessor/blob/master/twitter_preprocessor</code></pre></div></body></html>", "fir_7": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_7\"><pre id=\"fir_7_code\"><code class=\"python\">#! /usr/bin/env python2\n\n\"\"\"\nFilename: characterExtraction\nAuthor: Emily Daniels\nDate: April 2014\nPurpose: Extracts character names from a text file and performs analysis of\ntext sentences containing the names.\n\"\"\"\n\nimport json\nimport nltk\nimport re\n\nfrom collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom pattern.en import parse, Sentence, mood\nfrom pattern.db import csv\nfrom pattern.vector import Document, NB\n\ndef readText():\n    \"\"\"\n    Reads the text from a text file.\n    \"\"\"\n    with open(\"730.txt\", \"rb\") as f:\n        text = f.read().decode('utf-8-sig')\n    return text\n\n\ndef chunkSentences(text):\n    \"\"\"\n    Parses text into parts of speech tagged with parts of speech labels.\n\n    Used for reference: https://gist.github.com/onyxfish/322906\n    \"\"\"\n    sentences = nltk.sent_tokenize(text)\n    tokenizedSentences = [nltk.word_tokenize(sentence)\n                          for sentence in sentences]\n    taggedSentences = [nltk.pos_tag(sentence)\n                       for sentence in tokenizedSentences]\n    if nltk.__version__[0:2] == \"2.\":\n        chunkedSentences = nltk.batch_ne_chunk(taggedSentences, binary=True)\n    else:\n        chunkedSentences = nltk.ne_chunk_sents(taggedSentences, binary=True)\n    return chunkedSentences\n\n\ndef extractEntityNames(tree, _entityNames=None):\n    \"\"\"\n    Creates a local list to hold nodes of tree passed through, extracting named\n    entities from the chunked sentences.\n\n    Used for reference: https://gist.github.com/onyxfish/322906\n    \"\"\"\n    if _entityNames is None:\n        _entityNames = []\n    try:\n        if nltk.__version__[0:2] == \"2.\":\n            label = tree.node\n        else:\n            label = tree.label()\n    except AttributeError:\n        pass\n    else:\n        if label == 'NE':\n            _entityNames.append(' '.join([child[0] for child in tree]))\n        else:\n            for child in tree:\n                extractEntityNames(child, _entityNames=_entityNames)\n    return _entityNames\n\n\ndef buildDict(chunkedSentences, _entityNames=None):\n    \"\"\"\n    Uses the global entity list, creating a new dictionary with the properties\n    extended by the local list, without overwriting.\n\n    Used for reference: https://gist.github.com/onyxfish/322906\n    \"\"\"\n    if _entityNames is None:\n        _entityNames = []\n\n    for tree in chunkedSentences:\n        extractEntityNames(tree, _entityNames=_entityNames)\n\n    return _entityNames\n\n\ndef removeStopwords(entityNames, customStopWords=None):\n    \"\"\"\n    Brings in stopwords and custom stopwords to filter mismatches out.\n    \"\"\"\n    # Memoize custom stop words\n    if customStopWords is None:\n        with open(\"customStopWords.txt\", \"rb\") as f:\n            customStopwords = f.read().split(', ')\n\n    for name in entityNames:\n        if name in <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">stopwords.words('english')</div> or name in customStopwords:\n            entityNames.remove(name)\n\n\ndef getMajorCharacters(entityNames):\n    \"\"\"\n    Adds names to the major character list if they appear frequently.\n    \"\"\"\n    return {name for name in entityNames if entityNames.count(name) &gt; 10}\n\n\ndef splitIntoSentences(text):\n    \"\"\"\n    Split sentences on .?! \"\" and not on abbreviations of titles.\n    Used for reference: http://stackoverflow.com/a/8466725\n    \"\"\"\n    sentenceEnders = re.compile(r\"\"\"\n    # Split sentences on whitespace between them.\n    (?:               # Group for two positive lookbehinds.\n      (?&lt;=[.!?])      # Either an end of sentence punct,\n    | (?&lt;=[.!?]['\"])  # or end of sentence punct and quote.\n    )                 # End group of two positive lookbehinds.\n    (?&lt;!  Mr\\.   )    # Don't end sentence on \"Mr.\"\n    (?&lt;!  Mrs\\.  )    # Don't end sentence on \"Mrs.\"\n    (?&lt;!  Ms\\.   )    # Don't end sentence on \"Ms.\"\n    (?&lt;!  Jr\\.   )    # Don't end sentence on \"Jr.\"\n    (?&lt;!  Dr\\.   )    # Don't end sentence on \"Dr.\"\n    (?&lt;!  Prof\\. )    # Don't end sentence on \"Prof.\"\n    (?&lt;!  Sr\\.   )    # Don't end sentence on \"Sr.\"\n    \\s+               # Split on whitespace between sentences.\n    \"\"\", re.IGNORECASE | re.VERBOSE)\n    return sentenceEnders.split(text)\n\n\ndef compareLists(sentenceList, majorCharacters):\n    \"\"\"\n    Compares the list of sentences with the character names and returns\n    sentences that include names.\n    \"\"\"\n    characterSentences = defaultdict(list)\n    for sentence in sentenceList:\n        for name in majorCharacters:\n            if re.search(r\"\\b(?=\\w)%s\\b(?!\\w)\" % re.escape(name),\n                         sentence,\n                         re.IGNORECASE):\n                characterSentences[name].append(sentence)\n    return characterSentences\n\n\ndef extractMood(characterSentences):\n    \"\"\"\n    Analyzes the sentence using grammatical mood module from pattern.\n    \"\"\"\n    characterMoods = defaultdict(list)\n    for key, value in characterSentences.iteritems():\n        for x in value:\n            characterMoods[key].append(mood(Sentence(parse(str(x),\n                                                           lemmata=True))))\n    return characterMoods\n\n\ndef extractSentiment(characterSentences):\n    \"\"\"\n    Trains a Naive Bayes classifier object with the reviews.csv file, analyzes\n    the sentence, and returns the tone.\n    \"\"\"\n    nb = NB()\n    characterTones = defaultdict(list)\n    for review, rating in csv(\"reviews.csv\"):\n        nb.train(Document(review, type=int(rating), stopwords=True))\n    for key, value in characterSentences.iteritems():\n        for x in value:\n            characterTones[key].append(nb.classify(str(x)))\n    return characterTones\n\n\ndef writeAnalysis(sentenceAnalysis):\n    \"\"\"\n    Writes the sentence analysis to a text file in the same directory.\n    \"\"\"\n    with open(\"sentenceAnalysis.txt\", \"wb\") as f:\n        for item in sentenceAnalysis.items():\n            f.write(\"%s:%s\\n\" % item)\n\n\ndef writeToJSON(sentenceAnalysis):\n    \"\"\"\n    Writes the sentence analysis to a JSON file in the same directory.\n    \"\"\"\n    with open(\"sentenceAnalysis.json\", \"wb\") as f:\n        json.dump(sentenceAnalysis, f)\n\n\nif __name__ == \"__main__\":\n    text = readText()\n\n    chunkedSentences = chunkSentences(text)\n    entityNames = buildDict(chunkedSentences)\n    removeStopwords(entityNames)\n    majorCharacters = getMajorCharacters(entityNames)\n    \n    sentenceList = splitIntoSentences(text)\n    characterSentences = compareLists(sentenceList, majorCharacters)\n    characterMoods = extractMood(characterSentences)\n    characterTones = extractSentiment(characterSentences)\n\n    # Merges sentences, moods and tones together into one dictionary on each\n    # character.\n    sentenceAnalysis = defaultdict(list,\n                                   [(k, [characterSentences[k],\n                                         characterTones[k],\n                                         characterMoods[k]])\n                                    for k in characterSentences])\n    \n    writeAnalysis(sentenceAnalysis)\n    writeToJSON(sentenceAnalysis)\n    #https://github.com/emdaniels/character-extraction/blob/master/characterExtraction</code></pre></div></body></html>", "fir_28": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_28\"><pre id=\"fir_28_code\"><code class=\"python\">from nltk.corpus import subjectivity\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.sentiment import SentimentAnalyzer # SentimentAnalyzer is a tool to implement and facilitate Sentiment Analysis.\nfrom nltk.sentiment.util import (mark_negation, extract_unigram_feats) # mark_negation(): Append _NEG suffix to words that appear in the scope between a negation and a punctuation mark. extract_unigram_feats(): Populate a dictionary of unigram features, reflecting the presence/absence in the document of each of the tokens in unigrams.\n\nn_instances = 100\nobj_docs = [(sent, 'obj') for sent in <div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">subjectivity.sents(categories='obj')[:n_instances]]</div>\nsubj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\ntrain_obj_docs = obj_docs[:80]\ntest_obj_docs = obj_docs[80:100]\ntrain_subj_docs = subj_docs[:80]\ntest_subj_docs = subj_docs[80:100]\n\ntraining_docs = train_obj_docs + train_subj_docs\ntesting_docs = test_obj_docs + test_subj_docs\n\n<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_funcs blodFunc\">sentim</span>_<span class=\"fea_sentiment_analysis_funcs blodFunc\">analyzer</span> = <span class=\"fea_sentiment_analysis_funcs blodFunc\">Sentiment</span><span class=\"fea_sentiment_analysis_funcs blodFunc\">Analyzer</span>()</div>\nall_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n\nunigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n\nsentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n\ntraining_set = sentim_analyzer.apply_features(training_docs)\ntest_set = sentim_analyzer.apply_features(testing_docs)\n\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_funcs blodFunc\">train</span>er = NaiveBayes<span class=\"fea_classification_funcs blodFunc\">Classifier</span>.<span class=\"fea_classification_funcs blodFunc\">train</span>\n<span class=\"fea_classification_funcs blodFunc\">classifier</span> = sentim_analyzer.<span class=\"fea_classification_funcs blodFunc\">train</span>(<span class=\"fea_classification_funcs blodFunc\">train</span>er, <span class=\"fea_classification_funcs blodFunc\">train</span>ing_set)</div>\n\nfor key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n    print('{0}: {1}'.format(key, value))\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsentences = [\n    \"You are a piece of shit, and I will step on you.\",\n    \"THIS SUX!!!\",\n    \"This kinda sux...\",\n    \"You're good, man\",\n    \"HAHAHA YOU ARE THE BEST!!!!! VERY FUNNY!!!\"\n            ]\n\n\n<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_1\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_funcs blodFunc\">sid</span> = <span class=\"fea_sentiment_analysis_funcs blodFunc\">Sentiment</span><span class=\"fea_sentiment_analysis_funcs blodFunc\">Intensity</span><span class=\"fea_sentiment_analysis_funcs blodFunc\">Analyzer</span>()</div>\n\nfor sentence in sentences:\n    print('\\n' + sentence)\n    ss = sid.polarity_scores(sentence)\n    for k in sorted(ss):\n        print('{0}: {1}, '.format(k, ss[k]), end='')</code></pre></div></body></html>", "fir_22": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_22\"><pre id=\"fir_22_code\"><code class=\"python\">\n# coding=utf-8\nimport utils\nimport nltk\n\ndata = utils.getTrainData()\n\ndef get_words_in_tweets(tweets):\n    all_words = []\n    for (words, sentiment) in tweets:\n      all_words.extend(words)\n    return all_words\n\ndef get_word_features(wordlist):\n    <div class=\"highlights fea_word_frequency\" id=\"word_frequency_0\" style=\"display: inline;\">wordlist = nltk.<span class=\"fea_word_frequency_funcs blodFunc\">Freq</span><span class=\"fea_word_frequency_funcs blodFunc\">Dist</span>(wordlist)</div>\n    word_features = wordlist.keys()\n    return word_features\n\nword_features = get_word_features(get_words_in_tweets(data))\n\ndef extract_features(document):\n    document_words = set(document)\n    features = {}\n    for word in word_features:\n        features[word.decode(\"utf8\")] = (word in document_words)\n    return features\n\nallsetlength = len(data)\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_funcs blodFunc\">train</span>ing_set = nltk.classify.apply_features(extract_features, data[:allsetlength/10*8])</div>\ntest_set = data[allsetlength/10*8:]\n<div class=\"highlights fea_classification\" id=\"classification_1\" style=\"display: inline;\"><span class=\"fea_classification_funcs blodFunc\">classifier</span> = nltk.NaiveBayes<span class=\"fea_classification_funcs blodFunc\">Classifier</span>.<span class=\"fea_classification_funcs blodFunc\">train</span>(<span class=\"fea_classification_funcs blodFunc\">train</span>ing_set)</div>\n\ndef classify(tweet):\n\tprint classifier.classify(extract_features(tweet.split()))\n\nclassify(\"Bug\u00fcn \u00e7ok g\u00fczel bir g\u00fcn\")\n#https://github.com/mertkahyaoglu/twitter-sentiment-analysis/blob/master/classify</code></pre></div></body></html>", "fir_27": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_27\"><pre id=\"fir_27_code\"><code class=\"python\">from nltk.corpus import brown\nfrom nltk import FreqDist\n\n<div class=\"highlights fea_word_frequency\" id=\"word_frequency_0\" style=\"display: inline;\">suffix_f<span class=\"fea_word_frequency_funcs blodFunc\">dist</span> = <span class=\"fea_word_frequency_funcs blodFunc\">Freq</span><span class=\"fea_word_frequency_funcs blodFunc\">Dist</span>()</div>\nfor word in brown.words():\n    word = word.lower()\n    suffix_fdist[word[-1:]] += 1\n    suffix_fdist[word[-2:]] += 1\n    suffix_fdist[word[-3:]] += 1\ncommon_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n\ndef pos_features(word):\n    features = {}\n    for suffix in common_suffixes:\n        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n    return features\n\n<div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">tagged_words = brown.tagged_words(categories='news')</div>\nfeaturesets = [(pos_features(n), g) for (n,g) in tagged_words]\n\nfrom nltk import DecisionTreeClassifier\nfrom nltk.classify import accuracy\n\ncutoff = int(len(featuresets) * 0.1)\ntrain_set, test_set = featuresets[cutoff:], featuresets[:cutoff]\n\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_funcs blodFunc\">classifier</span> = DecisionTree<span class=\"fea_classification_funcs blodFunc\">Classifier</span>.<span class=\"fea_classification_funcs blodFunc\">train</span>(<span class=\"fea_classification_funcs blodFunc\">train</span>_set)</div> # NLTK is a teaching toolkit which is not really optimized for speed. Therefore, this may take forever. For speed, use scikit-learn for the classifiers.\n\naccuracy(classifier, test_set)\n\nclassifier.classify(pos_features('cats'))\n\nclassifier.pseudocode(depth=4)</code></pre></div></body></html>", "fir_26": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_26\"><pre id=\"fir_26_code\"><code class=\"python\">s = \"Le temps est un grand ma\u00eetre, dit-on, le malheur est qu'il tue ses \u00e9l\u00e8ves.\"\ns = s.lower()\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(\"[a-zA-Z'`\u00e9\u00e8\u00ee]+\")\ns_tokenized = tokenizer.tokenize(s)\n\nfrom nltk.util import ngrams\ngenerated_4grams = []\n\nfor word in s_tokenized:\n    generated_4grams.append(list(<div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">ngrams(word, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')</div>)) # n = 4.\n\ngenerated_4grams = [word for sublist in generated_4grams for word in sublist]\n\nng_list_4grams = generated_4grams\nfor idx, val in enumerate(generated_4grams):\n    ng_list_4grams[idx] = ''.join(val)\n\nfreq_4grams = {}\n\nfor ngram in ng_list_4grams:\n    if ngram not in freq_4grams:\n        freq_4grams.update({ngram: 1})\n    else:\n        ngram_occurrences = freq_4grams[ngram]\n        freq_4grams.update({ngram: ngram_occurrences + 1})\n        \nfrom operator import itemgetter # The operator module exports a set of efficient functions corresponding to the intrinsic operators of Python. For example, operator.add(x, y) is equivalent to the expression x + y.\n\nfreq_4grams_sorted = sorted(freq_4grams.items(), key=itemgetter(1), reverse=True)[0:300] # We only keep the 300 most popular n-grams. This was suggested in the original paper written about n-grams.\n\nfrom nltk import everygrams\n\ns_clean = ' '.join(s_tokenized) # For the code below we need the raw sentence as opposed to the tokens.\n\ndef ngram_extractor(sent):\n    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4) \n            if ' ' not in ng and '\\n' not in ng and ng != ('_',)]\n\nngram_extractor(s_clean)</code></pre></div></body></html>", "fir_15": "<html><body><div class=\"codeBlock hljs python\" id=\"fir_15\"><pre id=\"fir_15_code\"><code class=\"python\"># -*- coding: utf-8 -*-\n# Maximum Entropy Part-of-Speech Tagger for NLTK (Natural Language Toolkit)\n# Author: Arne Neumann\n# Licence: GPL 3\n\n#__docformat__ = 'epytext en'\n\n\"\"\"\nA I{part-of-speech tagger} that uses NLTK's build-in L{Maximum Entropy\nmodels&lt;nltk.MaxentClassifier&gt;} to find the most likely I{part-of-speech\ntag} (POS) for each word in a given sequence.\n\nThe tagger will be trained on a corpus of tagged sentences. For every word\nin the corpus, a C{tuple} consisting of a C{dictionary} of features from\nthe word's context (e.g. preceding/succeeding words and tags, word\nprefixes/suffixes etc.) and the word's tag will be generated.\nThe maximum entropy classifier will learn a model from these tuples that\nwill be used by the tagger to find the most likely POS-tag for any given\nword, even unseen ones.\n\nThe tagger and the featuresets chosen for training are implemented as described\nin Ratnaparkhi, Adwait (1996). A Maximum Entropy Model for Part-Of-Speech\nTagging. In Proceedings of the ARPA Human Language Technology Workshop. Pages\n250-255.\n\nUsage notes:\n============\n\nPlease install the MEGAM package (http://hal3.name/megam),\notherwise training will take forever.\n\nTo use the demo, please install either 'brown' or 'treebank' with::\n\n    import nltk\n    nltk.download()\n\nin the Python interpreter. Proper usage of demo() and all other functions and\nmethods is described below.\n\"\"\"\n\nimport time\nimport re\nfrom collections import defaultdict\n\nfrom nltk import TaggerI, FreqDist, untag, config_megam\nfrom nltk.classify.maxent import MaxentClassifier\n                  \n\nPATH_TO_MEGAM_EXECUTABLE = \"/usr/bin/megam\"\nconfig_megam(PATH_TO_MEGAM_EXECUTABLE)\n\n\nclass MaxentPosTagger(TaggerI):\n    \"\"\"\n    MaxentPosTagger is a part-of-speech tagger based on Maximum Entropy models.\n    \"\"\"\n    def train(self, train_sents, algorithm='megam', rare_word_cutoff=5,\n              rare_feat_cutoff=5, uppercase_letters='[A-Z]', trace=3,\n              **cutoffs):\n        \"\"\"\n        MaxentPosTagger trains a Maximum Entropy model from a C{list} of tagged\n        sentences.\n\n        @type train_sents: C{list} of C{list} of tuples of (C{str}, C{str})\n        @param train_sents: A list of tagged sentences. Each sentence is\n        represented by a list of tuples. Each tuple holds two strings, a\n        word and its tag, e.g. ('company','NN').\n\n        @type algorithm: C{str}\n        @param algorithm: The algorithm that is used by\n        L{nltk.MaxentClassifier.train()} to train and optimise the model. It is\n        B{strongly recommended} to use the C{LM-BFGS} algorithm provided by the\n        external package U{megam&lt;http://hal3.name/megam/&gt;} as it is much faster\n        and uses less memory than any of the algorithms provided by NLTK (i.e.\n        C{GIS}, C{IIS}) or L{scipy} (e.g. C{CG} and C{BFGS}).\n\n        @type rare_word_cutoff: C{int}\n        @param rare_word_cutoff: Words with less occurrences than\n        C{rare_word_cutoff} will be treated differently by L{extract_feats}\n        than non-rare words (cf. Ratnaparkhi 1996).\n\n        @type rare_feat_cutoff: C{int}\n        @param rare_feat_cutoff: ignore features that occur less than\n        C{rare_feat_cutoff} during training.\n\n        @type uppercase_letters: C{regex}\n        @param uppercase_letters: a regular expression that covers all\n        uppercase letters of the language of your corpus (e.g. '[A-Z\u00c4\u00d6\u00dc]' for\n        German)\n\n        @type trace: C{int}\n        @param trace: The level of diagnostic output to produce. C{0} doesn't\n        produce any output, while C{3} will give all the output that C{megam}\n        produces plus the time it took to train the model.\n\n        @param cutoffs: Arguments specifying various conditions under\n            which the training should be halted. When using C{MEGAM}, only\n            C{max_iter} should be relevant. For other cutoffs see\n            L{nltk.MaxentClassifier}\n\n              - C{max_iter=v}: Terminate after C{v} iterations.\n       \"\"\"\n        self.uppercase_letters = uppercase_letters\n        self.word_freqdist = self.gen_word_freqs(train_sents)\n        self.featuresets = self.gen_featsets(train_sents,\n                rare_word_cutoff)\n        self.features_freqdist = self.gen_feat_freqs(self.featuresets)\n        self.cutoff_rare_feats(self.featuresets, rare_feat_cutoff)\n\n        t1 = time.time()\n        <div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">self.<span class=\"fea_classification_funcs blodFunc\">classifier</span> = Maxent<span class=\"fea_classification_funcs blodFunc\">Classifier</span>.<span class=\"fea_classification_funcs blodFunc\">train</span>(self.featuresets, algorithm,\n                                                 trace, **cutoffs)</div>\n        t2 = time.time()\n        if trace &gt; 0:\n            print \"time to train the classifier: {0}\".format(round(t2-t1, 3))\n\n    def gen_feat_freqs(self, featuresets):\n        \"\"\"\n        Generates a frequency distribution of joint features (feature, tag)\n        tuples. The frequency distribution will be used by the tagger to\n        determine which (rare) features should not be considered during\n        training (feature cutoff).\n\n        This is how joint features look like::\n            (('t-2 t-1', 'IN DT'), 'NN')\n            (('w-2', '&lt;START&gt;'), 'NNP')\n            (('w+1', 'of'), 'NN')\n\n        @type featuresets: {list} of C{tuples} of (C{dict}, C{str})\n        @param featuresets: a list of tuples that contain the featureset of a\n        word from the training set and its POS tag.\n\n        @rtype: C{FreqDist}\n        @return: a L{frequency distribution&lt;nltk.FreqDist()&gt;},\n        counting how often each (context information feature, tag) tuple occurs\n        in the training sentences.\n        \"\"\"\n        features_freqdist = defaultdict(int)\n        for (feat_dict, tag) in featuresets:\n            for (feature, value) in feat_dict.items():\n                features_freqdist[ ((feature, value), tag) ] += 1\n        return features_freqdist\n\n    def gen_word_freqs(self, train_sents):\n        \"\"\"\n        Generates word frequencies from the training sentences for the feature\n        extractor.\n\n        @type train_sents: C{list} of C{list} of tuples of (C{str}, C{str})\n        @param train_sents: A list of tagged sentences.\n\n        @rtype: C{FreqDist}\n        @return: a L{frequency distribution&lt;nltk.FreqDist()&gt;},\n        counting how often each word occurs in the training sentences.\n        \"\"\"\n        word_freqdist = FreqDist()\n        for tagged_sent in train_sents:\n            for (word, _tag) in tagged_sent:\n                word_freqdist[word] += 1\n        return word_freqdist\n\n    def gen_featsets(self, train_sents, rare_word_cutoff):\n        \"\"\"\n        Generates featuresets for each token in the training sentences.\n\n        @type train_sents: C{list} of C{list} of tuples of (C{str}, C{str})\n        @param train_sents: A list of tagged sentences.\n\n        @type rare_word_cutoff: C{int}\n        @param rare_word_cutoff: Words with less occurrences than\n        C{rare_word_cutoff} will be treated differently by L{extract_feats}\n        than non-rare words (cf. Ratnaparkhi 1996).\n\n        @rtype: {list} of C{tuples} of (C{dict}, C{str})\n        @return:  a list of tuples that contains the featureset of\n        a token and its POS-tag.\n        \"\"\"\n        featuresets = []\n        for tagged_sent in train_sents:\n            history = []\n            untagged_sent = untag(tagged_sent)\n            for (i, (_word, tag)) in enumerate(tagged_sent):\n                featuresets.append( (self.extract_feats(untagged_sent, i,\n                    history, rare_word_cutoff), tag) )\n                history.append(tag)\n        return featuresets\n\n\n    def cutoff_rare_feats(self, featuresets, rare_feat_cutoff):\n        \"\"\"\n        Cuts off rare features to reduce training time and prevent overfitting.\n\n        Example\n        =======\n\n            Let's say, the suffixes of this featureset are too rare to learn.\n\n            &gt;&gt;&gt; featuresets[46712]\n            ({'suffix(1)': 't',\n            'prefix(1)': 'L',\n            'prefix(2)': 'Le',\n            'prefix(3)': 'Lem',\n            'suffix(3)': 'ont',\n            'suffix(2)': 'nt',\n            'contains-uppercase': True,\n            'prefix(4)': 'Lemo',\n            'suffix(4)': 'mont'},\n            'NNP')\n\n            C{cutoff_rare_feats} would then remove the rare joint features::\n\n                (('suffix(1)', 't'), 'NNP')\n                (('suffix(3)', 'ont'), 'NNP')\n                ((suffix(2)': 'nt'), 'NNP')\n                (('suffix(4)', 'mont'), 'NNP')\n\n            and return a featureset that only contains non-rare features:\n\n            &gt;&gt;&gt; featuresets[46712]\n            ({'prefix(1)': 'L',\n            'prefix(2)': 'Le',\n            'prefix(3)': 'Lem',\n            'contains-uppercase': True,\n            'prefix(4)': 'Lemo'},\n            'NNP')\n\n\n        @type featuresets: {list} of C{tuples} of (C{dict}, C{str})\n        @param featuresets: a list of tuples that contain the featureset of a\n        word from the training set and its POS tag\n\n        @type rare_feat_cutoff: C{int}\n        @param rare_feat_cutoff: if a (context information feature, tag)\n        tuple occurs less than C{rare_feat_cutoff} times in the training\n        set, then its corresponding feature will be removed from the\n        C{featuresets} to be learned.\n        \"\"\"\n        never_cutoff_features = set(['w','t'])\n\n        for (feat_dict, tag) in featuresets:\n            for (feature, value) in feat_dict.items():\n                feat_value_tag = ((feature, value), tag)\n                if self.features_freqdist[feat_value_tag] &lt; rare_feat_cutoff:\n                    if feature not in never_cutoff_features:\n                        feat_dict.pop(feature)\n\n\n    def extract_feats(self, sentence, i, history, rare_word_cutoff=5):\n        \"\"\"\n        Generates a featureset from a word (in a sentence). The features\n        were chosen as described in Ratnaparkhi (1996) and his Java\n        software package U{MXPOST&lt;ftp://ftp.cis.upenn.edu/pub/adwait/jmx&gt;}.\n\n        The following features are extracted:\n\n            - features for all words: last tag (C{t-1}), last two tags (C{t-2\n              t-1}), last words (C{w-1}) and (C{w-2}), next words (C{w+1}) and\n              (C{w+2})\n            - features for non-rare words: current word (C{w})\n            - features for rare words: word suffixes (last 1-4 letters),\n              word prefixes (first 1-4 letters),\n              word contains number (C{bool}), word contains uppercase character\n              (C{bool}), word contains hyphen (C{bool})\n\n        Ratnaparkhi experimented with his tagger on the Wall Street Journal\n        corpus (Penn Treebank project). He found that the tagger yields\n        better results when words which occur less than 5 times are treated\n        as rare. As your mileage may vary, please adjust\n        L{rare_word_cutoff} accordingly.\n\n        Examples\n        ========\n\n            1. This is a featureset extracted from the nonrare (word, tag)\n            tuple ('considerably', 'RB')\n\n            &gt;&gt;&gt; featuresets[22356]\n            ({'t-1': 'VB',\n            't-2 t-1': 'TO VB',\n            'w': 'considerably',\n            'w+1': '.',\n            'w+2': '&lt;END&gt;',\n            'w-1': 'improve',\n            'w-2': 'to'},\n            'RB')\n\n            2. A featureset extracted from the rare tuple ('Lemont', 'NN')\n\n            &gt;&gt;&gt; featuresets[46712]\n            ({'suffix(1)': 't',\n            'prefix(1)': 'L',\n            'prefix(2)': 'Le',\n            'prefix(3)': 'Lem',\n            'suffix(3)': 'ont',\n            'suffix(2)': 'nt',\n            'contains-uppercase': True,\n            'prefix(4)': 'Lemo',\n            'suffix(4)': 'mont'},\n            'NNP')\n\n\n        @type sentence: C{list} of C{str}\n        @param sentence: A list of words, usually a sentence.\n\n        @type i: C{int}\n        @param i: The index of a word in a sentence, where C{sentence[0]} would\n        represent the first word of a sentence.\n\n        @type history: C{int} of C{str}\n        @param history: A list of POS-tags that have been assigned to the\n        preceding words in a sentence.\n\n        @type rare_word_cutoff: C{int}\n        @param rare_word_cutoff: Words with less occurrences than\n        C{rare_word_cutoff} will be treated differently than non-rare words\n        (cf. Ratnaparkhi 1996).\n\n        @rtype: C{dict}\n        @return: a dictionary of features extracted from a word's\n        context.\n        \"\"\"\n        features = {}\n        hyphen = re.compile(\"-\")\n        number = re.compile(\"\\d\")\n        uppercase = re.compile(self.uppercase_letters)\n\n        #get features: w-1, w-2, t-1, t-2.\n        #takes care of the beginning of a sentence\n        if i == 0: #first word of sentence\n            features.update({\"w-1\": \"&lt;START&gt;\", \"t-1\": \"&lt;START&gt;\",\n                             \"w-2\": \"&lt;START&gt;\", \"t-2 t-1\": \"&lt;START&gt; &lt;START&gt;\"})\n        elif i == 1: #second word of sentence\n            features.update({\"w-1\": sentence[i-1], \"t-1\": history[i-1],\n                             \"w-2\": \"&lt;START&gt;\",\n                             \"t-2 t-1\": \"&lt;START&gt; %s\" % (history[i-1])})\n        else:\n            features.update({\"w-1\": sentence[i-1], \"t-1\": history[i-1],\n                \"w-2\": sentence[i-2],\n                \"t-2 t-1\": \"%s %s\" % (history[i-2], history[i-1])})\n\n        #get features: w+1, w+2. takes care of the end of a sentence.\n        for inc in [1, 2]:\n            try:\n                features[\"w+%i\" % (inc)] = sentence[i+inc]\n            except IndexError:\n                features[\"w+%i\" % (inc)] = \"&lt;END&gt;\"\n\n        if self.word_freqdist[sentence[i]] &gt;= rare_word_cutoff:\n            #additional features for 'non-rare' words\n            features[\"w\"] = sentence[i]\n\n        else: #additional features for 'rare' or 'unseen' words\n            features.update({\"suffix(1)\": sentence[i][-1:],\n                \"suffix(2)\": sentence[i][-2:], \"suffix(3)\": sentence[i][-3:],\n                \"suffix(4)\": sentence[i][-4:], \"prefix(1)\": sentence[i][:1],\n                \"prefix(2)\": sentence[i][:2], \"prefix(3)\": sentence[i][:3],\n                \"prefix(4)\": sentence[i][:4]})\n            if hyphen.search(sentence[i]) != None:\n                #set True, if regex is found at least once\n                features[\"contains-hyphen\"] = True\n            if number.search(sentence[i]) != None:\n                features[\"contains-number\"] = True\n            if uppercase.search(sentence[i]) != None:\n                features[\"contains-uppercase\"] = True\n\n        return features\n\n\n    def tag(self, sentence, rare_word_cutoff=5):\n        \"\"\"\n        Attaches a part-of-speech tag to each word in a sequence.\n\n        @type sentence: C{list} of C{str}\n        @param sentence: a list of words to be tagged.\n\n        @type rare_word_cutoff: C{int}\n        @param rare_word_cutoff: words with less occurrences than\n        C{rare_word_cutoff} will be treated differently than non-rare words\n        (cf. Ratnaparkhi 1996).\n\n        @rtype: C{list} of C{tuples} of (C{str}, C{str})\n        @return: a list of tuples consisting of a word and its corresponding\n        part-of-speech tag.\n        \"\"\"\n        history = []\n        for i in xrange(len(sentence)):\n            featureset = self.extract_feats(sentence, i, history,\n                                               rare_word_cutoff)\n            tag = self.classifier.classify(featureset)\n            history.append(tag)\n        return zip(sentence, history)\n\n\ndef demo(corpus, num_sents):\n    \"\"\"\n    Loads a few sentences from the Brown corpus or the Wall Street Journal\n    corpus, trains them, tests the tagger's accuracy and tags an unseen\n    sentence.\n\n    @type corpus: C{str}\n    @param corpus: Name of the corpus to load, either C{brown} or C{treebank}.\n\n    @type num_sents: C{int}\n    @param num_sents: Number of sentences to load from a corpus. Use a small\n    number, as training might take a while.\n    \"\"\"\n    if corpus.lower() == \"brown\":\n        from nltk.corpus import brown\n        tagged_sents = brown.tagged_sents()[:num_sents]\n    elif corpus.lower() == \"treebank\":\n        from nltk.corpus import treebank\n        tagged_sents = treebank.tagged_sents()[:num_sents]\n    else:\n        print \"Please load either the 'brown' or the 'treebank' corpus.\"\n\n    size = int(len(tagged_sents) * 0.1)\n    train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n    maxent_tagger = MaxentPosTagger()\n    maxent_tagger.train(train_sents)\n    print \"tagger accuracy (test %i sentences, after training %i):\" % \\\n        (size, (num_sents - size)), maxent_tagger.evaluate(test_sents)\n    print \"\\n\\n\"\n    print \"classify unseen sentence: \", maxent_tagger.tag([\"This\", \"is\", \"so\",\n        \"slow\", \"!\"])\n    print \"\\n\\n\"\n    print \"show the 10 most informative features:\"\n    print maxent_tagger.classifier.show_most_informative_features(10)\n\n\nif __name__ == '__main__':\n    demo(\"treebank\", 200)\n    #~ featuresets = demo_debugger(\"treebank\", 10000)\n    print \"\\n\\n\\n\"\n\n#https://github.com/arne-cl/nltk-maxent-pos-tagger/blob/master/mxpost</code></pre></div></body></html>", "sec_7": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"sec_7\"><pre id=\"sec_7_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob.parsers import PatternParser\n<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\">blob = <span class=\"fea_parsing_funcs blodFunc\">Text</span>Blob(\"Parsing is fun.\", <span class=\"fea_parsing_funcs blodFunc\">parse</span>r=Pattern<span class=\"fea_parsing_funcs blodFunc\">Parse</span>r())</div>\n<div class=\"highlights fea_parsing\" id=\"parsing_1\" style=\"display: inline;\">blob.<span class=\"fea_parsing_funcs blodFunc\">parse</span>()</div>\n#https://textblob.readthedocs.io/en/dev/advanced_usage.html</code></pre></div></body></html>", "sec_10": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_10\"><pre id=\"sec_10_code\"><code class=\"python\">\nfrom flask import Flask, render_template, request\napp = Flask(__name__)\nfrom textblob import TextBlob\nimport nltk\nfrom textblob import Word\nimport sys\n\n\ndef parse(string):\n   \"\"\"\n   Parse a paragraph. Devide it into sentences and try to generate quesstions from each sentences.\n   \"\"\"\n   data = []\n   try:\n      txt = TextBlob(string)\n      # Each sentence is taken from the string input and passed to genQuestion() to generate questions.\n      for sentence in txt.sentences:\n         question = genQuestion(sentence)\n         if question != None:\n            data.append(question)\n      return data\n   except Exception as e:\n      raise e\n\n\n\ndef genQuestion(line):\n\n   \"\"\"\n   outputs question from the given text\n   \"\"\"\n   answer = line\n   if type(line) is str:\n      line = TextBlob(line) # Create object of type textblob.blob.TextBlob\n\n   bucket = {}               # Create an empty dictionary\n   for i,j in enumerate(<div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">line.tags</div>):  # line.tags are the parts-of-speach in English\n      if j[1] not in bucket:\n         bucket[j[1]] = i  # Add all tags to the dictionary or bucket variable\n    \n   if verbose:               # In verbose more print the key,values of dictionary\n      print('\\n','-'*20)\n      print(line ,'\\n')  \n      print(\"TAGS:\",line.tags, '\\n')  \n      print(bucket)\n    \n   question = ''            # Create an empty string \n\n    # These are the english part-of-speach tags used in this demo program.\n    #.....................................................................\n    # NNS     Noun, plural\n    # JJ  Adjective \n    # NNP     Proper noun, singular \n    # VBG     Verb, gerund or present participle \n    # VBN     Verb, past participle \n    # VBZ     Verb, 3rd person singular present \n    # VBD     Verb, past tense \n    # IN      Preposition or subordinating conjunction \n    # PRP     Personal pronoun \n    # NN  Noun, singular or mass \n    #.....................................................................\n\n    # Create a list of tag-combination\n\n   l1 = ['NNP', 'VBG', 'VBZ', 'IN']\n   l2 = ['NNP', 'VBG', 'VBZ']\n    \n\n   l3 = ['PRP', 'VBG', 'VBZ', 'IN']\n   l4 = ['PRP', 'VBG', 'VBZ']\n   l5 = ['PRP', 'VBG', 'VBD']\n   l6 = ['NNP', 'VBG', 'VBD']\n   l7 = ['NN', 'VBG', 'VBZ']\n\n   l8 = ['NNP', 'VBZ', 'JJ']\n   l9 = ['NNP', 'VBZ', 'NN']\n\n   l10 = ['NNP', 'VBZ']\n   l11 = ['PRP', 'VBZ']\n   l12 = ['NNP', 'NN', 'IN']\n   l13 = ['NN', 'VBZ']\n\n   l14 = ['DT', 'NNP', 'VBZ', 'JJ', 'IN']\n\n\n    # With the use of conditional statements the dictionary is compared with the list created above\n\n   if all(key in bucket for key in l14): #'NN', 'VBZ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + ' ' + line.words[bucket['JJ']] + ' ' + line.words[bucket['IN']] + '?'\n\n   elif all(key in  bucket for key in l1): #'NNP', 'VBG', 'VBZ', 'IN' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NNP']]+ ' '+ line.words[bucket['VBG']] + '?'\n\n    \n   elif all(key in  bucket for key in l2): #'NNP', 'VBG', 'VBZ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NNP']] +' '+ line.words[bucket['VBG']] + '?'\n\n    \n   elif all(key in  bucket for key in l3): #'PRP', 'VBG', 'VBZ', 'IN' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['PRP']]+ ' '+ line.words[bucket['VBG']] + '?'\n\n    \n   elif all(key in  bucket for key in l4): #'PRP', 'VBG', 'VBZ' in sentence.\n      question = 'What ' + line.words[bucket['PRP']] +' '+  ' does ' + line.words[bucket['VBG']]+ ' '+  line.words[bucket['VBG']] + '?'\n\n   elif all(key in  bucket for key in l7): #'NN', 'VBG', 'VBZ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NN']] +' '+ line.words[bucket['VBG']] + '?'\n\n   elif all(key in bucket for key in l8): #'NNP', 'VBZ', 'JJ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + '?'\n\n   elif all(key in bucket for key in l9): #'NNP', 'VBZ', 'NN' in sentence\n      question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + '?'\n\n   elif all(key in bucket for key in l11): #'PRP', 'VBZ' in sentence.\n      if line.words[bucket['PRP']] in ['she','he']:\n          question = 'What' + ' does ' + line.words[bucket['PRP']].lower() + ' ' + line.words[bucket['VBZ']].singularize() + '?'\n\n   elif all(key in bucket for key in l10): #'NNP', 'VBZ' in sentence.\n      question = 'What' + ' does ' + line.words[bucket['NNP']] + ' ' + line.words[bucket['VBZ']].singularize() + '?'\n\n   elif all(key in bucket for key in l13): #'NN', 'VBZ' in sentence.\n      question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NN']] + '?'\n    \n \n\n    # When the tags are generated 's is split to ' and s. To overcome this issue.\n   if 'VBZ' in bucket and line.words[bucket['VBZ']] == \"\u2019\":\n      question = question.replace(\" \u2019 \",\"'s \")\n\n   # Print the genetated questions as output.\n   if question != '':\n      print('\\n', 'Question: ' + question )\n\n      return {'question':question,'answer':answer}\n      # print('\\n', 'Question: ' + question )\n   \n\n@app.route('/')\ndef student():\n   return render_template('form.html')\n\n@app.route('/result',methods = ['POST', 'GET'])\ndef result():\n   global verbose \n   verbose = False\n   text_input = ''\n   if request.method == 'POST':\n      result = request.form\n      for key, value in result.items():\n         text_input += value\n      data = (<div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\"><span class=\"fea_parsing_funcs blodFunc\">parse</span>(<span class=\"fea_parsing_funcs blodFunc\">text</span>_input)</div>)\n     \n      return render_template(\"result.html\",result = data)\n   else:\n      return render_template('form.html')\n\n\nif __name__ == '__main__':\n   app.run(debug = True)\n   #https://github.com/huudangdev/generator-question-textblob-nlp/blob/master/app</code></pre></div></body></html>", "sec_2": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"sec_2\"><pre id=\"sec_2_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob import Word\n\nword1 = Word(\"apples\")\nprint(\"apples:\", <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">word1.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>()</div>)\n\nword2 = Word(\"media\")\nprint(\"media:\", word2.lemmatize())\n\nworfir = Word(\"greater\")\nprint(\"greater:\", worfir.lemmatize(\"a\"))\n\nfor word, pos in <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\">text_<span class=\"fea_Part_of_Speech_funcs blodFunc\">blob</span>_object.<span class=\"fea_Part_of_Speech_funcs blodFunc\">tags</span></div>:\n    print(word + \" =&gt; \" + pos)\n\ntext = (\"Football is a good game. It has many health benefit\")\ntext_blob_object = TextBlob(text)\n<div class=\"highlights fea_text_simplify\" id=\"text_simplify_0\" style=\"display: inline;\">print(text_blob_object.words.pluralize())\nprint(text_blob_object.words.singularize())</div>\n\n    #https://stackabuse.com/python-for-nlp-introduction-to-the-textblob-library/</code></pre></div></body></html>", "sec_12": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_12\"><pre id=\"sec_12_code\"><code class=\"python\">from flask import Flask, request, jsonify\nfrom textblob import TextBlob, Word\nfrom textblob.exceptions import NotTranslated\napp = Flask(__name__)\n\nfrom signal import *\n\n@app.route(\"/sentiment\")\ndef singularize():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\t\n\treturn jsonify(blob.sentiment)\n\n@app.route(\"/singularize\")\ndef sentiment():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\t\n\treturn jsonify(blob.words.singularize())\n\n@app.route(\"/lemmatize\")\ndef lemmatize():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\t\n\treturn jsonify(<div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">blob.words.<span class=\"fea_lemmatization_funcs blodFunc\">lemmatize</span>()</div>)\n\n@app.route(\"/correct\")\ndef correct():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\t\n\treturn jsonify({'correct':str(<div class=\"highlights fea_spellcheck\" id=\"spellcheck_0\" style=\"display: inline;\">blob.<span class=\"fea_spellcheck_funcs blodFunc\">correct</span>()</div>)})\n\n@app.route(\"/spelling\")\ndef spelling():\n\ttext = request.args.get('text').strip().encode('utf-8', \"ignore\")\n\tblob = TextBlob(text)\n\n\tsuggestions = {}\n\tfor token in blob.words:\n\t\tword = Word(token)\n\t\tsuggestions[token] = word.spellcheck()\n\t\t\n\treturn jsonify(suggestions)\n\n@app.route(\"/language\")\ndef language():\n\ttext = request.args.get('text').strip()\n\tblob = TextBlob(text)\n\t\n\treturn jsonify({\"language\":blob.detect_language()})\n\n@app.route(\"/translate\")\ndef translate():\n\ttext = request.args.get('text').strip()\n\tl_from = request.args.get('from')\n\tl_to = request.args.get('to')\n\n\tblob = TextBlob(text)\n\n\tif l_from is None:\n\t\t<div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">l_from = <span class=\"fea_language_detection_funcs blodFunc\">blob</span>.<span class=\"fea_language_detection_funcs blodFunc\">detect</span>_<span class=\"fea_language_detection_funcs blodFunc\">language</span>()</div>\n\t\n\ttry:\n\t\t<div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\"><span class=\"fea_translation_funcs blodFunc\">translate</span>d = blob.<span class=\"fea_translation_funcs blodFunc\">translate</span>(from_lang = l_from, to = l_to)</div>\n\texcept NotTranslated:\n\t\ttranslated = text\t\t\n\n\treturn jsonify({\"translation\":str(translated)})\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=8593)\n    #https://github.com/dpasch01/textblob-service/blob/master/textblob-service</code></pre></div></body></html>", "sec_6": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"sec_6\"><pre id=\"sec_6_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob.taggers import NLTKTagger\n<div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\">nltk_tagger = NLTKTagger()</div>\nblob = TextBlob(\"Tag! You're It!\", pos_tagger=nltk_tagger)\n<div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_funcs blodFunc\">blob</span>.pos_<span class=\"fea_Part_of_Speech_funcs blodFunc\">tags</span></div>\n#https://textblob.readthedocs.io/en/dev/advanced_usage.html</code></pre></div></body></html>", "sec_9": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_9\"><pre id=\"sec_9_code\"><code class=\"python\">import textblob\n<div class=\"highlights fea_text_simplify\" id=\"text_simplify_0\" style=\"display: inline;\">stringText = textblob.TextBlob(str(list(dataset[\"Summary\"]))).lower()</div>\nwords = stringText.words\nwordCount = {}\nignore = ['a', 'an', 'the', \"'the\", 'and', 'to', 'of', 'in', 'into', 'is', 'was', 'on', 'at', 'from', 'with',\n          'while', 'for', \"'s\", 'as', 'not', 'by', 'after', 'during']\n\nfor word in words:\n    if word in ignore:\n        continue\n    if word in wordCount:\n        wordCount[word] = wordCount[word] + 1\n    else:\n        wordCount[word] = 1\n\nimport operator\nsorted_word = sorted(wordCount.items(), key=operator.itemgetter(1), reverse=True)[:500]\nwith open(\"sorted-word-count.txt\", \"w\") as f:\n    f.write(str(sorted_word))\n\nreasons = ['weather', 'fire', 'shot down', 'stall/runway', 'pilot/crew error', 'systems failure']\n\nexpresion = ['((poor|bad).*(weather|visibility)|thunderstorm|fog)','(caught fire)|(caught on fire)', \n           '(shot down) | (terrorist) | (terrorism)', '(stall)|(runway)', '(pilot|crew) (error|fatigue)',\n            '(engine.*(fire|fail))|(structural fail)|(fuel leak)|(langing gear)|(turbulence)|(electrical)|(out of fuel)|(fuel.*exhaust)']\n\ndataset['Label'] = pd.Series(np.nan, index=dataset.index)\n\ntrainData = []\nfor x in range(len(dataset)):\n    if dataset.loc[x,\"Summary\"] is np.nan:\n        dataset.loc[x,\"Label\"] = \"unknown\"\n    else:\n        for y in range(len(expresion)):\n            if re.search(expresion[y], dataset.loc[x,\"Summary\"].lower()):\n                dataset.loc[x,\"Label\"] = reasons[y]\n                temp = dataset.loc[x,\"Summary\"].lower(), dataset.loc[x,\"Label\"]\n                trainData.append(temp)\n                break\n\nfrom textblob.classifiers import NaiveBayesClassifier\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">cl = <span class=\"fea_classification_funcs blodFunc\">Naive</span><span class=\"fea_classification_funcs blodFunc\">Bayes</span><span class=\"fea_classification_funcs blodFunc\">Classifier</span>(<span class=\"fea_classification_funcs blodFunc\">train</span>Data)</div>\n\nreasons.append(\"unknown\")\nfor x in range(30,len(dataset)):\n    if dataset.loc[x,\"Label\"] in reasons:\n       continue\n    else:\n        dataset.loc[x,\"Label\"] = cl.classify(dataset.loc[x,\"Summary\"])\n        #https://github.com/arif-zaman/airplane-crash/blob/master/Airplane.ipynb</code></pre></div></body></html>", "sec_27": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_27\"><pre id=\"sec_27_code\"><code class=\"python\">import argparse\nimport sys\nfrom textblob import TextBlob\n\nDEFAULT_SUBJECT_LIMIT = 50\nDEFAULT_BODY_LIMIT = 72\n\n\nclass CliColors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\n\ndef check_subject_is_separated_from_body(commit_message):\n    lines = commit_message.splitlines()\n    if len(lines) &gt; 1:\n        # The second line should be empty\n        check_result = not lines[1]\n    else:\n        # If there is just one line then this rule doesn't apply\n        check_result = True\n    print_result(check_result, \"Separate subject from body with a blank line\")\n\n    return check_result\n\n\ndef check_subject_is_not_too_long(commit_message, subject_limit):\n    lines = commit_message.splitlines()\n    check_result = len(lines[0]) &lt;= subject_limit\n    print_result(check_result, \"Limit the subject line to \" +\n                 str(subject_limit) + \" characters\")\n\n    return check_result\n\n\ndef check_subject_is_capitalized(commit_message):\n    lines = commit_message.splitlines()\n    # Check if first character is in upper case\n    check_result = lines[0][0].isupper()\n    print_result(check_result, \"Capitalize the subject line\")\n\n    return check_result\n\n\ndef check_subject_does_not_end_with_period(commit_message):\n    lines = commit_message.splitlines()\n    check_result = not lines[0].endswith(\".\")\n    print_result(check_result, \"Do not end the subject line with a period\")\n\n    return check_result\n\n\ndef check_subject_uses_imperative(commit_message):\n    first = commit_message.splitlines()[0]\n    <div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">third_person_singular_present_verb = \"VBZ\"\n    non_third_person_singular_present_verb = \"VBP\"</div>\n    # The default NLTK parser is not very good with imperative sentences\n    # so we prefix the commit message with a personal pronoun so to\n    # help it determine easier whether the upcoming word is a verb\n    # and not a noun.\n    # We will prefix in two different ways, so to avoid false results.\n    # Read more here: https://stackoverflow.com/a/30823202/6485320\n    # and here: https://stackoverflow.com/a/9572724/6485320\n    third_person_prefix = \"It \"\n    words_in_third_person_prefix_blob = len(third_person_prefix.split())\n    non_third_person_prefix = \"You \"\n    words_in_non_third_person_prefix_blob = len(\n        non_third_person_prefix.split())\n    # Turn the first character into a lowercase so to make it easier for\n    # the parser to determine whether the word is a verb and its tense\n    first_character_in_lowercase = first[0].lower()\n    first = first_character_in_lowercase + first[1:]\n    third_person_blob = TextBlob(third_person_prefix + first)\n    non_third_person_blob = TextBlob(non_third_person_prefix + first)\n\n    first_word, third_person_result = third_person_blob.tags[words_in_third_person_prefix_blob]\n    _, non_third_person_result = non_third_person_blob.tags[words_in_non_third_person_prefix_blob]\n\n    # We need to determine whether the first word is a non-third person verb\n    # when parsed in a non-third person blob. However, there were some\n    # false positives so we use a third person blob to ensure it is not a\n    # third person verb. Unfortunately, there were now some false negatives\n    # due to verbs in a non-third person form, being classified as being in\n    # third person, when parsed in the third person blob.\n    # So, we ultimately check if the verb ends with an 's' which is a pretty\n    # good indicator of a third person, simple present tense verb.\n    <div class=\"highlights fea_text_simplify\" id=\"text_simplify_0\" style=\"display: inline;\">check_result = non_third_person_result == non_third_person_singular_present_verb and (\n        third_person_result != third_person_singular_present_verb or not first_word.endswith(\"s\"))</div>\n    print_result(check_result, \"Use the imperative mood in the subject line\")\n\n    return check_result\n#https://github.com/platisd/bad-commit-message-blocker/blob/master/bad_commit_message_blocker</code></pre></div></body></html>", "sec_1": "<html><body><div class=\"codeBlock hljs swift\" id=\"sec_1\"><pre id=\"sec_1_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob import Word\ndocument = (\"In computer science, artificial intelligence (AI), \\\n            sometimes called machine intelligence, is intelligence \\\n            demonstrated by machines, in contrast to the natural intelligence \\\n            displayed by humans and animals. Computer science defines AI \\\n            research as the study of \\\"intelligent agents\\\": any device that \\\n            perceives its environment and takes actions that maximize its\\\n            chance of successfully achieving its goals.[1] Colloquially,\\\n            the term \\\"artificial intelligence\\\" is used to describe machines\\\n            that mimic \\\"cognitive\\\" functions that humans associate with other\\\n            human minds, such as \\\"learning\\\" and \\\"problem solving\\\".[2]\")\ntext_blob_object = TextBlob(document)\n<div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">for noun_phrase in text_blob_object.noun_phrases:\n    print(noun_phrase)</div>\n\ntext = \"I love to watch football, but I have never played it\"\ntext_blob_object = TextBlob(text)\nfor ngram in <div class=\"highlights fea_n_grams\" id=\"n_grams_1\" style=\"display: inline;\">text_blob_object.ngrams(2)</div>:\n    print(ngram)\n    #https://stackabuse.com/python-for-nlp-introduction-to-the-textblob-library/</code></pre></div></body></html>", "sec_8": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"sec_8\"><pre id=\"sec_8_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob.np_extractors import ConllExtractor\nextractor = ConllExtractor()\nblob = TextBlob(\"Python is a high-level programming language.\", np_extractor=extractor)\n<div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">blob.noun_phrases</div>\n#https://textblob.readthedocs.io/en/dev/advanced_usage.html</code></pre></div></body></html>", "sec_17": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_17\"><pre id=\"sec_17_code\"><code class=\"python\">from textblob import TextBlob\nfrom spellchecker import SpellChecker\n\n\na = input('Enter an Incorrect String : ')\nprint('Original Text : ' + str(a))\nb = TextBlob(a)\n<div class=\"highlights fea_spellcheck\" id=\"spellcheck_0\" style=\"display: inline;\">print('<span class=\"fea_spellcheck_funcs blodFunc\">Correct</span>ed Text : ' + str(b.<span class=\"fea_spellcheck_funcs blodFunc\">correct</span>()))</div>\n\n\nspell = SpellChecker()\n# Find those words that may be misspelled\nmisspelled = spell.unknown(['Good', 'Evening'])\n\nfor word in misspelled:\n    # getting the one `most likely` answer\n    print(spell.correction(word))\n    # getting a list of `likely` options\n    print(spell.candidates(word))\n    #https://github.com/OjasBarawal/Spell-Checker/blob/main/app</code></pre></div></body></html>", "sec_26": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_26\"><pre id=\"sec_26_code\"><code class=\"python\"># The main package to help us with our text analysis\nfrom textblob import TextBlob\n\n# For reading input files in CSV format\nimport csv\n\n# For doing cool regular expressions\nimport re\n\n# For sorting dictionaries\nimport operator\n\n\n# For plotting results\nimport numpy as np\nimport matplotlib.mlab as mlab\nimport matplotlibplot as thr\n\n# Intialize an empty list to hold all of our tweets\ntweets = []\n\n\n# A helper function that removes all the non ASCII characters\n# from the given string. Retuns a string with only ASCII characters.\ndef strip_non_ascii(string):\n    ''' Returns the string without non ASCII characters'''\n    stripped = (c for c in string if 0 &lt; ord(c) &lt; 127)\n    return ''.join(stripped)\n\n\n\n# LOAD AND CLEAN DATA\n\n# Load in the input file and process each row at a time.\n# We assume that the file has three columns:\n# 0. The tweet text.\n# 1. The tweet ID.\n# 2. The tweet publish date\n#\n# We create a data structure for each tweet:\n#\n# id:       The ID of the tweet\n# pubdate:  The publication date of the tweet\n# orig:     The original, unpreprocessed string of characters\n# clean:    The preprocessed string of characters\n# TextBlob: The TextBlob object, created from the 'clean' string\n\nwith open('newtwitter.csv', 'rb') as csvfile:\n    reader = csv.reader(csvfile, delimiter=',')\n    reader.next()\n    for row in reader:\n\n        tweet= dict()\n        tweet['orig'] = row[0]\n        tweet['id'] = int(row[1])\n        tweet['pubdate'] = int(row[2])\n\n        # Ignore retweets\n        if re.match(r'^RT.*', tweet['orig']):\n            continue\n\n        tweet['clean'] = tweet['orig']\n\n        # Remove all non-ascii characters\n        tweet['clean'] = strip_non_ascii(tweet['clean'])\n\n        # Normalize case\n        tweet['clean'] = tweet['clean'].lower()\n\n        # Remove URLS. (I stole this regex from the internet.)\n        tweet['clean'] = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', tweet['clean'])\n\n        # Fix classic tweet lingo\n        tweet['clean'] = re.sub(r'\\bthats\\b', 'that is', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bive\\b', 'i have', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bim\\b', 'i am', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bya\\b', 'yeah', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bcant\\b', 'can not', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bwont\\b', 'will not', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bid\\b', 'i would', tweet['clean'])\n        tweet['clean'] = re.sub(r'wtf', 'what the fuck', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bwth\\b', 'what the hell', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\br\\b', 'are', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bu\\b', 'you', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bk\\b', 'OK', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bsux\\b', 'sucks', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bno+\\b', 'no', tweet['clean'])\n        tweet['clean'] = re.sub(r'\\bcoo+\\b', 'cool', tweet['clean'])\n\n        # Emoticons?\n        # NOTE: Turns out that TextBlob already handles emoticons well, so the\n        # following is not actually needed.\n        # See http://www.datagenetics.com/blog/october52012/index.html\n        # tweet['clean'] = re.sub(r'\\b:\\)\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b:D\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b:\\(\\b', 'sad', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b:-\\)\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b=\\)\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b\\(:\\b', 'good', tweet['clean'])\n        # tweet['clean'] = re.sub(r'\\b:\\\\\\b', 'annoyed', tweet['clean'])\n\n        # Create textblob object\n        tweet['TextBlob'] = TextBlob(tweet['clean'])\n\n        # Correct spelling (WARNING: SLOW)\n        #tweet['TextBlob'] = tweet['TextBlob'].correct()\n\n        tweets.append(tweet)\n\n\n\n# DEVELOP MODELS\n\n<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">for tweet in tweets:\n    tweet['<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span>'] = float(tweet['TextBlob'].<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>.<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span>)\n    tweet['subjectivity'] = float(tweet['TextBlob'].<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>.subjectivity)\n\n    if tweet['<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span>'] &gt;= 0.1:\n        tweet['<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>'] = 'positive'\n    elif tweet['<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span>'] &lt;= -0.1:\n        tweet['<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>'] = 'negative'\n    else:\n        tweet['<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>'] = 'neutral'</div>\n\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">tweets_sorted = sorted(tweets, key=lambda k: k['<span class=\"fea_text_scoring_funcs blodFunc\">polarity</span>'])</div>\n\n\n# EVALUATE RESULTS\n\n# First, print out a few example tweets from each sentiment category.\n\nprint \"\\n\\nTOP NEGATIVE TWEETS\"\nnegative_tweets = [d for d in tweets_sorted if d['sentiment'] == 'negative']\nfor tweet in negative_tweets[0:100]:\n    print \"id=%d, polarity=%.2f, clean=%s\" % (tweet['id'], tweet['polarity'], tweet['clean'])\n\nprint \"\\n\\nTOP POSITIVE TWEETS\"\npositive_tweets = [d for d in tweets_sorted if d['sentiment'] == 'positive']\nfor tweet in positive_tweets[-100:]:\n    print \"id=%d, polarity=%.2f, clean=%s\" % (tweet['id'], tweet['polarity'], tweet['clean'])\n\nprint \"\\n\\nTOP NEUTRAL TWEETS\"\nneutral_tweets = [d for d in tweets_sorted if d['sentiment'] == 'neutral']\nfor tweet in neutral_tweets[0:500]:\n    print \"id=%d, polarity=%.2f, clean=%s\" % (tweet['id'], tweet['polarity'], tweet['clean'])\n\n#https://github.com/stepthom/textblob-sentiment-analysis/blob/master/doAnalysis</code></pre></div></body></html>", "sec_29": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_29\"><pre id=\"sec_29_code\"><code class=\"python\">import sys\nimport json\nimport time\nimport re\nimport requests\nimport nltk\nimport argparse\nimport logging\nimport string\ntry:\n    import urllib.parse as urlparse\nexcept ImportError:\n    import urlparse\nfrom tweepy.streaming import StreamListener\nfrom tweepy import API, Stream, OAuthHandler, TweepError\nfrom textblob import TextBlob\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom bs4 import BeautifulSoup\nfrom elasticsearch import Elasticsearch\nfrom random import randint, randrange\nfrom datetime import datetime\nfrom newspaper import Article, ArticleException\n\n# import elasticsearch host, twitter keys and tokens\nfrom config import *\n\n\nSTOCKSIGHT_VERSION = '0.1-b.12'\n__version__ = STOCKSIGHT_VERSION\n\nIS_PY3 = sys.version_info &gt;= (3, 0)\n\nif not IS_PY3:\n    print(\"Sorry, stocksight requires Python 3.\")\n    sys.exit(1)\n\n# sentiment text-processing url\nsentimentURL = 'http://text-processing.com/api/sentiment/'\n\n# tweet id list\ntweet_ids = []\n\n# file to hold twitter user ids\ntwitter_users_file = './twitteruserids.txt'\n\nprev_time = time.time()\nsentiment_avg = [0.0,0.0,0.0]\n\ndef sentiment_analysis(text):\n    \"\"\"Determine if sentiment is positive, negative, or neutral\n    algorithm to figure out if sentiment is positive, negative or neutral\n    uses sentiment polarity from TextBlob, VADER Sentiment and\n    sentiment from text-processing URL\n    could be made better :)\n    \"\"\"\n\n    # pass text into sentiment url\n    if args.websentiment:\n        ret = get_sentiment_from_url(text, sentimentURL)\n        if ret is None:\n            sentiment_url = None\n        else:\n            sentiment_url, neg_url, pos_url, neu_url = ret\n    else:\n        sentiment_url = None\n\n    # pass text into TextBlob\n    text_tb = TextBlob(text)\n\n    # pass text into VADER Sentiment\n    analyzer = SentimentIntensityAnalyzer()\n    <div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\">text_vs = analyzer.<span class=\"fea_text_scoring_funcs blodFunc\">polarity</span>_scores(text)</div>\n\n    # determine sentiment from our sources\n    <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">if <span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>_url is None:\n        if text_tb.<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>.<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span> &lt; 0 and text_vs['compound'] &lt;= -0.05:\n            <span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span> = \"negative\"\n        elif text_tb.<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>.<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span> &gt; 0 and text_vs['compound'] &gt;= 0.05:\n            <span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span> = \"positive\"\n        else:\n            <span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span> = \"neutral\"\n    else:\n        if text_tb.<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>.<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span> &lt; 0 and text_vs['compound'] &lt;= -0.05 and <span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>_url == \"negative\":\n            <span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span> = \"negative\"\n        elif text_tb.<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>.<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span> &gt; 0 and text_vs['compound'] &gt;= 0.05 and <span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>_url == \"positive\":\n            <span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span> = \"positive\"\n        else:\n            <span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span> = \"neutral\"</div>\n\n    # calculate average polarity from TextBlob and VADER\n    polarity = (text_tb.sentiment.polarity + text_vs['compound']) / 2\n\n    # output sentiment polarity\n    print(\"************\")\n    print(\"Sentiment Polarity: \" + str(round(polarity, 3)))\n\n    # output sentiment subjectivity (TextBlob)\n    print(\"Sentiment Subjectivity: \" + str(round(text_tb.sentiment.subjectivity, 3)))\n\n    # output sentiment\n    print(\"Sentiment (url): \" + str(sentiment_url))\n    print(\"Sentiment (algorithm): \" + str(sentiment))\n    print(\"Overall sentiment (textblob): \", text_tb.sentiment) \n    print(\"Overall sentiment (vader): \", text_vs) \n    print(\"sentence was rated as \", round(text_vs['neg']*100, 3), \"% Negative\") \n    print(\"sentence was rated as \", round(text_vs['neu']*100, 3), \"% Neutral\") \n    print(\"sentence was rated as \", round(text_vs['pos']*100, 3), \"% Positive\") \n    print(\"************\")\n\n    return polarity, text_tb.sentiment.subjectivity, sentiment\n#https://github.com/shirosaidev/stocksight/blob/master/sentiment</code></pre></div></body></html>", "sec_15": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_15\"><pre id=\"sec_15_code\"><code class=\"python\">from textblob import TextBlob\n\n\ndef tweet_sentiment(text, verbose=False):\n    \"\"\"\n    The sentiment function of textblob returns two properties, polarity, and subjectivity.\n    Polarity is float which lies in the range of [-1,1] where 1 means positive statement\n    and -1 means a negative statement.\n    Subjective sentences generally refer to personal opinion, emotion or judgment whereas\n    objective refers to factual information. Subjectivity is also a float which lies in\n    the range of [0,1].\n    \"\"\"\n    # parse the tweet into textblob object\n    blob = TextBlob(text)\n    # we define the sentiment of sentence to be the product of its polarity and subjectivity\n    # tweet sentiment is the sum of sentiment for all sentences in a tweet\n    <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span> = sum(s.<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span> * s.subjectivity for s in blob.sentences)</div>\n    # print if verbose\n    if verbose:\n        polarity = sum(s.polarity for s in blob.sentences)\n        subjectivity = sum(s.subjectivity for s in blob.sentences)\n        num_sentence = len(blob.sentences)\n        return text, num_sentence, polarity, subjectivity, sentiment\n    else:\n        return sentiment\n\n\ndef test():\n    sentences = [\n        '$AAPL so is this the price that gets split? If so, looks like it\u2019ll be $125.50 a share on Monday. Nice.',\n        'Stocks head into September in high gear as Apple and Tesla split, and markets await the August jobs report',\n        'S&amp;P 500 SETS FRESH RECORD CLOSING HIGH OF 3,508.01',\n        'Massive $tsla dump be careful out there short term oversold tho $spy $amzn',\n        '$SPX is overbought but momentum is very very strong. My bet is unless we correct quickly this week, we are looking for a blow off top. ',\n        '$SPY reached 350 2 points from our target of 352.. RSI is overbought - sell and wait ti buy for later. Short $SHOP and $NVAX.',\n        'Slight setback, nothing to worry about. Outlook dismal. 28 trade session left - Target $SPX 2394.25',\n        'Russell looks bad. Big bearish RSI divergence and ejected from the channel after riding up the bottom rail.',\n    ]\n    print(' | '.join(['',' #','Sentence'+' '*92,'# sentence','polarity','subjectivity','sentiment','']))\n    print('-'*162)\n    for i,sentence in enumerate(sentences):\n        text, num_sentence, polarity, subjectivity, sentiment = tweet_sentiment(sentence, verbose=True)\n        print(f' | {i+1:2d} | {text[:100]: &lt;100} | {num_sentence: &gt;10} | {polarity:+8.2f} | {subjectivity:+12.2f} | {sentiment:+9.2f} |')\n\n\nif __name__ == '__main__':\n    test()\n    #https://github.com/quantumsnowball/AppleDaily20200907/blob/master/sentiment</code></pre></div></body></html>", "sec_20": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_20\"><pre id=\"sec_20_code\"><code class=\"python\">import random\nimport re\nimport csv\nimport string\nimport operator\n\nfrom textblob import TextBlob\nfrom textblob.classifiers import NaiveBayesClassifier # update sentiment, if textblob returns neutral\n\ndef determineSentiment(sent_dict):\n\t# takes in a dictionary or sub-dictionary to return the sentiment in a list\n\tfinal_sent_dict = {}\n\tsentence_list = []\n\tfor speech in sent_dict:\n\t\ttext_sent = TextBlob(sent_dict[speech])\n\t\t#text_tag = text_sent.tags\n\t\tcounter = 1\n\t\tfor sentence in text_sent.sentences:\n\t\t\t#print(speech)\n\t\t\tfinal_sent_dict[speech + '_' + str(counter)] = (sentence.sentiment, sentence)\n\t\t\tcounter += 1 # each sub-sentence in a speech has it's own dictionary key\n\t<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">final_sent_dict[\"_average\"] = text_sent.<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span></div> # beginning of an ordered dict\n\treturn final_sent_dict\n\ndef trainSentiment():\n\t# if the sentence is neutral, update to attribute sentiment based on key words\n\t# example: villian -&gt; negative, dying -&gt; negative, etc...\n\t# https://textblob.readthedocs.io/en/dev/classifiers.html#classifiers\n\n\t# train classifers on actual hamlet data\n\thamlet_train = [\n\t# act 1\n\t\t('this dreaded sight', 'neg'),\n\t\t('o god!', 'neg'),\n\t\t('o fie!', 'neg'),\n\t\t('break my heart,  for i must hold my tongue!', 'neg'),\n\t\t('funeral', 'neg'),\n\t\t('he was a man,  take him for all in all, i shall not look upon his like again', 'neg'),\n\t\t('i doubt some foul play would the night were come!', 'neg'),\n\t\t('foul deeds will rise', 'neg'),\n\t\t('pooh!', 'neg'),\n\t\t('angels and ministers of grace defend us!', 'neg'),\n\t\t('you shall not go', 'neg'),\n\t\t('hold off your hands', 'neg'),\n\t\t('my fate cries out' , 'neg'),\n\t\t(\"i'll make a ghost of him that lets me\" , 'neg'),\n\t\t('something is rotten in the state of denmark', 'neg'),\n\t\t('harrow up thy soul', 'neg'),\n\t\t('revenge', 'neg'),\n\t\t('incest', 'neg'),\n\t\t('adulterate', 'neg'),\n\t\t('beast', 'neg'),\n\t\t('lust', 'neg'),\n\t\t('a serpent stung me', 'neg'),\n\t\t('villain', 'neg'),\n\t\t('perturbed spirit!', 'neg')\n\t]\n\n\thamlet_test = [\n\t# act 2\n\t\t('dishonour', 'neg'),\n\t\t('taints of liberty', 'neg'),\n\t\t('flash and outbreak of a fiery mind', 'neg'),\n\t\t('falsehood', 'neg'),\n\t\t('fouled', 'neg'),\n\t\t('piteous', 'neg'),\n\t\t('i do not know', 'neg'),\n\t\t('i do fear it', 'neg'),\n\t\t('madness wherein now he raves', 'neg'),\n\t\t('madness', 'neg'),\n\t\t('indifferent children of the earth', 'neg'),\n\t\t('beggars bodies', 'neg'),\n\t\t('murder', 'neg'),\n\t\t('that he should weep for her?', 'neg'),\n\t\t('am i a coward?', 'neg'),\n\t\t('who calls me villain?', 'neg'),\n\t]\n\t<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">cl = <span class=\"fea_classification_funcs blodFunc\">Naive</span><span class=\"fea_classification_funcs blodFunc\">Bayes</span><span class=\"fea_classification_funcs blodFunc\">Classifier</span>(hamlet_<span class=\"fea_classification_funcs blodFunc\">train</span>)</div>\n\treturn cl\n\t#https://github.com/cyschneck/Billy-Bot/blob/master/shakespeare_sentiment</code></pre></div></body></html>", "sec_21": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_21\"><pre id=\"sec_21_code\"><code class=\"python\">from TwitterSearch import *\nfrom textblob import TextBlob\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf8')\n\nfilepath = \"2017.txt\"\ntry:\n\tfp = open(\"2017.txt\",\"r\")\n\tdi = { }\n\tfor line in fp.read().splitlines():\n\t\tcnt = 0\n\t\tscore = 0\n\t\ttemp = 0;\n\t\t#print line\n\t\n\t\t\n\t\ttso = TwitterSearchOrder() \n\t\ttso.set_keywords([line]) \n\t\ttso.set_language('en') \n\t\ttso.set_include_entities(False)\n\t\ttso.set_count(100)\n\n# it's about time to create a TwitterSearch object with our secret tokens\n\t\tts = TwitterSearch(\n\t\t\t\tconsumer_key = \"XXXX\",\n        \t\tconsumer_secret = \"YY\",\n        \t\taccess_token = \"ZZ\",\n        \t\taccess_token_secret = \"MM\"\n\t\t )\n\n\n # this is where the fun actually starts :)\n\t\tfor tweet in ts.search_tweets_iterable(tso):\n\t\t\tif(cnt&lt;20):\n\t\t\t\t<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">analysis = TextBlob(tweet['text'])\n\t\t\t\tcnt=cnt+1;\n\t\t\t\ttemp = temp+1;\n\t\t\t\tif analysis.<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>.<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span> &gt; 0:\n\t\t\t\t\t#print '1'\n\t\t\t\t\tscore=score+1\n\t\t\t\telif analysis.<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>.<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span> == 0:\n\t\t\t\t\t#print '0'\n\t\t\t\t\tscore = score;\n\t\t\t\telse:\n\t\t\t\t\tscore=score-1</div>\n\t\t\telse:\n\t\t\t\tbreak\n\t#F.write(tweet['text'])\n\t\t\t#print(tweet['text'])\n\t\tx = float(score)/float(temp)\n\t\t#print x\n\t\tdi.update({line : x})\n\td_view = [ (v,k) for k,v in di.iteritems() ]\n\td_view.sort(reverse=True) # natively sort tuples by first element\n\tfor v,k in d_view:\n\t\t\tprint v, k\n\t\t\nexcept TwitterSearchException as e: # take care of all those ugly errors if there are some\n\tprint(e)\n\t#https://github.com/avaiyang/Movie-Rating-and-Prediction-Model/blob/master/twittersearch</code></pre></div></body></html>", "sec_24": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_24\"><pre id=\"sec_24_code\"><code class=\"python\">import os\nfrom datetime import datetime, timedelta\nfrom pickle import load\n\nimport pytz\nfrom flask import Flask, jsonify, redirect, render_template, request, session, url_for\nfrom flask_sqlalchemy import SQLAlchemy\nfrom textblob import TextBlob\n\nfrom model_nltk import predict_sentiment\n\napp = Flask(__name__, template_folder=\"templates\")\n\n# \"sqlite:///data.sqlite\"\n# /// for relative path\n# //// for absolute path\napp.config[\"SQLALCHEMY_DATABASE_URI\"] = os.environ.get(\n    \"DATABASE_URL\", \"sqlite:///data.sqlite\"\n)\napp.config[\"SQLALCHEMY_TRACK_MODIFICATIONS\"] = False\napp.config[\"SECRET_KEY\"] = os.environ.get(\"SECRET_KEY\", \"thisissecret\")\napp.config[\"PERMANENT_SESSION_LIFETIME\"] = timedelta(hours=12)\n\ndb = SQLAlchemy(app)\n\n# since the app is hosted on heroku so this line of code is to change the timezone\nIST = pytz.timezone(\"Asia/Kolkata\")\n\n\n# I have creted two models but I am using model_nltk because of its high accurcy and less execution time.\n# textblob is used for ploting the subjectivity and polarity curve for the input data\n\n# class for creating and initialising database\nclass New_Data(db.Model):\n\n    Id = db.Column(db.Integer, primary_key=True)\n    Text = db.Column(db.Text)\n    Sentiment = db.Column(db.String(20))\n    # .now(IST).strftime('%Y-%m-%d %H:%M:%S'))\n    Date = db.Column(\n        db.DateTime, default=datetime.now(IST).strftime(\"%Y-%m-%d %H:%M:%S\")\n    )\n\n    def __init__(self, Text, Sentiment):\n        self.Text = Text\n        self.Sentiment = Sentiment\n\n\n# loading classifier\nwith open(\"my_classifier.pickle\", \"rb\") as f:\n    classifier = load(f)\n\n\ndef allowed_file(filename):\n    \"\"\"Checking file extension i.e. text file or not\"\"\"\n    return \".\" in filename and filename.split(\".\")[1] == \"txt\"\n\n\n# route for home page\n@app.route(\"/\", methods=[\"POST\", \"GET\"])\ndef home():\n    if request.method == \"POST\":\n        sentence = str(request.form.get(\"twt\"))\n\n        <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span> = predict_<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>(sentence, classifier)</div>\n\n        # adding emoji to the sentiment\n        if sentiment == \"Positive\":\n            sentiment += \" \\U0001f600\"\n\n        elif sentiment == \"Negative\":\n            sentiment += \" \\U0001F641\"\n\n        else:\n            pass\n\n        # creating an instance of the data table for the database and commiting the changes\n        usr_data = New_Data(sentence, sentiment.split()[0])\n        try:\n            db.session.add(usr_data)\n            db.session.commit()\n        except:\n            pass\n\n        text = 'You have entered \"' + sentence + '\"'\n        return render_template(\n            \"index.html\", text=text, sentiment=\"Sentiment: \" + sentiment\n        )\n\n    return render_template(\"index.html\")\n\n\n# route for about page\n@app.route(\"/about\")\ndef about():\n    return render_template(\"about.html\")\n\n\n# route for members page\n@app.route(\"/member\")\ndef contact():\n    return render_template(\"members.html\")\n\n\n# route for fastapi\n# setting default value for the api\n@app.route(\"/fast-api/\", defaults={\"sentence\": \"Great\"})\n@app.route(\"/fast-api/&lt;sentence&gt;\")\ndef fast_api(sentence):\n    sentiment = predict_sentiment(sentence, classifier)\n\n    return jsonify({\"sentence\": sentence, \"sentiment\": sentiment})\n\n\n# setting post method for the api\n@app.route(\"/fastapi\", methods=[\"POST\"])\ndef fastapi():\n    text = request.form[\"text\"]\n    <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_1\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span> = TextBlob(text).<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>.<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span>\n    if <span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span> &gt; 0:\n        <span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span> = \"Positive\"\n    elif <span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span> &lt; 0:\n        <span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span> = \"Negative\"\n    else:\n        <span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span> = \"Neutral\"\n    return jsonify({\"<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>\": <span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>})</div>\n#https://github.com/g-paras/sentiment-analysis-api/blob/master/app</code></pre></div></body></html>", "sec_25": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_25\"><pre id=\"sec_25_code\"><code class=\"python\">import facebook as fb\nimport requests\nimport argparse\nimport textblob as tb\n\nFLAGS = None\n\ndef sentiment_analysis(post):\n\n    # Here's where the magic happens\n    <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">tb_msg = tb(post['message'])\n    score = tb_msg.<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span></div>\n\n    print(\"Date: %s, From: %s\\n\", post['created_time'], post['from'])\n    print(\"%s\\nShared: %s, Score: %f\", post['message'], post['share'], score)\n\n\n\ndef connect(access_token, user):\n    graph = fb.GraphAPI(access_token)\n    profile = graph.get_object(user)\n\n    return graph, profile\n\n\ndef main():\n\n    access_token = FLAGS.access_token\n    user = FLAGS.profile\n\n    graph, profile = connect(access_token, user)\n    \n    posts = graph.get_connections(profile['id'], 'posts')\n\n\n    #Let's grab all the posts and analyze them!\n    while True:\n        try:\n            [sentiment_analysis(post=post) for post in posts['data']]\n            posts= requests.get(posts['paging']['next']).json()\n        except KeyError:\n            break\n            \n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Simple Facebook Sentiment Analysis Script')\n    parser.add_argument('--access_token', type=str, required=True, default='', help='Your Facebook API Access Token: https://developers.facebook.com/docs/graph-api/overview')\n    parser.add_argument('--profile', type=str, required=True, default='', help='The profile name to retrieve the posts from')\n    FLAGS = parser.parse_args()\n    main()\n    #https://github.com/cosimoiaia/Facebook-Sentiment-Analysis/blob/master/simple_facebook_sentiment_analysis</code></pre></div></body></html>", "sec_28": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_28\"><pre id=\"sec_28_code\"><code class=\"python\">import re\nimport tweepy\nfrom tweepy import OAuthHandler\nfrom textblob import TextBlob\n \nclass TwitterClient(object):\n    '''\n    Generic Twitter Class for sentiment analysis.\n    '''\n    def __init__(self):\n        '''\n        Class constructor or initialization method.\n        '''\n       \n        consumer_key = 'XXXXXXXXXXXX'\n        consumer_secret = 'XXXXXXXXXXXX'\n        access_token = 'XXXXXXXXXXXX'\n        access_token_secret = 'XXXXXXXXXXXX'\n \n       \n        try:\n         \n            self.auth = OAuthHandler(consumer_key, consumer_secret)\n         \n            self.auth.set_access_token(access_token, access_token_secret)\n        \n            self.api = tweepy.API(self.auth)\n        except:\n            print(\"Error: Authentication Failed\")\n \n    def clean_tweet(self, tweet):\n        '''\n        Utility function to clean tweet text by removing links, special characters\n        using simple regex statements.\n        '''\n        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n \n    def get_tweet_sentiment(self, tweet):\n        '''\n        Utility function to classify sentiment of passed tweet\n        using textblob's sentiment method\n        '''\n        \n        <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\">analysis = TextBlob(self.clean_tweet(tweet))\n       \n        if analysis.<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>.<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span> &gt; 0:\n            return 'positive'\n        elif analysis.<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>.<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span> == 0:\n            return 'neutral'\n        else:\n            return 'negative'</div>\n    #https://github.com/vinitshahdeo/jobtweets/blob/master/jobtweets</code></pre></div></body></html>", "sec_30": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_30\"><pre id=\"sec_30_code\"><code class=\"python\">import sys,tweepy,csv,re\nfrom textblob import TextBlob\nimport matplotlibplot as thr\n\n\nclass SentimentAnalysis:\n\n    def __init__(self):\n        self.tweets = []\n        self.tweetText = []\n\n    def DownloadData(self):\n        # authenticating\n        consumerKey = 'your key here'\n        consumerSecret = 'your key here'\n        accessToken = 'your key here'\n        accessTokenSecret = 'your key here'\n        auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n        auth.set_access_token(accessToken, accessTokenSecret)\n        api = tweepy.API(auth)\n\n        # input for term to be searched and how many tweets to search\n        searchTerm = input(\"Enter Keyword/Tag to search about: \")\n        NoOfTerms = int(input(\"Enter how many tweets to search: \"))\n\n        # searching for tweets\n        self.tweets = tweepy.Cursor(api.search, q=searchTerm, lang = \"en\").items(NoOfTerms)\n\n        # Open/create a file to append data to\n        csvFile = open('result.csv', 'a')\n\n        # Use csv writer\n        csvWriter = csv.writer(csvFile)\n\n\n        # creating some variables to store info\n        polarity = 0\n        positive = 0\n        wpositive = 0\n        spositive = 0\n        negative = 0\n        wnegative = 0\n        snegative = 0\n        neutral = 0\n\n\n        # iterating through tweets fetched\n        for tweet in self.tweets:\n            #Append to temp so that we can store in csv later. I use encode UTF-8\n            self.tweetText.append(self.cleanTweet(tweet.text).encode('utf-8'))\n            # print (tweet.text.translate(non_bmp_map))    #print tweet's text\n            analysis = TextBlob(tweet.text)\n            # print(analysis.sentiment)  # print tweet's polarity\n            <div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span> += analysis.<span class=\"fea_sentiment_analysis_funcs blodFunc\">sentiment</span>.<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span></div>  # adding up polarities to find the average later\n\n            if (analysis.sentiment.polarity == 0):  # adding reaction of how people are reacting to find average later\n                neutral += 1\n            elif (analysis.sentiment.polarity &gt; 0 and analysis.sentiment.polarity &lt;= 0.3):\n                wpositive += 1\n            elif (analysis.sentiment.polarity &gt; 0.3 and analysis.sentiment.polarity &lt;= 0.6):\n                positive += 1\n            elif (analysis.sentiment.polarity &gt; 0.6 and analysis.sentiment.polarity &lt;= 1):\n                spositive += 1\n            elif (analysis.sentiment.polarity &gt; -0.3 and analysis.sentiment.polarity &lt;= 0):\n                wnegative += 1\n            elif (analysis.sentiment.polarity &gt; -0.6 and analysis.sentiment.polarity &lt;= -0.3):\n                negative += 1\n            elif (analysis.sentiment.polarity &gt; -1 and analysis.sentiment.polarity &lt;= -0.6):\n                snegative += 1\n\n\n        # Write to csv and close csv file\n        csvWriter.writerow(self.tweetText)\n        csvFile.close()\n\n        # finding average of how people are reacting\n        positive = self.percentage(positive, NoOfTerms)\n        wpositive = self.percentage(wpositive, NoOfTerms)\n        spositive = self.percentage(spositive, NoOfTerms)\n        negative = self.percentage(negative, NoOfTerms)\n        wnegative = self.percentage(wnegative, NoOfTerms)\n        snegative = self.percentage(snegative, NoOfTerms)\n        neutral = self.percentage(neutral, NoOfTerms)\n\n        # finding average reaction\n        polarity = polarity / NoOfTerms\n\n        # printing out data\n        print(\"How people are reacting on \" + searchTerm + \" by analyzing \" + str(NoOfTerms) + \" tweets.\")\n        print()\n        print(\"General Report: \")\n\n        if (polarity == 0):\n            print(\"Neutral\")\n        elif (polarity &gt; 0 and polarity &lt;= 0.3):\n            print(\"Weakly Positive\")\n        elif (polarity &gt; 0.3 and polarity &lt;= 0.6):\n            print(\"Positive\")\n        elif (polarity &gt; 0.6 and polarity &lt;= 1):\n            print(\"Strongly Positive\")\n        elif (polarity &gt; -0.3 and polarity &lt;= 0):\n            print(\"Weakly Negative\")\n        elif (polarity &gt; -0.6 and polarity &lt;= -0.3):\n            print(\"Negative\")\n        elif (polarity &gt; -1 and polarity &lt;= -0.6):\n            print(\"Strongly Negative\")\n\n        print()\n        print(\"Detailed Report: \")\n        print(str(positive) + \"% people thought it was positive\")\n        print(str(wpositive) + \"% people thought it was weakly positive\")\n        print(str(spositive) + \"% people thought it was strongly positive\")\n        print(str(negative) + \"% people thought it was negative\")\n        print(str(wnegative) + \"% people thought it was weakly negative\")\n        print(str(snegative) + \"% people thought it was strongly negative\")\n        print(str(neutral) + \"% people thought it was neutral\")\n\n        self.plotPieChart(positive, wpositive, spositive, negative, wnegative, snegative, neutral, searchTerm, NoOfTerms)\n\n\n    def cleanTweet(self, tweet):\n        # Remove Links, Special Characters etc from tweet\n        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) | (\\w +:\\ / \\ / \\S +)\", \" \", tweet).split())\n\n    # function to calculate percentage\n    def percentage(self, part, whole):\n        temp = 100 * float(part) / float(whole)\n        return format(temp, '.2f')\n\n    def plotPieChart(self, positive, wpositive, spositive, negative, wnegative, snegative, neutral, searchTerm, noOfSearchTerms):\n        labels = ['Positive [' + str(positive) + '%]', 'Weakly Positive [' + str(wpositive) + '%]','Strongly Positive [' + str(spositive) + '%]', 'Neutral [' + str(neutral) + '%]',\n                  'Negative [' + str(negative) + '%]', 'Weakly Negative [' + str(wnegative) + '%]', 'Strongly Negative [' + str(snegative) + '%]']\n        sizes = [positive, wpositive, spositive, neutral, negative, wnegative, snegative]\n        colors = ['yellowgreen','lightgreen','darkgreen', 'gold', 'red','lightsalmon','darkred']\n        patches, texts = thr.pie(sizes, colors=colors, startangle=90)\n        thr.legend(patches, labels, loc=\"best\")\n        thr.title('How people are reacting on ' + searchTerm + ' by analyzing ' + str(noOfSearchTerms) + ' Tweets.')\n        thr.axis('equal')\n        thr.tight_layout()\n        thr.show()\n\n\n\nif __name__== \"__main__\":\n    sa = SentimentAnalysis()\n    sa.DownloadData()\n    #https://github.com/the-javapocalypse/Twitter-Sentiment-Analysis/blob/master/main</code></pre></div></body></html>", "sec_14": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_14\"><pre id=\"sec_14_code\"><code class=\"python\">from textblob import TextBlob                   ##language translation API\nfrom tkinter.scrolledtext import ScrolledText   ##for scrollable text box\n\n#-----translation function----------------------------------------------------------------------\ndef toLang(lang):\n    try:\n        output.delete(\"1.0\",END)            ##to delete previous entry in the output box\n        inputSTR = input_str.get(\"1.0\",END)\n        obj = TextBlob(str(inputSTR))\n        <div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">outputSTR = obj.<span class=\"fea_translation_funcs blodFunc\">translate</span>(to=lang)</div>\n        output.insert(END,str(outputSTR))   ##insert output to the output box\n    except:\n        output.insert(END, \"**Please enter a meaningful word/sentence**\")\n        #https://github.com/DeepakJha01/GUI-Translator/blob/master/main/gui_translator</code></pre></div></body></html>", "sec_16": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_16\"><pre id=\"sec_16_code\"><code class=\"python\">import webbrowser\nfrom textblob import TextBlob, exceptions\nfrom wox import Wox, WoxAPI\n\nLANGUAGE = 'ru'\n\ndef translate(query):\n    query_modified = query.strip().lower()\n    en = set(chr(i) for i in range(ord('a'), ord('z') + 1))\n    results = []\n    if query_modified:\n        try:\n            from_lang, to_lang = ('en', LANGUAGE) if query_modified[0] in en else (LANGUAGE, 'en')\n            translation = TextBlob(query_modified).translate(from_lang, to_lang)\n            results.append({\n                \"Title\": str(translation),\n                \"SubTitle\": query,\n                \"IcoPath\":\"Images/app.png\",\n                \"JsonRPCAction\":{'method': 'openUrl',\n                                 'parameters': [r'http://translate.google.com/#{}/{}/{}'.format(from_lang, to_lang, query)],\n                                 'dontHideAfterAction': False}\n            })\n        except exceptions.NotTranslated:\n            pass\n    if not results:\n        results.append({\n                \"Title\": 'Not found',\n                \"SubTitle\": '',\n                \"IcoPath\":\"Images/app.png\"\n            })\n    return results\n\nclass Translate(Wox):\n    def query(self, query):\n        return <div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\"><span class=\"fea_translation_funcs blodFunc\">translate</span>(query)</div>\n\n    def openUrl(self, url):\n        webbrowser.open(url)\n\nif __name__ == \"__main__\":\n    Translate()\n    #https://github.com/RomanKornev/Translate/blob/master/main</code></pre></div></body></html>", "sec_19": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_19\"><pre id=\"sec_19_code\"><code class=\"python\">import googletrans\nfrom googletrans import Translator\n\n#from pygoogletranslation import Translator\n\nfrom textblob import TextBlob\n\nimport time\nepis = {}\n\ntranslator = Translator(service_urls = ['translate.google.com', 'translate.google.co.kr'])\n#translator = Translator()\n\n#bad_result_message = '**!!! BAD RESULT OF RECOGNITION. U CAN TRY AGAIN**'\n\n\nlang_dic = {value.title(): key for key, value in googletrans.LANGUAGES.items()}\nlang_dic_reversed = {key: f'*{value.capitalize()}*' for key, value in googletrans.LANGUAGES.items()}\n\nall_langs = list(lang_dic.keys())\n\n\n\ndef from_code_to_name(language):\n    return  lang_dic_reversed[language]\n\ndef smart_to_tidy(langs):\n    return [lang_dic_reversed[l] for l in langs]\n\ndef get_code_from_lang(lang):\n    return lang_dic[lang.tolower()]\n\n\ndef log_text(text, lang_list = ['en','ru']):\n    \n    result = []\n    \n    if len(text) &lt; 3:\n        result.append(f'*too shirt text*: {text}')\n        return result\n    \n\n    lang_of_text = translator.detect(text).lang\n    #if len(lang_of_text) == 0: lang_of_text = 'en'\n    #print(lang_of_text)\n\n    bool_list = [r != lang_of_text for r in lang_list]\n    \n    if all(bool_list):\n        bool_list.append(False)\n        lang_list.append(lang_of_text)\n    \n    for lang, it in zip(lang_list, bool_list):\n        result.append(f'{lang_dic_reversed[lang].upper()}:')\n        if it:\n            time.sleep(0.7)\n            #print(f\"{text}, {lang}, {lang_of_text}\")\n            txt = translator.translate(text, dest = lang, src = lang_of_text).text\n            result.append(txt)\n        else:\n            txt = text\n            result.append(f'_(original text)_ {text}')\n        result.append('')\n    \n    return result\n\n\ndef log_text_better(text, lang_list = ['en','ru']):\n    \n    result = []\n    \n    if len(text) &lt; 3:\n        result.append(f'*too shirt text*: {text}')\n        return result\n    \n    blob = TextBlob(text)\n\n    <div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">lang_of_text = <span class=\"fea_language_detection_funcs blodFunc\">blob</span>.<span class=\"fea_language_detection_funcs blodFunc\">detect</span>_<span class=\"fea_language_detection_funcs blodFunc\">language</span>()</div>\n\n    bool_list = [r != lang_of_text for r in lang_list]\n    \n    if all(bool_list):\n        bool_list.append(False)\n        lang_list.append(lang_of_text)\n    \n    for lang, it in zip(lang_list, bool_list):\n        result.append(f'{lang_dic_reversed[lang].upper()}:')\n        if it:\n            time.sleep(1.3)\n            <div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">txt = str(blob.<span class=\"fea_translation_funcs blodFunc\">translate</span>(from_lang = lang_of_text, to=lang))</div>\n            result.append(txt)\n        else:\n            txt = text\n            result.append(f'_(original text)_ {text}')\n        result.append('')\n    \n    return result\n\n\ndef get_langs_from_numbers(numbers):\n    \n    l1 = [all_langs[k-1] for k in numbers]\n    \n    return l1, [lang_dic[k] for k in l1]\n\n\n\n\n\n\nif __name__ == '__main__':\n\n    #trans = Translator()\n    #print(trans.detect('\u041f\u0440\u0438\u0432\u0435\u0442'))\n    #print(trans.detect('Hello').lang)\n    #print(trans.translate('\u041f\u0440\u0438\u0432\u0435\u0442'))\n\n\n\n\n   \n    defs = ['en','ru']\n    \n    r = log_text('hello my friend',defs)\n    \n    print('\\n'.join(r))\n\n    r = log_text('Ich will',defs)\n    \n    print('\\n'.join(r))\n    \n    print(defs)\n    \n    r = log_text_better('hello my friend',defs)\n    \n    print('\\n'.join(r))\n\n    lang = Translator().detect('Hello boy')\n    print(lang)\n\n#https://github.com/PasaOpasen/TranslatorBot/blob/master/translator_tools</code></pre></div></body></html>", "sec_22": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_22\"><pre id=\"sec_22_code\"><code class=\"python\"># -*- coding: utf-8 -*-\n\"\"\"\nTranslator module that uses the Google Translate API.\n\nAdapted from Terry Yin's google-translate-python.\nLanguage detection added by Steven Loria.\n\"\"\"\n# I(Dhyey thumar) have done some modifications in this file.\n\n# import codecs\n# import re\nimport ctypes\nimport json\nimport sys\n\nfrom textblob.compat import PY2, request, urlencode\n# from textblob.exceptions import TranslatorError, NotTranslated\n\nsource_lang_code = str(sys.argv[1])\ninput_string = str(sys.argv[2])\ndest_lang_code = str(sys.argv[3])\n\n\nclass Translator(object):\n\n    \"\"\"A language translator and detector.\n\n    Usage:\n    ::\n        &gt;&gt;&gt; from textblob.translate import Translator\n        &gt;&gt;&gt; t = Translator()\n        &gt;&gt;&gt; t.translate('hello', from_lang='en', to_lang='fr')\n        u'bonjour'\n        &gt;&gt;&gt; t.detect(\"hola\")\n        u'es'\n    \"\"\"\n\n    url = \"http://translate.google.com/translate_a/t?client=webapp&amp;dt=bd&amp;dt=ex&amp;dt=ld&amp;dt=md&amp;dt=qca&amp;dt=rw&amp;dt=rm&amp;dt=ss&amp;dt=t&amp;dt=at&amp;ie=UTF-8&amp;oe=UTF-8&amp;otf=2&amp;ssel=0&amp;tsel=0&amp;kc=1\"\n\n    headers = {\n        'Accept': '*/*',\n        'Connection': 'keep-alive',\n        'User-Agent': (\n            'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36')\n    }\n\n    def translate(self, source, from_lang='auto', to_lang='en', host=None, type_=None):\n        \"\"\"Translate the source text from one language to another.\"\"\"\n        if PY2:\n            source = source.encode('utf-8')\n        data = {\"q\": source}\n        url = u'{url}&amp;sl={from_lang}&amp;tl={to_lang}&amp;hl={to_lang}&amp;tk={tk}'.format(\n            url=self.url,\n            from_lang=from_lang,\n            to_lang=to_lang,\n            tk=_calculate_tk(source),\n        )\n        response = self._request(url, host=host, type_=type_, data=data)\n        result = json.loads(response)\n        if isinstance(result, list):\n            try:\n                result = result[0]  # ignore detected language\n            except IndexError:\n                pass\n        # self._validate_translation(source, result)\n        return result\n\n    def detect(self, source, host=None, type_=None):\n        \"\"\"Detect the source text's language.\"\"\"\n        if PY2:\n            source = source.encode('utf-8')\n        # if len(source) &lt; 3:\n        #     return 1\n            # raise TranslatorError('Must provide a string with at least 3 characters.')\n        data = {\"q\": source}\n        url = u'{url}&amp;sl=auto&amp;tk={tk}'.format(\n            url=self.url, tk=_calculate_tk(source))\n        response = self._request(url, host=host, type_=type_, data=data)\n        result, language = json.loads(response)\n        return language\n\n    # def _validate_translation(self, source, result):\n    #     \"\"\"Validate API returned expected schema, and that the translated text\n    #     is different than the original string.\n    #     \"\"\"\n    #     if not result:\n    #         raise NotTranslated('Translation API returned and empty response.')\n    #     if PY2:\n    #         result = result.encode('utf-8')\n    #     if result.strip() == source.strip():\n    #         raise NotTranslated('Translation API returned the input string unchanged.')\n\n    def _request(self, url, host=None, type_=None, data=None):\n        encoded_data = urlencode(data).encode('utf-8')\n        req = request.Request(url=url, headers=self.headers, data=encoded_data)\n        if host or type_:\n            req.set_proxy(host=host, type=type_)\n        resp = request.urlopen(req)\n        content = resp.read()\n        return content.decode('utf-8')\n\n\n# def _unescape(text):\n#     \"\"\"Unescape unicode character codes within a string.\n#     \"\"\"\n#     pattern = r'\\\\{1,2}u[0-9a-fA-F]{4}'\n#     decode = lambda x: codecs.getdecoder('unicode_escape')(x.group())[0]\n#     return re.sub(pattern, decode, text)\n\n\ndef _calculate_tk(source):\n    \"\"\"Reverse engineered cross-site request protection.\"\"\"\n    # Source: https://github.com/soimort/translate-shell/issues/94#issuecomment-165433715\n    # Source: http://www.liuxiatool.com/t.php\n\n    tkk = [406398, 561666268 + 1526272306]\n    b = tkk[0]\n\n    if PY2:\n        d = map(ord, source)\n    else:\n        d = source.encode('utf-8')\n\n    def RL(a, b):\n        for c in range(0, len(b) - 2, 3):\n            d = b[c + 2]\n            d = ord(d) - 87 if d &gt;= 'a' else int(d)\n            xa = ctypes.c_uint32(a).value\n            d = xa &gt;&gt; d if b[c + 1] == '+' else xa &lt;&lt; d\n            a = a + d &amp; 4294967295 if b[c] == '+' else a ^ d\n        return ctypes.c_int32(a).value\n\n    a = b\n\n    for di in d:\n        a = RL(a + di, \"+-a^+6\")\n\n    a = RL(a, \"+-3^+b+-f\")\n    a ^= tkk[1]\n    a = a if a &gt;= 0 else ((a &amp; 2147483647) + 2147483648)\n    a %= pow(10, 6)\n\n    tk = '{0:d}.{1:d}'.format(a, a ^ b)\n    return tk\n\n\n<div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">translator_instance = Translator()</div>\nif source_lang_code == \"null\":\n    source_lang_code = translator_instance.detect(input_string)\n\ntranslated_string = translator_instance.translate(input_string, source_lang_code, dest_lang_code)\n\nif translated_string:\n    translated_string = str(translated_string)\n    result = []\n    for char in translated_string:\n        result.append(str(ord(char)))\n    seperator = ','\n    trans_string = seperator.join(result)\n    print(trans_string, end='\\n')\nelse:\n    print('empty response')\n    #https://github.com/dhyeythumar/Search-Engine/blob/master/Python_scripts/lang_trans1</code></pre></div></body></html>", "sec_23": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_23\"><pre id=\"sec_23_code\"><code class=\"python\">''' This is language translation script '''\nfrom textblob import TextBlob\nimport sys\n\n\nsource_lang_code = str(sys.argv[1])\ninput_string = str(sys.argv[2])\ndest_lang_code = str(sys.argv[3])\n\ninput_blob = TextBlob(input_string)\n\nif source_lang_code == \"null\":\n    try:\n        <div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">source_lang_code = input_<span class=\"fea_language_detection_funcs blodFunc\">blob</span>.<span class=\"fea_language_detection_funcs blodFunc\">detect</span>_<span class=\"fea_language_detection_funcs blodFunc\">language</span>()</div>\n        # print(\"Detected language:  \", source_lang_code)\n    except Exception as e:  # What if the input_string language is not detected\n        print(\"Error_1\", e)\n\ntry:\n    translated_string = <div class=\"highlights fea_translation\" id=\"translation_0\" style=\"display: inline;\">input_blob.<span class=\"fea_translation_funcs blodFunc\">translate</span>(\n        from_lang=source_lang_code, to=dest_lang_code)</div>\n    # print(translated_string) give character in unicode format.\n    # translated_string =&gt; is a &lt;class 'textblob.blob.TextBlob'&gt; type of object\n    # str(translated_string) =&gt; is a &lt;class 'str'&gt; type of object\n    # str(translated_string).encode('utf8) =&gt; is a &lt;class 'bytes'&gt; type of object\n\n    translated_string = str(translated_string)\n    result = []\n    for char in translated_string:\n        result.append(str(ord(char)))\n    seperator = ', '\n    trans_string = seperator.join(result)\n    print(trans_string, end='\\n')\n\nexcept Exception as e:  # What if the dest_lang code is null\n    print(\"Error_2\", e)\n    #https://github.com/dhyeythumar/Search-Engine/blob/master/Python_scripts/lang_trans</code></pre></div></body></html>", "sec_11": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_11\"><pre id=\"sec_11_code\"><code class=\"python\">from textblob import TextBlob\nfrom time import sleep\nimport csv\nimport pandas as pd\n\ndf = pd.read_csv(\"cleaned_data01.csv\")\ntexts = df['cleaned_text']\n\ncounter = 188277\nremains = texts.shape[0] - 188277\nreq_counter = 0\n\n#========================= textblob ==========================\n\nwith open('tweet_lang02.csv', 'a', encoding=\"utf-8-sig\") as csvFile:\n    csvWriter = csv.writer(csvFile)\n    csvWriter.writerow(['text','language'])\n    for i in range(188277, texts.shape[0]):\n        counter +=1\n        remains -=1\n        req_counter +=1\n        print(counter, ' ', remains)\n        t = texts[i]\n        s = t.replace(\"#\",\"\")\n        s = s.replace(\"_\", \" \")\n        if req_counter == 10:\n            sleep(3)\n            req_counter = 0\n\n        b = TextBlob(s)\n        <div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">l = b.<span class=\"fea_language_detection_funcs blodFunc\">detect</span>_<span class=\"fea_language_detection_funcs blodFunc\">language</span>()</div>\n        csvWriter.writerow([t,l])\n        #https://github.com/khaledabbud/SA_of_Tweets_After_QS_Assassination_AR_FA/blob/master/textblob_lang_classification</code></pre></div></body></html>", "sec_18": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_18\"><pre id=\"sec_18_code\"><code class=\"python\">from textblob import TextBlob\ndef log_text(text, lang_of_text=None, lang_list = ['en','ru'], trans_list = [True, True]):\n    \n    if len(text) &lt; 3:\n        print_on_yellow('too small text:',end=' ')\n        print(text)\n        return\n    \n    blob = TextBlob(text)\n    if lang_of_text == None:\n        <div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\">lang_of_text = <span class=\"fea_language_detection_funcs blodFunc\">blob</span>.<span class=\"fea_language_detection_funcs blodFunc\">detect</span>_<span class=\"fea_language_detection_funcs blodFunc\">language</span>()</div>\n\n    bool_list = [r != lang_of_text for r in lang_list]\n    \n    for lang, it, tc in zip(lang_list, bool_list, trans_list):\n        print(colored(f'\\t {lang}:', color = 'cyan', attrs=['bold']), end=' ')\n        if it:\n           <div class=\"highlights fea_language_detection\" id=\"language_detection_1\" style=\"display: inline;\"> txt = str(<span class=\"fea_language_detection_funcs blodFunc\">blob</span>.translate(from_lang = lang_of_text, to = lang))</div>\n            print(txt)\n        else:\n            txt = text\n            print(f'{text} (original text)')\n        \n        if tc:\n            pron = epis[lang].transliterate(txt)\n            print('\\t\\t\\t',end=' ')\n            print_on_magenta(f'[{pron}]')\n#https://github.com/PasaOpasen/SpeechLogger/blob/master/ThirdTry/text_logger5</code></pre></div></body></html>", "sec_4": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_4\"><pre id=\"sec_4_code\"><code class=\"python\">\nfrom textblob.classifiers import NaiveBayesClassifier\n\ntrain = [\n    ('amor', \"spanish\"),\n    (\"perro\", \"spanish\"),\n    (\"playa\", \"spanish\"),\n    (\"sal\", \"spanish\"),\n    (\"oceano\", \"spanish\"),\n    (\"love\", \"english\"),\n    (\"dog\", \"english\"),\n    (\"beach\", \"english\"),\n    (\"salt\", \"english\"),\n    (\"ocean\", \"english\")\n]\ntest = [\n    (\"ropa\", \"spanish\"),\n    (\"comprar\", \"spanish\"),\n    (\"camisa\", \"spanish\"),\n    (\"agua\", \"spanish\"),\n    (\"telefono\", \"spanish\"),\n    (\"clothes\", \"english\"),\n    (\"buy\", \"english\"),\n    (\"shirt\", \"english\"),\n    (\"water\", \"english\"),\n    (\"telephone\", \"english\")\n]\n\ndef extractor(word):\n    '''Extract the last letter of a word as the only feature.'''\n    feats = {}\n    last_letter = word[-1]\n    feats[\"last_letter({0})\".format(last_letter)] = True\n    return feats\n\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">lang_detector = <span class=\"fea_classification_funcs blodFunc\">Naive</span><span class=\"fea_classification_funcs blodFunc\">Bayes</span><span class=\"fea_classification_funcs blodFunc\">Classifier</span>(<span class=\"fea_classification_funcs blodFunc\">train</span>, feature_extractor=extractor)</div>\nprint(lang_detector.accuracy(test))\nprint(lang_detector.show_informative_features(5))\n#https://gist.github.com/sloria/6342158</code></pre></div></body></html>", "sec_5": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_5\"><pre id=\"sec_5_code\"><code class=\"python\">from textblob.classifiers import NaiveBayesClassifier\nfrom textblob import TextBlob\ntrain = [\n    ('I love this sandwich.', 'pos'),\n    ('This is an amazing place!', 'pos'),\n    ('I feel very good about these beers.', 'pos'),\n    ('This is my best work.', 'pos'),\n    (\"What an awesome view\", 'pos'),\n    ('I do not like this restaurant', 'neg'),\n    ('I am tired of this stuff.', 'neg'),\n    (\"I can't deal with this\", 'neg'),\n    ('He is my sworn enemy!', 'neg'),\n    ('My boss is horrible.', 'neg')\n]\ntest = [\n    ('The beer was good.', 'pos'),\n    ('I do not enjoy my job', 'neg'),\n    (\"I ain't feeling dandy today.\", 'neg'),\n    (\"I feel amazing!\", 'pos'),\n    ('Gary is a friend of mine.', 'pos'),\n    (\"I can't believe I'm doing this.\", 'neg')\n]\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\">cl = <span class=\"fea_classification_funcs blodFunc\">Naive</span><span class=\"fea_classification_funcs blodFunc\">Bayes</span><span class=\"fea_classification_funcs blodFunc\">Classifier</span>(<span class=\"fea_classification_funcs blodFunc\">train</span>)</div>\n# Classify some text\nprint(cl.classify(\"Their burgers are amazing.\"))  # \"pos\"\nprint(cl.classify(\"I don't like their pizza.\"))   # \"neg\"\n# Classify a TextBlob\nblob = TextBlob(\"The beer was amazing. But the hangover was horrible. \"\n\"My boss was not pleased.\", classifier=cl)\nprint(blob)\nprint(blob.classify())\nfor sentence in blob.sentences:\nprint(sentence)\nprint(sentence.classify())\n# Compute accuracy\nprint(\"Accuracy: {0}\".format(cl.accuracy(test)))\n# Show 5 most informative features\ncl.show_informative_features(5)\n#https://gist.github.com/sloria/6338202#file-tweet_classify-py</code></pre></div></body></html>", "sec_13": "<html><body><div class=\"codeBlock hljs python\" id=\"sec_13\"><pre id=\"sec_13_code\"><code class=\"python\">from textblob import TextBlob\nfrom textblob.sentiments import NaiveBayesAnalyzer\nimport matplotlibplot as thr\nimport random\n\ndef twitter_analysis(string):\n\t## Aain function starts\n\t## =====\n\n\tprocessedTweet = []\n\tpos = 0\n\tneg = 0\n\tneutral = 0\n\n\t#start process_tweet\n\tdef processTweet(tweet):\n\t    # process the tweets\n\t    \n\t    #Convert to lower case\n\t    tweet = tweet.lower()\n\t    #Convert www.* or https?://* to URL\n\t    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n\t    #Convert @username to AT_USER\n\t    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n\t    #Remove additional white spaces\n\t    tweet = re.sub('[\\s]+', ' ', tweet)\n\t    #Replace #word with word\n\t    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n\t    #trim\n\t    tweet = tweet.strip('\\'\"')\n\t    return tweet\n\t#end\n\n\ttd = TwitterData()\n\trawtweet = td.getData(string)\n\n\t#print \"1. Tweets colleted and pre-processing steps started\"\n\n\t#pre-processing tweets    \n\tfor i in range(1,len(rawtweet)):\n\t    processedTweet.append(processTweet(rawtweet[i]))\n\n\t#print \"2. preprocessing over and classifer begins\"\n\n\t# classifying the processed tweets by NaiveBayesAnalyzer\n\n\tfor i in range(1,len(processedTweet)):\n\t   <div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"> <span class=\"fea_classification_funcs blodFunc\">classifier</span> = TextBlob(processedTweet[i], analyzer=<span class=\"fea_classification_funcs blodFunc\">Naive</span><span class=\"fea_classification_funcs blodFunc\">Bayes</span>Analyzer())\n\t    classification = <span class=\"fea_classification_funcs blodFunc\">classifier</span>.sentiment.classification</div>\n\t    #print processedTweet[i],\"Polarity=\",classification\n\t    \n\t    if classification == \"pos\":\n\t        pos = pos + 1\n\t        #print pos;\n\t    elif classification == \"neg\":     \n\t        neg = neg + 1\n\t        #print neg\n\t    else:\n\t        neutral = neutral + 1 \n\n\tfinal = []\n\tfinal.append(neg);\n\tfinal.append(neutral);\n\tfinal.append(pos);\n\n\treturn final  \n\t#https://github.com/muthuvenki/Trend-Analysis/blob/master/sentimental_anlysis/views</code></pre></div></body></html>", "thr_17": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_17\"><pre id=\"thr_17_code\"><code class=\"python\">import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\n\ndf_amazon = pd.read_csv (\"datasets/amazon_alexa.tsv\", sep=\"\\t\")\n\nimport string\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\n# Create our list of punctuation marks\npunctuations = string.punctuation\n\n# Create our list of stopwords\nnlp = spacy.load('en')\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">stop_words = spacy.lang.en.stop_words.STOP_WORDS</div>\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nparser = English()\n\n# Creating our tokenizer function\ndef spacy_tokenizer(sentence):\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    <div class=\"highlights fea_parsing\" id=\"parsing_0\" style=\"display: inline;\"><span class=\"fea_parsing_funcs blodFunc\">mytokens</span> = <span class=\"fea_parsing_funcs blodFunc\">parser</span>(<span class=\"fea_parsing_funcs blodFunc\">sentence</span>)</div>\n\n    # Lemmatizing each token and converting each token into lowercase\n    mytokens = [ <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\">word.<span class=\"fea_lemmatization_funcs blodFunc\">lemma</span>_.lower()</div>.strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n\n    # Removing stop words\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n\n    # return preprocessed list of tokens\n    return mytokens\n\n# Custom transformer using spaCy\nclass predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        # Cleaning Text\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\n# Basic function to clean the text\ndef clean_text(text):\n    # Removing spaces and converting text into lowercase\n    return text.strip().lower()\n\n<div class=\"highlights fea_word_vectors\" id=\"word_vectors_0\" style=\"display: inline;\">bow_vector = CountVectorizer(tokenizer = <span class=\"fea_word_vectors_funcs blodFunc\">spacy</span>_tokenizer, ngram_range=(1,1))</div>\n\ntfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)\n\nfrom sklearn.model_selection import train_test_split\n\nX = df_amazon['verified_reviews'] # the features we want to analyze\nylabels = df_amazon['feedback'] # the labels, or answers, we want to test against\n\nX_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)\n\n# Logistic Regression Classifier\nfrom sklearn.linear_model import LogisticRegression\n<div class=\"highlights fea_classification\" id=\"classification_0\" style=\"display: inline;\"><span class=\"fea_classification_funcs blodFunc\">classifier</span> = <span class=\"fea_classification_funcs blodFunc\">Logistic</span><span class=\"fea_classification_funcs blodFunc\">Regression</span>()</div>\n\n# Create pipeline using Bag of Words\npipe = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', bow_vector),\n                 ('classifier', classifier)])\n\n# model generation\npipe.fit(X_train,y_train)\n\nfrom sklearn import metrics\n# Predicting with a test dataset\npredicted = pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\nprint(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\nprint(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))</code></pre></div></body></html>", "thr_10": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"thr_10\"><pre id=\"thr_10_code\"><code class=\"python\"># Check if word vector is available\nimport spacy\n\n# Loading a spacy model\nnlp = spacy.load(\"en_core_web_md\")\ntokens = nlp(\"I am an excellent cook\")\n\nfor token in tokens:\n  print(token.text ,' ',token.has_vector)\n  print(token.text,' ',<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">token</span>.vector_norm</div>)\n\nreview_1=nlp(' The food was amazing')\nreview_2=nlp('The food was excellent')\nreview_3=nlp('I did not like the food')\nreview_4=nlp('It was very bad experience')\n\n<div class=\"highlights fea_text_similarity\" id=\"text_similarity_0\" style=\"display: inline;\"><span class=\"fea_text_similarity_funcs blodFunc\">score</span>_1=<span class=\"fea_text_similarity_funcs blodFunc\">review</span>_1.<span class=\"fea_text_similarity_funcs blodFunc\">similarity</span>(<span class=\"fea_text_similarity_funcs blodFunc\">review</span>_2)</div>\nprint('Similarity between review 1 and 2',score_1)\n\n<div class=\"highlights fea_text_similarity\" id=\"text_similarity_1\" style=\"display: inline;\"><span class=\"fea_text_similarity_funcs blodFunc\">score</span>_2=<span class=\"fea_text_similarity_funcs blodFunc\">review</span>_3.<span class=\"fea_text_similarity_funcs blodFunc\">similarity</span>(<span class=\"fea_text_similarity_funcs blodFunc\">review</span>_4)</div>\nprint('Similarity between review 3 and 4',score_2)\n\n#https://www.machinelearningplus.com/spacy-tutorial-nlp/#mergingandsplittingtokenswithretokenize</code></pre></div></body></html>", "thr_12": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_12\"><pre id=\"thr_12_code\"><code class=\"python\">\"\"\"\nThis script extracts features from the transcript txt file and saves them to .csv files\nso they can be used in any toolkkit.\n\"\"\"\n\nimport csv\nimport spacy\n\n\ndef main():\n    \"\"\"Loads the model and processes it.\n    \n    The model used can be installed by running this command on your CMD/Terminal:\n\n    python -m spacy download es_core_news_md\n    \n    \"\"\"\n\n    corpus = open(\"transcript_clean.txt\", \"r\", encoding=\"utf-8\").read()\n    nlp = spacy.load(\"es_core_news_md\")\n\n    # Our corpus is bigger than the default limit, we will set\n    # a new limit equal to its length.\n    nlp.max_length = len(corpus)\n\n    doc = nlp(corpus)\n\n    get_tokens(doc)\n    get_entities(doc)\n    get_sentences(doc)\n\n\ndef <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">get_<span class=\"fea_tokenization_funcs blodFunc\">token</span>s(doc)</div>:\n    \"\"\"Get the tokens and save them to .csv\n\n    Parameters\n    ----------\n    doc : spacy.doc\n        A doc object.\n\n    \"\"\"\n\n    data_list = [[\"text\", \"text_lower\", \"lemma\", \"lemma_lower\",\n                  \"part_of_speech\", \"is_alphabet\", \"is_stopword\"]]\n\n    for token in doc:\n        data_list.append([\n            <div class=\"highlights fea_tokenization\" id=\"tokenization_1\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">token</span>.text, <span class=\"fea_tokenization_funcs blodFunc\">token</span>.lower_</div>, <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">token</span>.<span class=\"fea_lemmatization_funcs blodFunc\">lemma</span>_, <span class=\"fea_lemmatization_funcs blodFunc\">token</span>.<span class=\"fea_lemmatization_funcs blodFunc\">lemma</span>_.lower()</div>,\n            <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_funcs blodFunc\">token</span>.<span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span>_</div>, token.is_alpha, token.is_stop\n        ])\n\n    with open(\"./tokens.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as tokens_file:\n        csv.writer(tokens_file).writerows(data_list)\n\n\ndef get_entities(doc):\n    \"\"\"Get the entities and save them to .csv\n\n    Parameters\n    ----------\n    doc : spacy.doc\n        A doc object.\n\n    \"\"\"\n\n    data_list = [[\"text\", \"text_lower\", \"label\"]]\n\n    for ent in doc.ents:\n        data_list.append([ent.text, ent.lower_, ent.label_])\n\n    with open(\"./entities.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as entities_file:\n        csv.writer(entities_file).writerows(data_list)\n\n\ndef get_sentences(doc):\n    \"\"\"Get the sentences, score and save them to .csv\n\n    You will require to download the dataset (zip) from the following url:\n\n    https://www.kaggle.com/rtatman/sentiment-lexicons-for-81-languages\n\n    Once downloaded you will require to extract 2 .txt files:\n\n    negative_words_es.txt\n    positive_words_es.txt\n\n    Parameters\n    ----------\n    doc : spacy.doc\n        A doc object.\n\n    \"\"\"\n\n    # Load positive and negative words into lists.\n    with open(\"positive_words_es.txt\", \"r\", encoding=\"utf-8\") as temp_file:\n        positive_words = temp_file.read().splitlines()\n\n    with open(\"negative_words_es.txt\", \"r\", encoding=\"utf-8\") as temp_file:\n        negative_words = temp_file.read().splitlines()\n\n    data_list = [[\"text\", \"score\"]]\n\n    for sent in doc.sents:\n\n        # Only take into account real sentences.\n        if len(sent.text) &gt; 10:\n\n            score = 0\n\n            # Start scoring the sentence.\n            for word in sent:\n\n                if word.lower_ in positive_words:\n                    score += 1\n\n                if word.lower_ in negative_words:\n                    score -= 1\n\n            data_list.append([sent.text, score])\n\n\n    with open(\"./sentences.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as sentences_file:\n        csv.writer(sentences_file).writerows(data_list)\n\n\nif __name__ == \"__main__\":\n\n    main()\n    https://github.com/PhantomInsights/mexican-government-report/blob/master/scripts/step2</code></pre></div></body></html>", "thr_18": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_18\"><pre id=\"thr_18_code\"><code class=\"python\">survey_text = ('Out of 5 people surveyed, James Robert,'\n               ' Julie Fuller and Benjamin Brooks like'\n               ' apples. Kelly Cox and Matthew Evans'\n               ' like oranges.')\n\ndef replace_person_names(token):\n    <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\">if <span class=\"fea_tokenization_funcs blodFunc\">token</span>.ent_iob != 0 and <span class=\"fea_tokenization_funcs blodFunc\">token</span>.ent_type_ == 'PERSON':</div>\n        return '[REDACTED] '\n    return token.string\n\ndef <div class=\"highlights fea_named_entity_recognition\" id=\"named_entity_recognition_0\" style=\"display: inline;\"><span class=\"fea_named_entity_recognition_funcs blodFunc\">redact</span>_<span class=\"fea_named_entity_recognition_funcs blodFunc\">names</span>(<span class=\"fea_named_entity_recognition_funcs blodFunc\">nlp</span>_<span class=\"fea_named_entity_recognition_funcs blodFunc\">doc</span>):</div>\n    for ent in nlp_doc.ents:\n        ent.merge()\n    tokens = map(replace_person_names, nlp_doc)\n    return ''.join(tokens)\n\nsurvey_doc = nlp(survey_text)\nredact_names(survey_doc)</code></pre></div></body></html>", "thr_20": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_20\"><pre id=\"thr_20_code\"><code class=\"python\">one_about_text = ('Gus Proto is a Python developer'\n    ' currently working for a London-based Fintech company')\none_about_doc = nlp(one_about_text)\n# Extract children of `developer`\nprint([token.text for token in one_about_doc[5].children])\n\n# Extract previous neighboring node of `developer`\nprint (one_about_doc[5].nbor(-1))\n\n# Extract next neighboring node of `developer`\nprint (one_about_doc[5].nbor())\n\n# Extract all tokens on the left of `developer`\nprint([token.text for token in one_about_doc[5].lefts])\n\n# Extract tokens on the right of `developer`\nprint([<div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">token</span>.text for <span class=\"fea_tokenization_funcs blodFunc\">token</span> in one_about_doc[5].rights</div>])\n\n# Print subtree of `developer`\nprint (list(one_about_doc[5].subtree))\n\ndef flatten_tree(tree):\n    return ''.join([token.text_with_ws for token in list(tree)]).strip()\n\n# Print flattened subtree of `developer`\nprint (flatten_tree(one_about_doc[5].subtree))</code></pre></div></body></html>", "thr_27": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_27\"><pre id=\"thr_27_code\"><code class=\"python\">import re\nimport spacy\nfrom spacy.tokenizer import Tokenizer\ncustom_nlp = spacy.load('en_core_web_sm')\n<div class=\"highlights fea_text_simplify\" id=\"text_simplify_0\" style=\"display: inline;\"><span class=\"fea_text_simplify_funcs blodFunc\">prefix</span>_re = <span class=\"fea_text_simplify_funcs blodFunc\">spacy</span>.<span class=\"fea_text_simplify_funcs blodFunc\">util</span>.<span class=\"fea_text_simplify_funcs blodFunc\">compile</span>_<span class=\"fea_text_simplify_funcs blodFunc\">prefix</span>_<span class=\"fea_text_simplify_funcs blodFunc\">regex</span>(<span class=\"fea_text_simplify_funcs blodFunc\">custom</span>_<span class=\"fea_text_simplify_funcs blodFunc\">nlp</span>.<span class=\"fea_text_simplify_funcs blodFunc\">Defaults</span>.<span class=\"fea_text_simplify_funcs blodFunc\">prefixes</span>)\n<span class=\"fea_text_simplify_funcs blodFunc\">suffix</span>_re = <span class=\"fea_text_simplify_funcs blodFunc\">spacy</span>.<span class=\"fea_text_simplify_funcs blodFunc\">util</span>.<span class=\"fea_text_simplify_funcs blodFunc\">compile</span>_<span class=\"fea_text_simplify_funcs blodFunc\">suffix</span>_<span class=\"fea_text_simplify_funcs blodFunc\">regex</span>(<span class=\"fea_text_simplify_funcs blodFunc\">custom</span>_<span class=\"fea_text_simplify_funcs blodFunc\">nlp</span>.<span class=\"fea_text_simplify_funcs blodFunc\">Defaults</span>.<span class=\"fea_text_simplify_funcs blodFunc\">suffixes</span>)</div>\n<div class=\"highlights fea_regular_expression\" id=\"regular_expression_0\" style=\"display: inline;\"><span class=\"fea_regular_expression_funcs blodFunc\">infix</span>_re = re.<span class=\"fea_regular_expression_funcs blodFunc\">compile</span>(r'''[-~]''')</div>\ndef customize_tokenizer(nlp):\n    # Adds support to use `-` as the delimiter for tokenization\n    return <div class=\"highlights fea_tokenization\" id=\"tokenization_0\" style=\"display: inline;\"><span class=\"fea_tokenization_funcs blodFunc\">Token</span>izer(nlp.vocab, prefix_search=prefix_re.search,\n                     suffix_search=suffix_re.search,\n                     infix_finditer=infix_re.finditer,\n                     <span class=\"fea_tokenization_funcs blodFunc\">token</span>_match=None\n                     )</div>\n\n\ncustom_nlp.tokenizer = customize_tokenizer(custom_nlp)\ncustom_tokenizer_about_doc = custom_nlp(about_text)\nprint([token.text for token in custom_tokenizer_about_doc])\n</code></pre></div></body></html>", "thr_22": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_22\"><pre id=\"thr_22_code\"><code class=\"python\">def is_token_allowed(token):\n    '''\n        Only allow valid tokens which are not stop words\n        and punctuation symbols.\n    '''\n    if (not token or not token.string.strip() or\n        token.is_stop or token.is_punct):\n        return False\n    return True\n\ndef preprocess_token(token):\n    # Reduce token to its lowercase lemma form\n    return <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">token</span>.<span class=\"fea_lemmatization_funcs blodFunc\">lemma</span>_</div>.strip().lower()\n\ncomplete_filtered_tokens = [preprocess_token(token)\n    for token in complete_doc if is_token_allowed(token)]\ncomplete_filtered_tokens</code></pre></div></body></html>", "thr_26": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"thr_26\"><pre id=\"thr_26_code\"><code class=\"python\">import spacy\nnlp = spacy.load('en_core_web_sm')\n\nconference_help_text = ('Gus is helping organize a developer'\n    'conference on Applications of Natural Language'\n    ' Processing. He keeps organizing local Python meetups'\n    ' and several internal talks at his workplace.')\nconference_help_doc = nlp(conference_help_text)\nfor token in conference_help_doc:\n    print (token, <div class=\"highlights fea_lemmatization\" id=\"lemmatization_0\" style=\"display: inline;\"><span class=\"fea_lemmatization_funcs blodFunc\">token</span>.<span class=\"fea_lemmatization_funcs blodFunc\">lemma</span>_</div>)</code></pre></div></body></html>", "thr_13": "<html><body><div class=\"codeBlock hljs makefile\" id=\"thr_13\"><pre id=\"thr_13_code\"><code class=\"python\"># Construction via add_pipe with default model\ntok2vec = nlp.add_pipe(\"tok2vec\")\n\n# Construction via add_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_tok2vec\"}}\nparser = nlp.add_pipe(\"tok2vec\", config=config)\n\n# Construction from class\n<div class=\"highlights fea_word_vectors\" id=\"word_vectors_0\" style=\"display: inline;\">from <span class=\"fea_word_vectors_funcs blodFunc\">spacy</span>.pipeline import Tok2Vec\ntok2vec = Tok2Vec(nlp.vocab, model)</div>\n#https://spacy.io/api/tok2vec</code></pre></div></body></html>", "thr_5": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"thr_5\"><pre id=\"thr_5_code\"><code class=\"python\">from spacy.training import JsonlCorpus\nimport spacy\n\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">corpus = JsonlCorpus(\"./texts.jsonl\")</div>\nnlp = spacy.blank(\"en\")\ndata = corpus(nlp)\n\n#https://spacy.io/api/corpus</code></pre></div></body></html>", "thr_6": "<html><body><div class=\"codeBlock hljs typescript\" id=\"thr_6\"><pre id=\"thr_6_code\"><code class=\"python\">import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom string import punctuation\nfrom collections import Counter\nfrom heapq import nlargest\n\ndoc =\"\"\"Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to progressively improve their performance on a specific task. Machine learning algorithms build a mathematical model of sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in the applications of email filtering, detection of network intruders, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning.In its application across business problems, machine learning is also referred to as predictive analytics.\"\"\"\n\nnlp = spacy.load('en')\ndoc = nlp(doc)\n\nkeyword = []\n<div class=\"highlights fea_nlp_datasets\" id=\"nlp_datasets_0\" style=\"display: inline;\">stopwords = list(STOP_WORDS)</div>\npos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\nfor token in doc:\n    if(token.text in stopwords or token.text in punctuation):\n        continue\n    if(token.pos_ in pos_tag):\n        keyword.append(token.text)\n\nfreq_word = Counter(keyword)\n\nmax_freq = Counter(keyword).most_common(1)[0][1]\nfor word in freq_word.keys():  \n        freq_word[word] = (freq_word[word]/max_freq)\n\nsent_strength={}\nfor sent in doc.sents:\n    for word in sent:\n        if word.text in freq_word.keys():\n            if sent in sent_strength.keys():\n                sent_strength[sent]+=freq_word[word.text]\n            else:\n                sent_strength[sent]=freq_word[word.text]\nprint(sent_strength)\n\n<div class=\"highlights fea_summarizer\" id=\"summarizer_0\" style=\"display: inline;\"><span class=\"fea_summarizer_funcs blodFunc\">summarized</span>_<span class=\"fea_summarizer_funcs blodFunc\">sentences</span> = <span class=\"fea_summarizer_funcs blodFunc\">nlargest</span>(3, <span class=\"fea_summarizer_funcs blodFunc\">sent</span>_<span class=\"fea_summarizer_funcs blodFunc\">strength</span>, <span class=\"fea_summarizer_funcs blodFunc\">key</span>=<span class=\"fea_summarizer_funcs blodFunc\">sent</span>_<span class=\"fea_summarizer_funcs blodFunc\">strength</span>.<span class=\"fea_summarizer_funcs blodFunc\">get</span>)</div>\n\nfinal_sentences = [ w.text for w in summarized_sentences ]\nsummary = ' '.join(final_sentences)</code></pre></div></body></html>", "thr_25": "<html><body><div class=\"codeBlock hljs vbnet\" id=\"thr_25\"><pre id=\"thr_25_code\"><code class=\"python\">from collections import Counter\ncomplete_text = ('Gus Proto is a Python developer currently'\n    'working for a London-based Fintech company. He is'\n    ' interested in learning Natural Language Processing.'\n    ' There is a developer conference happening on 21 July'\n    ' 2019 in London. It is titled \"Applications of Natural'\n    ' Language Processing\". There is a helpline number '\n    ' available at +1-1234567891. Gus is helping organize it.'\n    ' He keeps organizing local Python meetups and several'\n    ' internal talks at his workplace. Gus is also presenting'\n    ' a talk. The talk will introduce the reader about \"Use'\n    ' cases of Natural Language Processing in Fintech\".'\n    ' Apart from his work, he is very passionate about music.'\n    ' Gus is learning to play the Piano. He has enrolled '\n    ' himself in the weekend batch of Great Piano Academy.'\n    ' Great Piano Academy is situated in Mayfair or the City'\n    ' of London and has world-class piano instructors.')\n\ncomplete_doc = nlp(complete_text)\n# Remove stop words and punctuation symbols\nwords = [token.text for token in complete_doc\n         if not token.is_stop and not token.is_punct]\n<div class=\"highlights fea_word_frequency\" id=\"word_frequency_0\" style=\"display: inline;\"><span class=\"fea_word_frequency_funcs blodFunc\">word</span>_<span class=\"fea_word_frequency_funcs blodFunc\">freq</span> = <span class=\"fea_word_frequency_funcs blodFunc\">Counter</span>(<span class=\"fea_word_frequency_funcs blodFunc\">words</span>)</div>\n# 5 commonly occurring words with their frequencies\ncommon_words = word_freq.most_common(5)\nprint (common_words)\n\n# Unique words\nunique_words = [word for (word, freq) in word_freq.items() if freq == 1]\nprint (unique_words)</code></pre></div></body></html>", "thr_24": "<html><body><div class=\"codeBlock hljs go\" id=\"thr_24\"><pre id=\"thr_24_code\"><code class=\"python\">nouns = []\nadjectives = []\nfor token in about_doc:\n    if <div class=\"highlights fea_Part_of_Speech\" id=\"Part_of_Speech_0\" style=\"display: inline;\"><span class=\"fea_Part_of_Speech_funcs blodFunc\">token</span>.<span class=\"fea_Part_of_Speech_funcs blodFunc\">pos</span>_</div> == 'NOUN':\n        nouns.append(token)\n    if <div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">token.pos_ == 'ADJ':</div>\n        adjectives.append(token)\n    print (token, <div class=\"highlights fea_tagger\" id=\"tagger_0\" style=\"display: inline;\"><span class=\"fea_tagger_funcs blodFunc\">token</span>.<span class=\"fea_tagger_funcs blodFunc\">tag</span>_</div>, token.pos_, spacy.explain(token.tag_))</code></pre></div></body></html>", "thr_11": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_11\"><pre id=\"thr_11_code\"><code class=\"python\">from skweak.base import SpanAnnotator\nimport spacy\nimport itertools\nimport json\nfrom spacy.tokens import Doc, Span  # type: ignore\nfrom typing import Tuple, Iterable, List\n\n####################################################################\n# Labelling source based on neural models\n####################################################################\n\n\nclass ModelAnnotator(SpanAnnotator):\n    \"\"\"Annotation based on a spacy NER model\"\"\"\n\n    def __init__(self, name:str, model_path:str, \n                 disabled:List[str]=[\"parser\", \"tagger\", \"lemmatizer\", \"attribute_ruler\"]):\n        \"\"\"Creates a new annotator based on a Spacy model. \"\"\"\n        \n        super(ModelAnnotator, self).__init__(name)\n        self.model = spacy.load(model_path, disable=disabled)\n\n\n    def find_spans(self, doc: Doc) -&gt; Iterable[Tuple[int, int, str]]:\n        \"\"\"Annotates one single document using the Spacy NER model\"\"\"\n\n        # Create a new document (to avoid conflicting annotations)\n        doc2 = self.create_new_doc(doc)\n        # And run the model\n        for _, proc in self.model.pipeline:\n            doc2 = proc(doc2)\n        # Add the annotation\n        for ent in doc2.ents:\n            yield ent.start, ent.end, ent.label_\n\n    def pipe(self, docs: Iterable[Doc]) -&gt; Iterable[Doc]:\n        \"\"\"Annotates the stream of documents based on the Spacy model\"\"\"\n\n        stream1, stream2 = itertools.tee(docs, 2)\n\n        # Remove existing entities from the document\n        stream2 = (self.create_new_doc(d) for d in stream2)\n        \n        # And run the model\n        for _, proc in self.model.pipeline:\n            stream2 = proc.pipe(stream2)\n        \n        for doc, doc_copy in zip(stream1, stream2):\n\n            doc.spans[self.name] = []\n\n            # Add the annotation\n            for ent in doc_copy.ents:\n                doc.spans[self.name].append(Span(doc, ent.start, ent.end, ent.label_))\n\n            yield doc\n\n    def create_new_doc(self, doc: Doc) -&gt; Doc:\n        \"\"\"Create a new, empty Doc (but with the same tokenisation as before)\"\"\"\n\n        return spacy.tokens.Doc(self.model.vocab, [tok.text for tok in doc], #type: ignore\n                               [tok.whitespace_ for tok in doc])\n\n\nclass TruecaseAnnotator(ModelAnnotator):\n    \"\"\"Spacy model annotator that preprocess all texts to convert them to a \n    \"truecased\" representation (see below)\"\"\"\n\n    def __init__(self, name:str, model_path:str, form_frequencies:str,\n                 disabled:List[str]=[\"parser\", \"tagger\", \"lemmatizer\", \"attribute_ruler\"]):\n        \"\"\"Creates a new annotator based on a Spacy model, and a dictionary containing\n        the most common case forms for a given word (to be able to truecase the document).\"\"\"\n        \n        super(TruecaseAnnotator, self).__init__(name, model_path, disabled)\n        with open(form_frequencies) as fd:\n            self.form_frequencies = json.load(fd)\n\n    def create_new_doc(self, doc: Doc, min_prob: float = 0.25) -&gt; Doc:\n        \"\"\"Performs truecasing of the tokens in the spacy document. Based on relative \n        frequencies of word forms, tokens that \n        (1) are made of letters, with a first letter in uppercase\n        (2) and are not sentence start\n        (3) and have a relative frequency below min_prob\n        ... will be replaced by its most likely case (such as lowercase). \"\"\"\n\n  #      print(\"running on\", doc[:10])\n\n        if not self.form_frequencies:\n            raise RuntimeError(\n                \"Cannot truecase without a dictionary of form frequencies\")\n\n        tokens = []\n        spaces = []\n        doctext = doc.text\n        for tok in doc:\n            toktext = tok.text\n\n            # We only change casing for words in Title or UPPER\n            if tok.is_alpha and toktext[0].isupper():\n                cond1 = tok.is_upper and len(toktext) &gt; 2  # word in uppercase\n                cond2 = toktext[0].isupper(\n                ) and not tok.is_sent_start  # titled word\n                if cond1 or cond2:\n                    token_lc = toktext.lower()\n                    if token_lc in self.form_frequencies:\n                        frequencies = self.form_frequencies[token_lc]\n                        if frequencies.get(toktext, 0) &lt; min_prob:\n                            alternative = sorted(\n                                frequencies.keys(), key=lambda x: frequencies[x])[-1]\n\n                            # We do not change from Title to to UPPER\n                            if not tok.is_title or not alternative.isupper():\n                                toktext = alternative\n\n            tokens.append(toktext)\n\n            # Spacy needs to know whether the token is followed by a space\n            if tok.i &lt; len(doc)-1:\n                spaces.append(doctext[tok.idx+len(tok)].isspace())\n            else:\n                spaces.append(False)\n\n        # Creates a new document with the tokenised words and space information\n        doc2 = Doc(self.<div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">model.vocab</div>, words=tokens, spaces=spaces) #type: ignore\n #       print(\"finished with doc\", doc2[:10])\n        return doc2\n        #https://github.com/NorskRegnesentral/skweak/blob/main/skweak/spacy</code></pre></div></body></html>", "thr_19": "<html><body><div class=\"codeBlock hljs bash\" id=\"thr_19\"><pre id=\"thr_19_code\"><code class=\"python\">conference_text = ('There is a developer conference'\n    ' happening on 21 July 2019 in London.')\nconference_doc = nlp(conference_text)\n# Extract Noun Phrases\nfor chunk in <div class=\"highlights fea_n_grams\" id=\"n_grams_0\" style=\"display: inline;\">conference_doc.noun_chunks</div>:\n    print (chunk)</code></pre></div></body></html>", "thr_8": "<html><body><div class=\"codeBlock hljs coffeescript\" id=\"thr_8\"><pre id=\"thr_8_code\"><code class=\"python\">import spacy\nimport contextualSpellCheck\n\nnlp = spacy.load('en_core_web_sm')\n<div class=\"highlights fea_spellcheck\" id=\"spellcheck_0\" style=\"display: inline;\"><span class=\"fea_spellcheck_funcs blodFunc\">contextual</span><span class=\"fea_spellcheck_funcs blodFunc\">Spell</span><span class=\"fea_spellcheck_funcs blodFunc\">Check</span>.<span class=\"fea_spellcheck_funcs blodFunc\">add</span>_to_<span class=\"fea_spellcheck_funcs blodFunc\">pipe</span>(<span class=\"fea_spellcheck_funcs blodFunc\">nlp</span>)</div>\ndoc = nlp('Income was $9.4 milion compared to the prior year of $2.7 milion.')\n\nprint(doc._.performed_spellCheck) #Should be True\nprint(doc._.outcome_spellCheck) #Income was $9.4 million compared to the prior year of $2.7 million.\n#https://spacy.io/universe/project/contextualSpellCheck</code></pre></div></body></html>", "thr_7": "<html><body><div class=\"codeBlock hljs rust\" id=\"thr_7\"><pre id=\"thr_7_code\"><code class=\"python\">import spacy\nfrom spacytextblob.spacytextblob import SpacyTextBlob\n\nnlp = spacy.load('en_core_web_sm')\nnlp.add_pipe('spacytextblob')\ntext = 'I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.'\ndoc = nlp(text)\n<div class=\"highlights fea_sentiment_analysis\" id=\"sentiment_analysis_0\" style=\"display: inline;\"><span class=\"fea_sentiment_analysis_funcs blodFunc\">doc</span>._.<span class=\"fea_sentiment_analysis_funcs blodFunc\">polarity</span>      # <span class=\"fea_sentiment_analysis_funcs blodFunc\">Polarity</span>: -0.125\n<span class=\"fea_sentiment_analysis_funcs blodFunc\">doc</span>._.<span class=\"fea_sentiment_analysis_funcs blodFunc\">subjectivity</span></div>  # Sujectivity: 0.9\n<div class=\"highlights fea_text_scoring\" id=\"text_scoring_0\" style=\"display: inline;\"><span class=\"fea_text_scoring_funcs blodFunc\">doc</span>._.<span class=\"fea_text_scoring_funcs blodFunc\">assessments</span></div>   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n#https://spacy.io/universe/project/spacy-textblob</code></pre></div></body></html>", "thr_9": "<html><body><div class=\"codeBlock hljs python\" id=\"thr_9\"><pre id=\"thr_9_code\"><code class=\"python\">import spacy\nfrom spacy_langdetect import LanguageDetector\nnlp = spacy.load('en')\n<div class=\"highlights fea_language_detection\" id=\"language_detection_0\" style=\"display: inline;\"><span class=\"fea_language_detection_funcs blodFunc\">nlp</span>.<span class=\"fea_language_detection_funcs blodFunc\">add</span>_<span class=\"fea_language_detection_funcs blodFunc\">pipe</span>(<span class=\"fea_language_detection_funcs blodFunc\">Language</span><span class=\"fea_language_detection_funcs blodFunc\">Detector</span>(), <span class=\"fea_language_detection_funcs blodFunc\">name</span>='<span class=\"fea_language_detection_funcs blodFunc\">language</span>_<span class=\"fea_language_detection_funcs blodFunc\">detector</span>', <span class=\"fea_language_detection_funcs blodFunc\">last</span>=<span class=\"fea_language_detection_funcs blodFunc\">True</span>)</div>\ntext = 'This is an english text.'\ndoc = nlp(text)\n# document level language detection. Think of it like average language of the document!\nprint(doc._.language)\n# sentence level language detection\nfor sent in doc.sents:\n   print(sent, sent._.language)\n   #https://spacy.io/universe/project/spacy-langdetect</code></pre></div></body></html>"}}